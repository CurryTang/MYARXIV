<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-16T00:00:00Z">2025-01-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">58</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniThink: Expanding Knowledge Boundaries in Machine Writing through
  Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine writing with large language models often relies on
retrieval-augmented generation. However, these approaches remain confined
within the boundaries of the model's predefined scope, limiting the generation
of content with rich information. Specifically, vanilla-retrieved information
tends to lack depth, utility, and suffers from redundancy, which negatively
impacts the quality of generated articles, leading to shallow, repetitive, and
unoriginal outputs. To address these issues, we propose OmniThink, a machine
writing framework that emulates the human-like process of iterative expansion
and reflection. The core idea behind OmniThink is to simulate the cognitive
behavior of learners as they progressively deepen their knowledge of the
topics. Experimental results demonstrate that OmniThink improves the knowledge
density of generated articles without compromising metrics such as coherence
and depth. Human evaluations and expert feedback further highlight the
potential of OmniThink to address real-world challenges in the generation of
long-form articles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Lexicon-Based Text Embeddings with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Lei, Tao Shen, Yu Cao, Andrew Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have demonstrated exceptional performance
on general-purpose text embedding tasks. While dense embeddings have dominated
related research, we introduce the first Lexicon-based EmbeddiNgS (LENS)
leveraging LLMs that achieve competitive performance on these tasks. Regarding
the inherent tokenization redundancy issue and unidirectional attention
limitations in traditional causal LLMs, LENS consolidates the vocabulary space
through token embedding clustering, and investigates bidirectional attention
and various pooling strategies. Specifically, LENS simplifies lexicon matching
by assigning each dimension to a specific token cluster, where semantically
similar tokens are grouped together, and unlocking the full potential of LLMs
through bidirectional attention. Extensive experiments demonstrate that LENS
outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),
delivering compact feature representations that match the sizes of dense
counterparts. Notably, combining LENSE with dense embeddings achieves
state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suggesting Code Edits in Interactive Machine Learning Notebooks Using
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bihui Jin, Jiayue Wang, Pengyu Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning developers frequently use interactive computational
notebooks, such as Jupyter notebooks, to host code for data processing and
model training. Jupyter notebooks provide a convenient tool for writing machine
learning pipelines and interactively observing outputs, however, maintaining
Jupyter notebooks, e.g., to add new features or fix bugs, can be challenging
due to the length and complexity of the notebooks. Moreover, there is no
existing benchmark related to developer edits on Jupyter notebooks. To address
this, we present the first dataset of 48,398 Jupyter notebook edits derived
from 20,095 revisions of 792 machine learning repositories on GitHub, and
perform the first study of the using LLMs to predict code edits in Jupyter
notebooks. Our dataset captures granular details of cell-level and line-level
modifications, offering a foundation for understanding real-world maintenance
patterns in machine learning workflows. We observed that the edits on Jupyter
notebooks are highly localized, with changes averaging only 166 lines of code
in repositories. While larger models outperform smaller counterparts in code
editing, all models have low accuracy on our dataset even after finetuning,
demonstrating the complexity of real-world machine learning maintenance tasks.
Our findings emphasize the critical role of contextual information in improving
model performance and point toward promising avenues for advancing large
language models' capabilities in engineering machine learning code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention based Bidirectional GRU hybrid model for inappropriate content
  detection in Urdu language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezzah Shoukat, Rabia Irfan, Iqra Basharat, Muhammad Ali Tahir, Sameen Shaukat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increased use of the internet and social networks for online
discussions, the spread of toxic and inappropriate content on social networking
sites has also increased. Several studies have been conducted in different
languages. However, there is less work done for South Asian languages for
inappropriate content identification using deep learning techniques. In Urdu
language, the spellings are not unique, and people write different common
spellings for the same word, while mixing it other languages, like English in
the text makes it more challenging, and limited research work is available to
process such language with the finest algorithms. The use of attention layer
with a deep learning model can help handling the long-term dependencies and
increase its efficiency . To explore the effects of the attention layer, this
study proposes attention-based Bidirectional GRU hybrid model for identifying
inappropriate content in Urdu Unicode text language. Four different baseline
deep learning models; LSTM, Bi-LSTM, GRU, and TCN, are used to compare the
performance of the proposed model. The results of these models were compared
based on evaluation metrics, dataset size, and impact of the word embedding
layer. The pre-trained Urdu word2Vec embeddings were utilized for our case. Our
proposed model BiGRU-A outperformed all other baseline models by yielding 84\%
accuracy without using pre-trained word2Vec layer. From our experiments, we
have established that the attention layer improves the model's efficiency, and
pre-trained word2Vec embedding does not work well with an inappropriate content
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Insights from 12 Machine Learning Models in Extracting
  Economic Ideology from Political Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihed Ncib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study conducts a systematic assessment of the capabilities of 12 machine
learning models and model variations in detecting economic ideology. As an
evaluation benchmark, I use manifesto data spanning six elections in the United
Kingdom and pre-annotated by expert and crowd coders. The analysis assesses the
performance of several generative, fine-tuned, and zero-shot models at the
granular and aggregate levels. The results show that generative models such as
GPT-4o and Gemini 1.5 Flash consistently outperform other models against all
benchmarks. However, they pose issues of accessibility and resource
availability. Fine-tuning yielded competitive performance and offers a reliable
alternative through domain-specific optimization. But its dependency on
training data severely limits scalability. Zero-shot models consistently face
difficulties with identifying signals of economic ideology, often resulting in
negative associations with human coding. Using general knowledge for the
domain-specific task of ideology scaling proved to be unreliable. Other key
findings include considerable within-party variation, fine-tuning benefiting
from larger training data, and zero-shot's sensitivity to prompt content. The
assessments include the strengths and limitations of each model and derive
best-practices for automated analyses of political content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation of <span class="highlight-title">Foundation</span> LLMs for e-Commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Herold, Michael Kozielski, Tala Bazazo, Pavel Petrushkov, Hadi Hashemi, Patrycja Cieplicka, Dominika Basaj, Shahram Khadivi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the e-Llama models: 8 billion and 70 billion parameter large
language models that are adapted towards the e-commerce domain. These models
are meant as foundation models with deep knowledge about e-commerce, that form
a base for instruction- and fine-tuning. The e-Llama models are obtained by
continuously pretraining the Llama 3.1 base models on 1 trillion tokens of
domain-specific data.
  We discuss our approach and motivate our choice of hyperparameters with a
series of ablation studies. To quantify how well the models have been adapted
to the e-commerce domain, we define and implement a set of multilingual,
e-commerce specific evaluation tasks.
  We show that, when carefully choosing the training setup, the Llama 3.1
models can be adapted towards the new domain without sacrificing significant
performance on general domain tasks. We also explore the possibility of merging
the adapted model and the base model for a better control of the performance
trade-off between domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Large Reasoning Models: A <span class="highlight-title">Survey</span> of Reinforced Reasoning with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language has long been conceived as an essential tool for human reasoning.
The breakthrough of Large Language Models (LLMs) has sparked significant
research interest in leveraging these models to tackle complex reasoning tasks.
Researchers have moved beyond simple autoregressive token generation by
introducing the concept of "thought" -- a sequence of tokens representing
intermediate steps in the reasoning process. This innovative paradigm enables
LLMs' to mimic complex human reasoning processes, such as tree search and
reflective thinking. Recently, an emerging trend of learning to reason has
applied reinforcement learning (RL) to train LLMs to master reasoning
processes. This approach enables the automatic generation of high-quality
reasoning trajectories through trial-and-error search algorithms, significantly
expanding LLMs' reasoning capacity by providing substantially more training
data. Furthermore, recent studies demonstrate that encouraging LLMs to "think"
with more tokens during test-time inference can further significantly boost
reasoning accuracy. Therefore, the train-time and test-time scaling combined to
show a new research frontier -- a path toward Large Reasoning Model. The
introduction of OpenAI's o1 series marks a significant milestone in this
research direction. In this survey, we present a comprehensive review of recent
progress in LLM reasoning. We begin by introducing the foundational background
of LLMs and then explore the key technical components driving the development
of large reasoning models, with a focus on automated data construction,
learning-to-reason techniques, and test-time scaling. We also analyze popular
open-source projects at building large reasoning models, and conclude with open
challenges and future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Heap: A Contamination-Free Multilingual Code <span class="highlight-title">Dataset</span> for Evaluating
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Katzy, Razvan Mihai Popescu, Arie van Deursen, Maliheh Izadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent rise in the popularity of large language models has spurred the
development of extensive code datasets needed to train them. This has left
limited code available for collection and use in the downstream investigation
of specific behaviors, or evaluation of large language models without suffering
from data contamination. To address this problem, we release The Heap, a large
multilingual dataset covering 57 programming languages that has been
deduplicated with respect to other open datasets of code, enabling researchers
to conduct fair evaluations of large language models without significant data
cleaning overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-Print. Accepted to FORGE 2025 Dataset Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through
  Category-Bounding <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Kirmayr, Lukas Stappen, Phillip Schneider, Florian Matthes, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's assistant landscape, personalisation enhances interactions,
fosters long-term relationships, and deepens engagement. However, many systems
struggle with retaining user preferences, leading to repetitive user requests
and disengagement. Furthermore, the unregulated and opaque extraction of user
preferences in industry applications raises significant concerns about privacy
and trust, especially in regions with stringent regulations like Europe. In
response to these challenges, we propose a long-term memory system for voice
assistants, structured around predefined categories. This approach leverages
Large Language Models to efficiently extract, store, and retrieve preferences
within these categories, ensuring both personalisation and transparency. We
also introduce a synthetic multi-turn, multi-session conversation dataset
(CarMem), grounded in real industry data, tailored to an in-car voice assistant
setting. Benchmarked on the dataset, our system achieves an F1-score of .78 to
.95 in preference extraction, depending on category granularity. Our
maintenance strategy reduces redundant preferences by 95% and contradictory
ones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,
the results demonstrate the system's suitability for industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the International Conference on
  Computational Linguistics (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Scarcity to Capability: Empowering Fake News Detection in
  Low-Resource Languages with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrithik Majumdar Shibu, Shrestha Datta, Md. Sumon Miah, Nasrullah Sami, Mahruba Sharmin Chowdhury, Md. Saiful Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid spread of fake news presents a significant global challenge,
particularly in low-resource languages like Bangla, which lack adequate
datasets and detection tools. Although manual fact-checking is accurate, it is
expensive and slow to prevent the dissemination of fake news. Addressing this
gap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news
detection. This version includes 11,700 additional, meticulously curated fake
news articles validated from credible sources, creating a proportional dataset
of 47,000 authentic and 13,000 fake news items across 13 categories. In
addition, we created a manually curated independent test set of 460 fake and
540 authentic news items for rigorous evaluation. We invest efforts in
collecting fake news from credible sources and manually verified while
preserving the linguistic richness. We develop a benchmark system utilizing
transformer-based architectures, including fine-tuned Bidirectional Encoder
Representations from Transformers variants (F1-87\%) and Large Language Models
with Quantized Low-Rank Approximation (F1-89\%), that significantly outperforms
traditional methods. BanFakeNews-2.0 offers a valuable resource to advance
research and application in fake news detection for low-resourced languages. We
publicly release our dataset and model on Github to foster research in this
direction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stylomech: Unveiling Authorship via Computational Stylometry in English
  and Romanized Sinhala 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nabeelah Faumi, Adeepa Gunathilake, Benura Wickramanayake, Deelaka Dias, TGDK Sumanathilaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of Web 2.0, the development in social technology coupled with
global communication systematically brought positive and negative impacts to
society. Copyright claims and Author identification are deemed crucial as there
has been a considerable amount of increase in content violation owing to the
lack of proper ethics in society. The Author's attribution in both English and
Romanized Sinhala became a major requirement in the last few decades. As an
area largely unexplored, particularly within the context of Romanized Sinhala,
the research contributes significantly to the field of computational
linguistics. The proposed author attribution system offers a unique approach,
allowing for the comparison of only two sets of text: suspect author and
anonymous text, a departure from traditional methodologies which often rely on
larger corpora. This work focuses on using the numerical representation of
various pairs of the same and different authors allowing for, the model to
train on these representations as opposed to text, this allows for it to apply
to a multitude of authors and contexts, given that the suspected author text,
and the anonymous text are of reasonable quality. By expanding the scope of
authorship attribution to encompass diverse linguistic contexts, the work
contributes to fostering trust and accountability in digital communication,
especially in Sri Lanka. This research presents a pioneering approach to author
attribution in both English and Romanized Sinhala, addressing a critical need
for content verification and intellectual property rights enforcement in the
digital age.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 figure, 1 image</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Continuous Semantic Shifts with Diachronic Word Similarity
  Matrices <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hajime Kiyama, Taichi Aida, Mamoru Komachi, Toshinobu Ogiso, Hiroya Takamura, Daichi Mochihashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The meanings and relationships of words shift over time. This phenomenon is
referred to as semantic shift.Research focused on understanding how semantic
shifts occur over multiple time periods is essential for gaining a detailed
understanding of semantic shifts.However, detecting change points only between
adjacent time periods is insufficient for analyzing detailed semantic shifts,
and using BERT-based methods to examine word sense proportions incurs a high
computational cost.To address those issues, we propose a simple yet intuitive
framework for how semantic shifts occur over multiple time periods by
leveraging a similarity matrix between the embeddings of the same word through
time.We compute a diachronic word similarity matrix using fast and lightweight
word embeddings across arbitrary time periods, making it deeper to analyze
continuous semantic shifts.Additionally, by clustering the similarity matrices
for different words, we can categorize words that exhibit similar behavior of
semantic shift in an unsupervised manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence Estimation for Error Detection in Text-to-SQL Systems <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Somov, Elena Tutubalina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL enables users to interact with databases through natural
language, simplifying the retrieval and synthesis of information. Despite the
success of large language models (LLMs) in converting natural language
questions into SQL queries, their broader adoption is limited by two main
challenges: achieving robust generalization across diverse queries and ensuring
interpretative confidence in their predictions. To tackle these issues, our
research investigates the integration of selective classifiers into Text-to-SQL
systems. We analyse the trade-off between coverage and risk using entropy based
confidence estimation with selective classifiers and assess its impact on the
overall performance of Text-to-SQL models. Additionally, we explore the models'
initial calibration and improve it with calibration techniques for better model
alignment between confidence and accuracy. Our experimental results show that
encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and
decoder-only Llama 3, thus the designated external entropy-based selective
classifier has better performance. The study also reveal that, in terms of
error detection, selective classifier with a higher probability detects errors
associated with irrelevant questions rather than incorrect query generations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures, to be published in AAAI 2025 Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting a Large Language Model with a Combination of Text and Visual
  Data for Conversational Visualization of Global Geospatial Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Mena, Alexandre Kouyoumdjian, Lonni Besançon, Michael Gleicher, Ivan Viola, Anders Ynnerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for augmenting a Large Language Model (LLM) with a
combination of text and visual data to enable accurate question answering in
visualization of scientific data, making conversational visualization possible.
LLMs struggle with tasks like visual data interaction, as they lack contextual
visual information. We address this problem by merging a text description of a
visualization and dataset with snapshots of the visualization. We extract their
essential features into a structured text file, highly compact, yet descriptive
enough to appropriately augment the LLM with contextual information, without
any fine-tuning. This approach can be applied to any visualization that is
already finally rendered, as long as it is associated with some textual
description.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIER: A Novel Metric for Evaluating What Matters in Code-Switching <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enes Yavuz Ugan, Ngoc-Quan Pham, Leonard Bärmann, Alex Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching, the alternation of languages within a single discourse,
presents a significant challenge for Automatic Speech Recognition. Despite the
unique nature of the task, performance is commonly measured with established
metrics such as Word-Error-Rate (WER). However, in this paper, we question
whether these general metrics accurately assess performance on code-switching.
Specifically, using both Connectionist-Temporal-Classification and
Encoder-Decoder models, we show fine-tuning on non-code-switched data from both
matrix and embedded language improves classical metrics on code-switching test
sets, although actual code-switched words worsen (as expected). Therefore, we
propose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only
on specific words of interest. We instantiate PIER on code-switched utterances
and show that this more accurately describes the code-switching performance,
showing huge room for improvement in future work. This focused evaluation
allows for a more precise assessment of model performance, particularly in
challenging aspects such as inter-word and intra-word code-switching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Inquiry-Diagnosis Relationship with Advanced Patient
  Simulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, Jian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online medical consultation (OMC) restricts doctors to gathering patient
information solely through inquiries, making the already complex sequential
decision-making process of diagnosis even more challenging. Recently, the rapid
advancement of large language models has demonstrated a significant potential
to transform OMC. However, most studies have primarily focused on improving
diagnostic accuracy under conditions of relatively sufficient information,
while paying limited attention to the "inquiry" phase of the consultation
process. This lack of focus has left the relationship between "inquiry" and
"diagnosis" insufficiently explored. In this paper, we first extract real
patient interaction strategies from authentic doctor-patient conversations and
use these strategies to guide the training of a patient simulator that closely
mirrors real-world behavior. By inputting medical records into our patient
simulator to simulate patient responses, we conduct extensive experiments to
explore the relationship between "inquiry" and "diagnosis" in the consultation
process. Experimental results demonstrate that inquiry and diagnosis adhere to
the Liebig's law: poor inquiry quality limits the effectiveness of diagnosis,
regardless of diagnostic capability, and vice versa. Furthermore, the
experiments reveal significant differences in the inquiry performance of
various models. To investigate this phenomenon, we categorize the inquiry
process into four types: (1) chief complaint inquiry; (2) specification of
known symptoms; (3) inquiry about accompanying symptoms; and (4) gathering
family or medical history. We analyze the distribution of inquiries across the
four types for different models to explore the reasons behind their significant
performance differences. We plan to open-source the weights and related code of
our patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling <span class="highlight-title">Graph</span>-Based Dependency Parsing with Arc Vectorization and
  Attention-Based Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Floquet, Joseph Le Roux, Nadi Tomeh, Thierry Charnois
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel architecture for graph-based dependency parsing that
explicitly constructs vectors, from which both arcs and labels are scored. Our
method addresses key limitations of the standard two-pipeline approach by
unifying arc scoring and labeling into a single network, reducing scalability
issues caused by the information bottleneck and lack of parameter sharing.
Additionally, our architecture overcomes limited arc interactions with
transformer layers to efficiently simulate higher-order dependencies.
Experiments on PTB and UD show that our model outperforms state-of-the-art
parsers in both accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving the unsolvable: Translating case law in Hong Kong 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        King-kui Sin, Xi Xuan, Chunyu Kit, Clara Ho-yan Chan, Honic Ho-kin Ip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges translating case law under Hong Kong's
bilingual legal system. It highlights the initial success of translating all
written statutes into Chinese before the 1997 handover, a task mandated by the
Basic Law. The effort involved significant collaboration among legal,
linguistic, and translation experts, resulting in a comprehensive and
culturally appropriate bilingual legal system. However, translating case law
remains a significant challenge due to the sheer volume and continuous growth
of judicial decisions. The paper critiques the governments and judiciarys
sporadic and uncoordinated efforts to translate case law, contrasting it with
the thorough approach previously taken for statute translation. Although the
government acknowledges the importance of legal bilingualism, it lacks a
sustainable strategy for translating case law. The Judiciarys position that
translating all judgments is unnecessary, unrealistic, and not cost-effectiveis
analyzed and critiqued for its impact on legal transparency and public trust. A
proposed solution involves leveraging machine translation technology through a
human-machine interactive translation platform, which undergoes two major
transitions. Initially based on a neural model, the platform transitions to
using a large language model for improved translation accuracy. Furthermore, it
evolves from a single-agent system to a multi-agent system, incorporating
Translator, Annotator, and Proofreader agents. This multi-agent approach,
supported by a grant, aims to facilitate efficient, high-quality translation of
judicial judgments by integrating advanced artificial intelligence and
continuous feedback mechanisms, thus better meeting the needs of a bilingual
legal system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Responsible LLMs: Inherent Risk, Malicious Use, and
  Mitigation Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huandong Wang, Wenjie Fu, Yingzhou Tang, Zhilong Chen, Yuxi Huang, Jinghua Piao, Chen Gao, Fengli Xu, Tao Jiang, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) present significant potential for
supporting numerous real-world applications and delivering positive social
impacts, they still face significant challenges in terms of the inherent risk
of privacy leakage, hallucinated outputs, and value misalignment, and can be
maliciously used for generating toxic content and unethical purposes after been
jailbroken. Therefore, in this survey, we present a comprehensive review of
recent advancements aimed at mitigating these issues, organized across the four
phases of LLM development and usage: data collecting and pre-training,
fine-tuning and alignment, prompting and reasoning, and post-processing and
auditing. We elaborate on the recent advances for enhancing the performance of
LLMs in terms of privacy protection, hallucination reduction, value alignment,
toxicity elimination, and jailbreak defenses. In contrast to previous surveys
that focus on a single dimension of responsible LLMs, this survey presents a
unified framework that encompasses these diverse dimensions, providing a
comprehensive view of enhancing LLMs to better serve real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral
  Therapy in Psychological Counseling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ancheng Xu, Di Yang, Renhao Li, Jingwei Zhu, Minghuan Tan, Min Yang, Wanxin Qiu, Mingchen Ma, Haihong Wu, Bingyu Li, Feng Sha, Chengming Li, Xiping Hu, Qiang Qu, Derek F. Wong, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional in-person psychological counseling remains primarily niche, often
chosen by individuals with psychological issues, while online automated
counseling offers a potential solution for those hesitant to seek help due to
feelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and
widely used approach in psychological counseling. The advent of large language
models (LLMs) and agent technology enables automatic CBT diagnosis and
treatment. However, current LLM-based CBT systems use agents with a fixed
structure, limiting their self-optimization capabilities, or providing hollow,
unhelpful suggestions due to redundant response patterns. In this work, we
utilize Quora-like and YiXinLi single-round consultation models to build a
general agent framework that generates high-quality responses for single-turn
psychological consultation scenarios. We use a bilingual dataset to evaluate
the quality of single-response consultations generated by each framework. Then,
we incorporate dynamic routing and supervisory mechanisms inspired by real
psychological counseling to construct a CBT-oriented autonomous multi-agent
framework, demonstrating its general applicability. Experimental results
indicate that AutoCBT can provide higher-quality automated psychological
counseling services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Models Do Not Understand Negation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many practical vision-language applications require models that understand
negation, e.g., when using natural language to retrieve images which contain
certain objects but not others. Despite advancements in vision-language models
(VLMs) through large-scale training, their ability to comprehend negation
remains underexplored. This study addresses the question: how well do current
VLMs understand negation? We introduce NegBench, a new benchmark designed to
evaluate negation understanding across 18 task variations and 79k examples
spanning image, video, and medical datasets. The benchmark consists of two core
tasks designed to evaluate negation understanding in diverse multimodal
settings: Retrieval with Negation and Multiple Choice Questions with Negated
Captions. Our evaluation reveals that modern VLMs struggle significantly with
negation, often performing at chance level. To address these shortcomings, we
explore a data-centric approach wherein we finetune CLIP models on large-scale
synthetic datasets containing millions of negated captions. We show that this
approach can result in a 10% increase in recall on negated queries and a 40%
boost in accuracy on multiple-choice questions with negated captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://negbench.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mGeNTE: A Multilingual Resource for Gender-Neutral Language and
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Savoldi, Eleonora Cupin, Manjinder Thind, Anne Lauscher, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gender-neutral language reflects societal and linguistic shifts towards
greater inclusivity by avoiding the implication that one gender is the norm
over others. This is particularly relevant for grammatical gender languages,
which heavily encode the gender of terms for human referents and over-relies on
masculine forms, even when gender is unspecified or irrelevant. Language
technologies are known to mirror these inequalities, being affected by a male
bias and perpetuating stereotypical associations when translating into
languages with extensive gendered morphology. In such cases, gender-neutral
language can help avoid undue binary assumptions. However, despite its
importance for creating fairer multi- and cross-lingual technologies, inclusive
language research remains scarce and insufficiently supported in current
resources. To address this gap, we present the multilingual mGeNTe dataset.
Derived from the bilingual GeNTE (Piergentili et al., 2023), mGeNTE extends the
original corpus to include the English-Italian/German/Spanish language pairs.
Since each language pair is English-aligned with gendered and neutral sentences
in the target languages, mGeNTE enables research in both automatic
Gender-Neutral Translation (GNT) and language modelling for three grammatical
gender languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating LLM Abilities to Understand <span class="highlight-title">Tabular</span> Electronic Health
  Records: A Comprehensive Study of Patient Data Extraction and <span class="highlight-title">Retrie</span>val <span class="chip">ECIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus Lovon, Martin Mouysset, Jo Oleiwan, Jose G. Moreno, Christine Damase-Michel, Lynda Tamine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Record (EHR) tables pose unique challenges among which is
the presence of hidden contextual dependencies between medical features with a
high level of data dimensionality and sparsity. This study presents the first
investigation into the abilities of LLMs to comprehend EHRs for patient data
extraction and retrieval. We conduct extensive experiments using the MIMICSQL
dataset to explore the impact of the prompt structure, instruction, context,
and demonstration, of two backbone LLMs, Llama2 and Meditron, based on task
performance. Through quantitative and qualitative analyses, our findings show
that optimal feature selection and serialization methods can enhance task
performance by up to 26.79% compared to naive approaches. Similarly, in-context
learning setups with relevant example selection improve data extraction
performance by 5.95%. Based on our study findings, we propose guidelines that
we believe would help the design of LLM-based models to support health search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published as full paper in the Proceedings of the European
  Conference on Information Retrieval (ECIR) 2025. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChartInsighter: An Approach for Mitigating Hallucination in Time-series
  Chart Summary Generation with A Benchmark <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fen Wang, Bomiao Wang, Xueli Shu, Zhen Liu, Zekai Shao, Chao Liu, Siming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective chart summary can significantly reduce the time and effort decision
makers spend interpreting charts, enabling precise and efficient communication
of data insights. Previous studies have faced challenges in generating accurate
and semantically rich summaries of time-series data charts. In this paper, we
identify summary elements and common hallucination types in the generation of
time-series chart summaries, which serve as our guidelines for automatic
generation. We introduce ChartInsighter, which automatically generates chart
summaries of time-series data, effectively reducing hallucinations in chart
summary generation. Specifically, we assign multiple agents to generate the
initial chart summary and collaborate iteratively, during which they invoke
external data analysis modules to extract insights and compile them into a
coherent summary. Additionally, we implement a self-consistency test method to
validate and correct our summary. We create a high-quality benchmark of charts
and summaries, with hallucination types annotated on a sentence-by-sentence
basis, facilitating the evaluation of the effectiveness of reducing
hallucinations. Our evaluations using our benchmark show that our method
surpasses state-of-the-art models, and that our summary hallucination rate is
the lowest, which effectively reduces various hallucinations and improves
summary quality. The benchmark is available at
https://github.com/wangfen01/ChartInsighter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithm for Semantic Network Generation from Texts of Low Resource
  Languages Such as Kiswahili 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barack Wamkaya Wanjawa, Lawrence Muchemi, Evans Miriti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing low-resource languages, such as Kiswahili, using machine learning
is difficult due to lack of adequate training data. However, such low-resource
languages are still important for human communication and are already in daily
use and users need practical machine processing tasks such as summarization,
disambiguation and even question answering (QA). One method of processing such
languages, while bypassing the need for training data, is the use semantic
networks. Some low resource languages, such as Kiswahili, are of the
subject-verb-object (SVO) structure, and similarly semantic networks are a
triple of subject-predicate-object, hence SVO parts of speech tags can map into
a semantic network triple. An algorithm to process raw natural language text
and map it into a semantic network is therefore necessary and desirable in
structuring low resource languages texts. This algorithm tested on the
Kiswahili QA task with upto 78.6% exact match.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures, published in Open Journal for Information
  Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shape-Based Single Object Classification Using Ensemble Method
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nur Shazwani Kamarudin, Mokhairi Makhtar, Syadiah Nor Wan Shamsuddin, Syed Abdullah Fadzli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, more and more images are available. Annotation and retrieval of the
images pose classification problems, where each class is defined as the group
of database images labelled with a common semantic label. Various systems have
been proposed for content-based retrieval, as well as for image classification
and indexing. In this paper, a hierarchical classification framework has been
proposed for bridging the semantic gap effectively and achieving multi-category
image classification. A well known pre-processing and post-processing method
was used and applied to three problems; image segmentation, object
identification and image classification. The method was applied to classify
single object images from Amazon and Google datasets. The classification was
tested for four different classifiers; BayesNetwork (BN), Random Forest (RF),
Bagging and Vote. The estimated classification accuracies ranged from 20% to
99% (using 10-fold cross validation). The Bagging classifier presents the best
performance, followed by the Random Forest classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study of In-Context-Learning-Based Text-to-SQL Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Shen, Chengcheng Wan, Ruoyi Qiao, Jiazhen Zou, Hang Xu, Yuchen Shao, Yueling Zhang, Weikai Miao, Geguang Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been adopted to perform text-to-SQL tasks,
utilizing their in-context learning (ICL) capability to translate natural
language questions into structured query language (SQL). However, such a
technique faces correctness problems and requires efficient repairing
solutions. In this paper, we conduct the first comprehensive study of
text-to-SQL errors. Our study covers four representative ICL-based techniques,
five basic repairing methods, two benchmarks, and two LLM settings. We find
that text-to-SQL errors are widespread and summarize 29 error types of 7
categories. We also find that existing repairing attempts have limited
correctness improvement at the cost of high computational overhead with many
mis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL
error detection and repairing framework. The evaluation demonstrates that
MapleRepair outperforms existing solutions by repairing 13.8% more queries with
neglectable mis-repairs and 67.4% less overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Mental Health Content on Social Media and Its Effect
  Towards Suicidal Ideation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohaiminul Islam Bhuiyan, Nur Shazwani Kamarudin, Nur Hafieza Ismail
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This review underscores the critical need for effective strategies to
identify and support individuals with suicidal ideation, exploiting
technological innovations in ML and DL to further suicide prevention efforts.
The study details the application of these technologies in analyzing vast
amounts of unstructured social media data to detect linguistic patterns,
keywords, phrases, tones, and contextual cues associated with suicidal
thoughts. It explores various ML and DL models like SVMs, CNNs, LSTM, neural
networks, and their effectiveness in interpreting complex data patterns and
emotional nuances within text data. The review discusses the potential of these
technologies to serve as a life-saving tool by identifying at-risk individuals
through their digital traces. Furthermore, it evaluates the real-world
effectiveness, limitations, and ethical considerations of employing these
technologies for suicide prevention, stressing the importance of responsible
development and usage. The study aims to fill critical knowledge gaps by
analyzing recent studies, methodologies, tools, and techniques in this field.
It highlights the importance of synthesizing current literature to inform
practical tools and suicide prevention efforts, guiding innovation in reliable,
ethical systems for early intervention. This research synthesis evaluates the
intersection of technology and mental health, advocating for the ethical and
responsible application of ML, DL, and NLP to offer life-saving potential
worldwide while addressing challenges like generalizability, biases, privacy,
and the need for further research to ensure these technologies do not
exacerbate existing inequities and harms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive
  Vision-Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harrison Fuller, Fernando Gabriela Garcia, Victor Flores
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot learning in medical image classification presents a significant
challenge due to the limited availability of annotated data and the complex
nature of medical imagery. In this work, we propose Adaptive Vision-Language
Fine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework
that leverages the capabilities of Large Vision-Language Models (LVLMs) for
medical image analysis. HiCA introduces a two-stage fine-tuning strategy,
combining domain-specific pretraining and hierarchical contrastive learning to
align visual and textual representations at multiple levels. We evaluate our
approach on two benchmark datasets, Chest X-ray and Breast Ultrasound,
achieving state-of-the-art performance in both few-shot and zero-shot settings.
Further analyses demonstrate the robustness, generalizability, and
interpretability of our method, with substantial improvements in performance
compared to existing baselines. Our work highlights the potential of
hierarchical contrastive strategies in adapting LVLMs to the unique challenges
of medical imaging tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To <span class="highlight-title">Retrie</span>ve or Not to <span class="highlight-title">Retrie</span>ve? Uncertainty Detection for Dynamic
  <span class="highlight-title">Retrie</span>val Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation equips large language models with the
capability to retrieve external knowledge, thereby mitigating hallucinations by
incorporating information beyond the model's intrinsic abilities. However, most
prior works have focused on invoking retrieval deterministically, which makes
it unsuitable for tasks such as long-form question answering. Instead,
dynamically performing retrieval by invoking it only when the underlying LLM
lacks the required knowledge can be more efficient. In this context, we delve
deeper into the question, "To Retrieve or Not to Retrieve?" by exploring
multiple uncertainty detection methods. We evaluate these methods for the task
of long-form question answering, employing dynamic retrieval, and present our
comparisons. Our findings suggest that uncertainty detection metrics, such as
Degree Matrix Jaccard and Eccentricity, can reduce the number of retrieval
calls by almost half, with only a slight reduction in question-answering
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perspective Transition of Large Language Models for Solving Subjective
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Wang, Yuanchi Zhang, Ziyue Wang, Yuzhuang Xu, Fuwen Luo, Yile Wang, Peng Li, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized the field of natural
language processing, enabling remarkable progress in various tasks. Different
from objective tasks such as commonsense reasoning and arithmetic
question-answering, the performance of LLMs on subjective tasks is still
limited, where the perspective on the specific problem plays crucial roles for
better interpreting the context and giving proper response. For example, in
certain scenarios, LLMs may perform better when answering from an expert role
perspective, potentially eliciting their relevant domain knowledge. In
contrast, in some scenarios, LLMs may provide more accurate responses when
answering from a third-person standpoint, enabling a more comprehensive
understanding of the problem and potentially mitigating inherent biases. In
this paper, we propose Reasoning through Perspective Transition (RPT), a method
based on in-context learning that enables LLMs to dynamically select among
direct, role, and third-person perspectives for the best way to solve
corresponding subjective problem. Through extensive experiments on totally 12
subjective tasks by using both closed-source and open-source LLMs including
GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single
fixed perspective based methods such as chain-of-thought prompting and expert
prompting, highlights the intricate ways that LLMs can adapt their perspectives
to provide nuanced and contextually appropriate responses for different
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delayed Fusion: Integrating Large Language Models into First-Pass
  Decoding in End-to-end Speech Recognition <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takaaki Hori, Martin Kocour, Adnan Haider, Erik McDermott, Xiaodan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an efficient decoding approach for end-to-end automatic
speech recognition (E2E-ASR) with large language models (LLMs). Although
shallow fusion is the most common approach to incorporate language models into
E2E-ASR decoding, we face two practical problems with LLMs. (1) LLM inference
is computationally costly. (2) There may be a vocabulary mismatch between the
ASR model and the LLM. To resolve this mismatch, we need to retrain the ASR
model and/or the LLM, which is at best time-consuming and in many cases not
feasible. We propose "delayed fusion," which applies LLM scores to ASR
hypotheses with a delay during decoding and enables easier use of pre-trained
LLMs in ASR tasks. This method can reduce not only the number of hypotheses
scored by the LLM but also the number of LLM inference calls. It also allows
re-tokenizion of ASR hypotheses during decoding if ASR and LLM employ different
tokenizations. We demonstrate that delayed fusion provides improved decoding
speed and accuracy compared to shallow fusion and N-best rescoring using the
LibriHeavy ASR corpus and three public LLMs, OpenLLaMA 3B & 7B and Mistral 7B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Foundation</span>s of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is a book about large language models. As indicated by the title, it
primarily focuses on foundational concepts rather than comprehensive coverage
of all cutting-edge technologies. The book is structured into four main
chapters, each exploring a key area: pre-training, generative models, prompting
techniques, and alignment methods. It is intended for college students,
professionals, and practitioners in natural language processing and related
fields, and can serve as a reference for anyone interested in large language
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple <span class="highlight-title">Graph</span> Contrastive Learning Framework for Short Text
  Classification <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Liu, Fausto Giunchiglia, Lan Huang, Ximing Li, Xiaoyue Feng, Renchu Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short text classification has gained significant attention in the information
age due to its prevalence and real-world applications. Recent advancements in
graph learning combined with contrastive learning have shown promising results
in addressing the challenges of semantic sparsity and limited labeled data in
short text classification. However, existing models have certain limitations.
They rely on explicit data augmentation techniques to generate contrastive
views, resulting in semantic corruption and noise. Additionally, these models
only focus on learning the intrinsic consistency between the generated views,
neglecting valuable discriminative information from other potential views. To
address these issues, we propose a Simple graph contrastive learning framework
for Short Text Classification (SimSTC). Our approach involves performing graph
learning on multiple text-related component graphs to obtain multi-view text
embeddings. Subsequently, we directly apply contrastive learning on these
embeddings. Notably, our method eliminates the need for data augmentation
operations to generate contrastive views while still leveraging the benefits of
multi-view contrastive learning. Despite its simplicity, our model achieves
outstanding performance, surpassing large language models on various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Short Text Classification with Multi-Source Information
  Exploration and Dual-Level Contrastive Learning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Liu, Mengyu Li, Wei Pang, Fausto Giunchiglia, Lan Huang, Xiaoyue Feng, Renchu Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short text classification, as a research subtopic in natural language
processing, is more challenging due to its semantic sparsity and insufficient
labeled samples in practical scenarios. We propose a novel model named
MI-DELIGHT for short text classification in this work. Specifically, it first
performs multi-source information (i.e., statistical information, linguistic
information, and factual information) exploration to alleviate the sparsity
issues. Then, the graph learning approach is adopted to learn the
representation of short texts, which are presented in graph forms. Moreover, we
introduce a dual-level (i.e., instance-level and cluster-level) contrastive
learning auxiliary task to effectively capture different-grained contrastive
information within massive unlabeled data. Meanwhile, previous models merely
perform the main task and auxiliary tasks in parallel, without considering the
relationship among tasks. Therefore, we introduce a hierarchical architecture
to explicitly model the correlations between tasks. We conduct extensive
experiments across various benchmark datasets, demonstrating that MI-DELIGHT
significantly surpasses previous competitive models. It even outperforms
popular large language models on several datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from
  Supervised Fine-Tuning to Test-Time Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhou Yu, Tianhao Cheng, Ying Cheng, Rui Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have shown promise in
medical applications such as disease diagnosis and treatment planning. However,
most existing medical LLMs struggle with the advanced reasoning required for
complex clinical scenarios, such as differential diagnosis or personalized
treatment suggestions. We proposed FineMedLM-o1, which leverages high-quality
synthetic medical data and long-form reasoning data for Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and
deep reasoning capabilities. Additionally, we introduced Test-Time Training
(TTT) in the medical domain for the first time, facilitating domain adaptation
and ensuring reliable, accurate reasoning. Experimental results demonstrate
that FineMedLM-o1 achieves a 23% average performance improvement over prior
models on key medical benchmarks. Furthermore, the introduction of TTT provides
an additional 14% performance boost, highlighting its effectiveness in
enhancing medical reasoning capabilities. To support this process, we also
proposed a novel method for synthesizing medical dialogue. Compared to other
open-source datasets, our dataset stands out as superior in both quality and
complexity. The project and data will be released on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2KQL: From Natural Language to Kusto Query 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02933v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02933v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinye Tang, Amir H. Abdi, Jeremias Eichelbaum, Mahan Das, Alex Klein, Nihal Irmak Pakis, William Blum, Daniel L Mace, Tanvi Raja, Namrata Padmanabhan, Ye Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is growing rapidly in volume and complexity. Proficiency in database
query languages is pivotal for crafting effective queries. As coding assistants
become more prevalent, there is significant opportunity to enhance database
query languages. The Kusto Query Language (KQL) is a widely used query language
for large semi-structured data such as logs, telemetries, and time-series for
big data analytics platforms. This paper introduces NL2KQL an innovative
framework that uses large language models (LLMs) to convert natural language
queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several
key components: Schema Refiner which narrows down the schema to its most
pertinent elements; the Few-shot Selector which dynamically selects relevant
examples from a few-shot dataset; and the Query Refiner which repairs syntactic
and semantic errors in KQL queries. Additionally, this study outlines a method
for generating large datasets of synthetic NLQ-KQL pairs which are valid within
a specific database contexts. To validate NL2KQL's performance, we utilize an
array of online (based on query execution) and offline (based on query parsing)
metrics. Through ablation studies, the significance of each framework component
is examined, and the datasets used for benchmarking are made publicly
available. This work is the first of its kind and is compared with available
baselines to demonstrate its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Brain Activity with Advanced Transformer Models: Exploring the
  Role of Punctuation in Semantic Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenon Lamprou, Frank Polick, Yashar Moshfeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research examines the congruence between neural activity and advanced
transformer models, emphasizing the semantic significance of punctuation in
text understanding. Utilizing an innovative approach originally proposed by
Toneva and Wehbe, we evaluate four advanced transformer models RoBERTa,
DistiliBERT, ALBERT, and ELECTRA against neural activity data. Our findings
indicate that RoBERTa exhibits the closest alignment with neural activity,
surpassing BERT in accuracy. Furthermore, we investigate the impact of
punctuation removal on model performance and neural alignment, revealing that
BERT's accuracy enhances in the absence of punctuation. This study contributes
to the comprehension of how neural networks represent language and the
influence of punctuation on semantic processing within the human brain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFactor GNNs: Revisiting Factorisation-based Models from a
  Message-Passing Perspective <span class="chip">NeurIPS
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09980v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09980v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and generalise to unseen nodes in inductive settings. Our work bridges
the gap between FMs and GNNs by proposing ReFactor GNNs. This new architecture
draws upon both modelling paradigms, which previously were largely thought of
as disjoint. Concretely, using a message-passing formalism, we show how FMs can
be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our ReFactor GNNs. Across
a multitude of well-established KGC benchmarks, our ReFactor GNNs achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36th Conference on Neural Information Processing Systems (NeurIPS
  2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PolInterviews -- A <span class="highlight-title">Dataset</span> of German Politician Public Broadcast
  Interviews 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04484v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04484v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Birkenmaier, Laureen Sieber, Felix Bergstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel dataset of public broadcast interviews featuring
high-ranking German politicians. The interviews were sourced from YouTube,
transcribed, processed for speaker identification, and stored in a tidy and
open format. The dataset comprises 99 interviews with 33 different German
politicians across five major interview formats, containing a total of 28,146
sentences. As the first of its kind, this dataset offers valuable opportunities
for research on various aspects of political communication in the (German)
political contexts, such as agenda-setting, interviewer dynamics, or
politicians' self-presentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Crafting Customisable Characters with LLMs: Introducing SimsChat, a
  Persona-Driven Role-Playing Agent Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17962v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17962v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Yang, Dong Liu, Chenghao Xiao, Kun Zhao, Chen Tang, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate remarkable ability to comprehend
instructions and generate human-like text, enabling sophisticated agent
simulation beyond basic behavior replication. However, the potential for
creating freely customisable characters remains underexplored. We introduce the
Customisable Conversation Agent Framework, which employs LLMs to simulate
real-world characters through personalised characteristic feature injection,
enabling diverse character creation according to user preferences. We propose
the SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn
role-playing dialogues across 1,360 real-world scenes. Characters are initially
customised using pre-defined elements (career, aspiration, traits, skills),
then expanded through personal and social profiles. Building on this, we
present SimsChat, a freely customisable role-playing agent incorporating
various realistic settings and topic-specified character interactions.
Experimental results on both SimsConv and WikiRoleEval datasets demonstrate
SimsChat's superior performance in maintaining character consistency, knowledge
accuracy, and appropriate question rejection compared to existing models. Our
framework provides valuable insights for developing more accurate and
customisable human simulacra. Our data and code are publicly available at
https://github.com/Bernard-Yang/SimsChat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can linguists better understand DNA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual transfer ability, which reflects how well models fine-tuned on
one source language can be applied to other languages, has been well studied in
multilingual pre-trained models. However, the existence of such capability
transfer between natural language and gene sequences/languages remains under
explored.This study addresses this gap by drawing inspiration from the
sentence-pair classification task used for evaluating sentence similarity in
natural language. We constructed two analogous tasks: DNA-pair
classification(DNA sequence similarity) and DNA-protein-pair
classification(gene coding determination). These tasks were designed to
validate the transferability of capabilities from natural language to gene
sequences. Even a small-scale pre-trained model like GPT-2-small, which was
pre-trained on English, achieved an accuracy of 78% on the DNA-pair
classification task after being fine-tuned on English sentence-pair
classification data(XTREME PAWS-X). While training a BERT model on multilingual
text, the precision reached 89%. On the more complex DNA-protein-pair
classification task, however, the model's output was barely distinguishable
from random output.Experimental validation has confirmed that the transfer of
capabilities from natural language to biological language is unequivocally
present. Building on this foundation, we have also investigated the impact of
model parameter scale and pre-training on this capability transfer. We provide
recommendations for facilitating the transfer of capabilities from natural
language to genetic language,as well as new approaches for conducting
biological research based on this capability.This study offers an intriguing
new perspective on exploring the relationship between natural language and
genetic language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ aiXcoder-7B: A Lightweight and Effective Large Language Model for Code
  Processing <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13187v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13187v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Jiang, Jia Li, He Zong, Huanyu Liu, Hao Zhu, Shukai Hu, Erlu Li, Jiazheng Ding, Yu Han, Wei Ning, Gen Wang, Yihong Dong, Kechi Zhang, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been widely used in code completion, and
researchers are focusing on scaling up LLMs to improve their accuracy. However,
larger LLMs have lower inference efficiency, affecting developers' experience
and productivity. In this paper, we propose a lightweight and effective LLM for
code completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B
achieves higher code completion accuracy while having smaller scales (i.e., 7
billion parameters). We attribute the superiority of aiXcoder-7B to three key
factors: (1) Multi-objective training. We employ three training objectives, one
of which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers
the syntax structures in code and effectively improves the performance of LLMs
for code. (2) Diverse data sampling strategies. They consider inter-file
relationships and enhance the capability of LLMs in understanding cross-file
contexts. (3) Extensive high-quality data. We establish a rigorous data
collection pipeline and consume a total of 1.2 trillion unique tokens for
training aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a
broad distribution of code. We evaluate aiXcoder-7B in five popular code
completion benchmarks and a new benchmark collected by this paper. The results
show that aiXcoder-7B outperforms the latest six LLMs with similar sizes and
even surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),
positioning aiXcoder-7B as a lightweight and effective LLM for academia and
industry. Finally, we summarize three valuable insights for helping
practitioners train the next generations of LLMs for code. aiXcoder-7B has been
open-souced and gained significant attention. Until January 2025, aiXcoder-7B
has received 2,226 GitHub Stars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>(1) Accepted by the 47th International Conference on Software
  Engineering (ICSE 2025). (2) aiXcoder-7B is available at
  https://github.com/aixcoder-plugin/aiXcoder-7B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AudioBERT: Audio Knowledge Augmented Language Model <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjong Ok, Suho Yoo, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have identified that language models, pretrained on text-only
datasets, often lack elementary visual knowledge, \textit{e.g.,} colors of
everyday objects. Motivated by this observation, we ask whether a similar
shortcoming exists in terms of the \textit{auditory} knowledge. To answer this
question, we construct a new dataset called AuditoryBench, which consists of
two novel tasks for evaluating auditory knowledge. Based on our analysis using
the benchmark, we find that language models also suffer from a severe lack of
auditory knowledge. To address this limitation, we propose AudioBERT, a novel
method to augment the auditory knowledge of BERT through a retrieval-based
approach. First, we detect auditory knowledge spans in prompts to query our
retrieval model efficiently. Then, we inject audio knowledge into BERT and
switch on low-rank adaptation for effective adaptation when audio knowledge is
required. Our experiments demonstrate that AudioBERT is quite effective,
achieving superior performance on the AuditoryBench. The dataset and code are
available at \bulurl{https://github.com/HJ-Ok/AudioBERT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focus On This, Not That! Steering LLMs With Adaptive Feature
  Specification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H. S. Torr, Francesco Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Instruction Tuning (IT) in training large language
models (LLMs) to perform arbitrary user-specified tasks, these models often
still leverage spurious or biased features learned from their training data,
leading to undesired behaviours when deploying them in new contexts. In this
work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to
condition their responses by focusing on specific features whilst ignoring
others, leading to different behaviours based on what features are specified.
Across several experimental settings, we show that focus-tuned models can be
adaptively steered by focusing on different features at inference-time: for
instance, robustness can be improved by focusing on task-causal features and
ignoring spurious features, and social bias can be mitigated by ignoring
demographic categories. Furthermore, FIT can steer behaviour in new contexts,
generalising under distribution shift and to new unseen features at inference
time, and thereby facilitating more robust, fair, and controllable LLM
applications in real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Fine-Tuned <span class="highlight-title">Retrie</span>val-Augmented Generation with Long-Context
  Support: For 3GPP Standards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Erak, Nouf Alabbasi, Omar Alhussein, Ismail Lotfi, Amr Hussein, Sami Muhaidat, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show that large language models (LLMs) struggle with technical
standards in telecommunications. We propose a fine-tuned retrieval-augmented
generation (RAG) system based on the Phi-2 small language model (SLM) to serve
as an oracle for communication networks. Our developed system leverages
forward-looking semantic chunking to adaptively determine parsing breakpoints
based on embedding similarity, enabling effective processing of diverse
document formats. To handle the challenge of multiple similar contexts in
technical standards, we employ a re-ranking algorithm to prioritize the most
relevant retrieved chunks. Recognizing the limitations of Phi-2's small context
window, we implement a recent technique, namely SelfExtend, to expand the
context window during inference, which not only boosts the performance but also
can accommodate a wider range of user queries and design requirements from
customers to specialized technicians. For fine-tuning, we utilize the low-rank
adaptation (LoRA) technique to enhance computational efficiency during training
and enable effective fine-tuning on small datasets. Our comprehensive
experiments demonstrate substantial improvements over existing
question-answering approaches in the telecom domain, achieving performance that
exceeds larger language models such as GPT-4 (which is about 880 times larger
in size). This work presents a novel approach to leveraging SLMs for
communication networks, offering a balance of efficiency and performance. This
work can serve as a foundation towards agentic language models for networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Proc. IEEE Globecom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAGBench: Explainable Benchmark for <span class="highlight-title">Retrie</span>val-Augmented Generation
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Friel, Masha Belyi, Atindriyo Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has become a standard architectural
pattern for incorporating domain-specific knowledge into user-facing chat
applications powered by Large Language Models (LLMs). RAG systems are
characterized by (1) a document retriever that queries a domain-specific corpus
for context information relevant to an input query, and (2) an LLM that
generates a response based on the provided query and context. However,
comprehensive evaluation of RAG systems remains a challenge due to the lack of
unified evaluation criteria and annotated datasets. In response, we introduce
RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k
examples. It covers five unique industry-specific domains and various RAG task
types. RAGBench examples are sourced from industry corpora such as user
manuals, making it particularly relevant for industry applications. Further, we
formalize the TRACe evaluation framework: a set of explainable and actionable
RAG evaluation metrics applicable across all RAG domains. We release the
labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.
RAGBench explainable labels facilitate holistic evaluation of RAG systems,
enabling actionable feedback for continuous improvement of production
applications. Thorough extensive benchmarking, we find that LLM-based RAG
evaluation methods struggle to compete with a finetuned RoBERTa model on the
RAG evaluation task. We identify areas where existing approaches fall short and
propose the adoption of RAGBench with TRACe towards advancing the state of RAG
evaluation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SD-Eval: A Benchmark <span class="highlight-title">Dataset</span> for Spoken Dialogue Understanding Beyond
  Words <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, Zhizheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech encompasses a wealth of information, including but not limited to
content, paralinguistic, and environmental information. This comprehensive
nature of speech significantly impacts communication and is crucial for
human-computer interaction. Chat-Oriented Large Language Models (LLMs), known
for their general-purpose assistance capabilities, have evolved to handle
multi-modal inputs, including speech. Although these models can be adept at
recognizing and analyzing speech, they often fall short of generating
appropriate responses. We argue that this is due to the lack of principles on
task definition and model development, which requires open-source datasets and
metrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a
benchmark dataset aimed at multidimensional evaluation of spoken dialogue
understanding and generation. SD-Eval focuses on paralinguistic and
environmental information and includes 7,303 utterances, amounting to 8.76
hours of speech data. The data is aggregated from eight public datasets,
representing four perspectives: emotion, accent, age, and background sound. To
assess the SD-Eval benchmark dataset, we implement three different models and
construct a training set following a process similar to that of SD-Eval. The
training set contains 1,052.72 hours of speech data and 724.4k utterances. We
also conduct a comprehensive evaluation using objective evaluation methods
(e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the
generated responses. Models conditioned with paralinguistic and environmental
information outperform their counterparts in both objective and subjective
measures. Moreover, experiments demonstrate that LLM-based metrics show a
higher correlation with human evaluation compared to traditional metrics. We
open-source SD-Eval at https://github.com/amphionspace/SD-Eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discriminative Representation learning via Attention-Enhanced
  Contrastive Learning for Short Text Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has gained significant attention in short text
clustering, yet it has an inherent drawback of mistakenly identifying samples
from the same category as negatives and then separating them in the feature
space (false negative separation), which hinders the generation of superior
representations. To generate more discriminative representations for efficient
clustering, we propose a novel short text clustering method, called
Discriminative Representation learning via \textbf{A}ttention-\textbf{E}nhanced
\textbf{C}ontrastive \textbf{L}earning for Short Text Clustering
(\textbf{AECL}). The \textbf{AECL} consists of two modules which are the
pseudo-label generation module and the contrastive learning module. Both
modules build a sample-level attention mechanism to capture similarity
relationships between samples and aggregate cross-sample features to generate
consistent representations. Then, the former module uses the more
discriminative consistent representation to produce reliable supervision
information for assist clustering, while the latter module explores similarity
relationships and consistent representations optimize the construction of
positive samples to perform similarity-guided contrastive learning, effectively
addressing the false negative separation issue. Experimental results
demonstrate that the proposed \textbf{AECL} outperforms state-of-the-art
methods. If the paper is accepted, we will open-source the code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language models (LMs) have sparked growing interest in
developing LM agents. While fully autonomous agents could excel in many
scenarios, numerous use cases inherently require them to collaborate with
humans due to humans' latent preferences, domain expertise, or need for
control. To facilitate the study of human-agent collaboration, we present
Collaborative Gym (Co-Gym), a general framework enabling asynchronous,
tripartite interaction among agents, humans, and task environments. We
instantiate Co-Gym with three representative tasks in both simulated and
real-world conditions, and propose an evaluation framework that assesses both
the collaboration outcomes and processes. Our findings reveal that
collaborative agents consistently outperform their fully autonomous
counterparts in task performance within those delivered cases, achieving win
rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related
Work when evaluated by real users. However, our study also highlights
significant challenges in developing collaborative agents, requiring
advancements in core aspects of intelligence -- communication capabilities,
situational awareness, and balancing autonomy and human control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERaLiON-TextLLM: Cross-Lingual Understanding of Large Language Models
  in Chinese, Indonesian, Malay, and Singlish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Huang, Tarun Kumar Vangani, Minh Duc Pham, Xunlong Zou, Bin Wang, Zhengyuan Liu, Ai Ti Aw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual large language models (MLLMs) have shown impressive capabilities
across a variety of languages. However, efficacy can differ greatly between
different language families, especially for those with limited linguistic
resources. This report presents MERaLiON-TextLLM, a series of open-source
language models specifically tailored to improve understanding and generation
in Chinese, Indonesian, Malay, and Singlish. The initial released model is
built on Llama-3-8B-Base and refined through a meticulously crafted process of
continued pre-training and weight merging. Our approach achieves performance
improvements across benchmarks in these languages, exceeding the capabilities
of the official Llama-3 models. We provide the model checkpoints as a resource
to support further research and development in cross-lingual language
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do LLMs Really Think Step-by-step In Implicit Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15862v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15862v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been well-known that Chain-of-Thought can remarkably enhance LLMs'
performance on complex tasks. However, because it also introduces slower
inference speeds and higher computational costs, many researches have attempted
to use implicit CoT, which does not need LLMs to explicitly generate the
intermediate steps. However, the invisible reasoning process leaves us a doubt
that, can implicit CoT really be equal to explicit CoT? Therefore, in this
study, we address this question through experiments. We probe the information
of intermediate steps from the model's hidden states when it is either trained
or prompted to perform implicit CoT. The results surprisingly indicate that
when prompted, LLMs hardly think about intermediate steps, suggesting they may
just rely on experience rather than strict step-by-step reasoning. But when
trained, they indeed calculate intermediate steps. Moreover, in both
situations, we find the effect of using implicit CoT is susceptible to the
format of the problem, reaffirming the current deficiency of implicit CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is in
  https://github.com/yuyijiong/if_step_by_step_implicit_CoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERaLiON-AudioLLM: Bridging Audio and Language with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09818v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09818v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxu He, Zhuohan Liu, Shuo Sun, Bin Wang, Wenyu Zhang, Xunlong Zou, Nancy F. Chen, Ai Ti Aw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning
in One Network), the first speech-text model tailored for Singapore's
multilingual and multicultural landscape. Developed under the National Large
Language Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates
advanced speech and text processing to address the diverse linguistic nuances
of local accents and dialects, enhancing accessibility and usability in
complex, multilingual environments. Our results demonstrate improvements in
both speech recognition and task-specific understanding, positioning
MERaLiON-AudioLLM as a pioneering solution for region specific AI applications.
We envision this release to set a precedent for future models designed to
address localised linguistic and cultural contexts in a global framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for
  Multi-label Social Media Text Classification in Disaster Informatics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yin, Chengkai Liu, Ali Mostafavi, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of crisis/disaster informatics, social media is increasingly
being used for improving situational awareness to inform response and relief
efforts. Efficient and accurate text classification tools have been a focal
area of investigation in crisis informatics. However, current methods mostly
rely on single-label text classification models, which fails to capture
different insights embedded in dynamic and multifaceted disaster-related social
media data. This study introduces a novel approach to disaster text
classification by enhancing a pre-trained Large Language Model (LLM) through
instruction fine-tuning targeted for multi-label classification of
disaster-related tweets. Our methodology involves creating a comprehensive
instruction dataset from disaster-related tweets, which is then used to
fine-tune an open-source LLM, thereby embedding it with disaster-specific
knowledge. This fine-tuned model can classify multiple aspects of
disaster-related information simultaneously, such as the type of event,
informativeness, and involvement of human aid, significantly improving the
utility of social media data for situational awareness in disasters. The
results demonstrate that this approach enhances the categorization of critical
information from social media posts, thereby facilitating a more effective
deployment for situational awareness during emergencies. This research paves
the way for more advanced, adaptable, and robust disaster management tools,
leveraging the capabilities of LLMs to improve real-time situational awareness
and response strategies in disaster scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Relevant source code and data is available:
  https://github.com/KaiYin97/CrsisLLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Framework for Inference-time Scaling and Steering of Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models produce impressive results in modalities ranging from images
and video to protein design and text. However, generating samples with
user-specified properties remains a challenge. Recent research proposes
fine-tuning models to maximize rewards that capture desired properties, but
these methods require expensive training and are prone to mode collapse. In
this work, we propose Feynman Kac (FK) steering, an inference-time framework
for steering diffusion models with reward functions. FK steering works by
sampling a system of multiple interacting diffusion processes, called
particles, and resampling particles at intermediate steps based on scores
computed using functions called potentials. Potentials are defined using
rewards for intermediate states and are selected such that a high value
indicates that the particle will yield a high-reward sample. We explore various
choices of potentials, intermediate rewards, and samplers. We evaluate FK
steering on text-to-image and text diffusion models. For steering text-to-image
models with a human preference reward, we find that FK steering a 0.8B
parameter model outperforms a 2.6B parameter fine-tuned model on prompt
fidelity, with faster sampling and no training. For steering text diffusion
models with rewards for text quality and specific text attributes, we find that
FK steering generates lower perplexity, more linguistically acceptable outputs
and enables gradient-free control of attributes like toxicity. Our results
demonstrate that inference-time scaling and steering of diffusion models, even
with off-the-shelf rewards, can provide significant sample quality gains and
controllability benefits. Code is available at
https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span>ing Attitudinal Alignment Between Large Language Models Vs. Humans
  Towards 17 Sustainable Development Goals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Wu, Ying Xu, Tingsong Xiao, Yunze Xiao, Yitong Li, Tianyang Wang, Yichi Zhang, Shanghai Zhong, Yuwei Zhang, Wei Lu, Yifan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as potent tools for advancing the
United Nations' Sustainable Development Goals (SDGs). However, the attitudinal
disparities between LLMs and humans towards these goals can pose significant
challenges. This study conducts a comprehensive review and analysis of the
existing literature on the attitudes of LLMs towards the 17 SDGs, emphasizing
the comparison between their attitudes and support for each goal and those of
humans. We examine the potential disparities, primarily focusing on aspects
such as understanding and emotions, cultural and regional differences, task
objective variations, and factors considered in the decision-making process.
These disparities arise from the underrepresentation and imbalance in LLM
training data, historical biases, quality issues, lack of contextual
understanding, and skewed ethical values reflected. The study also investigates
the risks and harms that may arise from neglecting the attitudes of LLMs
towards the SDGs, including the exacerbation of social inequalities, racial
discrimination, environmental destruction, and resource wastage. To address
these challenges, we propose strategies and recommendations to guide and
regulate the application of LLMs, ensuring their alignment with the principles
and goals of the SDGs, and therefore creating a more just, inclusive, and
sustainable future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language
  Models for Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong He, Pengfei Li, Gang Liu, Genrong He, Zhaolin Chen, Shenjun Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) represent an evolutionary expansion
in the capabilities of traditional large language models, enabling them to
tackle challenges that surpass the scope of purely text-based applications. It
leverages the knowledge previously encoded within these language models,
thereby enhancing their applicability and functionality in the reign of
multimodal contexts. Recent works investigate the adaptation of MLLMs as a
universal solution to address medical multi-modal problems as a generative
task. In this paper, we propose a parameter efficient framework for fine-tuning
MLLMs, specifically validated on medical visual question answering (Med-VQA)
and medical report generation (MRG) tasks, using public benchmark datasets. We
also introduce an evaluation metric using the 5-point Likert scale and its
weighted average value to measure the quality of the generated reports for MRG
tasks, where the scale ratings are labelled by both humans manually and the
GPT-4 model. We further assess the consistency of performance metrics across
traditional measures, GPT-4, and human ratings for both VQA and MRG tasks. The
results indicate that semantic similarity assessments using GPT-4 align closely
with human annotators and provide greater stability, yet they reveal a
discrepancy when compared to conventional lexical similarity measurements. This
questions the reliability of lexical similarity metrics for evaluating the
performance of generative models in Med-VQA and report generation tasks.
Besides, our fine-tuned model significantly outperforms GPT-4v. This indicates
that without additional fine-tuning, multi-modal models like GPT-4v do not
perform effectively on medical imaging tasks. The code will be available here:
https://github.com/jinlHe/PeFoMed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures
  and Languages <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, Víctor Gutiérrez-Basulto, Yazmín Ibáñez-García, Hwaran Lee, Shamsuddeen Hassan Muhammad, Kiwoong Park, Anar Sabuhi Rzayev, Nina White, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jose Camacho-Collados, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often lack culture-specific knowledge of daily
life, especially across diverse regions and non-English languages. Existing
benchmarks for evaluating LLMs' cultural sensitivities are limited to a single
language or collected from online sources such as Wikipedia, which do not
reflect the mundane everyday lifestyles of diverse regions. That is,
information about the food people eat for their birthday celebrations, spices
they typically use, musical instruments youngsters play, or the sports they
practice in school is common cultural knowledge but uncommon in easily
collected online sources, especially for underrepresented cultures. To address
this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate
LLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises
52.6k question-answer pairs from 16 countries/regions, in 13 different
languages, including low-resource ones such as Amharic, Assamese, Azerbaijani,
Hausa, and Sundanese. We construct the benchmark to include two formats of
questions: short-answer and multiple-choice. We show that LLMs perform better
for cultures that are highly represented online, with a maximum 57.34%
difference in GPT-4, the best-performing model, in the short-answer format. For
cultures represented by mid-to-high-resource languages, LLMs perform better in
their local languages, but for cultures represented by low-resource languages,
LLMs perform better in English than the local languages. We make our dataset
publicly available at: https://github.com/nlee0212/BLEnD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Datasets & Benchmark Track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">110</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Multi-modal Large Language Models for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, Fatih Porikli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving demands safe motion planning, especially in critical
"long-tail" scenarios. Recent end-to-end autonomous driving systems leverage
large language models (LLMs) as planners to improve generalizability to rare
events. However, using LLMs at test time introduces high computational costs.
To address this, we propose DiMA, an end-to-end autonomous driving system that
maintains the efficiency of an LLM-free (or vision-based) planner while
leveraging the world knowledge of an LLM. DiMA distills the information from a
multi-modal LLM to a vision-based end-to-end planner through a set of specially
designed surrogate tasks. Under a joint training strategy, a scene encoder
common to both networks produces structured representations that are
semantically grounded as well as aligned to the final planning objective.
Notably, the LLM is optional at inference, enabling robust planning without
compromising on efficiency. Training with DiMA results in a 37% reduction in
the L2 trajectory error and an 80% reduction in the collision rate of the
vision-based planner, as well as a 44% trajectory error reduction in longtail
scenarios. DiMA also achieves state-of-the-art performance on the nuScenes
planning benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynthLight: Portrait Relighting with Diffusion Model by Learning to
  Re-render Synthetic Faces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumit Chaturvedi, Mengwei Ren, Yannick Hold-Geoffroy, Jingyuan Liu, Julie Dorsey, Zhixin Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SynthLight, a diffusion model for portrait relighting. Our
approach frames image relighting as a re-rendering problem, where pixels are
transformed in response to changes in environmental lighting conditions. Using
a physically-based rendering engine, we synthesize a dataset to simulate this
lighting-conditioned transformation with 3D head assets under varying lighting.
We propose two training and inference strategies to bridge the gap between the
synthetic and real image domains: (1) multi-task training that takes advantage
of real human portraits without lighting labels; (2) an inference time
diffusion sampling procedure based on classifier-free guidance that leverages
the input portrait to better preserve details. Our method generalizes to
diverse real photographs and produces realistic illumination effects, including
specular highlights and cast shadows, while preserving the subject's identity.
Our quantitative experiments on Light Stage data demonstrate results comparable
to state-of-the-art relighting methods. Our qualitative results on in-the-wild
images showcase rich and unprecedented illumination effects. Project Page:
\url{https://vrroom.github.io/synthlight/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 25 figures, Project Page
  https://vrroom.github.io/synthlight/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learnings from Scaling Visual Tokenizers for Reconstruction and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual tokenization via auto-encoding empowers state-of-the-art image and
video generative models by compressing pixels into a latent space. Although
scaling Transformer-based generators has been central to recent advances, the
tokenizer component itself is rarely scaled, leaving open questions about how
auto-encoder design choices influence both its objective of reconstruction and
downstream generative performance. Our work aims to conduct an exploration of
scaling in auto-encoders to fill in this blank. To facilitate this exploration,
we replace the typical convolutional backbone with an enhanced Vision
Transformer architecture for Tokenization (ViTok). We train ViTok on
large-scale image and video datasets far exceeding ImageNet-1K, removing data
constraints on tokenizer scaling. We first study how scaling the auto-encoder
bottleneck affects both reconstruction and generation -- and find that while it
is highly correlated with reconstruction, its relationship with generation is
more complex. We next explored the effect of separately scaling the
auto-encoders' encoder and decoder on reconstruction and generation
performance. Crucially, we find that scaling the encoder yields minimal gains
for either reconstruction or generation, while scaling the decoder boosts
reconstruction but the benefits for generation are mixed. Building on our
exploration, we design ViTok as a lightweight auto-encoder that achieves
competitive performance with state-of-the-art auto-encoders on ImageNet-1K and
COCO reconstruction tasks (256p and 512p) while outperforming existing
auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x
fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates
competitive performance on image generation for ImageNet-1K and sets new
state-of-the-art benchmarks for class-conditional video generation on UCF-101.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 25 figures, 7 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Translation, Found in Context: Sign Language Translation with
  Contextual Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngjoon Jang, Haran Raajesh, Liliane Momeni, Gül Varol, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our objective is to translate continuous sign language into spoken language
text. Inspired by the way human interpreters rely on context for accurate
translation, we incorporate additional contextual cues together with the
signing video, into a new translation framework. Specifically, besides visual
sign recognition features that encode the input video, we integrate
complementary textual information from (i) captions describing the background
show, (ii) translation of previous sentences, as well as (iii) pseudo-glosses
transcribing the signing. These are automatically extracted and inputted along
with the visual features to a pre-trained large language model (LLM), which we
fine-tune to generate spoken language translations in text form. Through
extensive ablation studies, we show the positive contribution of each input cue
to the translation performance. We train and evaluate our approach on BOBSL --
the largest British Sign Language dataset currently available. We show that our
contextual approach significantly enhances the quality of the translations
compared to previously reported results on BOBSL, and also to state-of-the-art
methods that we implement as baselines. Furthermore, we demonstrate the
generality of our approach by applying it also to How2Sign, an American Sign
Language dataset, and achieve competitive results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexi Du, Jiazhen Zhang, Tal Zeevi, Nicha C. Dvornek, John A. Onofrey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are essential tools for computer vision
tasks, but they lack traditionally desired properties of extracted features
that could further improve model performance, e.g., rotational equivariance.
Such properties are ubiquitous in biomedical images, which often lack explicit
orientation. While current work largely relies on data augmentation or explicit
modules to capture orientation information, this comes at the expense of
increased training costs or ineffective approximations of the desired
equivariance. To overcome these challenges, we propose a novel and efficient
implementation of the Symmetric Rotation-Equivariant (SRE) Convolution
(SRE-Conv) kernel, designed to learn rotation-invariant features while
simultaneously compressing the model size. The SRE-Conv kernel can easily be
incorporated into any CNN backbone. We validate the ability of a deep SRE-CNN
to capture equivariance to rotation using the public MedMNISTv2 dataset (16
total tasks). SRE-Conv-CNN demonstrated improved rotated image classification
performance accuracy on all 16 test datasets in both 2D and 3D images, all
while increasing efficiency with fewer parameters and reduced memory footprint.
The code is available at https://github.com/XYPB/SRE-Conv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ISBI 2025 4-page paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComplexVAD: Detecting Interaction Anomalies in Video <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furkan Mumcu, Michael J. Jones, Yasin Yilmaz, Anoop Cherian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video anomaly detection datasets are inadequate for representing
complex anomalies that occur due to the interactions between objects. The
absence of complex anomalies in previous video anomaly detection datasets
affects research by shifting the focus onto simple anomalies. To address this
problem, we introduce a new large-scale dataset: ComplexVAD. In addition, we
propose a novel method to detect complex anomalies via modeling the
interactions between objects using a scene graph with spatio-temporal
attributes. With our proposed method and two other state-of-the-art video
anomaly detection methods, we obtain baseline scores on ComplexVAD and
demonstrate that our new method outperforms existing works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference-Time Scaling for Diffusion Models beyond Scaling Denoising
  Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have made significant impacts across various domains,
largely due to their ability to scale during training by increasing data,
computational resources, and model size, a phenomenon characterized by the
scaling laws. Recent research has begun to explore inference-time scaling
behavior in Large Language Models (LLMs), revealing how performance can further
improve with additional computation during inference. Unlike LLMs, diffusion
models inherently possess the flexibility to adjust inference-time computation
via the number of denoising steps, although the performance gains typically
flatten after a few dozen. In this work, we explore the inference-time scaling
behavior of diffusion models beyond increasing denoising steps and investigate
how the generation performance can further improve with increased computation.
Specifically, we consider a search problem aimed at identifying better noises
for the diffusion sampling process. We structure the design space along two
axes: the verifiers used to provide feedback, and the algorithms used to find
better noise candidates. Through extensive experiments on class-conditioned and
text-conditioned image generation benchmarks, our findings reveal that
increasing inference-time compute leads to substantial improvements in the
quality of samples generated by diffusion models, and with the complicated
nature of images, combinations of the components in the framework can be
specifically chosen to conform with different application scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Aerial Detection Baseline of Multimodal Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multimodal language models (MLMs) based on generative pre-trained
Transformer are considered powerful candidates for unifying various domains and
tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding
performance in multiple tasks, such as visual question answering and visual
grounding. In addition to visual grounding that detects specific objects
corresponded to given instruction, aerial detection, which detects all objects
of multiple categories, is also a valuable and challenging task for RS
foundation models. However, aerial detection has not been explored by existing
RS MLMs because the autoregressive prediction mechanism of MLMs differs
significantly from the detection outputs. In this paper, we present a simple
baseline for applying MLMs to aerial detection for the first time, named
LMMRotate. Specifically, we first introduce a normalization method to transform
detection outputs into textual outputs to be compatible with the MLM framework.
Then, we propose a evaluation method, which ensures a fair comparison between
MLMs and conventional object detection models. We construct the baseline by
fine-tuning open-source general-purpose MLMs and achieve impressive detection
performance comparable to conventional detector. We hope that this baseline
will serve as a reference for future MLM development, enabling more
comprehensive capabilities for understanding RS images. Code is available at
https://github.com/Li-Qingyun/mllm-mmrotate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 table, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLOL: Fast Baselines for Real-World Low-Light Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan C. Benito, Daniel Feijoo, Alvaro Garcia, Marcos V. Conde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Light Image Enhancement (LLIE) is a key task in computational photography
and imaging. The problem of enhancing images captured during night or in dark
environments has been well-studied in the image signal processing literature.
However, current deep learning-based solutions struggle with efficiency and
robustness in real-world scenarios (e.g. scenes with noise, saturated pixels,
bad illumination). We propose a lightweight neural network that combines image
processing in the frequency and spatial domains. Our method, FLOL+, is one of
the fastest models for this task, achieving state-of-the-art results on popular
real scenes datasets such as LOL and LSRW. Moreover, we are able to process
1080p images under 12ms. Code and models at https://github.com/cidautai/FLOL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Continual Forgetting for <span class="highlight-title">Pre-train</span>ed Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For privacy and security concerns, the need to erase unwanted information
from pre-trained vision models is becoming evident nowadays. In real-world
scenarios, erasure requests originate at any time from both users and model
owners, and these requests usually form a sequence. Therefore, under such a
setting, selective information is expected to be continuously removed from a
pre-trained model while maintaining the rest. We define this problem as
continual forgetting and identify three key challenges. (i) For unwanted
knowledge, efficient and effective deleting is crucial. (ii) For remaining
knowledge, the impact brought by the forgetting procedure should be minimal.
(iii) In real-world scenarios, the training samples may be scarce or partially
missing during the process of forgetting. To address them, we first propose
Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA
modules to fine-tune the FFN layers in Transformer blocks for each forgetting
task independently, and towards (ii), a simple group sparse regularization is
adopted, enabling automatic selection of specific LoRA groups and zeroing out
the others. To further extend GS-LoRA to more practical scenarios, we
incorporate prototype information as additional supervision and introduce a
more practical approach, GS-LoRA++. For each forgotten class, we move the
logits away from its original prototype. For the remaining classes, we pull the
logits closer to their respective prototypes. We conduct extensive experiments
on face recognition, object detection and image classification and demonstrate
that our method manages to forget specific classes with minimal impact on other
classes. Codes have been released on https://github.com/bjzhb666/GS-LoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models via DPO:
  On-Policy Data Hold the Key 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination remains a major challenge for Large Vision-Language Models
(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention
as a simple solution to hallucination issues. It directly learns from
constructed preference pairs that reflect the severity of hallucinations in
responses to the same prompt and image. Nonetheless, different data
construction methods in existing works bring notable performance variations. We
identify a crucial factor here: outcomes are largely contingent on whether the
constructed data aligns on-policy w.r.t the initial (reference) policy of DPO.
Theoretical analysis suggests that learning from off-policy data is impeded by
the presence of KL-divergence between the updated policy and the reference
policy. From the perspective of dataset distribution, we systematically
summarize the inherent flaws in existing algorithms that employ DPO to address
hallucination issues. To alleviate the problems, we propose On-Policy Alignment
(OPA)-DPO framework, which uniquely leverages expert feedback to correct
hallucinated responses and aligns both the original and expert-revised
responses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO
achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:
13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared
to the previous SOTA algorithm trained with 16k samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Image-Text Correspondence with Cost Aggregation for
  Open-Vocabulary Part Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiho Choi, Seonho Lee, Minhyun Lee, Seungho Lee, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing
fine-grained parts in unseen categories. We identify two primary challenges in
OVPS: (1) the difficulty in aligning part-level image-text correspondence, and
(2) the lack of structural understanding in segmenting object parts. To address
these issues, we propose PartCATSeg, a novel framework that integrates
object-aware part-level cost aggregation, compositional loss, and structural
guidance from DINO. Our approach employs a disentangled cost aggregation
strategy that handles object and part-level costs separately, enhancing the
precision of part-level segmentation. We also introduce a compositional loss to
better capture part-object relationships, compensating for the limited part
annotations. Additionally, structural guidance from DINO features improves
boundary delineation and inter-part understanding. Extensive experiments on
Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that
our method significantly outperforms state-of-the-art approaches, setting a new
baseline for robust generalization to unseen part categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP
  Evaluation Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Roger, Prateek Humane, Daniel Z. Kaplan, Kshitij Gupta, Qi Sun, George Adamopoulos, Jonathan Siu Chi Lim, Quentin Anthony, Edwin Fennell, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of Vision-Language Models (VLMs) in the past several years
calls for rigorous and comprehensive evaluation methods and benchmarks. This
work analyzes existing VLM evaluation techniques, including automated metrics,
AI-based assessments, and human evaluations across diverse tasks. We first
introduce Robin - a novel suite of VLMs that we built by combining Large
Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use
Robin to identify shortcomings of current evaluation approaches across scales.
Next, to overcome the identified limitations, we introduce CHIRP - a new long
form response benchmark we developed for more robust and complete VLM
evaluation. We provide open access to the Robin training code, model suite, and
CHIRP benchmark to promote reproducibility and advance VLM research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Face Matching and Physical-Digital Spoofing Attack Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Kunwar, Ajita Rattani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition technology has dramatically transformed the landscape of
security, surveillance, and authentication systems, offering a user-friendly
and non-invasive biometric solution. However, despite its significant
advantages, face recognition systems face increasing threats from physical and
digital spoofing attacks. Current research typically treats face recognition
and attack detection as distinct classification challenges. This approach
necessitates the implementation of separate models for each task, leading to
considerable computational complexity, particularly on devices with limited
resources. Such inefficiencies can stifle scalability and hinder performance.
In response to these challenges, this paper introduces an innovative unified
model designed for face recognition and detection of physical and digital
attacks. By leveraging the advanced Swin Transformer backbone and incorporating
HiLo attention in a convolutional neural network framework, we address unified
face recognition and spoof attack detection more effectively. Moreover, we
introduce augmentation techniques that replicate the traits of physical and
digital spoofing cues, significantly enhancing our model robustness. Through
comprehensive experimental evaluation across various datasets, we showcase the
effectiveness of our model in unified face recognition and spoof detection.
Additionally, we confirm its resilience against unseen physical and digital
spoofing attacks, underscoring its potential for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WMamba: Wavelet-based Mamba for Face Forgery Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siran Peng, Tianshuo Zhang, Li Gao, Xiangyu Zhu, Haoyuan Zhang, Kai Pang, Zhen Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of deepfake generation technologies, the demand
for robust and accurate face forgery detection algorithms has become
increasingly critical. Recent studies have demonstrated that wavelet analysis
can uncover subtle forgery artifacts that remain imperceptible in the spatial
domain. Wavelets effectively capture important facial contours, which are often
slender, fine-grained, and global in nature. However, existing wavelet-based
approaches fail to fully leverage these unique characteristics, resulting in
sub-optimal feature extraction and limited generalizability. To address this
challenge, we introduce WMamba, a novel wavelet-based feature extractor built
upon the Mamba architecture. WMamba maximizes the utility of wavelet
information through two key innovations. First, we propose Dynamic Contour
Convolution (DCConv), which employs specially crafted deformable kernels to
adaptively model slender facial contours. Second, by leveraging the Mamba
architecture, our method captures long-range spatial relationships with linear
computational complexity. This efficiency allows for the extraction of
fine-grained, global forgery artifacts from small image patches. Extensive
experimental results show that WMamba achieves state-of-the-art (SOTA)
performance, highlighting its effectiveness and superiority in face forgery
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metric Learning with Progressive Self-Distillation for Audio-Visual
  Embedding Learning <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghuo Zeng, Kazushi Ikeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning projects samples into an embedded space, where similarities
and dissimilarities are quantified based on their learned representations.
However, existing methods often rely on label-guided representation learning,
where representations of different modalities, such as audio and visual data,
are aligned based on annotated labels. This approach tends to underutilize
latent complex features and potential relationships inherent in the
distributions of audio and visual data that are not directly tied to the
labels, resulting in suboptimal performance in audio-visual embedding learning.
To address this issue, we propose a novel architecture that integrates
cross-modal triplet loss with progressive self-distillation. Our method
enhances representation learning by leveraging inherent distributions and
dynamically refining soft audio-visual alignments -- probabilistic alignments
between audio and visual data that capture the inherent relationships beyond
explicit labels. Specifically, the model distills audio-visual
distribution-based knowledge from annotated labels in a subset of each batch.
This self-distilled knowledge is used t
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 2 tables. Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid
  Prototyping in Virtual Reality Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Augusto Pinheiro de Sousa, Heiko Hamann, Oliver Deussen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLAM is a foundational technique with broad applications in robotics and
AR/VR. SLAM simulations evaluate new concepts, but testing on
resource-constrained devices, such as VR HMDs, faces challenges: high
computational cost and restricted sensor data access. This work proposes a
sparse framework using mesh geometry projections as features, which improves
efficiency and circumvents direct sensor data access, advancing SLAM research
as we demonstrate in VR and through numerical evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential PatchCore: Anomaly Detection for Surface Inspection using
  Synthetic Impurities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runzhou Mao, Juraj Fulir, Christoph Garth, Petra Gospodnetić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The appearance of surface impurities (e.g., water stains, fingerprints,
stickers) is an often-mentioned issue that causes degradation of automated
visual inspection systems. At the same time, synthetic data generation
techniques for visual surface inspection have focused primarily on generating
perfect examples and defects, disregarding impurities. This study highlights
the importance of considering impurities when generating synthetic data. We
introduce a procedural method to include photorealistic water stains in
synthetic data. The synthetic datasets are generated to correspond to real
datasets and are further used to train an anomaly detection model and
investigate the influence of water stains. The high-resolution images used for
surface inspection lead to memory bottlenecks during anomaly detection
training. To address this, we introduce Sequential PatchCore - a method to
build coresets sequentially and make training on large images using
consumer-grade hardware tractable. This allows us to perform transfer learning
using coresets pre-trained on different dataset versions. Our results show the
benefits of using synthetic data for pre-training an explicit coreset anomaly
model and the extended performance benefits of finetuning the coreset using
real data. We observed how the impurities and labelling ambiguity lower the
model performance and have additionally reported the defect-wise recall to
provide an industrially relevant perspective on model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Teacher-<span class="highlight-title">Review</span>er-Student Framework for Semi-supervised 2D Human
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wulian Yun, Mengshi Qi, Fei Peng, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional 2D human pose estimation methods typically require extensive
labeled annotations, which are both labor-intensive and expensive. In contrast,
semi-supervised 2D human pose estimation can alleviate the above problems by
leveraging a large amount of unlabeled data along with a small portion of
labeled data. Existing semi-supervised 2D human pose estimation methods update
the network through backpropagation, ignoring crucial historical information
from the previous training process. Therefore, we propose a novel
semi-supervised 2D human pose estimation method by utilizing a newly designed
Teacher-Reviewer-Student framework. Specifically, we first mimic the phenomenon
that human beings constantly review previous knowledge for consolidation to
design our framework, in which the teacher predicts results to guide the
student's learning and the reviewer stores important historical parameters to
provide additional supervision signals. Secondly, we introduce a Multi-level
Feature Learning strategy, which utilizes the outputs from different stages of
the backbone to estimate the heatmap to guide network training, enriching the
supervisory information while effectively capturing keypoint relationships.
Finally, we design a data augmentation strategy, i.e., Keypoint-Mix, to perturb
pose information by mixing different keypoints, thus enhancing the network's
ability to discern keypoints. Extensive experiments on publicly available
datasets, demonstrate our method achieves significant improvements compared to
the existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-driven Adaptation of <span class="highlight-title">Foundation</span> Models for Few-shot Surgical
  Workflow Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingxuan Chen, Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Surgical workflow analysis is crucial for improving surgical
efficiency and safety. However, previous studies rely heavily on large-scale
annotated datasets, posing challenges in cost, scalability, and reliance on
expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven
Adaptation), designed to handle various surgical workflow analysis tasks with
minimal paired image-label data.
  Methods: Our approach has two key components. First, Few-shot selection-based
modality alignment selects a small subset of images and aligns their embeddings
with text embeddings from the downstream task, bridging the modality gap.
Second, Text-driven adaptation leverages only text data to train a decoder,
eliminating the need for paired image-text data. This decoder is then applied
to aligned image embeddings, enabling image-related tasks without explicit
image-text pairs.
  Results: We evaluate our approach to generative tasks (image captioning) and
discriminative tasks (triplet recognition and phase recognition). Results show
that Surg-FTDA outperforms baselines and generalizes well across downstream
tasks.
  Conclusion: We propose a text-driven adaptation approach that mitigates the
modality gap and handles multiple downstream tasks in surgical workflow
analysis, with minimal reliance on large annotated datasets. The code and
dataset will be released in https://github.com/TingxuanSix/Surg-FTDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring AI-based System Design for Pixel-level Protected Health
  Information Detection in Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Truong, Ivo M. Baltruschat, Mark Klemens, Grit Werner, Matthias Lenga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  De-identification of medical images is a critical step to ensure privacy
during data sharing in research and clinical settings. The initial step in this
process involves detecting Protected Health Information (PHI), which can be
found in image metadata or imprinted within image pixels. Despite the
importance of such systems, there has been limited evaluation of existing
AI-based solutions, creating barriers to the development of reliable and robust
tools. In this study, we present an AI-based pipeline for PHI detection,
comprising three key components: text detection, text extraction, and analysis
of PHI content in medical images. By experimenting with exchanging roles of
vision and language models within the pipeline, we evaluate the performance and
recommend the best setup for the PHI detection task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention
  Mixture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Han, Liang Du, Yiwen Wu, Xiangguo Zhou, Hongwei Du, Weibo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of VLMs often relies on the dynamic high-resolution schema that
adaptively augments the input images to multiple crops, so that the details of
the images can be retained. However, such approaches result in a large number
of redundant visual tokens, thus significantly reducing the efficiency of the
VLMs. To improve the VLMs' efficiency without introducing extra training costs,
many research works are proposed to reduce the visual tokens by filtering the
uninformative visual tokens or aggregating their information. Some approaches
propose to reduce the visual tokens according to the self-attention of VLMs,
which are biased, to result in inaccurate responses. The token reduction
approaches solely rely on visual cues are text-agnostic, and fail to focus on
the areas that are most relevant to the question, especially when the queried
objects are non-salient to the image. In this work, we first conduct
experiments to show that the original text embeddings are aligned with the
visual tokens, without bias on the tailed visual tokens. We then propose a
self-adaptive cross-modality attention mixture mechanism that dynamically
leverages the effectiveness of visual saliency and text-to-image similarity in
the pre-LLM layers to select the visual tokens that are informative. Extensive
experiments demonstrate that the proposed approach achieves state-of-the-art
training-free VLM acceleration performance, especially when the reduction rate
is sufficiently large.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HydraMix: Multi-Image Feature Mixing for Small Data Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Reinders, Frederik Schubert, Bodo Rosenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks requires datasets with a large number of
annotated examples. The collection and annotation of these datasets is not only
extremely expensive but also faces legal and privacy problems. These factors
are a significant limitation for many real-world applications. To address this,
we introduce HydraMix, a novel architecture that generates new image
compositions by mixing multiple different images from the same class. HydraMix
learns the fusion of the content of various images guided by a
segmentation-based mixing mask in feature space and is optimized via a
combination of unsupervised and adversarial training. Our data augmentation
scheme allows the creation of models trained from scratch on very small
datasets. We conduct extensive experiments on ciFAIR-10, STL-10, and
ciFAIR-100. Additionally, we introduce a novel text-image metric to assess the
generality of the augmented datasets. Our results show that HydraMix
outperforms existing state-of-the-art methods for image classification on small
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnyStory: Towards Unified Single and Multiple Subject Personalization in
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie He, Yuxiang Tuo, Binghui Chen, Chongyang Zhong, Yifeng Geng, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large-scale generative models have demonstrated outstanding
text-to-image generation capabilities. However, generating high-fidelity
personalized images with specific subjects still presents challenges,
especially in cases involving multiple subjects. In this paper, we propose
AnyStory, a unified approach for personalized subject generation. AnyStory not
only achieves high-fidelity personalization for single subjects, but also for
multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory
models the subject personalization problem in an "encode-then-route" manner. In
the encoding step, AnyStory utilizes a universal and powerful image encoder,
i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve
high-fidelity encoding of subject features. In the routing step, AnyStory
utilizes a decoupled instance-aware subject router to accurately perceive and
predict the potential location of the corresponding subject in the latent
space, and guide the injection of subject conditions. Detailed experimental
results demonstrate the excellent performance of our method in retaining
subject details, aligning text descriptions, and personalizing for multiple
subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report; Project page:
  https://aigcdesigngroup.github.io/AnyStory/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling
  for Multimodal Emotion Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qize Yang, Detao Bai, Yi-Xing Peng, Xihan Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding emotions accurately is essential for fields like human-computer
interaction. Due to the complexity of emotions and their multi-modal nature
(e.g., emotions are influenced by facial expressions and audio), researchers
have turned to using multi-modal models to understand human emotions rather
than single-modality. However, current video multi-modal large language models
(MLLMs) encounter difficulties in effectively integrating audio and identifying
subtle facial micro-expressions. Furthermore, the lack of detailed emotion
analysis datasets also limits the development of multimodal emotion analysis.
To address these issues, we introduce a self-reviewed dataset and a
human-reviewed dataset, comprising 24,137 coarse-grained samples and 3,500
manually annotated samples with detailed emotion annotations, respectively.
These datasets allow models to learn from diverse scenarios and better
generalize to real-world applications. Moreover, in addition to the audio
modeling, we propose to explicitly integrate facial encoding models into the
existing advanced Video MLLM, enabling the MLLM to effectively unify audio and
the subtle facial cues for emotion understanding. By aligning these features
within a unified space and employing instruction tuning in our proposed
datasets, our Omni-Emotion achieves state-of-the-art performance in both
emotion recognition and reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VanGogh: A Unified Multimodal Diffusion-based Framework for Video
  Colorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixun Fang, Zhiheng Liu, Kai Zhu, Yu Liu, Ka Leong Cheng, Wei Zhai, Yang Cao, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video colorization aims to transform grayscale videos into vivid color
representations while maintaining temporal consistency and structural
integrity. Existing video colorization methods often suffer from color bleeding
and lack comprehensive control, particularly under complex motion or diverse
semantic cues. To this end, we introduce VanGogh, a unified multimodal
diffusion-based framework for video colorization. VanGogh tackles these
challenges using a Dual Qformer to align and fuse features from multiple
modalities, complemented by a depth-guided generation process and an optical
flow loss, which help reduce color overflow. Additionally, a color injection
strategy and luma channel replacement are implemented to improve generalization
and mitigate flickering artifacts. Thanks to this design, users can exercise
both global and local control over the generation process, resulting in
higher-quality colorized videos. Extensive qualitative and quantitative
evaluations, and user studies, demonstrate that VanGogh achieves superior
temporal consistency and color fidelity.Project page:
https://becauseimbatman0.github.io/VanGogh.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of Various SLAM Systems for Mobile Robot in an Indoor
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Filipenko, Ilya Afanasyev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a comparative analysis of a mobile robot trajectories
computed by various ROS-based SLAM systems. For this reason we developed a
prototype of a mobile robot with common sensors: 2D lidar, a monocular and ZED
stereo cameras. Then we conducted experiments in a typical office environment
and collected data from all sensors, running all tested SLAM systems based on
the acquired dataset. We studied the following SLAM systems: (a) 2D
lidar-based: GMapping, Hector SLAM, Cartographer; (b) monocular camera-based:
Large Scale Direct monocular SLAM (LSD SLAM), ORB SLAM, Direct Sparse Odometry
(DSO); and (c) stereo camera-based: ZEDfu, Real-Time Appearance-Based Mapping
(RTAB map), ORB SLAM, Stereo Parallel Tracking and Mapping (S-PTAM). Since all
SLAM methods were tested on the same dataset we compared results for different
SLAM systems with appropriate metrics, demonstrating encouraging results for
lidar-based Cartographer SLAM, Monocular ORB SLAM and Stereo RTAB Map methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Devil is in the Details: Simple Remedies for Image-to-LiDAR
  Representation Learning <span class="chip">ACCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjun Jo, Kwon Byung-Ki, Kim Ji-Yeon, Hawook Jeong, Kyungdon Joo, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR is a crucial sensor in autonomous driving, commonly used alongside
cameras. By exploiting this camera-LiDAR setup and recent advances in image
representation learning, prior studies have shown the promising potential of
image-to-LiDAR distillation. These prior arts focus on the designs of their own
losses to effectively distill the pre-trained 2D image representations into a
3D model. However, the other parts of the designs have been surprisingly
unexplored. We find that fundamental design elements, e.g., the LiDAR
coordinate system, quantization according to the existing input interface, and
data utilization, are more critical than developing loss functions, which have
been overlooked in prior works. In this work, we show that simple fixes to
these designs notably outperform existing methods by 16% in 3D semantic
segmentation on the nuScenes dataset and 13% in 3D object detection on the
KITTI dataset in downstream task performance. We focus on overlooked design
choices along the spatial and temporal axes. Spatially, prior work has used
cylindrical coordinate and voxel sizes without considering their side effects
yielded with a commonly deployed sparse convolution layer input interface,
leading to spatial quantization errors in 3D models. Temporally, existing work
has avoided cumbersome data curation by discarding unsynced data, limiting the
use to only the small portion of data that is temporally synced across sensors.
We analyze these effects and propose simple solutions for each overlooked
aspect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonoSOWA: Scalable monocular 3D Object detector Without human
  Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Skvrna, Lukas Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting the three-dimensional position and orientation of objects using a
single RGB camera is a foundational task in computer vision with many important
applications. Traditionally, 3D object detection methods are trained in a
fully-supervised setup, requiring vast amounts of human annotations, which are
laborious, costly, and do not scale well with the ever-increasing amounts of
data being captured.
  In this paper, we present the first method to train 3D object detectors for
monocular RGB cameras without domain-specific human annotations, thus making
orders of magnitude more data available for training. Thanks to newly proposed
Canonical Object Space, the method can not only exploit data across a variety
of datasets and camera setups to train a single 3D detector, but unlike
previous work it also works out of the box in previously unseen camera setups.
All this is crucial for practical applications, where the data and cameras are
extremely heterogeneous.
  The method is evaluated on two standard autonomous driving datasets, where it
outperforms previous works, which, unlike our method, still rely on 2D human
annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEFOM-Stereo: Depth <span class="highlight-title">Foundation</span> Model Based Stereo Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hualie Jiang, Zhiqiang Lou, Laiyan Ding, Rui Xu, Minglang Tan, Wenjie Jiang, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching is a key technique for metric depth estimation in computer
vision and robotics. Real-world challenges like occlusion and non-texture
hinder accurate disparity estimation from binocular matching cues. Recently,
monocular relative depth estimation has shown remarkable generalization using
vision foundation models. Thus, to facilitate robust stereo matching with
monocular depth cues, we incorporate a robust monocular relative depth model
into the recurrent stereo-matching framework, building a new framework for
depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature
extraction stage, we construct the combined context and matching feature
encoder by integrating features from conventional CNNs and DEFOM. In the update
stage, we use the depth predicted by DEFOM to initialize the recurrent
disparity and introduce a scale update module to refine the disparity at the
correct scale. DEFOM-Stereo is verified to have comparable performance on the
Scene Flow dataset with state-of-the-art (SOTA) methods and notably shows much
stronger zero-shot generalization. Moreover, DEFOM-Stereo achieves SOTA
performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks,
ranking 1st on many metrics. In the joint evaluation under the robust vision
challenge, our model simultaneously outperforms previous models on the
individual benchmarks. Both results demonstrate the outstanding capabilities of
the proposed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Insta360-Research-Team/DEFOM-Stereo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and
  Offloading for Edge Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrui Shi, Yong Zhao, Zeyang Cui, Xiaoming Shen, Minhang Zeng, Xiaojie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection plays a crucial role in smart video analysis, with
applications ranging from autonomous driving and security to smart cities.
However, achieving real-time object detection on edge devices presents
significant challenges due to their limited computational resources and the
high demands of deep neural network (DNN)-based detection models, particularly
when processing high-resolution video. Conventional strategies, such as input
down-sampling and network up-scaling, often compromise detection accuracy for
faster performance or lead to higher inference latency. To address these
issues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven
Partitioning and Edge Offloading framework designed to optimize the
accuracy-latency trade-off in resource-constrained edge environments. Our
approach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that
partitions video frames into non-uniform blocks based on object distribution
and the computational characteristics of DNNs. Furthermore, a parallel edge
offloading scheme is implemented to distribute these blocks across multiple
edge servers for concurrent processing. Experimental evaluations show that
RE-POSE significantly enhances detection accuracy and reduces inference
latency, surpassing existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective
  Scenes <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, Wenzhen Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) often struggle with reconstructing and
rendering highly reflective scenes. Recent advancements have developed various
reflection-aware appearance models to enhance NeRF's capability to render
specular reflections. However, the robust reconstruction of highly reflective
scenes is still hindered by the inherent shape ambiguity on specular surfaces.
Existing methods typically rely on additional geometry priors to regularize the
shape prediction, but this can lead to oversmoothed geometry in complex scenes.
Observing the critical role of surface normals in parameterizing reflections,
we introduce a transmittance-gradient-based normal estimation technique that
remains robust even under ambiguous shape conditions. Furthermore, we propose a
dual activated densities module that effectively bridges the gap between smooth
surface normals and sharp object boundaries. Combined with a reflection-aware
appearance model, our proposed method achieves robust reconstruction and
high-fidelity rendering of scenes featuring both highly specular reflections
and intricate geometric structures. Extensive experiments demonstrate that our
method outperforms existing state-of-the-art methods on various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025, code available at https://github.com/sjj118/Normal-NeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Relation between Optical Aperture and Automotive Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ofer Bar-Shalom, Tzvi Philipp, Eran Kishon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the impact of aperture size and shape on automotive camera systems
for deep-learning-based tasks like traffic sign recognition and light state
detection. A method is proposed to simulate optical effects using the point
spread function (PSF), enhancing realism and reducing the domain gap between
synthetic and real-world images. Computer-generated scenes are refined with
this technique to model optical distortions and improve simulation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double Visual Defense: Adversarial <span class="highlight-title">Pre-train</span>ing and Instruction Tuning
  for Improving Vision-Language Model Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Wang, Cihang Xie, Brian Bartoldson, Bhavya Kailkhura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the robustness of vision-language models against
adversarial visual perturbations and introduces a novel ``double visual
defense" to enhance this robustness. Unlike previous approaches that resort to
lightweight adversarial fine-tuning of a pre-trained CLIP model, we perform
large-scale adversarial vision-language pre-training from scratch using
web-scale data. We then strengthen the defense by incorporating adversarial
visual instruction tuning. The resulting models from each stage, $\Delta$CLIP
and $\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a
new state-of-the-art in adversarial defense for vision-language models. For
example, the adversarial robustness of $\Delta$CLIP surpasses that of the
previous best models on ImageNet-1k by ~20%. %For example, $\Delta$CLIP
surpasses the previous best models on ImageNet-1k by ~20% in terms of
adversarial robustness. Similarly, compared to prior art, $\Delta^2$LLaVA
brings a ~30% robustness improvement to image captioning task and a ~20%
robustness improvement to visual question answering task. Furthermore, our
models exhibit stronger zero-shot recognition capability, fewer hallucinations,
and superior reasoning performance compared to baselines. Our project page is
https://doublevisualdefense.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling up <span class="highlight-title">self-supervised</span> learning for improved surgical <span class="highlight-title">foundation</span>
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim J. M. Jaspers, Ronald L. P. D. de Jong, Yiping Li, Carolus H. J. Kusters, Franciscus H. A. Bakker, Romy C. van Jaarsveld, Gino M. Kuiper, Richard van Hillegersberg, Jelle P. Ruurda, Willem M. Brinkman, Josien P. W. Pluim, Peter H. N. de With, Marcel Breeuwer, Yasmina Al Khalil, Fons van der Sommen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have revolutionized computer vision by achieving vastly
superior performance across diverse tasks through large-scale pretraining on
extensive datasets. However, their application in surgical computer vision has
been limited. This study addresses this gap by introducing SurgeNetXL, a novel
surgical foundation model that sets a new benchmark in surgical computer
vision. Trained on the largest reported surgical dataset to date, comprising
over 4.7 million video frames, SurgeNetXL achieves consistent top-tier
performance across six datasets spanning four surgical procedures and three
tasks, including semantic segmentation, phase recognition, and critical view of
safety (CVS) classification. Compared with the best-performing surgical
foundation models, SurgeNetXL shows mean improvements of 2.4, 9.0, and 12.6
percent for semantic segmentation, phase recognition, and CVS classification,
respectively. Additionally, SurgeNetXL outperforms the best-performing
ImageNet-based variants by 14.4, 4.0, and 1.6 percent in the respective tasks.
In addition to advancing model performance, this study provides key insights
into scaling pretraining datasets, extending training durations, and optimizing
model architectures specifically for surgical computer vision. These findings
pave the way for improved generalizability and robustness in data-scarce
scenarios, offering a comprehensive framework for future research in this
domain. All models and a subset of the SurgeNetXL dataset, including over 2
million video frames, are publicly available at:
https://github.com/TimJaspers0801/SurgeNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwan Heo, Jangyeong Kim, Seongyeong Lee, Jeong A Wi, Junyoung Choi, Sangjun Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The synthesis of high-quality 3D assets from textual or visual inputs has
become a central objective in modern generative modeling. Despite the
proliferation of 3D generation algorithms, they frequently grapple with
challenges such as multi-view inconsistency, slow generation times, low
fidelity, and surface reconstruction problems. While some studies have
addressed some of these issues, a comprehensive solution remains elusive. In
this paper, we introduce \textbf{CaPa}, a carve-and-paint framework that
generates high-fidelity 3D assets efficiently. CaPa employs a two-stage
process, decoupling geometry generation from texture synthesis. Initially, a 3D
latent diffusion model generates geometry guided by multi-view inputs, ensuring
structural consistency across perspectives. Subsequently, leveraging a novel,
model-agnostic Spatially Decoupled Attention, the framework synthesizes
high-resolution textures (up to 4K) for a given geometry. Furthermore, we
propose a 3D-aware occlusion inpainting algorithm that fills untextured
regions, resulting in cohesive results across the entire model. This pipeline
generates high-quality 3D assets in less than 30 seconds, providing
ready-to-use outputs for commercial applications. Experimental results
demonstrate that CaPa excels in both texture fidelity and geometric stability,
establishing a new standard for practical, scalable 3D asset generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://ncsoft.github.io/CaPa/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AugRefer: Advancing 3D Visual Grounding via Cross-Modal Augmentation and
  Spatial Relation-based Referring <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wang, Na Zhao, Zhiyuan Han, Dan Guo, Xun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding (3DVG), which aims to correlate a natural language
description with the target object within a 3D scene, is a significant yet
challenging task. Despite recent advancements in this domain, existing
approaches commonly encounter a shortage: a limited amount and diversity of
text3D pairs available for training. Moreover, they fall short in effectively
leveraging different contextual clues (e.g., rich spatial relations within the
3D visual space) for grounding. To address these limitations, we propose
AugRefer, a novel approach for advancing 3D visual grounding. AugRefer
introduces cross-modal augmentation designed to extensively generate diverse
text-3D pairs by placing objects into 3D scenes and creating accurate and
semantically rich descriptions using foundation models. Notably, the resulting
pairs can be utilized by any existing 3DVG methods for enriching their training
data. Additionally, AugRefer presents a language-spatial adaptive decoder that
effectively adapts the potential referring objects based on the language
description and various 3D spatial relations. Extensive experiments on three
benchmark datasets clearly validate the effectiveness of AugRefer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Models Do Not Understand Negation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many practical vision-language applications require models that understand
negation, e.g., when using natural language to retrieve images which contain
certain objects but not others. Despite advancements in vision-language models
(VLMs) through large-scale training, their ability to comprehend negation
remains underexplored. This study addresses the question: how well do current
VLMs understand negation? We introduce NegBench, a new benchmark designed to
evaluate negation understanding across 18 task variations and 79k examples
spanning image, video, and medical datasets. The benchmark consists of two core
tasks designed to evaluate negation understanding in diverse multimodal
settings: Retrieval with Negation and Multiple Choice Questions with Negated
Captions. Our evaluation reveals that modern VLMs struggle significantly with
negation, often performing at chance level. To address these shortcomings, we
explore a data-centric approach wherein we finetune CLIP models on large-scale
synthetic datasets containing millions of negated captions. We show that this
approach can result in a 10% increase in recall on negated queries and a 40%
boost in accuracy on multiple-choice questions with negated captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://negbench.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Neural Style Transfer for Artistic Image Generation using VGG19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kapil Kashyap, Mehak Garg, Sean Fargose, Sindhu Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Throughout history, humans have created remarkable works of art, but
artificial intelligence has only recently started to make strides in generating
visually compelling art. Breakthroughs in the past few years have focused on
using convolutional neural networks (CNNs) to separate and manipulate the
content and style of images, applying texture synthesis techniques.
Nevertheless, a number of current techniques continue to encounter obstacles,
including lengthy processing times, restricted choices of style images, and the
inability to modify the weight ratio of styles. We proposed a neural style
transfer system that can add various artistic styles to a desired image to
address these constraints allowing flexible adjustments to style weight ratios
and reducing processing time. The system uses the VGG19 model for feature
extraction, ensuring high-quality, flexible stylization without compromising
content integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust and Realistic Human Pose Estimation via WiFi Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Jingcai Guo, Song Guo, Jingren Zhou, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust WiFi-based human pose estimation is a challenging task that bridges
discrete and subtle WiFi signals to human skeletons. This paper revisits this
problem and reveals two critical yet overlooked issues: 1) cross-domain gap,
i.e., due to significant variations between source-target domain pose
distributions; and 2) structural fidelity gap, i.e., predicted skeletal poses
manifest distorted topology, usually with misplaced joints and disproportionate
bone lengths. This paper fills these gaps by reformulating the task into a
novel two-phase framework dubbed DT-Pose: Domain-consistent representation
learning and Topology-constrained Pose decoding. Concretely, we first propose a
temporal-consistent contrastive learning strategy with uniformity
regularization, coupled with self-supervised masking-reconstruction operations,
to enable robust learning of domain-consistent and motion-discriminative
WiFi-specific representations. Beyond this, we introduce a simple yet effective
pose decoder with task prompts, which integrates Graph Convolution Network
(GCN) and Transformer layers to constrain the topology structure of the
generated skeleton by exploring the adjacent-overarching relationships among
human joints. Extensive experiments conducted on various benchmark datasets
highlight the superior performance of our method in tackling these fundamental
challenges in both 2D/3D human pose estimation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PISCO: <span class="highlight-title">Self-Supervised</span> k-Space Regularization for Improved Neural
  Implicit k-Space Representations of Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronika Spieker, Hannah Eichhorn, Wenqi Huang, Jonathan K. Stelter, Tabita Catalan, Rickmer F. Braren, Daniel Rueckert, Francisco Sahli Costabal, Kerstin Hammernik, Dimitrios C. Karampinos, Claudia Prieto, Julia A. Schnabel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit k-space representations (NIK) have shown promising results
for dynamic magnetic resonance imaging (MRI) at high temporal resolutions. Yet,
reducing acquisition time, and thereby available training data, results in
severe performance drops due to overfitting. To address this, we introduce a
novel self-supervised k-space loss function $\mathcal{L}_\mathrm{PISCO}$,
applicable for regularization of NIK-based reconstructions. The proposed loss
function is based on the concept of parallel imaging-inspired self-consistency
(PISCO), enforcing a consistent global k-space neighborhood relationship
without requiring additional data. Quantitative and qualitative evaluations on
static and dynamic MR reconstructions show that integrating PISCO significantly
improves NIK representations. Particularly for high acceleration factors
(R$\geq$54), NIK with PISCO achieves superior spatio-temporal reconstruction
quality compared to state-of-the-art methods. Furthermore, an extensive
analysis of the loss assumptions and stability shows PISCO's potential as
versatile self-supervised k-space loss function for further applications and
architectures. Code is available at:
https://github.com/compai-lab/2025-pisco-spieker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Transmission and Deblurring: A Semantic Communication Approach
  Using Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pujing Yang, Guangyi Zhang, Yunlong Cai, Lei Yu, Guanding Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based joint source-channel coding (JSCC) is emerging as a
promising technology for effective image transmission. However, most existing
approaches focus on transmitting clear images, overlooking real-world
challenges such as motion blur caused by camera shaking or fast-moving objects.
Motion blur often degrades image quality, making transmission and
reconstruction more challenging. Event cameras, which asynchronously record
pixel intensity changes with extremely low latency, have shown great potential
for motion deblurring tasks. However, the efficient transmission of the
abundant data generated by event cameras remains a significant challenge. In
this work, we propose a novel JSCC framework for the joint transmission of
blurry images and events, aimed at achieving high-quality reconstructions under
limited channel bandwidth. This approach is designed as a deblurring
task-oriented JSCC system. Since RGB cameras and event cameras capture the same
scene through different modalities, their outputs contain both shared and
domain-specific information. To avoid repeatedly transmitting the shared
information, we extract and transmit their shared information and
domain-specific information, respectively. At the receiver, the received
signals are processed by a deblurring decoder to generate clear images.
Additionally, we introduce a multi-stage training strategy to train the
proposed model. Simulation results demonstrate that our method significantly
outperforms existing JSCC-based image transmission schemes, addressing motion
blur effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVIA: A Street View Image Anonymization Framework for Self-Driving
  Applications <span class="chip">SC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyu Liu, Xuhong Wang, Cen Chen, Yanhao Wang, Shengyue Yao, Yilun Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been an increasing interest in image
anonymization, particularly focusing on the de-identification of faces and
individuals. However, for self-driving applications, merely de-identifying
faces and individuals might not provide sufficient privacy protection since
street views like vehicles and buildings can still disclose locations,
trajectories, and other sensitive information. Therefore, it remains crucial to
extend anonymization techniques to street view images to fully preserve the
privacy of users, pedestrians, and vehicles. In this paper, we propose a Street
View Image Anonymization (SVIA) framework for self-driving applications. The
SVIA framework consists of three integral components: a semantic segmenter to
segment an input image into functional regions, an inpainter to generate
alternatives to privacy-sensitive regions, and a harmonizer to seamlessly
stitch modified regions to guarantee visual coherence. Compared to existing
methods, SVIA achieves a much better trade-off between image generation quality
and privacy protection, as evidenced by experimental results for five common
metrics on two widely used public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables. Accepted by IEEE ITSC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Segmentation with transformers: An <span class="highlight-title">Overview</span>, Challenges and Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepjyoti Chetia, Debasish Dutta, Sanjib Kr Kalita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation, a key task in computer vision, has traditionally relied
on convolutional neural networks (CNNs), yet these models struggle with
capturing complex spatial dependencies, objects with varying scales, need for
manually crafted architecture components and contextual information. This paper
explores the shortcomings of CNN-based models and the shift towards transformer
architectures -to overcome those limitations. This work reviews
state-of-the-art transformer-based segmentation models, addressing
segmentation-specific challenges and their solutions. The paper discusses
current challenges in transformer-based segmentation and outlines promising
future trends, such as lightweight architectures and enhanced data efficiency.
This survey serves as a guide for understanding the impact of transformers in
advancing segmentation capabilities and overcoming the limitations of
traditional models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification of Traditional Medicinal Plant Leaves Using an effective
  Deep Learning model and Self-Curated <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepjyoti Chetia, Sanjib Kr Kalita, Prof Partha Pratim Baruah, Debasish Dutta, Tanaz Akhter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medicinal plants have been a key component in producing traditional and
modern medicines, especially in the field of Ayurveda, an ancient Indian
medical system. Producing these medicines and collecting and extracting the
right plant is a crucial step due to the visually similar nature of some
plants. The extraction of these plants from nonmedicinal plants requires human
expert intervention. To solve the issue of accurate plant identification and
reduce the need for a human expert in the collection process; employing
computer vision methods will be efficient and beneficial. In this paper, we
have proposed a model that solves such issues. The proposed model is a custom
convolutional neural network (CNN) architecture with 6 convolution layers,
max-pooling layers, and dense layers. The model was tested on three different
datasets named Indian Medicinal Leaves Image Dataset,MED117 Medicinal Plant
Leaf Dataset, and the self-curated dataset by the authors. The proposed model
achieved respective accuracies of 99.5%, 98.4%, and 99.7% using various
optimizers including Adam, RMSprop, and SGD with momentum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strategic Base Representation Learning via Feature Augmentations for
  Few-Shot Class Incremental Learning <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parinita Nema, Vinod K Kurmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot class incremental learning implies the model to learn new classes
while retaining knowledge of previously learned classes with a small number of
training instances. Existing frameworks typically freeze the parameters of the
previously learned classes during the incorporation of new classes. However,
this approach often results in suboptimal class separation of previously
learned classes, leading to overlap between old and new classes. Consequently,
the performance of old classes degrades on new classes. To address these
challenges, we propose a novel feature augmentation driven contrastive learning
framework designed to enhance the separation of previously learned classes to
accommodate new classes. Our approach involves augmenting feature vectors and
assigning proxy labels to these vectors. This strategy expands the feature
space, ensuring seamless integration of new classes within the expanded space.
Additionally, we employ a self-supervised contrastive loss to improve the
separation between previous classes. We validate our framework through
experiments on three FSCIL benchmark datasets: CIFAR100, miniImageNet, and
CUB200. The results demonstrate that our Feature Augmentation driven
Contrastive Learning framework significantly outperforms other approaches,
achieving state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents
  in Augmented Reality Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saptarashmi Bandyopadhyay, Vikas Bahirwani, Lavisha Aggarwal, Bhanu Guda, Lin Li, Andrea Colaco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal AI Agents are AI models that have the capability of interactively
and cooperatively assisting human users to solve day-to-day tasks. Augmented
Reality (AR) head worn devices can uniquely improve the user experience of
solving procedural day-to-day tasks by providing egocentric multimodal (audio
and video) observational capabilities to AI Agents. Such AR capabilities can
help AI Agents see and listen to actions that users take which can relate to
multimodal capabilities of human users. Existing AI Agents, either Large
Language Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive
in nature, which means that models cannot take an action without reading or
listening to the human user's prompts. Proactivity of AI Agents on the other
hand can help the human user detect and correct any mistakes in agent observed
tasks, encourage users when they do tasks correctly or simply engage in
conversation with the user - akin to a human teaching or assisting a user. Our
proposed YET to Intervene (YETI) multimodal agent focuses on the research
question of identifying circumstances that may require the agent to intervene
proactively. This allows the agent to understand when it can intervene in a
conversation with human users that can help the user correct mistakes on tasks,
like cooking, using AR. Our YETI Agent learns scene understanding signals based
on interpretable notions of Structural Similarity (SSIM) on consecutive video
frames. We also define the alignment signal which the AI Agent can learn to
identify if the video frames corresponding to the user's actions on the task
are consistent with expected actions. These signals are used by our AI Agent to
determine when it should proactively intervene. We compare our results on the
instances of proactive intervention in the HoloAssist multimodal benchmark for
an expert agent guiding a user to complete procedural tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Your Dreams A Reality: Decoding the Dreams into a Coherent Video
  Story from fMRI Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Fu, Jianxiong Gao, Baofeng Yang, Jianfeng Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the brave new idea for Multimedia community, and proposes
a novel framework to convert dreams into coherent video narratives using fMRI
data. Essentially, dreams have intrigued humanity for centuries, offering
glimpses into our subconscious minds. Recent advancements in brain imaging,
particularly functional magnetic resonance imaging (fMRI), have provided new
ways to explore the neural basis of dreaming. By combining subjective dream
experiences with objective neurophysiological data, we aim to understand the
visual aspects of dreams and create complete video narratives. Our process
involves three main steps: reconstructing visual perception, decoding dream
imagery, and integrating dream stories. Using innovative techniques in fMRI
analysis and language modeling, we seek to push the boundaries of dream
research and gain deeper insights into visual experiences during sleep. This
technical report introduces a novel approach to visually decoding dreams using
fMRI signals and weaving dream visuals into narratives using language models.
We gather a dataset of dreams along with descriptions to assess the
effectiveness of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UVRM: A Scalable 3D Reconstruction Model from Unposed Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiu-hong Kao, Xiao Li, Jinglu Wang, Chi-Keung Tang, Yu-Wing Tai, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reconstruction Models (LRMs) have recently become a popular method for
creating 3D foundational models. Training 3D reconstruction models with 2D
visual data traditionally requires prior knowledge of camera poses for the
training samples, a process that is both time-consuming and prone to errors.
Consequently, 3D reconstruction training has been confined to either synthetic
3D datasets or small-scale datasets with annotated poses. In this study, we
investigate the feasibility of 3D reconstruction using unposed video data of
various objects. We introduce UVRM, a novel 3D reconstruction model capable of
being trained and evaluated on monocular videos without requiring any
information about the pose. UVRM uses a transformer network to implicitly
aggregate video frames into a pose-invariant latent feature space, which is
then decoded into a tri-plane 3D representation. To obviate the need for
ground-truth pose annotations during training, UVRM employs a combination of
the score distillation sampling (SDS) method and an analysis-by-synthesis
approach, progressively synthesizing pseudo novel-views using a pre-trained
diffusion model. We qualitatively and quantitatively evaluate UVRM's
performance on the G-Objaverse and CO3D datasets without relying on pose
information. Extensive experiments show that UVRM is capable of effectively and
efficiently reconstructing a wide range of 3D objects from unposed videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SE-BSFV: Online Subspace Learning based Shadow Enhancement and
  Background Suppression for ViSAR under Complex Background 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangqu Yan, Chenyang Luo, Yaowen Fu, Wenpeng Zhang, Wei Yang, Ruofeng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video synthetic aperture radar (ViSAR) has attracted substantial attention in
the moving target detection (MTD) field due to its ability to continuously
monitor changes in the target area. In ViSAR, the moving targets' shadows will
not offset and defocus, which is widely used as a feature for MTD. However, the
shadows are difficult to distinguish from the low scattering region in the
background, which will cause more missing and false alarms. Therefore, it is
worth investigating how to enhance the distinction between the shadows and
background. In this study, we proposed the Shadow Enhancement and Background
Suppression for ViSAR (SE-BSFV) algorithm. The SE-BSFV algorithm is based on
the low-rank representation (LRR) theory and adopts online subspace learning
technique to enhance shadows and suppress background for ViSAR images. Firstly,
we use a registration algorithm to register the ViSAR images and utilize
Gaussian mixture distribution (GMD) to model the ViSAR data. Secondly, the
knowledge learned from the previous frames is leveraged to estimate the GMD
parameters of the current frame, and the Expectation-maximization (EM)
algorithm is used to estimate the subspace parameters. Then, the foreground
matrix of the current frame can be obtained. Finally, the alternating direction
method of multipliers (ADMM) is used to eliminate strong scattering objects in
the foreground matrix to obtain the final results. The experimental results
indicate that the SE-BSFV algorithm significantly enhances the shadows'
saliency and greatly improves the detection performance while ensuring
efficiency compared with several other advanced pre-processing algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-CAM: A Simpler Interpre<span class="highlight-title">table</span> Transformer for Fine-Grained
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpita Chowdhury, Dipanjyoti Paul, Zheda Mai, Jianyang Gu, Ziheng Zhang, Kazi Sajeed Mehrab, Elizabeth G. Campolongo, Daniel Rubenstein, Charles V. Stewart, Anuj Karpatne, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple usage of pre-trained Vision Transformers (ViTs) for
fine-grained analysis, aiming to identify and localize the traits that
distinguish visually similar categories, such as different bird species or dog
breeds. Pre-trained ViTs such as DINO have shown remarkable capabilities to
extract localized, informative features. However, using saliency maps like
Grad-CAM can hardly point out the traits: they often locate the whole object by
a blurred, coarse heatmap, not traits. We propose a novel approach Prompt Class
Attention Map (Prompt-CAM) to the rescue. Prompt-CAM learns class-specific
prompts to a pre-trained ViT and uses the corresponding outputs for
classification. To classify an image correctly, the true-class prompt must
attend to the unique image patches not seen in other classes' images, i.e.,
traits. As such, the true class's multi-head attention maps reveal traits and
their locations. Implementation-wise, Prompt-CAM is almost a free lunch by
simply modifying the prediction head of Visual Prompt Tuning (VPT). This makes
Prompt-CAM fairly easy to train and apply, sharply contrasting other
interpretable methods that design specific models and training processes. It is
even simpler than the recently published INterpretable TRansformer (INTR),
whose encoder-decoder architecture prevents it from leveraging pre-trained
ViTs. Extensive empirical studies on a dozen datasets from various domains
(e.g., birds, fishes, insects, fungi, flowers, food, and cars) validate
Prompt-CAM superior interpretation capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention
  for Image Restoration Models Compression <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongheng Zhang, Danfeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based encoder-decoder models have achieved remarkable success in
image-to-image transfer tasks, particularly in image restoration. However,
their high computational complexity-manifested in elevated FLOPs and parameter
counts-limits their application in real-world scenarios. Existing knowledge
distillation methods in image restoration typically employ lightweight student
models that directly mimic the intermediate features and reconstruction results
of the teacher, overlooking the implicit attention relationships between them.
To address this, we propose a Soft Knowledge Distillation (SKD) strategy that
incorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for
compressing image restoration models. This mechanism facilitates interaction
between the student and teacher across both channel and spatial dimensions,
enabling the student to implicitly learn the attention matrices. Additionally,
we employ a Gaussian kernel function to measure the distance between student
and teacher features in kernel space, ensuring stable and efficient feature
learning. To further enhance the quality of reconstructed images, we replace
the commonly used L1 or KL divergence loss with a contrastive learning loss at
the image level. Experiments on three tasks-image deraining, deblurring, and
denoising-demonstrate that our SKD strategy significantly reduces computational
complexity while maintaining strong image restoration capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shape-Based Single Object Classification Using Ensemble Method
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nur Shazwani Kamarudin, Mokhairi Makhtar, Syadiah Nor Wan Shamsuddin, Syed Abdullah Fadzli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, more and more images are available. Annotation and retrieval of the
images pose classification problems, where each class is defined as the group
of database images labelled with a common semantic label. Various systems have
been proposed for content-based retrieval, as well as for image classification
and indexing. In this paper, a hierarchical classification framework has been
proposed for bridging the semantic gap effectively and achieving multi-category
image classification. A well known pre-processing and post-processing method
was used and applied to three problems; image segmentation, object
identification and image classification. The method was applied to classify
single object images from Amazon and Google datasets. The classification was
tested for four different classifiers; BayesNetwork (BN), Random Forest (RF),
Bagging and Vote. The estimated classification accuracies ranged from 20% to
99% (using 10-fold cross validation). The Bagging classifier presents the best
performance, followed by the Random Forest classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-conditioned and Temporal-guided Diffusion Modeling for
  Accelerated Dynamic MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liping Zhang, Iris Yuwen Zhou, Sydney B. Montesi, Li Feng, Fang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To propose a domain-conditioned and temporal-guided diffusion
modeling method, termed dynamic Diffusion Modeling (dDiMo), for accelerated
dynamic MRI reconstruction, enabling diffusion process to characterize
spatiotemporal information for time-resolved multi-coil Cartesian and
non-Cartesian data. Methods: The dDiMo framework integrates temporal
information from time-resolved dimensions, allowing for the concurrent capture
of intra-frame spatial features and inter-frame temporal dynamics in diffusion
modeling. It employs additional spatiotemporal ($x$-$t$) and self-consistent
frequency-temporal ($k$-$t$) priors to guide the diffusion process. This
approach ensures precise temporal alignment and enhances the recovery of fine
image details. To facilitate a smooth diffusion process, the nonlinear
conjugate gradient algorithm is utilized during the reverse diffusion steps.
The proposed model was tested on two types of MRI data: Cartesian-acquired
multi-coil cardiac MRI and Golden-Angle-Radial-acquired multi-coil
free-breathing lung MRI, across various undersampling rates. Results: dDiMo
achieved high-quality reconstructions at various acceleration factors,
demonstrating improved temporal alignment and structural recovery compared to
other competitive reconstruction methods, both qualitatively and
quantitatively. This proposed diffusion framework exhibited robust performance
in handling both Cartesian and non-Cartesian acquisitions, effectively
reconstructing dynamic datasets in cardiac and lung MRI under different imaging
conditions. Conclusion: This study introduces a novel diffusion modeling method
for dynamic MRI reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding the Trigger: Causal Abductive Reasoning on Video Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Minh Le, Vuong Le, Kien Do, Sunil Gupta, Svetha Venkatesh, Truyen Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new problem, Causal Abductive Reasoning on Video
Events (CARVE), which involves identifying causal relationships between events
in a video and generating hypotheses about causal chains that account for the
occurrence of a target event. To facilitate research in this direction, we
create two new benchmark datasets with both synthetic and realistic videos,
accompanied by trigger-target labels generated through a novel counterfactual
synthesis approach. To explore the challenge of solving CARVE, we present a
Causal Event Relation Network (CERN) that examines the relationships between
video events in temporal and semantic spaces to efficiently determine the
root-cause trigger events. Through extensive experiments, we demonstrate the
critical roles of event relational representation learning and interaction
modeling in solving video causal reasoning challenges. The introduction of the
CARVE task, along with the accompanying datasets and the CERN framework, will
advance future research on video causal reasoning and significantly facilitate
various applications, including video surveillance, root-cause analysis and
movie content management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creating Virtual Environments with 3D Gaussian Splatting: A Comparative
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently emerged as an innovative and
efficient 3D representation technique. While its potential for extended reality
(XR) applications is frequently highlighted, its practical effectiveness
remains underexplored. In this work, we examine three distinct 3DGS-based
approaches for virtual environment (VE) creation, leveraging their unique
strengths for efficient and visually compelling scene representation. By
conducting a comparable study, we evaluate the feasibility of 3DGS in creating
immersive VEs, identify its limitations in XR applications, and discuss future
research and development opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE VR 2025 Posters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive
  Vision-Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harrison Fuller, Fernando Gabriela Garcia, Victor Flores
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot learning in medical image classification presents a significant
challenge due to the limited availability of annotated data and the complex
nature of medical imagery. In this work, we propose Adaptive Vision-Language
Fine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework
that leverages the capabilities of Large Vision-Language Models (LVLMs) for
medical image analysis. HiCA introduces a two-stage fine-tuning strategy,
combining domain-specific pretraining and hierarchical contrastive learning to
align visual and textual representations at multiple levels. We evaluate our
approach on two benchmark datasets, Chest X-ray and Breast Ultrasound,
achieving state-of-the-art performance in both few-shot and zero-shot settings.
Further analyses demonstrate the robustness, generalizability, and
interpretability of our method, with substantial improvements in performance
compared to existing baselines. Our work highlights the potential of
hierarchical contrastive strategies in adapting LVLMs to the unique challenges
of medical imaging tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoccerSynth-Detection: A Synthetic <span class="highlight-title">Dataset</span> for Soccer Player Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobin Qin, Calvin Yeung, Rikuhei Umemoto, Keisuke Fujii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In soccer video analysis, player detection is essential for identifying key
events and reconstructing tactical positions. The presence of numerous players
and frequent occlusions, combined with copyright restrictions, severely
restricts the availability of datasets, leaving limited options such as
SoccerNet-Tracking and SportsMOT. These datasets suffer from a lack of
diversity, which hinders algorithms from adapting effectively to varied soccer
video contexts. To address these challenges, we developed
SoccerSynth-Detection, the first synthetic dataset designed for the detection
of synthetic soccer players. It includes a broad range of random lighting and
textures, as well as simulated camera motion blur. We validated its efficacy
using the object detection model (Yolov8n) against real-world datasets
(SoccerNet-Tracking and SportsMoT). In transfer tests, it matched the
performance of real datasets and significantly outperformed them in images with
motion blur; in pre-training tests, it demonstrated its efficacy as a
pre-training dataset, significantly enhancing the algorithm's overall
performance. Our work demonstrates the potential of synthetic datasets to
replace real datasets for algorithm training in the field of soccer video
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-guided Synthetic Geometric Augmentation for Zero-shot 3D
  Understanding <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kohei Torimi, Ryosuke Yamada, Daichi Otsuka, Kensho Hara, Yuki M. Asano, Hirokatsu Kataoka, Yoshimitsu Aoki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot recognition models require extensive training data for
generalization. However, in zero-shot 3D classification, collecting 3D data and
captions is costly and laborintensive, posing a significant barrier compared to
2D vision. Recent advances in generative models have achieved unprecedented
realism in synthetic data production, and recent research shows the potential
for using generated data as training data. Here, naturally raising the
question: Can synthetic 3D data generated by generative models be used as
expanding limited 3D datasets? In response, we present a synthetic 3D dataset
expansion method, Textguided Geometric Augmentation (TeGA). TeGA is tailored
for language-image-3D pretraining, which achieves SoTA in zero-shot 3D
classification, and uses a generative textto-3D model to enhance and extend
limited 3D datasets. Specifically, we automatically generate text-guided
synthetic 3D data and introduce a consistency filtering strategy to discard
noisy samples where semantics and geometric shapes do not match with text. In
the experiment to double the original dataset size using TeGA, our approach
demonstrates improvements over the baselines, achieving zeroshot performance
gains of 3.0% on Objaverse-LVIS, 4.6% on ScanObjectNN, and 8.7% on ModelNet40.
These results demonstrate that TeGA effectively bridges the 3D data gap,
enabling robust zero-shot 3D classification even with limited real training
data and paving the way for zero-shot 3D vision application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, this paper is submitted to CVPR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias for Action: Video Implicit Neural Representations with Bias
  Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alper Kayabasi, Anil Kumar Vadathya, Guha Balakrishnan, Vishwanath Saragadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new continuous video modeling framework based on implicit neural
representations (INRs) called ActINR. At the core of our approach is the
observation that INRs can be considered as a learnable dictionary, with the
shapes of the basis functions governed by the weights of the INR, and their
locations governed by the biases. Given compact non-linear activation
functions, we hypothesize that an INR's biases are suitable to capture motion
across images, and facilitate compact representations for video sequences.
Using these observations, we design ActINR to share INR weights across frames
of a video sequence, while using unique biases for each frame. We further model
the biases as the output of a separate INR conditioned on time index to promote
smoothness. By training the video INR and this bias INR together, we
demonstrate unique capabilities, including $10\times$ video slow motion,
$4\times$ spatial super resolution along with $2\times$ slow motion, denoising,
and video inpainting. ActINR performs remarkably well across numerous video
processing tasks (often achieving more than 6dB improvement), setting a new
standard for continuous modeling of videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Distillation for Image Restoration : Simultaneous Learning
  from Degraded and Clean Images <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongheng Zhang, Danfeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model compression through knowledge distillation has seen extensive
application in classification and segmentation tasks. However, its potential in
image-to-image translation, particularly in image restoration, remains
underexplored. To address this gap, we propose a Simultaneous Learning
Knowledge Distillation (SLKD) framework tailored for model compression in image
restoration tasks. SLKD employs a dual-teacher, single-student architecture
with two distinct learning strategies: Degradation Removal Learning (DRL) and
Image Reconstruction Learning (IRL), simultaneously. In DRL, the student
encoder learns from Teacher A to focus on removing degradation factors, guided
by a novel BRISQUE extractor. In IRL, the student decoder learns from Teacher B
to reconstruct clean images, with the assistance of a proposed PIQE extractor.
These strategies enable the student to learn from degraded and clean images
simultaneously, ensuring high-quality compression of image restoration models.
Experimental results across five datasets and three tasks demonstrate that SLKD
achieves substantial reductions in FLOPs and parameters, exceeding 80\%, while
maintaining strong image restoration performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Open-Vocabulary Models Ready for Detection of MEP Elements on
  Construction Sites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdalwhab Abdalwhab, Ali Imran, Sina Heydarian, Ivanka Iordanova, David St-Onge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The construction industry has long explored robotics and computer vision, yet
their deployment on construction sites remains very limited. These technologies
have the potential to revolutionize traditional workflows by enhancing
accuracy, efficiency, and safety in construction management. Ground robots
equipped with advanced vision systems could automate tasks such as monitoring
mechanical, electrical, and plumbing (MEP) systems. The present research
evaluates the applicability of open-vocabulary vision-language models compared
to fine-tuned, lightweight, closed-set object detectors for detecting MEP
components using a mobile ground robotic platform. A dataset collected with
cameras mounted on a ground robot was manually annotated and analyzed to
compare model performance. The results demonstrate that, despite the
versatility of vision-language models, fine-tuned lightweight models still
largely outperform them in specialized environments and for domain-specific
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpticFusion: Multi-Modal Neural Implicit 3D Reconstruction of
  Microstructures by Fusing White Light Interferometry and Optical Microscopy <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Chen, Yijin Li, Guofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  White Light Interferometry (WLI) is a precise optical tool for measuring the
3D topography of microstructures. However, conventional WLI cannot capture the
natural color of a sample's surface, which is essential for many microscale
research applications that require both 3D geometry and color information.
Previous methods have attempted to overcome this limitation by modifying WLI
hardware and analysis software, but these solutions are often costly. In this
work, we address this challenge from a computer vision multi-modal
reconstruction perspective for the first time. We introduce OpticFusion, a
novel approach that uses an additional digital optical microscope (OM) to
achieve 3D reconstruction with natural color textures using multi-view WLI and
OM images. Our method employs a two-step data association process to obtain the
poses of WLI and OM data. By leveraging the neural implicit representation, we
fuse multi-modal data and apply color decomposition technology to extract the
sample's natural color. Tested on our multi-modal dataset of various microscale
samples, OpticFusion achieves detailed 3D reconstructions with color textures.
Our method provides an effective tool for practical applications across
numerous microscale research fields. The source code and our real-world dataset
are available at https://github.com/zju3dv/OpticFusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Scale-aware Representations for improved
  Concept-Representation Alignment in ViTs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanchit Sinha, Guangzhi Xiong, Aidong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) are increasingly being adopted in various
sensitive vision applications - like medical diagnosis, facial recognition,
etc. To improve the interpretability of such models, many approaches attempt to
forward-align them with carefully annotated abstract, human-understandable
semantic entities - concepts. Concepts provide global rationales to the model
predictions and can be quickly understood/intervened on by domain experts. Most
current research focuses on designing model-agnostic, plug-and-play generic
concept-based explainability modules that do not incorporate the inner workings
of foundation models (e.g., inductive biases, scale invariance, etc.) during
training. To alleviate this issue for ViTs, in this paper, we propose a novel
Concept Representation Alignment Module (CRAM) which learns both scale and
position-aware representations from multi-scale feature pyramids and patch
representations respectively. CRAM further aligns these representations with
concept annotations through an attention matrix. The proposed CRAM module
improves the predictive performance of ViT architectures and also provides
accurate and robust concept explanations as demonstrated on five datasets -
including three widely used benchmarks (CUB, Pascal APY, Concept-MNIST) and 2
real-world datasets (AWA2, KITS).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Law-Based Transformation (ALT): A Lightweight Feature
  Representation for Time Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcell T. Kurbucz, Balázs Hajós, Balázs P. Halmos, Vince Á. Molnár, Antal Jakovác
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series classification (TSC) is fundamental in numerous domains,
including finance, healthcare, and environmental monitoring. However,
traditional TSC methods often struggle with the inherent complexity and
variability of time series data. Building on our previous work with the linear
law-based transformation (LLT) - which improved classification accuracy by
transforming the feature space based on key data patterns - we introduce
adaptive law-based transformation (ALT). ALT enhances LLT by incorporating
variable-length shifted time windows, enabling it to capture distinguishing
patterns of various lengths and thereby handle complex time series more
effectively. By mapping features into a linearly separable space, ALT provides
a fast, robust, and transparent solution that achieves state-of-the-art
performance with only a few hyperparameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surgical Visual Understanding (SurgVU) <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneeq Zia, Max Berniker, Rogerio Nespolo, Conor Perreault, Ziheng Wang, Benjamin Mueller, Ryan Schmidt, Kiran Bhattacharyya, Xi Liu, Anthony Jarc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to recent advances in machine learning and the ability to harvest large
amounts of data during robotic-assisted surgeries, surgical data science is
ripe for foundational work. We present a large dataset of surgical videos and
their accompanying labels for this purpose. We describe how the data was
collected and some of its unique attributes. Multiple example problems are
outlined. Although the dataset was curated for a particular set of scientific
challenges (in an accompanying paper), it is general enough to be used for a
broad range machine learning questions. Our hope is that this dataset exposes
the larger machine learning community to the challenging problems within
surgical data science, and becomes a touchstone for future research. The videos
are available at
https://storage.googleapis.com/isi-surgvu/surgvu24_videos_only.zip, the labels
at https://storage.googleapis.com/isi-surgvu/surgvu24_labels_updated_v2.zip,
and a validation set for tool detection problem at
https://storage.googleapis.com/isi-surgvu/cat1_test_set_public.zip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FutureDepth: Learning to Predict the Future Improves Video Depth
  Estimation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajeev Yasarla, Manish Kumar Singh, Hong Cai, Yunxiao Shi, Jisoo Jeong, Yinhao Zhu, Shizhong Han, Risheek Garrepalli, Fatih Porikli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel video depth estimation approach,
FutureDepth, which enables the model to implicitly leverage multi-frame and
motion cues to improve depth estimation by making it learn to predict the
future at training. More specifically, we propose a future prediction network,
F-Net, which takes the features of multiple consecutive frames and is trained
to predict multi-frame features one time step ahead iteratively. In this way,
F-Net learns the underlying motion and correspondence information, and we
incorporate its features into the depth decoding process. Additionally, to
enrich the learning of multiframe correspondence cues, we further leverage a
reconstruction network, R-Net, which is trained via adaptively masked
auto-encoding of multiframe feature volumes. At inference time, both F-Net and
R-Net are used to produce queries to work with the depth decoder, as well as a
final refinement network. Through extensive experiments on several benchmarks,
i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and
open-domain scenarios, we show that FutureDepth significantly improves upon
baseline models, outperforms existing video depth estimation methods, and sets
new state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more
efficient than existing SOTA video depth estimation models and has similar
latencies when comparing to monocular models
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAMo: Leveraging Memory and Attention for Monocular Video Depth
  Estimation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajeev Yasarla, Hong Cai, Jisoo Jeong, Yunxiao Shi, Risheek Garrepalli, Fatih Porikli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MAMo, a novel memory and attention frame-work for monocular video
depth estimation. MAMo can augment and improve any single-image depth
estimation networks into video depth estimation models, enabling them to take
advantage of the temporal information to predict more accurate depth. In MAMo,
we augment model with memory which aids the depth prediction as the model
streams through the video. Specifically, the memory stores learned visual and
displacement tokens of the previous time instances. This allows the depth
network to cross-reference relevant features from the past when predicting
depth on the current frame. We introduce a novel scheme to continuously update
the memory, optimizing it to keep tokens that correspond with both the past and
the present visual information. We adopt attention-based approach to process
memory features where we first learn the spatio-temporal relation among the
resultant visual and displacement memory tokens using self-attention module.
Further, the output features of self-attention are aggregated with the current
visual features through cross-attention. The cross-attended features are
finally given to a decoder to predict depth on the current frame. Through
extensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and
DDAD, we show that MAMo consistently improves monocular depth estimation
networks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo video
depth estimation provides higher accuracy with lower latency, when omparing to
SOTA cost-volume-based video depth models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vulnerability-Aware Spatio-Temporal Learning for Generalizable and
  Interpre<span class="highlight-title">table</span> Deepfake Video Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dat Nguyen, Marcella Astrid, Anis Kacem, Enjie Ghorbel, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting deepfake videos is highly challenging due to the complex
intertwined spatial and temporal artifacts in forged sequences. Most recent
approaches rely on binary classifiers trained on both real and fake data.
However, such methods may struggle to focus on important artifacts, which can
hinder their generalization capability. Additionally, these models often lack
interpretability, making it difficult to understand how predictions are made.
To address these issues, we propose FakeSTormer, offering two key
contributions. First, we introduce a multi-task learning framework with
additional spatial and temporal branches that enable the model to focus on
subtle spatio-temporal artifacts. These branches also provide interpretability
by highlighting video regions that may contain artifacts. Second, we propose a
video-level data synthesis algorithm that generates pseudo-fake videos with
subtle artifacts, providing the model with high-quality samples and ground
truth data for our spatial and temporal branches. Extensive experiments on
several challenging benchmarks demonstrate the competitiveness of our approach
compared to recent state-of-the-art methods. The code is available at
https://github.com/10Ring/FakeSTormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Super-class guided Transformer for Zero-Shot Attribute Classification <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehyung Kim, Chanhyeong Yang, Jihwan Park, Taehoon Song, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attribute classification is crucial for identifying specific characteristics
within image regions. Vision-Language Models (VLMs) have been effective in
zero-shot tasks by leveraging their general knowledge from large-scale
datasets. Recent studies demonstrate that transformer-based models with
class-wise queries can effectively address zero-shot multi-label
classification. However, poor utilization of the relationship between seen and
unseen attributes makes the model lack generalizability. Additionally,
attribute classification generally involves many attributes, making maintaining
the model's scalability difficult. To address these issues, we propose
Super-class guided transFormer (SugaFormer), a novel framework that leverages
super-classes to enhance scalability and generalizability for zero-shot
attribute classification. SugaFormer employs Super-class Query Initialization
(SQI) to reduce the number of queries, utilizing common semantic information
from super-classes, and incorporates Multi-context Decoding (MD) to handle
diverse visual cues. To strengthen generalizability, we introduce two knowledge
transfer strategies that utilize VLMs. During training, Super-class guided
Consistency Regularization (SCR) aligns model's features with VLMs using
super-class guided prompts, and during inference, Zero-shot Retrieval-based
Score Enhancement (ZRSE) refines predictions for unseen attributes. Extensive
experiments demonstrate that SugaFormer achieves state-of-the-art performance
across three widely-used attribute classification benchmarks under zero-shot,
and cross-dataset transfer settings. Our code is available at
https://github.com/mlvlab/SugaFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIS-MAE: An Efficient <span class="highlight-title">Self-supervised</span> Learning Approach on Medical Image
  Segmentation and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelong Liu, Andrew Tieu, Nikhil Patel, Georgios Soultanidis, Louisa Deyer, Ying Wang, Sean Huver, Alexander Zhou, Yunhao Mei, Zahi A. Fayad, Timothy Deyer, Xueyan Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has the potential to revolutionize diagnosis and
segmentation in medical imaging. However, development and clinical
implementation face multiple challenges including limited data availability,
lack of generalizability, and the necessity to incorporate multi-modal data
effectively. A foundation model, which is a large-scale pre-trained AI model,
offers a versatile base that can be adapted to a variety of specific tasks and
contexts. Here, we present VIsualization and Segmentation Masked AutoEncoder
(VIS-MAE), novel model weights specifically designed for medical imaging.
Specifically, VIS-MAE is trained on a dataset of 2.5 million unlabeled images
from various modalities (CT, MR, PET,X-rays, and ultrasound), using
self-supervised learning techniques. It is then adapted to classification and
segmentation tasks using explicit labels. VIS-MAE has high label efficiency,
outperforming several benchmark models in both in-domain and out-of-domain
applications. In addition, VIS-MAE has improved label efficiency as it can
achieve similar performance to other models with a reduced amount of labeled
training data (50% or 80%) compared to other pre-trained weights. VIS-MAE
represents a significant advancement in medical imaging AI, offering a
generalizable and robust solution for improving segmentation and classification
tasks while reducing the data annotation workload. The source code of this work
is available at https://github.com/lzl199704/VIS-MAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study on Multi-task Uncertainty Quantification in Semantic
  Segmentation and Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Landgraf, Markus Hillemann, Theodor Kapler, Markus Ulrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks excel in perception tasks such as semantic segmentation
and monocular depth estimation, making them indispensable in safety-critical
applications like autonomous driving and industrial inspection. However, they
often suffer from overconfidence and poor explainability, especially for
out-of-domain data. While uncertainty quantification has emerged as a promising
solution to these challenges, multi-task settings have yet to be explored. In
an effort to shed light on this, we evaluate Monte Carlo Dropout, Deep
Sub-Ensembles, and Deep Ensembles for joint semantic segmentation and monocular
depth estimation. Thereby, we reveal that Deep Ensembles stand out as the
preferred choice, particularly in out-of-domain scenarios, and show the
potential benefit of multi-task learning with regard to the uncertainty quality
in comparison to solving both tasks separately. Additionally, we highlight the
impact of employing different uncertainty thresholds to classify pixels as
certain or uncertain, with the median uncertainty emerging as a robust default.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is an extended version of a previously published
  conference paper and is currently in review for a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of <span class="highlight-title">Foundation</span> Models in Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) are large-scale deep learning models trained on
massive datasets, often using self-supervised learning techniques. These models
serve as a versatile base for a wide range of downstream tasks, including those
in medicine and healthcare. FMs have demonstrated remarkable success across
multiple healthcare domains. However, existing surveys in this field do not
comprehensively cover all areas where FMs have made significant strides. In
this survey, we present a comprehensive review of FMs in medicine, focusing on
their evolution, learning strategies, flagship models, applications, and
associated challenges. We examine how prominent FMs, such as the BERT and GPT
families, are transforming various aspects of healthcare, including clinical
large language models, medical image analysis, and omics research.
Additionally, we provide a detailed taxonomy of FM-enabled healthcare
applications, spanning clinical natural language processing, medical computer
vision, graph learning, and other biology- and omics- related tasks. Despite
the transformative potentials of FMs, they also pose unique challenges. This
survey delves into these challenges and highlights open research questions and
lessons learned to guide researchers and practitioners. Our goal is to provide
valuable insights into the capabilities of FMs in health, facilitating
responsible deployment and mitigating associated risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Zero-Shot Object-Level Change Detection by Incorporating
  Visual Correspondence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Huy Nguyen, Pooyan Rahmanzadehgervi, Long Mai, Anh Totti Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting object-level changes between two images across possibly different
views is a core task in many applications that involve visual inspection or
camera surveillance. Existing change-detection approaches suffer from three
major limitations: (1) lack of evaluation on image pairs that contain no
changes, leading to unreported false positive rates; (2) lack of
correspondences (i.e., localizing the regions before and after a change); and
(3) poor zero-shot generalization across different domains. To address these
issues, we introduce a novel method that leverages change correspondences (a)
during training to improve change detection accuracy, and (b) at test time, to
minimize false positives. That is, we harness the supervision labels of where
an object is added or removed to supervise change detectors, improving their
accuracy over previous work by a large margin. Our work is also the first to
predict correspondences between pairs of detected changes using estimated
homography and the Hungarian algorithm. Our model demonstrates superior
performance over existing methods, achieving state-of-the-art results in change
detection and change correspondence accuracy across both in-distribution and
zero-shot benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MECD+: Unlocking Event-Level Causal <span class="highlight-title">Graph</span> Discovery for Video Reasoning <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tieyuan Chen, Huabin Liu, Yi Wang, Yihang Chen, Tianyao He, Chaofan Gan, Huanyu He, Weiyao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video causal reasoning aims to achieve a high-level understanding of videos
from a causal perspective. However, it exhibits limitations in its scope,
primarily executed in a question-answering paradigm and focusing on brief video
segments containing isolated events and basic causal relations, lacking
comprehensive and structured causality analysis for videos with multiple
interconnected events. To fill this gap, we introduce a new task and dataset,
Multi-Event Causal Discovery (MECD). It aims to uncover the causal relations
between events distributed chronologically across long videos. Given visual
segments and textual descriptions of events, MECD identifies the causal
associations between these events to derive a comprehensive and structured
event-level video causal graph explaining why and how the result event
occurred. To address the challenges of MECD, we devise a novel framework
inspired by the Granger Causality method, incorporating an efficient mask-based
event prediction model to perform an Event Granger Test. It estimates causality
by comparing the predicted result event when premise events are masked versus
unmasked. Furthermore, we integrate causal inference techniques such as
front-door adjustment and counterfactual inference to mitigate challenges in
MECD like causality confounding and illusory causality. Additionally, context
chain reasoning is introduced to conduct more robust and generalized reasoning.
Experiments validate the effectiveness of our framework in reasoning complete
causal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,
respectively. Further experiments demonstrate that causal relation graphs can
also contribute to downstream video understanding tasks such as video question
answering and video event prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TPAMI Submission. continuous work of arXiv:2409.17647 (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Multimodal Large Language Models (MLLMs) have typically focused on
integrating visual and textual modalities, with less emphasis placed on the
role of speech in enhancing interaction. However, speech plays a crucial role
in multimodal dialogue systems, and implementing high-performance in both
vision and speech tasks remains a significant challenge due to the fundamental
modality differences. In this paper, we propose a carefully designed
multi-stage training methodology that progressively trains LLM to understand
both visual and speech information, ultimately enabling fluent vision and
speech interaction. Our approach not only preserves strong vision-language
capacity, but also enables efficient speech-to-speech dialogue capabilities
without separate ASR and TTS modules, significantly accelerating multimodal
end-to-end response speed. By comparing our method against state-of-the-art
counterparts across benchmarks for image, video, and speech tasks, we
demonstrate that our model is equipped with both strong visual and speech
capabilities, making near real-time vision and speech interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/VITA-MLLM/VITA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian
  Neural Networks <span class="chip">AAAI'2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20891v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20891v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bao Gia Doan, Afshar Shamsi, Xiao-Yu Guo, Arash Mohammadi, Hamid Alinejad-Rokny, Dino Sejdinovic, Damien Teney, Damith C. Ranasinghe, Ehsan Abbasnejad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational complexity of Bayesian learning is impeding its adoption in
practical, large-scale tasks. Despite demonstrations of significant merits such
as improved robustness and resilience to unseen or out-of-distribution inputs
over their non- Bayesian counterparts, their practical use has faded to near
insignificance. In this study, we introduce an innovative framework to mitigate
the computational burden of Bayesian neural networks (BNNs). Our approach
follows the principle of Bayesian techniques based on deep ensembles, but
significantly reduces their cost via multiple low-rank perturbations of
parameters arising from a pre-trained neural network. Both vanilla version of
ensembles as well as more sophisticated schemes such as Bayesian learning with
Stein Variational Gradient Descent (SVGD), previously deemed impractical for
large models, can be seamlessly implemented within the proposed framework,
called Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a
dramatic reduction in the number of trainable parameters required to
approximate a Bayesian posterior; and ii) it not only maintains, but in some
instances, surpasses the performance of conventional Bayesian learning methods
and non-Bayesian baselines. Our results with large-scale tasks such as
ImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the
effectiveness and versatility of Bella in building highly scalable and
practical Bayesian deep models for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in AAAI'2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Space Characterization of Autoencoder Variants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anika Shrivastava, Renu Rameshan, Samar Agnihotri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the latent spaces learned by deep learning models is crucial in
exploring how they represent and generate complex data. Autoencoders (AEs) have
played a key role in the area of representation learning, with numerous
regularization techniques and training principles developed not only to enhance
their ability to learn compact and robust representations, but also to reveal
how different architectures influence the structure and smoothness of the
lower-dimensional non-linear manifold. We strive to characterize the structure
of the latent spaces learned by different autoencoders including convolutional
autoencoders (CAEs), denoising autoencoders (DAEs), and variational
autoencoders (VAEs) and how they change with the perturbations in the input. By
characterizing the matrix manifolds corresponding to the latent spaces, we
provide an explanation for the well-known observation that the latent spaces of
CAE and DAE form non-smooth manifolds, while that of VAE forms a smooth
manifold. We also map the points of the matrix manifold to a Hilbert space
using distance preserving transforms and provide an alternate view in terms of
the subspaces generated in the Hilbert space as a function of the distortion in
the input. The results show that the latent manifolds of CAE and DAE are
stratified with each stratum being a smooth product manifold, while the
manifold of VAE is a smooth product manifold of two symmetric positive definite
matrices and a symmetric positive semi-definite matrix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, and 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STROOBnet Optimization via GPU-Accelerated Proximal Recurrence
  Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14388v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14388v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ted Edward Holmberg, Mahdi Abdelguerfi, Elias Ioup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal networks' observational capabilities are crucial for accurate
data gathering and informed decisions across multiple sectors. This study
focuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network
(STROOBnet), linking observational nodes (e.g., surveillance cameras) to events
within defined geographical regions, enabling efficient monitoring. Using data
from Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New
Orleans, where RTCC combats rising crime amidst reduced police presence, we
address the network's initial observational imbalances. Aiming for uniform
observational efficacy, we propose the Proximal Recurrence approach. It
outperformed traditional clustering methods like k-means and DBSCAN by offering
holistic event frequency and spatial consideration, enhancing observational
coverage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures, 2023 IEEE International Conference on Big Data
  (BigData)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Few-Shot Image Classification through Learnable Multi-Scale
  Embedding and Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Askari, Amirreza Fateh, Mohammad Reza Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of few-shot classification, the goal is to train a classifier
using a limited number of samples while maintaining satisfactory performance.
However, traditional metric-based methods exhibit certain limitations in
achieving this objective. These methods typically rely on a single distance
value between the query feature and support feature, thereby overlooking the
contribution of shallow features. To overcome this challenge, we propose a
novel approach in this paper. Our approach involves utilizing a multi-output
embedding network that maps samples into distinct feature spaces. The proposed
method extracts feature vectors at different stages, enabling the model to
capture both global and abstract features. By utilizing these diverse feature
spaces, our model enhances its performance. Moreover, employing a
self-attention mechanism improves the refinement of features at each stage,
leading to even more robust representations and improved overall performance.
Furthermore, assigning learnable weights to each stage significantly improved
performance and results. We conducted comprehensive evaluations on the
MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way
5-shot scenarios. Additionally, we performed cross-domain tasks across eight
benchmark datasets, achieving high accuracy in the testing domains. These
evaluations demonstrate the efficacy of our proposed method in comparison to
state-of-the-art approaches. https://github.com/FatemehAskari/MSENet
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems
  using Disparity Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24031v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24031v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Larey, Eyal Rond, Omer Achrack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition technologies are increasingly used in various applications,
yet they are vulnerable to face spoofing attacks. These spoofing attacks often
involve unique 3D structures, such as printed papers or mobile device screens.
Although stereo-depth cameras can detect such attacks effectively, their
high-cost limits their widespread adoption. Conversely, two-sensor systems
without extrinsic calibration offer a cost-effective alternative but are unable
to calculate depth using stereo techniques. In this work, we propose a method
to overcome this challenge by leveraging facial attributes to derive disparity
information and estimate relative depth for anti-spoofing purposes, using
non-calibrated systems. We introduce a multi-modal anti-spoofing model, coined
Disparity Model, that incorporates created disparity maps as a third modality
alongside the two original sensor modalities. We demonstrate the effectiveness
of the Disparity Model in countering various spoof attacks using a
comprehensive dataset collected from the Intel RealSense ID Solution F455. Our
method outperformed existing methods in the literature, achieving an Equal
Error Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False
Positive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the
errors of the best comparison method, respectively. Additionally, we introduce
a model ensemble that addresses 3D spoof attacks as well, achieving an EER of
2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a
state-of-the-art solution for the challenging task of anti-spoofing in
non-calibrated systems that lack depth information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating alignment between humans and neural network representations
  in image-based learning tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09377v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09377v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Demircan, Tankred Saanum, Leonardo Pettini, Marcel Binz, Blazej M Baczkowski, Christian F Doeller, Mona M Garvert, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans represent scenes and objects in rich feature spaces, carrying
information that allows us to generalise about category memberships and
abstract functions with few examples. What determines whether a neural network
model generalises like a human? We tested how well the representations of $86$
pretrained neural network models mapped to human learning trajectories across
two tasks where humans had to learn continuous relationships and categories of
natural images. In these tasks, both human participants and neural networks
successfully identified the relevant stimulus features within a few trials,
demonstrating effective generalisation. We found that while training dataset
size was a core determinant of alignment with human choices, contrastive
training with multi-modal data (text and imagery) was a common feature of
currently publicly available models that predicted human generalisation.
Intrinsic dimensionality of representations had different effects on alignment
for different model types. Lastly, we tested three sets of human-aligned
representations and found no consistent improvements in predictive accuracy
compared to the baselines. In conclusion, pretrained neural networks can serve
to extract representations for cognitive models, as they appear to capture some
fundamental aspects of cognition that are transferable across tasks. Both our
paradigms and modelling approach offer a novel way to quantify alignment
between neural networks and humans and extend cognitive science into more
naturalistic domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction-Guided Fusion of Multi-Layer Visual Features in Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Li, Yi Zheng, Haotian Chen, Xiaolei Chen, Yuxuan Liang, Chenghang Lai, Bin Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have achieved significant success in
multimodal tasks by combining pre-trained vision encoders and large language
models. However, current LVLMs mainly rely on features from the final layers of
the vision encoder, neglecting complementary information in shallower layers.
While recent methods have explored multi-layer features, they are often
task-agnostic. We investigate the contributions of visual features from
different encoder layers across 18 benchmarks and 6 task categories. Our
results show that multi-layer features provide complementary strengths with
varying task dependencies, and uniform fusion performs suboptimally. Based on
these findings, we propose an instruction-guided vision aggregator that
dynamically integrates multi-layer features based on textual instructions,
without increasing the number of visual tokens. Extensive evaluations show
superior performance, and analysis reveals the dominance of mid-to-high-level
features in semantic tasks and the critical role of low-level features in
fine-grained perception. This work provides valuable insights into the adaptive
use of hierarchical visual features in LVLMs, advancing more flexible
multimodal systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models in Vision: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04747v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04747v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models represent a recent emerging topic in computer
vision, demonstrating remarkable results in the area of generative modeling. A
diffusion model is a deep generative model that is based on two stages, a
forward diffusion stage and a reverse diffusion stage. In the forward diffusion
stage, the input data is gradually perturbed over several steps by adding
Gaussian noise. In the reverse stage, a model is tasked at recovering the
original input data by learning to gradually reverse the diffusion process,
step by step. Diffusion models are widely appreciated for the quality and
diversity of the generated samples, despite their known computational burdens,
i.e. low speeds due to the high number of steps involved during sampling. In
this survey, we provide a comprehensive review of articles on denoising
diffusion models applied in vision, comprising both theoretical and practical
contributions in the field. First, we identify and present three generic
diffusion modeling frameworks, which are based on denoising diffusion
probabilistic models, noise conditioned score networks, and stochastic
differential equations. We further discuss the relations between diffusion
models and other deep generative models, including variational auto-encoders,
generative adversarial networks, energy-based models, autoregressive models and
normalizing flows. Then, we introduce a multi-perspective categorization of
diffusion models applied in computer vision. Finally, we illustrate the current
limitations of diffusion models and envision some interesting directions for
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence. 25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DriveLM: Driving with <span class="highlight-title">Graph</span> Visual Question Answering <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14150v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14150v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, Hongyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how vision-language models (VLMs) trained on web-scale data can be
integrated into end-to-end driving systems to boost generalization and enable
interactivity with human users. While recent approaches adapt VLMs to driving
via single-round visual question answering (VQA), human drivers reason about
decisions in multiple steps. Starting from the localization of key objects,
humans estimate object interactions before taking actions. The key insight is
that with our proposed task, Graph VQA, where we model graph-structured
reasoning through perception, prediction and planning question-answer pairs, we
obtain a suitable proxy task to mimic the human reasoning process. We
instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose
a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA
and end-to-end driving. The experiments demonstrate that Graph VQA provides a
simple, principled framework for reasoning about a driving scene, and
DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent
baseline performs end-to-end autonomous driving competitively in comparison to
state-of-the-art driving-specific architectures. Notably, its benefits are
pronounced when it is evaluated zero-shot on unseen objects or sensor
configurations. We hope this work can be the starting point to shed new light
on how to apply VLMs for autonomous driving. To facilitate future research, all
code, data, and models are available to the public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024 as Oral paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards an End-to-End (E2E) Adversarial Learning and Application in the
  Physical World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dudi Biton, Jacob Shams, Satoru Koda, Asaf Shabtai, Yuval Elovici, Ben Nassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traditional learning process of patch-based adversarial attacks,
conducted in the digital domain and then applied in the physical domain (e.g.,
via printed stickers), may suffer from reduced performance due to adversarial
patches' limited transferability from the digital domain to the physical
domain. Given that previous studies have considered using projectors to apply
adversarial attacks, we raise the following question: can adversarial learning
(i.e., patch generation) be performed entirely in the physical domain with a
projector? In this work, we propose the Physical-domain Adversarial Patch
Learning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework
that converts adversarial learning from the digital domain to the physical
domain using a projector. We evaluate PAPLA across multiple scenarios,
including controlled laboratory settings and realistic outdoor environments,
demonstrating its ability to ensure attack success compared to conventional
digital learning-physical application (DL-PA) methods. We also analyze the
impact of environmental factors, such as projection surface color, projector
strength, ambient light, distance, and angle of the target object relative to
the camera, on the effectiveness of projected patches. Finally, we demonstrate
the feasibility of the attack against a parked car and a stop sign in a
real-world outdoor environment. Our results show that under specific
conditions, E2E adversarial learning in the physical domain eliminates the
transferability issue and ensures evasion by object detectors. Finally, we
provide insights into the challenges and opportunities of applying adversarial
learning in the physical domain and explain where such an approach is more
effective than using a sticker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TextureCrop: Enhancing Synthetic Image Detection through Texture-based
  Cropping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15500v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15500v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Despina Konstantinidou, Christos Koutlis, Symeon Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI technologies produce increasingly realistic imagery, which,
despite its potential for creative applications, can also be misused to produce
misleading and harmful content. This renders Synthetic Image Detection (SID)
methods essential for identifying AI-generated content online. State-of-the-art
SID methods typically resize or center-crop input images due to architectural
or computational constraints, which hampers the detection of artifacts that
appear in high-resolution images. To address this limitation, we propose
TextureCrop, an image pre-processing component that can be plugged in any
pre-trained SID model to improve its performance. By focusing on high-frequency
image parts where generative artifacts are prevalent, TextureCrop enhances SID
performance with manageable memory requirements. Experimental results
demonstrate a consistent improvement in AUC across various detectors by 6.1%
compared to center cropping and by 15% compared to resizing, across
high-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.
Code available at https : //github.com/mever-team/texture-crop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IOR: Inversed Objects Replay for Incremental Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04829v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04829v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijia An, Boyu Diao, Libo Huang, Ruiqi Liu, Zhulin An, Yongjun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Incremental Object Detection (IOD) methods partially alleviate
catastrophic forgetting when incrementally detecting new objects in real-world
scenarios. However, many of these methods rely on the assumption that unlabeled
old-class objects may co-occur with labeled new-class objects in the
incremental data. When unlabeled old-class objects are absent, the performance
of existing methods tends to degrade. The absence can be mitigated by
generating old-class samples, but it incurs high costs. This paper argues that
previous generation-based IOD suffers from redundancy, both in the use of
generative models, which require additional training and storage, and in the
overproduction of generated samples, many of which do not contribute
significantly to performance improvements. To eliminate the redundancy, we
propose Inversed Objects Replay (IOR). Specifically, we generate old-class
samples by inversing the original detectors, thus eliminating the necessity of
training and storing additional generative models. We propose augmented replay
to reuse the objects in generated samples, reducing redundant generations.
Moreover, we propose high-value knowledge distillation focusing on the
positions of old-class objects overwhelmed by the background, which transfers
the knowledge to the incremental detector. Extensive experiments conducted on
MS COCO 2017 demonstrate that our method can efficiently improve detection
performance in IOD scenarios with the absence of old-class objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skinned Motion Retargeting with Dense Geometric Interaction Perception <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Ye, Jia-Wei Liu, Jia Jia, Shikun Sun, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing and maintaining geometric interactions among different body parts
is crucial for successful motion retargeting in skinned characters. Existing
approaches often overlook body geometries or add a geometry correction stage
after skeletal motion retargeting. This results in conflicts between skeleton
interaction and geometry correction, leading to issues such as jittery,
interpenetration, and contact mismatches. To address these challenges, we
introduce a new retargeting framework, MeshRet, which directly models the dense
geometric interactions in motion retargeting. Initially, we establish dense
mesh correspondences between characters using semantically consistent sensors
(SCS), effective across diverse mesh topologies. Subsequently, we develop a
novel spatio-temporal representation called the dense mesh interaction (DMI)
field. This field, a collection of interacting SCS feature vectors, skillfully
captures both contact and non-contact interactions between body geometries. By
aligning the DMI field during retargeting, MeshRet not only preserves motion
semantics but also prevents self-interpenetration and ensures contact
preservation. Extensive experiments on the public Mixamo dataset and our
newly-collected ScanRet dataset demonstrate that MeshRet achieves
state-of-the-art performance. Code available at
https://github.com/abcyzj/MeshRet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ reBEN: Refined BigEarthNet <span class="highlight-title">Dataset</span> for Remote Sensing Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03653v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03653v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Norman Clasen, Leonard Hackel, Tom Burgert, Gencer Sumbul, Begüm Demir, Volker Markl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents refined BigEarthNet (reBEN) that is a large-scale,
multi-modal remote sensing dataset constructed to support deep learning (DL)
studies for remote sensing image analysis. The reBEN dataset consists of
549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN,
we initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the
BigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m.
We apply atmospheric correction to the Sentinel-2 patches using the latest
version of the sen2cor tool, resulting in higher-quality patches compared to
those present in BigEarthNet. Each patch is then associated with a pixel-level
reference map and scene-level multi-labels. This makes reBEN suitable for
pixel- and scene-based learning tasks. The labels are derived from the most
recent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class
nomenclature as in BigEarthNet. The use of the most recent CLC map results in
overcoming the label noise present in BigEarthNet. Furthermore, we introduce a
new geographical-based split assignment algorithm that significantly reduces
the spatial correlation among the train, validation, and test sets with respect
to those present in BigEarthNet. This increases the reliability of the
evaluation of DL models. To minimize the DL model training time, we introduce
software tools that convert the reBEN dataset into a DL-optimized data format.
In our experiments, we show the potential of reBEN for multi-modal multi-label
image classification problems by considering several state-of-the-art DL
models. The pre-trained model weights, associated code, and complete dataset
are available at https://bigearth.net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DehazeGS: Seeing Through Fog with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03659v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03659v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current novel view synthesis tasks primarily rely on high-quality and clear
images. However, in foggy scenes, scattering and attenuation can significantly
degrade the reconstruction and rendering quality. Although NeRF-based dehazing
reconstruction algorithms have been developed, their use of deep fully
connected neural networks and per-ray sampling strategies leads to high
computational costs. Moreover, NeRF's implicit representation struggles to
recover fine details from hazy scenes. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly
modeling point clouds into 3D Gaussians. In this paper, we propose leveraging
the explicit Gaussian representation to explain the foggy image formation
process through a physically accurate forward rendering process. We introduce
DehazeGS, a method capable of decomposing and rendering a fog-free background
from participating media using only muti-view foggy images as input. We model
the transmission within each Gaussian distribution to simulate the formation of
fog. During this process, we jointly learn the atmospheric light and scattering
coefficient while optimizing the Gaussian representation of the hazy scene. In
the inference stage, we eliminate the effects of scattering and attenuation on
the Gaussians and directly project them onto a 2D plane to obtain a clear view.
Experiments on both synthetic and real-world foggy datasets demonstrate that
DehazeGS achieves state-of-the-art performance in terms of both rendering
quality and computational efficiency. visualizations are available at
https://dehazegs.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StructSR: Refuse Spurious Details in Real-World Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yachao Li, Dong Liang, Tianyu Ding, Sheng-Jun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based models have shown great promise in real-world image
super-resolution (Real-ISR), but often generate content with structural errors
and spurious texture details due to the empirical priors and illusions of these
models. To address this issue, we introduce StructSR, a simple, effective, and
plug-and-play method that enhances structural fidelity and suppresses spurious
details for diffusion-based Real-ISR. StructSR operates without the need for
additional fine-tuning, external model priors, or high-level semantic
knowledge. At its core is the Structure-Aware Screening (SAS) mechanism, which
identifies the image with the highest structural similarity to the
low-resolution (LR) input in the early inference stage, allowing us to leverage
it as a historical structure knowledge to suppress the generation of spurious
details. By intervening in the diffusion inference process, StructSR seamlessly
integrates with existing diffusion-based Real-ISR models. Our experimental
results demonstrate that StructSR significantly improves the fidelity of
structure and texture, improving the PSNR and SSIM metrics by an average of
5.27% and 9.36% on a synthetic dataset (DIV2K-Val) and 4.13% and 8.64% on two
real-world datasets (RealSR and DRealSR) when integrated with four
state-of-the-art diffusion-based Real-ISR methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Direct Unlearning Optimization for Robust and Safe Text-to-Image Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong-Hyun Park, Sangdoo Yun, Jin-Hwa Kim, Junho Kim, Geonhui Jang, Yonghyun Jeong, Junghyo Jo, Gayoung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image (T2I) models have unlocked a wide range
of applications but also present significant risks, particularly in their
potential to generate unsafe content. To mitigate this issue, researchers have
developed unlearning techniques to remove the model's ability to generate
potentially harmful content. However, these methods are easily bypassed by
adversarial attacks, making them unreliable for ensuring the safety of
generated images. In this paper, we propose Direct Unlearning Optimization
(DUO), a novel framework for removing Not Safe For Work (NSFW) content from T2I
models while preserving their performance on unrelated topics. DUO employs a
preference optimization approach using curated paired image data, ensuring that
the model learns to remove unsafe visual concepts while retaining unrelated
features. Furthermore, we introduce an output-preserving regularization term to
maintain the model's generative capabilities on safe content. Extensive
experiments demonstrate that DUO can robustly defend against various
state-of-the-art red teaming methods without significant performance
degradation on unrelated topics, as measured by FID and CLIP scores. Our work
contributes to the development of safer and more reliable T2I models, paving
the way for their responsible deployment in both closed-source and open-source
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Distortion Guided Transformer for Omnidirectional Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cuixin Yang, Rongkang Dong, Jun Xiao, Cong Zhang, Kin-Man Lam, Fei Zhou, Guoping Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As virtual and augmented reality applications gain popularity,
omnidirectional image (ODI) super-resolution has become increasingly important.
Unlike 2D plain images that are formed on a plane, ODIs are projected onto
spherical surfaces. Applying established image super-resolution methods to
ODIs, therefore, requires performing equirectangular projection (ERP) to map
the ODIs onto a plane. ODI super-resolution needs to take into account
geometric distortion resulting from ERP. However, without considering such
geometric distortion of ERP images, previous deep-learning-based methods only
utilize a limited range of pixels and may easily miss self-similar textures for
reconstruction. In this paper, we introduce a novel Geometric Distortion Guided
Transformer for Omnidirectional image Super-Resolution (GDGT-OSR).
Specifically, a distortion modulated rectangle-window self-attention mechanism,
integrated with deformable self-attention, is proposed to better perceive the
distortion and thus involve more self-similar textures. Distortion modulation
is achieved through a newly devised distortion guidance generator that produces
guidance by exploiting the variability of distortion across latitudes.
Furthermore, we propose a dynamic feature aggregation scheme to adaptively fuse
the features from different self-attention modules. We present extensive
experimental results on public datasets and show that the new GDGT-OSR
outperforms methods in existing literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures, journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iFADIT: Invertible Face Anonymization via Disentangled Identity
  Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Yuan, Kai Liang, Xiong Li, Tao Wu, Nannan Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anonymization aims to conceal the visual identity of a face to safeguard
the individual's privacy. Traditional methods like blurring and pixelation can
largely remove identifying features, but these techniques significantly degrade
image quality and are vulnerable to deep reconstruction attacks. Generative
models have emerged as a promising solution for anonymizing faces while
preserving a natural appearance. However, many still face limitations in visual
quality and often overlook the potential to recover the original face from the
anonymized version, which can be valuable in specific contexts such as image
forensics. This paper proposes a novel framework named iFADIT, an acronym for
Invertible Face Anonymization via Disentangled Identity Transform. The
framework features a disentanglement architecture coupled with a secure
flow-based model: the former decouples identity information from
non-identifying attributes, while the latter transforms the decoupled identity
into an anonymized version in an invertible manner controlled by a secret key.
The anonymized face can then be reconstructed based on a pre-trained StyleGAN
that ensures high image quality and realistic facial details. Recovery of the
original face (aka de-anonymization) is possible upon the availability of the
matching secret, by inverting the anonymization process based on the same set
of model parameters. Furthermore, a dedicated secret-key mechanism along with a
dual-phase training strategy is devised to ensure the desired properties of
face anonymization. Qualitative and quantitative experiments demonstrate the
superiority of the proposed approach in anonymity, reversibility, security,
diversity, and interpretability over competing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using
  Real-Time Warped Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling aims to transform random noise into structured outputs.
In this work, we enhance video diffusion models by allowing motion control via
structured latent noise sampling. This is achieved by just a change in data: we
pre-process training videos to yield structured noise. Consequently, our method
is agnostic to diffusion model design, requiring no changes to model
architectures or training pipelines. Specifically, we propose a novel noise
warping algorithm, fast enough to run in real time, that replaces random
temporal Gaussianity with correlated warped noise derived from optical flow
fields, while preserving the spatial Gaussianity. The efficiency of our
algorithm enables us to fine-tune modern video diffusion base models using
warped noise with minimal overhead, and provide a one-stop solution for a wide
range of user-friendly motion control: local object motion control, global
camera movement control, and motion transfer. The harmonization between
temporal coherence and spatial Gaussianity in our warped noise leads to
effective motion control while maintaining per-frame pixel quality. Extensive
experiments and user studies demonstrate the advantages of our method, making
it a robust and scalable approach for controlling motion in video diffusion
models. Video results are available on our webpage:
https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code
and model checkpoints are available on GitHub:
https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point-PRC: A <span class="highlight-title">Prompt</span> Learning Based Regulation Framework for
  Generalizable Point Cloud Analysis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Sun, Qiuhong Ke, Yongcai Wang, Wang Chen, Kang Yang, Deying Li, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the 3D domain generalization (3DDG) ability of large
3D models based on prevalent prompt learning. Recent works demonstrate the
performances of 3D point cloud recognition can be boosted remarkably by
parameter-efficient prompt tuning. However, we observe that the improvement on
downstream tasks comes at the expense of a severe drop in 3D domain
generalization. To resolve this challenge, we present a comprehensive
regulation framework that allows the learnable prompts to actively interact
with the well-learned general knowledge in large 3D models to maintain good
generalization. Specifically, the proposed framework imposes multiple explicit
constraints on the prompt learning trajectory by maximizing the mutual
agreement between task-specific predictions and task-agnostic knowledge. We
design the regulation framework as a plug-and-play module to embed into
existing representative large 3D models. Surprisingly, our method not only
realizes consistently increasing generalization ability but also enhances
task-specific 3D recognition performances across various 3DDG benchmarks by a
clear margin. Considering the lack of study and evaluation on 3DDG, we also
create three new benchmarks, namely base-to-new, cross-dataset and few-shot
generalization benchmarks, to enrich the field and inspire future research.
Code and benchmarks are available at
\url{https://github.com/auniquesun/Point-PRC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 figures, 14 tables; accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMRxRecon2024: A Multi-Modality, Multi-View K-Space <span class="highlight-title">Dataset</span> Boosting
  Universal Machine Learning for Accelerated Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Wang, Fanwen Wang, Chen Qin, Jun Lyu, Cheng Ouyang, Shuo Wang, Yan Li, Mengyao Yu, Haoyu Zhang, Kunyuan Guo, Zhang Shi, Qirong Li, Ziqiang Xu, Yajing Zhang, Hao Li, Sha Hua, Binghua Chen, Longyu Sun, Mengting Sun, Qin Li, Ying-Hua Chu, Wenjia Bai, Jing Qin, Xiahai Zhuang, Claudia Prieto, Alistair Young, Michael Markl, He Wang, Lianming Wu, Guang Yang, Xiaobo Qu, Chengyan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically
gold-standard technique for diagnosing cardiac diseases, thanks to its ability
to provide diverse information with multiple modalities and anatomical views.
Accelerated cardiac MRI is highly expected to achieve time-efficient and
patient-friendly imaging, and then advanced image reconstruction approaches are
required to recover high-quality, clinically interpretable images from
undersampled measurements. However, the lack of publicly available cardiac MRI
k-space dataset in terms of both quantity and diversity has severely hindered
substantial technological progress, particularly for data-driven artificial
intelligence. Here, we provide a standardized, diverse, and high-quality
CMRxRecon2024 dataset to facilitate the technical development, fair evaluation,
and clinical transfer of cardiac MRI reconstruction approaches, towards
promoting the universal frameworks that enable fast and robust reconstructions
across different cardiac MRI protocols in clinical practice. To the best of our
knowledge, the CMRxRecon2024 dataset is the largest and most protocal-diverse
publicly available cardiac k-space dataset. It is acquired from 330 healthy
volunteers, covering commonly used modalities, anatomical views, and
acquisition trajectories in clinical cardiac MRI workflows. Besides, an open
platform with tutorials, benchmarks, and data processing tools is provided to
facilitate data usage, advanced method development, and fair performance
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLG-CBM: Training Concept Bottleneck Models with Vision-Language
  Guidance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01432v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01432v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyansh Srivastava, Ge Yan, Tsui-Wei Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept Bottleneck Models (CBMs) provide interpretable prediction by
introducing an intermediate Concept Bottleneck Layer (CBL), which encodes
human-understandable concepts to explain models' decision. Recent works
proposed to utilize Large Language Models and pre-trained Vision-Language
Models to automate the training of CBMs, making it more scalable and automated.
However, existing approaches still fall short in two aspects: First, the
concepts predicted by CBL often mismatch the input image, raising doubts about
the faithfulness of interpretation. Second, it has been shown that concept
values encode unintended information: even a set of random concepts could
achieve comparable test accuracy to state-of-the-art CBMs. To address these
critical limitations, in this work, we propose a novel framework called
Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful
interpretability with the benefits of boosted performance. Our method leverages
off-the-shelf open-domain grounded object detectors to provide visually
grounded concept annotation, which largely enhances the faithfulness of concept
prediction while further improving the model performance. In addition, we
propose a new metric called Number of Effective Concepts (NEC) to control the
information leakage and provide better interpretability. Extensive evaluations
across five standard benchmarks show that our method, VLG-CBM, outperforms
existing methods by at least 4.27% and up to 51.09% on Accuracy at NEC=5
(denoted as ANEC-5), and by at least 0.45% and up to 29.78% on average accuracy
(denoted as ANEC-avg), while preserving both faithfulness and interpretability
of the learned concepts as demonstrated in extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing Forestry Images Conditioned on Plant Phenotype Using a
  Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasmita Pal, Arun Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plant phenology and phenotype prediction using remote sensing data are
increasingly gaining attention within the plant science community as a
promising approach to enhance agricultural productivity. This work focuses on
generating synthetic forestry images that satisfy certain phenotypic
attributes, viz. canopy greenness. We harness a Generative Adversarial Network
(GAN) to synthesize biologically plausible and phenotypically stable forestry
images conditioned on the greenness of vegetation (a continuous attribute) over
a specific region of interest, describing a particular vegetation type in a
mixed forest. The training data is based on the automated digital camera
imagery provided by the National Ecological Observatory Network (NEON) and
processed by the PhenoCam Network. Our method helps render the appearance of
forest sites specific to a greenness value. The synthetic images are
subsequently utilized to predict another phenotypic attribute, viz., redness of
plants. The quality of the synthetic images is assessed using the Structural
SIMilarity (SSIM) index and Fr\'echet Inception Distance (FID). Further, the
greenness and redness indices of the synthetic images are compared against
those of the original images using Root Mean Squared Percentage Error (RMSPE)
to evaluate their accuracy and integrity. The generalizability and scalability
of our proposed GAN model are established by effectively transforming it to
generate synthetic images for other forest sites and vegetation types. From a
broader perspective, this approach could be leveraged to visualize forestry
based on different phenotypic attributes in the context of various
environmental parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Pattern Recognition journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with
  Multi-modality Refinement Module 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongzhihan Wang, Yang Yang, Liang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry (VO) plays a crucial role in autonomous driving, robotic
navigation, and other related tasks by estimating the position and orientation
of a camera based on visual input. Significant progress has been made in
data-driven VO methods, particularly those leveraging deep learning techniques
to extract image features and estimate camera poses. However, these methods
often struggle in low-light conditions because of the reduced visibility of
features and the increased difficulty of matching keypoints. To address this
limitation, we introduce BrightVO, a novel VO model based on Transformer
architecture, which not only performs front-end visual feature extraction, but
also incorporates a multi-modality refinement module in the back-end that
integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,
this module iteratively refines pose estimates to reduce errors and improve
both accuracy and robustness. Furthermore, we create a synthetic low-light
dataset, KiC4R, which includes a variety of lighting conditions to facilitate
the training and evaluation of VO frameworks in challenging environments.
Experimental results demonstrate that BrightVO achieves state-of-the-art
performance on both the KiC4R dataset and the KITTI benchmarks. Specifically,
it provides an average improvement of 20% in pose estimation accuracy in normal
outdoor environments and 259% in low-light conditions, outperforming existing
methods. For widespread use and further development, the research work is fully
open-source at https://github.com/Anastasiawd/BrightVO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have identified significant issues in the methodology and data
  analysis that impact the validity of our conclusions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Framework for Inference-time Scaling and Steering of Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models produce impressive results in modalities ranging from images
and video to protein design and text. However, generating samples with
user-specified properties remains a challenge. Recent research proposes
fine-tuning models to maximize rewards that capture desired properties, but
these methods require expensive training and are prone to mode collapse. In
this work, we propose Feynman Kac (FK) steering, an inference-time framework
for steering diffusion models with reward functions. FK steering works by
sampling a system of multiple interacting diffusion processes, called
particles, and resampling particles at intermediate steps based on scores
computed using functions called potentials. Potentials are defined using
rewards for intermediate states and are selected such that a high value
indicates that the particle will yield a high-reward sample. We explore various
choices of potentials, intermediate rewards, and samplers. We evaluate FK
steering on text-to-image and text diffusion models. For steering text-to-image
models with a human preference reward, we find that FK steering a 0.8B
parameter model outperforms a 2.6B parameter fine-tuned model on prompt
fidelity, with faster sampling and no training. For steering text diffusion
models with rewards for text quality and specific text attributes, we find that
FK steering generates lower perplexity, more linguistically acceptable outputs
and enables gradient-free control of attributes like toxicity. Our results
demonstrate that inference-time scaling and steering of diffusion models, even
with off-the-shelf rewards, can provide significant sample quality gains and
controllability benefits. Code is available at
https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery
  from Videos <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13397v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13397v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery (HMR) provides rich human body information for various
real-world applications. While image-based HMR methods have achieved impressive
results, they often struggle to recover humans in dynamic scenarios, leading to
temporal inconsistencies and non-smooth 3D motion predictions due to the
absence of human motion. In contrast, video-based approaches leverage temporal
information to mitigate this issue. In this paper, we present DiffMesh, an
innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh
establishes a bridge between diffusion models and human motion, efficiently
generating accurate and smooth output mesh sequences by incorporating human
motion within the forward process and reverse process in the diffusion model.
Extensive experiments are conducted on the widely used datasets (Human3.6M
\cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness
and efficiency of our DiffMesh. Visual comparisons in real-world scenarios
further highlight DiffMesh's suitability for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05264v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05264v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Peng, Mengshi Qi, Dong Zhao, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human pose estimation (3D HPE) has emerged as a prominent research topic,
particularly in the realm of RGB-based methods. However, RGB images are
susceptible to limitations such as sensitivity to lighting conditions and
potential user discomfort. Consequently, multi-modal sensing, which leverages
non-intrusive sensors, is gaining increasing attention. Nevertheless,
multi-modal 3D HPE still faces challenges, including modality imbalance and the
imperative for continual learning. In this work, we introduce a novel balanced
continual multi-modal learning method for 3D HPE, which harnesses the power of
RGB, LiDAR, mmWave, and WiFi. Specifically, we propose a Shapley value-based
contribution algorithm to quantify the contribution of each modality and
identify modality imbalance. To address this imbalance, we employ a re-learning
strategy. Furthermore, recognizing that raw data is prone to noise
contamination, we develop a novel denoising continual learning approach. This
approach incorporates a noise identification and separation module to mitigate
the adverse effects of noise and collaborates with the balanced learning
strategy to enhance optimization. Additionally, an adaptive EWC mechanism is
employed to alleviate catastrophic forgetting. We conduct extensive experiments
on the widely-adopted multi-modal dataset, MM-Fi, which demonstrate the
superiority of our approach in boosting 3D pose estimation and mitigating
catastrophic forgetting in complex scenarios. We will release our codes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaboration in Immersive Environments: Challenges and Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00689v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00689v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahin Doroudian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in
all engineering fields in order to avoid the use of physical prototypes, to
train in high-risk situations, and to interpret real or simulated results. In
order to complete a shared task or assign tasks to the agents in such immersive
environments, collaboration or Shared Cooperative Activities are a necessity.
Collaboration in immersive environments is an emerging field of research that
aims to study and enhance the ways in which people interact and work together
in Virtual and Augmented Reality settings. Collaboration in immersive
environments is a complex process that involves different factors such as
communication, coordination, and social presence. This paper provides an
overview of the current state of research on collaboration in immersive
environments. It discusses the different types of immersive environments,
including VR and AR, and the different forms of collaboration that can occur in
these environments. The paper also highlights the challenges and limitations of
collaboration in immersive environments, such as the lack of physical cues,
cost and usability and the need for further research in this area. Overall,
collaboration in immersive environments is a promising field with a wide range
of potential applications, from education to industry, and it can benefit both
individuals and groups by enhancing their ability to work together effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added new references in Networking section</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking <span class="highlight-title">Pre-Train</span>ed Feature Extractor Selection in Multiple Instance
  Learning for Whole Slide Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01167v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01167v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Wong, Mun Yong Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) has become a preferred method for gigapixel
whole slide image (WSI) classification without requiring patch-level
annotations. Current MIL research primarily relies on embedding-based
approaches, which extract patch features using a pre-trained feature extractor
and aggregate them for slide-level prediction. Despite the critical role of
feature extraction, there is limited guidance on selecting optimal feature
extractors to maximize WSI performance. This study addresses this gap by
systematically evaluating MIL feature extractors across three dimensions:
pre-training dataset, backbone model, and pre-training method. Extensive
experiments were conducted on two public WSI datasets (TCGA-NSCLC and
Camelyon16) using four state-of-the-art (SOTA) MIL models. Our findings reveal
that: 1) selecting a robust self-supervised learning (SSL) method has a greater
impact on performance than relying solely on an in-domain pre-training dataset;
2) prioritizing Transformer-based backbones with deeper architectures over
CNN-based models; and 3) using larger, more diverse pre-training datasets
significantly enhances classification outcomes. We hope that these insights can
provide practical guidance for optimizing WSI classification and explain the
reasons behind the performance advantages of the current SOTA pathology
foundation models. Furthermore, this work may inform the development of more
effective pathology foundation models. Our code is publicly available at
https://github.com/bryanwong17/MIL-Feature-Extractor-Selection
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysMamba: State Space Duality Model for Remote Physiological
  Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01077v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01077v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Yan, Yan Zhong, Hongbin Xu, Wenjun Zhang, Shangru Yi, Lin Shu, Wenxiong Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote Photoplethysmography (rPPG) enables non-contact physiological signal
extraction from facial videos, offering applications in psychological state
analysis, medical assistance, and anti-face spoofing. However, challenges such
as motion artifacts, lighting variations, and noise limit its real-world
applicability. To address these issues, we propose PhysMamba, a novel
dual-pathway time-frequency interaction model based on Synergistic State Space
Duality (SSSD), which for the first time integrates state space models with
attention mechanisms in a dual-branch framework. Combined with a Multi-Scale
Query (MQ) mechanism, PhysMamba achieves efficient information exchange and
enhanced feature representation, ensuring robustness under noisy and dynamic
conditions. Experiments on PURE, UBFC-rPPG, and MMPD datasets demonstrate that
PhysMamba outperforms state-of-the-art methods, offering superior accuracy and
generalization. This work lays a strong foundation for practical applications
in non-contact health monitoring, including real-time remote patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Masked Image Modeling to Avoid Model Collapse on Multi-modal
  MRI <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10377v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10377v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linxuan Han, Sa Xiao, Zimeng Li, Haidong Li, Xiuchao Zhao, Yeqing Han, Fumin Guo, Xin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal magnetic resonance imaging (MRI) provides information of lesions
for computer-aided diagnosis from different views. Deep learning algorithms are
suitable for identifying specific anatomical structures, segmenting lesions,
and classifying diseases. Manual labels are limited due to the high expense,
which hinders further improvement of accuracy. Self-supervised learning,
particularly masked image modeling (MIM), has shown promise in utilizing
unlabeled data. However, we spot model collapse when applying MIM to
multi-modal MRI datasets. The performance of downstream tasks does not see any
improvement following the collapsed model. To solve model collapse, we analyze
and address it in two types: complete collapse and dimensional collapse. We
find complete collapse occurs because the collapsed loss value in multi-modal
MRI datasets falls below the normally converged loss value. Based on this, the
hybrid mask pattern (HMP) masking strategy is introduced to elevate the
collapsed loss above the normally converged loss value and avoid complete
collapse. Additionally, we reveal that dimensional collapse stems from
insufficient feature uniformity in MIM. We mitigate dimensional collapse by
introducing the pyramid barlow twins (PBT) module as an explicit regularization
method. Overall, we construct the enhanced MIM (E-MIM) with HMP and PBT module
to avoid model collapse multi-modal MRI. Experiments are conducted on three
multi-modal MRI datasets to validate the effectiveness of our approach in
preventing both types of model collapse. By preventing model collapse, the
training of the model becomes more stable, resulting in a decent improvement in
performance for segmentation and classification tasks. The code is available at
https://github.com/LinxuanHan/E-MIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the lEEE for possible publication.
  copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryoBench: Diverse and challenging <span class="highlight-title">dataset</span>s for the heterogeneity
  problem in cryo-EM <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05526v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05526v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkyu Jeon, Rishwanth Raghu, Miro Astore, Geoffrey Woollard, Ryan Feathers, Alkin Kaz, Sonya M. Hanson, Pilar Cossio, Ellen D. Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryo-electron microscopy (cryo-EM) is a powerful technique for determining
high-resolution 3D biomolecular structures from imaging data. Its unique
ability to capture structural variability has spurred the development of
heterogeneous reconstruction algorithms that can infer distributions of 3D
structures from noisy, unlabeled imaging data. Despite the growing number of
advanced methods, progress in the field is hindered by the lack of standardized
benchmarks with ground truth information and reliable validation metrics. Here,
we introduce CryoBench, a suite of datasets, metrics, and benchmarks for
heterogeneous reconstruction in cryo-EM. CryoBench includes five datasets
representing different sources of heterogeneity and degrees of difficulty.
These include conformational heterogeneity generated from designed motions of
antibody complexes or sampled from a molecular dynamics simulation, as well as
compositional heterogeneity from mixtures of ribosome assembly states or 100
common complexes present in cells. We then analyze state-of-the-art
heterogeneous reconstruction tools, including neural and non-neural methods,
assess their sensitivity to noise, and propose new metrics for quantitative
evaluation. We hope that CryoBench will be a foundational resource for
accelerating algorithmic development and evaluation in the cryo-EM and machine
learning communities. Project page: https://cryobench.cs.princeton.edu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Swin transformers are robust to distribution and concept drift in
  endoscopy-based longitudinal rectal cancer assessment <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Tapias Gomez, Aneesh Rangnekar, Hannah Williams, Hannah Thompson, Julio Garcia-Aguilar, Joshua Jesse Smith, Harini Veeraraghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endoscopic images are used at various stages of rectal cancer treatment
starting from cancer screening, diagnosis, during treatment to assess response
and toxicity from treatments such as colitis, and at follow up to detect new
tumor or local regrowth (LR). However, subjective assessment is highly variable
and can underestimate the degree of response in some patients, subjecting them
to unnecessary surgery, or overestimate response that places patients at risk
of disease spread. Advances in deep learning has shown the ability to produce
consistent and objective response assessment for endoscopic images. However,
methods for detecting cancers, regrowth, and monitoring response during the
entire course of patient treatment and follow-up are lacking. This is because,
automated diagnosis and rectal cancer response assessment requires methods that
are robust to inherent imaging illumination variations and confounding
conditions (blood, scope, blurring) present in endoscopy images as well as
changes to the normal lumen and tumor during treatment. Hence, a hierarchical
shifted window (Swin) transformer was trained to distinguish rectal cancer from
normal lumen using endoscopy images. Swin as well as two convolutional
(ResNet-50, WideResNet-50), and vision transformer (ViT) models were trained
and evaluated on follow-up longitudinal images to detect LR on private dataset
as well as on out-of-distribution (OOD) public colonoscopy datasets to detect
pre/non-cancerous polyps. Color shifts were applied using optimal transport to
simulate distribution shifts. Swin and ResNet models were similarly accurate in
the in-distribution dataset. Swin was more accurate than other methods
(follow-up: 0.84, OOD: 0.83) even when subject to color shifts (follow-up:
0.83, OOD: 0.87), indicating capability to provide robust performance for
longitudinal cancer assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The work has been accepted for publication in 2024 SPIE Medical
  Imaging conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Lexicon-Based Text Embeddings with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Lei, Tao Shen, Yu Cao, Andrew Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have demonstrated exceptional performance
on general-purpose text embedding tasks. While dense embeddings have dominated
related research, we introduce the first Lexicon-based EmbeddiNgS (LENS)
leveraging LLMs that achieve competitive performance on these tasks. Regarding
the inherent tokenization redundancy issue and unidirectional attention
limitations in traditional causal LLMs, LENS consolidates the vocabulary space
through token embedding clustering, and investigates bidirectional attention
and various pooling strategies. Specifically, LENS simplifies lexicon matching
by assigning each dimension to a specific token cluster, where semantically
similar tokens are grouped together, and unlocking the full potential of LLMs
through bidirectional attention. Extensive experiments demonstrate that LENS
outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),
delivering compact feature representations that match the sizes of dense
counterparts. Notably, combining LENSE with dense embeddings achieves
state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metric Learning with Progressive Self-Distillation for Audio-Visual
  Embedding Learning <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghuo Zeng, Kazushi Ikeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning projects samples into an embedded space, where similarities
and dissimilarities are quantified based on their learned representations.
However, existing methods often rely on label-guided representation learning,
where representations of different modalities, such as audio and visual data,
are aligned based on annotated labels. This approach tends to underutilize
latent complex features and potential relationships inherent in the
distributions of audio and visual data that are not directly tied to the
labels, resulting in suboptimal performance in audio-visual embedding learning.
To address this issue, we propose a novel architecture that integrates
cross-modal triplet loss with progressive self-distillation. Our method
enhances representation learning by leveraging inherent distributions and
dynamically refining soft audio-visual alignments -- probabilistic alignments
between audio and visual data that capture the inherent relationships beyond
explicit labels. Specifically, the model distills audio-visual
distribution-based knowledge from annotated labels in a subset of each batch.
This self-distilled knowledge is used t
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 2 tables. Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Conversational <span class="highlight-title">Recommend</span>er Systems with Large Language
  Models: A User-Centric Evaluation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Quanyu Dai, Xiaoyu Dong, Xiao-Ming Wu, Zhenhua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRS) involve both recommendation and
dialogue tasks, which makes their evaluation a unique challenge. Although past
research has analyzed various factors that may affect user satisfaction with
CRS interactions from the perspective of user studies, few evaluation metrics
for CRS have been proposed. Recent studies have shown that LLMs can align with
human preferences, and several LLM-based text quality evaluation measures have
been introduced. However, the application of LLMs in CRS evaluation remains
relatively limited. To address this research gap and advance the development of
user-centric conversational recommender systems, this study proposes an
automated LLM-based CRS evaluation framework, building upon existing research
in human-computer interaction and psychology. The framework evaluates CRS from
four dimensions: dialogue behavior, language expression, recommendation items,
and response content. We use this framework to evaluate four different
conversational recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating LLM Abilities to Understand <span class="highlight-title">Tabular</span> Electronic Health
  Records: A Comprehensive Study of Patient Data Extraction and <span class="highlight-title">Retrie</span>val <span class="chip">ECIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus Lovon, Martin Mouysset, Jo Oleiwan, Jose G. Moreno, Christine Damase-Michel, Lynda Tamine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Record (EHR) tables pose unique challenges among which is
the presence of hidden contextual dependencies between medical features with a
high level of data dimensionality and sparsity. This study presents the first
investigation into the abilities of LLMs to comprehend EHRs for patient data
extraction and retrieval. We conduct extensive experiments using the MIMICSQL
dataset to explore the impact of the prompt structure, instruction, context,
and demonstration, of two backbone LLMs, Llama2 and Meditron, based on task
performance. Through quantitative and qualitative analyses, our findings show
that optimal feature selection and serialization methods can enhance task
performance by up to 26.79% compared to naive approaches. Similarly, in-context
learning setups with relevant example selection improve data extraction
performance by 5.95%. Based on our study findings, we propose guidelines that
we believe would help the design of LLM-based models to support health search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published as full paper in the Proceedings of the European
  Conference on Information Retrieval (ECIR) 2025. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-tiered Solution for Personalized Baggage Item <span class="highlight-title">Recommend</span>ations
  using FastText and Association Rule Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mudavath Ravi, Atul Negi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an intelligent baggage item recommendation system to
optimize packing for air travelers by providing tailored suggestions based on
specific travel needs and destinations. Using FastText word embeddings and
Association Rule Mining (ARM), the system ensures efficient luggage space
utilization, compliance with weight limits, and an enhanced travel experience.
The methodology comprises four phases: (1) data collection and preprocessing
with pre-trained FastText embeddings for text representation and similarity
scoring (2) a content-based recommendation system enriched by user search
history (3) application of ARM to user interactions to uncover meaningful item
associations and (4) integration of FastText and ARM for accurate, personalized
recommendations. Performance is evaluated using metrics such as coverage,
support, confidence, lift, leverage, and conviction. Results demonstrate the
system's effectiveness in providing relevant suggestions, improving customer
satisfaction, and simplifying the packing process. These insights advance
personalized recommendations, targeted marketing, and product optimization in
air travel and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Style4Rec: Enhancing Transformer-based E-commerce <span class="highlight-title">Recommend</span>ation Systems
  with Style and Shopping Cart Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berke Ugurlu, Ming-Yi Hong, Che Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding users' product preferences is essential to the efficacy of a
recommendation system. Precision marketing leverages users' historical data to
discern these preferences and recommends products that align with them.
However, recent browsing and purchase records might better reflect current
purchasing inclinations. Transformer-based recommendation systems have made
strides in sequential recommendation tasks, but they often fall short in
utilizing product image style information and shopping cart data effectively.
In light of this, we propose Style4Rec, a transformer-based e-commerce
recommendation system that harnesses style and shopping cart information to
enhance existing transformer-based sequential product recommendation systems.
Style4Rec represents a significant step forward in personalized e-commerce
recommendations, outperforming benchmarks across various evaluation metrics.
Style4Rec resulted in notable improvements: HR@5 increased from 0.681 to 0.735,
NDCG@5 increased from 0.594 to 0.674, and MRR@5 increased from 0.559 to 0.654.
We tested our model using an e-commerce dataset from our partnering company and
found that it exceeded established transformer-based sequential recommendation
benchmarks across various evaluation metrics. Thus, Style4Rec presents a
significant step forward in personalized e-commerce recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 images, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To <span class="highlight-title">Retrie</span>ve or Not to <span class="highlight-title">Retrie</span>ve? Uncertainty Detection for Dynamic
  <span class="highlight-title">Retrie</span>val Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation equips large language models with the
capability to retrieve external knowledge, thereby mitigating hallucinations by
incorporating information beyond the model's intrinsic abilities. However, most
prior works have focused on invoking retrieval deterministically, which makes
it unsuitable for tasks such as long-form question answering. Instead,
dynamically performing retrieval by invoking it only when the underlying LLM
lacks the required knowledge can be more efficient. In this context, we delve
deeper into the question, "To Retrieve or Not to Retrieve?" by exploring
multiple uncertainty detection methods. We evaluate these methods for the task
of long-form question answering, employing dynamic retrieval, and present our
comparisons. Our findings suggest that uncertainty detection metrics, such as
Degree Matrix Jaccard and Eccentricity, can reduce the number of retrieval
calls by almost half, with only a slight reduction in question-answering
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Integration of Data Lake <span class="highlight-title">Table</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aamod Khatiwada, Roee Shraga, Renée J. Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data integration is an important step in any data science pipeline where the
objective is to unify the information available in different datasets for
comprehensive analysis. Full Disjunction, which is an associative extension of
the outer join operator, has been shown to be an effective operator for
integrating datasets. It fully preserves and combines the available
information. Existing Full Disjunction algorithms only consider the equi-join
scenario where only tuples having the same value on joining columns are
integrated. This, however, does not realistically represent an open data
scenario, where datasets come from diverse sources with inconsistent values
(e.g., synonyms, abbreviations, etc.) and with limited metadata. So, joining
just on equal values severely limits the ability of Full Disjunction to fully
combine datasets. Thus, in this work, we propose an extension of Full
Disjunction to also account for "fuzzy" matches among tuples. We present a
novel data-driven approach to enable the joining of approximate or fuzzy
matches within Full Disjunction. Experimentally, we show that fuzzy Full
Disjunction does not add significant time overhead over a state-of-the-art Full
Disjunction implementation and also that it enhances the integration
effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic Collective Action in <span class="highlight-title">Recommend</span>er Systems: Promoting Songs by
  Reordering Playlists <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joachim Baumann, Celestine Mendler-Dünner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate algorithmic collective action in transformer-based recommender
systems. Our use case is a music streaming platform where a collective of fans
aims to promote the visibility of an underrepresented artist by strategically
placing one of their songs in the existing playlists they control. We introduce
two easily implementable strategies to select the position at which to insert
the song with the goal to boost recommendations at test time. The strategies
exploit statistical properties of the learner by targeting discontinuities in
the recommendations, and leveraging the long-tail nature of song distributions.
We evaluate the efficacy of our strategies using a publicly available
recommender system model released by a major music streaming platform. Our
findings reveal that through strategic placement even small collectives
(controlling less than 0.01\% of the training data) can achieve up to
$40\times$ more test time recommendations than an average song with the same
number of training set occurrences. Focusing on the externalities of the
strategy, we find that the recommendations of other songs are largely
preserved, and the newly gained recommendations are distributed across various
artists. Together, our findings demonstrate how carefully designed collective
action strategies can be effective while not necessarily being adversarial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024, camera-ready updates</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPRec: Leveraging Self-Play to Debias Preference Alignment for Large
  Language Model-based <span class="highlight-title">Recommend</span>ations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongming Gao, Ruijun Chen, Shuai Yuan, Kexin Huang, Yuanqing Yu, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have attracted significant attention in
recommendation systems. Current LLM-based recommender systems primarily rely on
supervised fine-tuning (SFT) to train the model for recommendation tasks.
However, relying solely on positive samples limits the model's ability to align
with user satisfaction and expectations. To address this, researchers have
introduced Direct Preference Optimization (DPO), which explicitly aligns
recommendations with user preferences using offline preference ranking data.
Despite its advantages, our theoretical analysis reveals that DPO inherently
biases the model towards a few items, exacerbating the filter bubble issue and
ultimately degrading user experience. In this paper, we propose SPRec, a novel
self-play recommendation framework designed to mitigate over-recommendation and
improve fairness without requiring additional data or manual intervention. In
each self-play iteration, the model undergoes an SFT step followed by a DPO
step, treating offline interaction data as positive samples and the predicted
outputs from the previous iteration as negative samples. This effectively
re-weights the DPO loss function using the model's logits, adaptively
suppressing biased items. Extensive experiments on multiple real-world datasets
demonstrate SPRec's effectiveness in enhancing recommendation accuracy and
addressing fairness concerns. The implementation is available via
https://github.com/RegionCh/SPRec
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tapping the Potential of Large Language Models as <span class="highlight-title">Recommend</span>er Systems: A
  Comprehensive Framework and Empirical Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Sheng Chen, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models~(LLMs) such as ChatGPT have showcased
remarkable abilities in solving general tasks, demonstrating the potential for
applications in recommender systems. To assess how effectively LLMs can be used
in recommendation tasks, our study primarily focuses on employing LLMs as
recommender systems through prompting engineering. We propose a general
framework for utilizing LLMs in recommendation tasks, focusing on the
capabilities of LLMs as recommenders. To conduct our analysis, we formalize the
input of LLMs for recommendation into natural language prompts with two key
aspects, and explain how our framework can be generalized to various
recommendation scenarios. As for the use of LLMs as recommenders, we analyze
the impact of public availability, tuning strategies, model architecture,
parameter scale, and context length on recommendation results based on the
classification of LLMs. As for prompt engineering, we further analyze the
impact of four important components of prompts, \ie task descriptions, user
interest modeling, candidate items construction and prompting strategies. In
each section, we first define and categorize concepts in line with the existing
literature. Then, we propose inspiring research questions followed by detailed
experiments on two public datasets, in order to systematically analyze the
impact of different factors on performance. Based on our empirical analysis, we
finally summarize promising directions to shed lights on future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge <span class="highlight-title">Retrie</span>val Based on Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Te-Lun Yang, Jyi-Shane Liu, Yuen-Hsien Tseng, Jyh-Shing Roger Jang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study develops a question-answering system based on Retrieval-Augmented
Generation (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.
Using TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for
dense vector retrieval to obtain highly relevant search results and
BGE-reranker to reorder these results based on query relevance. The most
pertinent retrieval outcomes serve as reference knowledge for a Large Language
Model (LLM), enhancing its ability to answer questions and establishing a
knowledge retrieval system grounded in generative AI. The system's
effectiveness is assessed through a two-stage evaluation: automatic and
assisted performance evaluations. The automatic evaluation calculates accuracy
by comparing the model's auto-generated labels with ground truth answers,
measuring performance under standardized conditions without human intervention.
The assisted performance evaluation involves 20 finance-related multiple-choice
questions answered by 20 participants without financial backgrounds. Initially,
participants answer independently. Later, they receive system-generated
reference information to assist in answering, examining whether the system
improves accuracy when assistance is provided. The main contributions of this
research are: (1) Enhanced LLM Capability: By integrating BGE-M3 and
BGE-reranker, the system retrieves and reorders highly relevant results,
reduces hallucinations, and dynamically accesses authorized or public knowledge
sources. (2) Improved Data Privacy: A customized RAG architecture enables local
operation of the LLM, eliminating the need to send private data to external
servers. This approach enhances data security, reduces reliance on commercial
services, lowers operational costs, and mitigates privacy risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 13 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing User Interest based on Stream Clustering and Memory Networks
  in Large-Scale <span class="highlight-title">Recommend</span>er Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13238v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13238v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Nian Wang, Cong Xu, Ming Zhao, Bin Wang, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RSs) provide personalized recommendation service based
on user interest, which are widely used in various platforms. However, there
are lots of users with sparse interest due to lacking consumption behaviors,
which leads to poor recommendation results for them. This problem is widespread
in large-scale RSs and is particularly difficult to address. To solve this
problem, we propose a novel solution named User Interest Enhancement (UIE)
which enhances user interest including user profile and user history behavior
sequences using the enhancement vectors and personalized enhancement vector
generated based on stream clustering and memory networks from different
perspectives. UIE not only remarkably improves model performance on the users
with sparse interest but also significantly enhance model performance on other
users. UIE is an end-to-end solution which is easy to be implemented based on
ranking model. Moreover, we expand our solution and apply similar methods to
long-tail items, which also achieves excellent improvement. Furthermore, we
conduct extensive offline and online experiments in a large-scale industrial
RS. The results demonstrate that our model outperforms other models remarkably,
especially for the users with sparse interest. Until now, UIE has been fully
deployed in multiple large-scale RSs and achieved remarkable improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSD4Rec: A Structured State Space Duality Model for Efficient Sequential
  <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohao Qu, Yifeng Zhang, Liangbo Ning, Wenqi Fan, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation methods are crucial in modern recommender systems
for their remarkable capability to understand a user's changing interests based
on past interactions. However, a significant challenge faced by current methods
(e.g., RNN- or Transformer-based models) is to effectively and efficiently
capture users' preferences by modeling long behavior sequences, which impedes
their various applications like short video platforms where user interactions
are numerous. Recently, an emerging architecture named Mamba, built on state
space models (SSM) with efficient hardware-aware designs, has showcased the
tremendous potential for sequence modeling, presenting a compelling avenue for
addressing the challenge effectively. Inspired by this, we propose a novel
generic and efficient sequential recommendation backbone, SSD4Rec, which
explores the seamless adaptation of Mamba for sequential recommendations.
Specifically, SSD4Rec marks the variable- and long-length item sequences with
sequence registers and processes the item representations with bidirectional
Structured State Space Duality (SSD) blocks. This not only allows for
hardware-aware matrix multiplication but also empowers outstanding capabilities
in variable-length and long-range sequence modeling. Extensive evaluations on
four benchmark datasets demonstrate that the proposed model achieves
state-of-the-art performance while maintaining near-linear scalability with
user sequence length. Our code is publicly available at
https://github.com/ZhangYifeng1995/SSD4Rec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Significant revisions have been implemented in our paper,
  particularly focusing on both the methodology and experimental sections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scenario-Wise Rec: A Multi-Scenario <span class="highlight-title">Recommend</span>ation Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaopeng Li, Jingtong Gao, Pengyue Jia, Xiangyu Zhao, Yichao Wang, Wanyu Wang, Yejing Wang, Yuhao Wang, Xiangyu Zhao, Huifeng Guo, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi Scenario Recommendation (MSR) tasks, referring to building a unified
model to enhance performance across all recommendation scenarios, have recently
gained much attention. However, current research in MSR faces two significant
challenges that hinder the field's development: the absence of uniform
procedures for multi-scenario dataset processing, thus hindering fair
comparisons, and most models being closed-sourced, which complicates
comparisons with current SOTA models. Consequently, we introduce our benchmark,
\textbf{Scenario-Wise Rec}, which comprises 6 public datasets and 12 benchmark
models, along with a training and evaluation pipeline. Additionally, we
validated the benchmark using an industrial advertising dataset, reinforcing
its reliability and applicability in real-world scenarios. We aim for this
benchmark to offer researchers valuable insights from prior work, enabling the
development of novel models based on our benchmark and thereby fostering a
collaborative research ecosystem in MSR. Our source code is also publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusion <span class="highlight-title">Self-supervised</span> Learning for <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19692v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19692v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Lei Sang, Yi Zhang, Yiwen Zhang, Yun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely deployed in various web environments, and
self-supervised learning (SSL) has recently attracted significant attention in
this field. Contrastive learning (CL) stands out as a major SSL paradigm due to
its robust ability to generate self-supervised signals. Mainstream graph
contrastive learning (GCL)-based methods typically implement CL by creating
contrastive views through various data augmentation techniques. Despite these
methods are effective, we argue that there still exist several challenges. i)
Data augmentation ($e.g.,$ discarding edges or adding noise) necessitates
additional graph convolution (GCN) or modeling operations, which are highly
time-consuming and potentially harm the embedding quality. ii) Existing
CL-based methods use traditional CL objectives to capture self-supervised
signals. However, few studies have explored obtaining CL objectives from more
perspectives and have attempted to fuse the varying signals from these CL
objectives to enhance recommendation performance.
  To overcome these challenges, we propose a Fusion Self-supervised Learning
framework for recommendation. Specifically, instead of facilitating data
augmentations, we use high-order information from GCN process to create
contrastive views. Additionally, to integrate self-supervised signals from
various CL objectives, we propose an advanced CL objective. By ensuring that
positive pairs are distanced from negative samples derived from both
contrastive views, we effectively fuse self-supervised signals from distinct CL
objectives, thereby enhancing the mutual information between positive pairs.
Experimental results on three public datasets demonstrate the superior
recommendation performance and efficiency of HFGCL compared to the
state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">137</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexi Du, Jiazhen Zhang, Tal Zeevi, Nicha C. Dvornek, John A. Onofrey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are essential tools for computer vision
tasks, but they lack traditionally desired properties of extracted features
that could further improve model performance, e.g., rotational equivariance.
Such properties are ubiquitous in biomedical images, which often lack explicit
orientation. While current work largely relies on data augmentation or explicit
modules to capture orientation information, this comes at the expense of
increased training costs or ineffective approximations of the desired
equivariance. To overcome these challenges, we propose a novel and efficient
implementation of the Symmetric Rotation-Equivariant (SRE) Convolution
(SRE-Conv) kernel, designed to learn rotation-invariant features while
simultaneously compressing the model size. The SRE-Conv kernel can easily be
incorporated into any CNN backbone. We validate the ability of a deep SRE-CNN
to capture equivariance to rotation using the public MedMNISTv2 dataset (16
total tasks). SRE-Conv-CNN demonstrated improved rotated image classification
performance accuracy on all 16 test datasets in both 2D and 3D images, all
while increasing efficiency with fewer parameters and reduced memory footprint.
The code is available at https://github.com/XYPB/SRE-Conv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ISBI 2025 4-page paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAST: Efficient Action Tokenization for Vision-Language-Action Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive sequence models, such as Transformer-based vision-language
action (VLA) policies, can be tremendously effective for capturing complex and
generalizable robotic behaviors. However, such models require us to choose a
tokenization of our continuous action signals, which determines how the
discrete symbols predicted by the model map to continuous robot actions. We
find that current approaches for robot action tokenization, based on simple
per-dimension, per-timestep binning schemes, typically perform poorly when
learning dexterous skills from high-frequency robot data. To address this
challenge, we propose a new compression-based tokenization scheme for robot
actions, based on the discrete cosine transform. Our tokenization approach,
Frequency-space Action Sequence Tokenization (FAST), enables us to train
autoregressive VLAs for highly dexterous and high-frequency tasks where
standard discretization methods fail completely. Based on FAST, we release
FAST+, a universal robot action tokenizer, trained on 1M real robot action
trajectories. It can be used as a black-box tokenizer for a wide range of robot
action sequences, with diverse action spaces and control frequencies. Finally,
we show that, when combined with the pi0 VLA, our method can scale to training
on 10k hours of robot data and match the performance of diffusion VLAs, while
reducing training time by up to 5x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.pi.website/research/fast</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suggesting Code Edits in Interactive Machine Learning Notebooks Using
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bihui Jin, Jiayue Wang, Pengyu Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning developers frequently use interactive computational
notebooks, such as Jupyter notebooks, to host code for data processing and
model training. Jupyter notebooks provide a convenient tool for writing machine
learning pipelines and interactively observing outputs, however, maintaining
Jupyter notebooks, e.g., to add new features or fix bugs, can be challenging
due to the length and complexity of the notebooks. Moreover, there is no
existing benchmark related to developer edits on Jupyter notebooks. To address
this, we present the first dataset of 48,398 Jupyter notebook edits derived
from 20,095 revisions of 792 machine learning repositories on GitHub, and
perform the first study of the using LLMs to predict code edits in Jupyter
notebooks. Our dataset captures granular details of cell-level and line-level
modifications, offering a foundation for understanding real-world maintenance
patterns in machine learning workflows. We observed that the edits on Jupyter
notebooks are highly localized, with changes averaging only 166 lines of code
in repositories. While larger models outperform smaller counterparts in code
editing, all models have low accuracy on our dataset even after finetuning,
demonstrating the complexity of real-world machine learning maintenance tasks.
Our findings emphasize the critical role of contextual information in improving
model performance and point toward promising avenues for advancing large
language models' capabilities in engineering machine learning code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random Subspace Cubic-Regularization Methods, with Applications to
  Low-Rank Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coralia Cartis, Zhen Shao, Edward Tansley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and analyze random subspace variants of the second-order Adaptive
Regularization using Cubics (ARC) algorithm. These methods iteratively restrict
the search space to some random subspace of the parameters, constructing and
minimizing a local model only within this subspace. Thus, our variants only
require access to (small-dimensional) projections of first- and second-order
problem derivatives and calculate a reduced step inexpensively. Under suitable
assumptions, the ensuing methods maintain the optimal first-order, and
second-order, global rates of convergence of (full-dimensional) cubic
regularization, while showing improved scalability both theoretically and
numerically, particularly when applied to low-rank functions. When applied to
the latter, our adaptive variant naturally adapts the subspace size to the true
rank of the function, without knowing it a priori.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictions as Surrogates: Revisiting Surrogate Outcomes in the Age of
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Ji, Lihua Lei, Tijana Zrnic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a formal connection between the decades-old surrogate outcome
model in biostatistics and economics and the emerging field of
prediction-powered inference (PPI). The connection treats predictions from
pre-trained models, prevalent in the age of AI, as cost-effective surrogates
for expensive outcomes. Building on the surrogate outcomes literature, we
develop recalibrated prediction-powered inference, a more efficient approach to
statistical inference than existing PPI proposals. Our method departs from the
existing proposals by using flexible machine learning techniques to learn the
optimal ``imputed loss'' through a step we call recalibration. Importantly, the
method always improves upon the estimator that relies solely on the data with
available true outcomes, even when the optimal imputed loss is estimated
imperfectly, and it achieves the smallest asymptotic variance among PPI
estimators if the estimate is consistent. Computationally, our optimization
objective is convex whenever the loss function that defines the target
parameter is convex. We further analyze the benefits of recalibration, both
theoretically and numerically, in several common scenarios where machine
learning predictions systematically deviate from the outcome of interest. We
demonstrate significant gains in effective sample size over existing PPI
proposals via three applications leveraging state-of-the-art machine
learning/AI models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating particle physics Lagrangians with transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Sheng Koay, Rikard Enberg, Stefano Moretti, Eliel Camargo-Molina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In physics, Lagrangians provide a systematic way to describe laws governing
physical systems. In the context of particle physics, they encode the
interactions and behavior of the fundamental building blocks of our universe.
By treating Lagrangians as complex, rule-based constructs similar to linguistic
expressions, we trained a transformer model -- proven to be effective in
natural language tasks -- to predict the Lagrangian corresponding to a given
list of particles. We report on the transformer's performance in constructing
Lagrangians respecting the Standard Model $\mathrm{SU}(3)\times
\mathrm{SU}(2)\times \mathrm{U}(1)$ gauge symmetries. The resulting model is
shown to achieve high accuracies (over 90\%) with Lagrangians up to six matter
fields, with the capacity to generalize beyond the training distribution,
albeit within architectural constraints. We show through an analysis of input
embeddings that the model has internalized concepts such as group
representations and conjugation operations as it learned to generate
Lagrangians. We make the model and training datasets available to the
community. An interactive demonstration can be found at:
\url{https://huggingface.co/spaces/JoseEliel/generate-lagrangians}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 11 figues, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention based Bidirectional GRU hybrid model for inappropriate content
  detection in Urdu language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezzah Shoukat, Rabia Irfan, Iqra Basharat, Muhammad Ali Tahir, Sameen Shaukat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increased use of the internet and social networks for online
discussions, the spread of toxic and inappropriate content on social networking
sites has also increased. Several studies have been conducted in different
languages. However, there is less work done for South Asian languages for
inappropriate content identification using deep learning techniques. In Urdu
language, the spellings are not unique, and people write different common
spellings for the same word, while mixing it other languages, like English in
the text makes it more challenging, and limited research work is available to
process such language with the finest algorithms. The use of attention layer
with a deep learning model can help handling the long-term dependencies and
increase its efficiency . To explore the effects of the attention layer, this
study proposes attention-based Bidirectional GRU hybrid model for identifying
inappropriate content in Urdu Unicode text language. Four different baseline
deep learning models; LSTM, Bi-LSTM, GRU, and TCN, are used to compare the
performance of the proposed model. The results of these models were compared
based on evaluation metrics, dataset size, and impact of the word embedding
layer. The pre-trained Urdu word2Vec embeddings were utilized for our case. Our
proposed model BiGRU-A outperformed all other baseline models by yielding 84\%
accuracy without using pre-trained word2Vec layer. From our experiments, we
have established that the attention layer improves the model's efficiency, and
pre-trained word2Vec embedding does not work well with an inappropriate content
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Continual Forgetting for <span class="highlight-title">Pre-train</span>ed Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For privacy and security concerns, the need to erase unwanted information
from pre-trained vision models is becoming evident nowadays. In real-world
scenarios, erasure requests originate at any time from both users and model
owners, and these requests usually form a sequence. Therefore, under such a
setting, selective information is expected to be continuously removed from a
pre-trained model while maintaining the rest. We define this problem as
continual forgetting and identify three key challenges. (i) For unwanted
knowledge, efficient and effective deleting is crucial. (ii) For remaining
knowledge, the impact brought by the forgetting procedure should be minimal.
(iii) In real-world scenarios, the training samples may be scarce or partially
missing during the process of forgetting. To address them, we first propose
Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA
modules to fine-tune the FFN layers in Transformer blocks for each forgetting
task independently, and towards (ii), a simple group sparse regularization is
adopted, enabling automatic selection of specific LoRA groups and zeroing out
the others. To further extend GS-LoRA to more practical scenarios, we
incorporate prototype information as additional supervision and introduce a
more practical approach, GS-LoRA++. For each forgotten class, we move the
logits away from its original prototype. For the remaining classes, we pull the
logits closer to their respective prototypes. We conduct extensive experiments
on face recognition, object detection and image classification and demonstrate
that our method manages to forget specific classes with minimal impact on other
classes. Codes have been released on https://github.com/bjzhb666/GS-LoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cueless EEG imagined speech for subject identification: <span class="highlight-title">dataset</span> and
  benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Derakhshesh, Zahra Dehghanian, Reza Ebrahimpour, Hamid R. Rabiee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalogram (EEG) signals have emerged as a promising modality for
biometric identification. While previous studies have explored the use of
imagined speech with semantically meaningful words for subject identification,
most have relied on additional visual or auditory cues. In this study, we
introduce a cueless EEG-based imagined speech paradigm, where subjects imagine
the pronunciation of semantically meaningful words without any external cues.
This innovative approach addresses the limitations of prior methods by
requiring subjects to select and imagine words from a predefined list
naturally. The dataset comprises over 4,350 trials from 11 subjects across five
sessions. We assess a variety of classification methods, including traditional
machine learning techniques such as Support Vector Machines (SVM) and XGBoost,
as well as time-series foundation models and deep learning architectures
specifically designed for EEG classification, such as EEG Conformer and Shallow
ConvNet. A session-based hold-out validation strategy was employed to ensure
reliable evaluation and prevent data leakage. Our results demonstrate
outstanding classification accuracy, reaching 97.93%. These findings highlight
the potential of cueless EEG paradigms for secure and reliable subject
identification in real-world applications, such as brain-computer interfaces
(BCIs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Near-optimal Algorithm for Learning Margin Halfspaces with Massart
  Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Diakonikolas, Nikos Zarifis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of PAC learning $\gamma$-margin halfspaces in the
presence of Massart noise. Without computational considerations, the sample
complexity of this learning problem is known to be
$\widetilde{\Theta}(1/(\gamma^2 \epsilon))$. Prior computationally efficient
algorithms for the problem incur sample complexity $\tilde{O}(1/(\gamma^4
\epsilon^3))$ and achieve 0-1 error of $\eta+\epsilon$, where $\eta<1/2$ is the
upper bound on the noise rate. Recent work gave evidence of an
information-computation tradeoff, suggesting that a quadratic dependence on
$1/\epsilon$ is required for computationally efficient algorithms. Our main
result is a computationally efficient learner with sample complexity
$\widetilde{\Theta}(1/(\gamma^2 \epsilon^2))$, nearly matching this lower
bound. In addition, our algorithm is simple and practical, relying on online
SGD on a carefully selected sequence of convex losses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer
  Depression Detection <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaee Cheong, Aditya Bangar, Sinan Kalkan, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning bias in mental health is becoming an increasingly pertinent
challenge. Despite promising efforts indicating that multitask approaches often
work better than unitask approaches, there is minimal work investigating the
impact of multitask learning on performance and fairness in depression
detection nor leveraged it to achieve fairer prediction outcomes. In this work,
we undertake a systematic investigation of using a multitask approach to
improve performance and fairness for depression detection. We propose a novel
gender-based task-reweighting method using uncertainty grounded in how the
PHQ-8 questionnaire is structured. Our results indicate that, although a
multitask approach improves performance and fairness compared to a unitask
approach, the results are not always consistent and we see evidence of negative
transfer and a reduction in the Pareto frontier, which is concerning given the
high-stake healthcare setting. Our proposed approach of gender-based
reweighting with uncertainty improves performance and fairness and alleviates
both challenges to a certain extent. Our findings on each PHQ-8 subitem task
difficulty are also in agreement with the largest study conducted on the PHQ-8
subitem discrimination capacity, thus providing the very first tangible
evidence linking ML findings with large-scale empirical population studies
conducted on the PHQ-8.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the Proceedings of Machine Learning Research 259, 1-14,
  2024 as part of the Machine Learning for Health (ML4H) Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward-Guided Controlled Generation for Inference-Time Alignment in
  Diffusion Models: Tutorial and <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, Tommaso Biancalani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This tutorial provides an in-depth guide on inference-time guidance and
alignment methods for optimizing downstream reward functions in diffusion
models. While diffusion models are renowned for their generative modeling
capabilities, practical applications in fields such as biology often require
sample generation that maximizes specific metrics (e.g., stability, affinity in
proteins, closeness to target structures). In these scenarios, diffusion models
can be adapted not only to generate realistic samples but also to explicitly
maximize desired measures at inference time without fine-tuning. This tutorial
explores the foundational aspects of such inference-time algorithms. We review
these methods from a unified perspective, demonstrating that current techniques
-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,
and classifier guidance -- aim to approximate soft optimal denoising processes
(a.k.a. policies in RL) that combine pre-trained denoising processes with value
functions serving as look-ahead functions that predict from intermediate states
to terminal rewards. Within this framework, we present several novel algorithms
not yet covered in the literature. Furthermore, we discuss (1) fine-tuning
methods combined with inference-time techniques, (2) inference-time algorithms
based on search algorithms such as Monte Carlo tree search, which have received
limited attention in current research, and (3) connections between
inference-time algorithms in language models and diffusion models. The code of
this tutorial on protein design is available at
https://github.com/masa-ue/AlignInversePro
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We plan to add more content/codes. Please let us know if there are
  any comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rough kernel hedging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Muca Cirone, Cristopher Salvi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building on the functional-analytic framework of operator-valued kernels and
un-truncated signature kernels, we propose a scalable, provably convergent
signature-based algorithm for a broad class of high-dimensional, path-dependent
hedging problems. We make minimal assumptions about market dynamics by
modelling them as general geometric rough paths, yielding a fully model-free
approach. Furthermore, through a representer theorem, we provide theoretical
guarantees on the existence and uniqueness of a global minimum for the
resulting optimization problem and derive an analytic solution under highly
general loss functions. Similar to the popular deep hedging approach, but in a
more rigorous fashion, our method can also incorporate additional features via
the underlying operator-valued kernel, such as trading signals, news analytics,
and past hedging decisions, closely aligning with true machine-learning
practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fokker-Planck to Callan-Symanzik: evolution of weight matrices under
  training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Bu, Uri Kol, Ziming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamical evolution of a neural network during training has been an
incredibly fascinating subject of study. First principal derivation of generic
evolution of variables in statistical physics systems has proved useful when
used to describe training dynamics conceptually, which in practice means
numerically solving equations such as Fokker-Planck equation. Simulating entire
networks inevitably runs into the curse of dimensionality. In this paper, we
utilize Fokker-Planck to simulate the probability density evolution of
individual weight matrices in the bottleneck layers of a simple
2-bottleneck-layered auto-encoder and compare the theoretical evolutions
against the empirical ones by examining the output data distributions. We also
derive physically relevant partial differential equations such as
Callan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation
we have.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Research in Large Language Models for Electronic Design
  Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Pan, Guanglei Zhou, Chen-Chia Chang, Isaac Jacobson, Jiang Hu, Yiran Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the rapidly evolving domain of Electronic Design Automation (EDA),
Large Language Models (LLMs) have emerged as transformative technologies,
offering unprecedented capabilities for optimizing and automating various
aspects of electronic design. This survey provides a comprehensive exploration
of LLM applications in EDA, focusing on advancements in model architectures,
the implications of varying model sizes, and innovative customization
techniques that enable tailored analytical insights. By examining the
intersection of LLM capabilities and EDA requirements, the paper highlights the
significant impact these models have on extracting nuanced understandings from
complex datasets. Furthermore, it addresses the challenges and opportunities in
integrating LLMs into EDA workflows, paving the way for future research and
application in this dynamic field. Through this detailed analysis, the survey
aims to offer valuable insights to professionals in the EDA industry, AI
researchers, and anyone interested in the convergence of advanced AI
technologies and electronic design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 2 figures, 3 tables, accepted by TODAES</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Ming Liu, Ming-Chih Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning and large language models (LLMs) have
facilitated the deployment of the mixture-of-experts (MoE) mechanism in the
stock investment domain. While these models have demonstrated promising trading
performance, they are often unimodal, neglecting the wealth of information
available in other modalities, such as textual data. Moreover, the traditional
neural network-based router selection mechanism fails to consider contextual
and real-world nuances, resulting in suboptimal expert selection. To address
these limitations, we propose LLMoE, a novel framework that employs LLMs as the
router within the MoE architecture. Specifically, we replace the conventional
neural network-based router with LLMs, leveraging their extensive world
knowledge and reasoning capabilities to select experts based on historical
price data and stock news. This approach provides a more effective and
interpretable selection mechanism. Our experiments on multimodal real-world
stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models
and other deep neural network approaches. Additionally, the flexible
architecture of LLMoE allows for easy adaptation to various downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging
  Innovations in Finance, Social Media, and Crime Prevention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Large Language Models in Wireless Communication: A Novel
  <span class="highlight-title">Dataset</span> and Fine-Tuning Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushen Lin, Ruichen Zhang, Wenqi Huang, Kaidi Wang, Zhiguo Ding, Daniel K. C. So, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we develop a specialized dataset aimed at enhancing the
evaluation and fine-tuning of large language models (LLMs) specifically for
wireless communication applications. The dataset includes a diverse set of
multi-hop questions, including true/false and multiple-choice types, spanning
varying difficulty levels from easy to hard. By utilizing advanced language
models for entity extraction and question generation, rigorous data curation
processes are employed to maintain high quality and relevance. Additionally, we
introduce a Pointwise V-Information (PVI) based fine-tuning method, providing a
detailed theoretical analysis and justification for its use in quantifying the
information content of training data with 2.24\% and 1.31\% performance boost
for different models compared to baselines, respectively. To demonstrate the
effectiveness of the fine-tuned models with the proposed methodologies on
practical tasks, we also consider different tasks, including summarizing
optimization problems from technical papers and solving the mathematical
problems related to non-orthogonal multiple access (NOMA), which are generated
by using the proposed multi-agent framework. Simulation results show
significant performance gain in summarization tasks with 20.9\% in the ROUGE-L
metrics. We also study the scaling laws of fine-tuning LLMs and the challenges
LLMs face in the field of wireless communications, offering insights into their
adaptation to wireless communication tasks. This dataset and fine-tuning
methodology aim to enhance the training and evaluation of LLMs, contributing to
advancements in LLMs for wireless communication research and applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figure, journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weight for Robustness: A Comprehensive Approach towards Optimal
  Fault-Tolerant Asynchronous ML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tehila Dahan, Kfir Y. Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenges of Byzantine-robust training in asynchronous
distributed machine learning systems, aiming to enhance efficiency amid massive
parallelization and heterogeneous computing resources. Asynchronous systems,
marked by independently operating workers and intermittent updates, uniquely
struggle with maintaining integrity against Byzantine failures, which encompass
malicious or erroneous actions that disrupt learning. The inherent delays in
such settings not only introduce additional bias to the system but also obscure
the disruptions caused by Byzantine faults. To tackle these issues, we adapt
the Byzantine framework to asynchronous dynamics by introducing a novel
weighted robust aggregation framework. This allows for the extension of robust
aggregators and a recent meta-aggregator to their weighted versions, mitigating
the effects of delayed updates. By further incorporating a recent
variance-reduction technique, we achieve an optimal convergence rate for the
first time in an asynchronous Byzantine environment. Our methodology is
rigorously validated through empirical and theoretical analysis, demonstrating
its effectiveness in enhancing fault tolerance and optimizing performance in
asynchronous ML systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have demonstrated significant
progress in performing complex tasks. While Reinforcement Learning from Human
Feedback (RLHF) has been effective in aligning LLMs with human preferences, it
is susceptible to spurious correlations in reward modeling. Consequently, it
often introduces biases-such as length bias, sycophancy, conceptual bias, and
discrimination that hinder the model's ability to capture true causal
relationships. To address this, we propose a novel causal reward modeling
approach that integrates causal inference to mitigate these spurious
correlations. Our method enforces counterfactual invariance, ensuring reward
predictions remain consistent when irrelevant variables are altered. Through
experiments on both synthetic and real-world datasets, we show that our
approach mitigates various types of spurious correlations effectively,
resulting in more reliable and fair alignment of LLMs with human preferences.
As a drop-in enhancement to the existing RLHF workflow, our causal reward
modeling provides a practical way to improve the trustworthiness and fairness
of LLM finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARMAX identification of low rank <span class="highlight-title">graph</span>ical models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Cao, Aming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large-scale systems, complex internal relationships are often present.
Such interconnected systems can be effectively described by low rank stochastic
processes. When identifying a predictive model of low rank processes from
sampling data, the rank-deficient property of spectral densities is often
obscured by the inevitable measurement noise in practice. However, existing low
rank identification approaches often did not take noise into explicit
consideration, leading to non-negligible inaccuracies even under weak noise. In
this paper, we address the identification issue of low rank processes under
measurement noise. We find that the noisy measurement model admits a sparse
plus low rank structure in latent-variable graphical models. Specifically, we
first decompose the problem into a maximum entropy covariance extension
problem, and a low rank graphical estimation problem based on an autoregressive
moving-average with exogenous input (ARMAX) model. To identify the ARMAX low
rank graphical models, we propose an estimation approach based on maximum
likelihood. The identifiability and consistency of this approach are proven
under certain conditions. Simulation results confirm the reliable performance
of the entire algorithm in both the parameter estimation and noisy data
filtering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVaDE : Event-Based Variational Thompson Sampling for Model-Based
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Aravindan, Dixant Mittal, Wee Sun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Posterior Sampling for Reinforcement Learning (PSRL) is a well-known
algorithm that augments model-based reinforcement learning (MBRL) algorithms
with Thompson sampling. PSRL maintains posterior distributions of the
environment transition dynamics and the reward function, which are intractable
for tasks with high-dimensional state and action spaces. Recent works show that
dropout, used in conjunction with neural networks, induces variational
distributions that can approximate these posteriors. In this paper, we propose
Event-based Variational Distributions for Exploration (EVaDE), which are
variational distributions that are useful for MBRL, especially when the
underlying domain is object-based. We leverage the general domain knowledge of
object-based domains to design three types of event-based convolutional layers
to direct exploration. These layers rely on Gaussian dropouts and are inserted
between the layers of the deep neural network model to help facilitate
variational Thompson sampling. We empirically show the effectiveness of
EVaDE-equipped Simulated Policy Learning (EVaDE-SimPLe) on the 100K Atari game
suite.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor
  Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal
  Manipulation Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitul Goswami, Romit Chatterjee, Somnath Mahato, Prasant Kumar Pattnaik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research presents a study on enhancing the robustness of Wi-Fi-based
indoor positioning systems against adversarial attacks. The goal is to improve
the positioning accuracy and resilience of these systems under two attack
scenarios: Wi-Fi Spoofing and Signal Strength Manipulation. Three models are
developed and evaluated: a baseline model (M_Base), an adversarially trained
robust model (M_Rob), and an ensemble model (M_Ens). All models utilize a
Kolmogorov-Arnold Network (KAN) architecture. The robust model is trained with
adversarially perturbed data, while the ensemble model combines predictions
from both the base and robust models. Experimental results show that the robust
model reduces positioning error by approximately 10% compared to the baseline,
achieving 2.03 meters error under Wi-Fi spoofing and 2.00 meters under signal
strength manipulation. The ensemble model further outperforms with errors of
2.01 meters and 1.975 meters for the respective attack types. This analysis
highlights the effectiveness of adversarial training techniques in mitigating
attack impacts. The findings underscore the importance of considering
adversarial scenarios in developing indoor positioning systems, as improved
resilience can significantly enhance the accuracy and reliability of such
systems in mission-critical environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing the Sensitivity of Neural Physics Simulators to Mesh Topology
  via <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Vaska, Justin Goodwin, Robin Walters, Rajmonda S. Caceres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meshes are used to represent complex objects in high fidelity physics
simulators across a variety of domains, such as radar sensing and aerodynamics.
There is growing interest in using neural networks to accelerate physics
simulations, and also a growing body of work on applying neural networks
directly to irregular mesh data. Since multiple mesh topologies can represent
the same object, mesh augmentation is typically required to handle topological
variation when training neural networks. Due to the sensitivity of physics
simulators to small changes in mesh shape, it is challenging to use these
augmentations when training neural network-based physics simulators. In this
work, we show that variations in mesh topology can significantly reduce the
performance of neural network simulators. We evaluate whether pretraining can
be used to address this issue, and find that employing an established
autoencoder pretraining technique with graph embedding models reduces the
sensitivity of neural network simulators to variations in mesh topology.
Finally, we highlight future research directions that may further reduce neural
simulator sensitivity to mesh topology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IFRA: a machine learning-based Instrumented Fall Risk Assessment Scale
  derived from Instrumented Timed Up and Go test in stroke patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Macciò, Alessandro Carfì, Alessio Capitanelli, Peppino Tropea, Massimo Corbo, Fulvio Mastrogiovanni, Michela Picardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective fall risk assessment is critical for post-stroke patients. The
present study proposes a novel, data-informed fall risk assessment method based
on the instrumented Timed Up and Go (ITUG) test data, bringing in many mobility
measures that traditional clinical scales fail to capture. IFRA, which stands
for Instrumented Fall Risk Assessment, has been developed using a two-step
process: first, features with the highest predictive power among those
collected in a ITUG test have been identified using machine learning
techniques; then, a strategy is proposed to stratify patients into low, medium,
or high-risk strata. The dataset used in our analysis consists of 142
participants, out of which 93 were used for training (15 synthetically
generated), 17 for validation and 32 to test the resulting IFRA scale (22
non-fallers and 10 fallers). Features considered in the IFRA scale include gait
speed, vertical acceleration during sit-to-walk transition, and turning angular
velocity, which align well with established literature on the risk of fall in
neurological patients. In a comparison with traditional clinical scales such as
the traditional Timed Up & Go and the Mini-BESTest, IFRA demonstrates
competitive performance, being the only scale to correctly assign more than
half of the fallers to the high-risk stratum (Fischer's Exact test p = 0.004).
Despite the dataset's limited size, this is the first proof-of-concept study to
pave the way for future evidence regarding the use of IFRA tool for continuous
patient monitoring and fall prevention both in clinical stroke rehabilitation
and at home post-discharge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 2 figures, submitted for review dec 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metrics for Inter-<span class="highlight-title">Dataset</span> Similarity with Example Applications in
  Synthetic Data and Feature Selection Evaluation -- Extended Version <span class="chip">SDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Rajabinasab, Anton D. Lautrup, Arthur Zimek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring inter-dataset similarity is an important task in machine learning
and data mining with various use cases and applications. Existing methods for
measuring inter-dataset similarity are computationally expensive, limited, or
sensitive to different entities and non-trivial choices for parameters. They
also lack a holistic perspective on the entire dataset. In this paper, we
propose two novel metrics for measuring inter-dataset similarity. We discuss
the mathematical foundation and the theoretical basis of our proposed metrics.
We demonstrate the effectiveness of the proposed metrics by investigating two
applications in the evaluation of synthetic data and in the evaluation of
feature selection methods. The theoretical and empirical studies conducted in
this paper illustrate the effectiveness of the proposed metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the extended version of a paper accepted at 2025 SIAM
  International Conference on Data Mining (SDM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atleus: Accelerating Transformers on the Edge Enabled by 3D
  Heterogeneous Manycore Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyush Dhingra, Janardhan Rao Doppa, Partha Pratim Pande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer architectures have become the standard neural network model for
various machine learning applications including natural language processing and
computer vision. However, the compute and memory requirements introduced by
transformer models make them challenging to adopt for edge applications.
Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is
a common task to enhance the model's predictive performance on specific
tasks/applications. Existing transformer accelerators are oblivious to
complexities introduced by fine-tuning. In this paper, we propose the design of
a three-dimensional (3D) heterogeneous architecture referred to as Atleus that
incorporates heterogeneous computing resources specifically optimized to
accelerate transformer models for the dual purposes of fine-tuning and
inference. Specifically, Atleus utilizes non-volatile memory and systolic array
for accelerating transformer computational kernels using an integrated 3D
platform. Moreover, we design a suitable NoC to achieve high performance and
energy efficiency. Finally, Atleus adopts an effective quantization scheme to
support model compression. Experimental results demonstrate that Atleus
outperforms existing state-of-the-art by up to 56x and 64.5x in terms of
performance and energy efficiency respectively
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication in IEEE Transactions on Computer-Aided
  Design of Integrated Circuits and Systems (TCAD)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential PatchCore: Anomaly Detection for Surface Inspection using
  Synthetic Impurities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runzhou Mao, Juraj Fulir, Christoph Garth, Petra Gospodnetić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The appearance of surface impurities (e.g., water stains, fingerprints,
stickers) is an often-mentioned issue that causes degradation of automated
visual inspection systems. At the same time, synthetic data generation
techniques for visual surface inspection have focused primarily on generating
perfect examples and defects, disregarding impurities. This study highlights
the importance of considering impurities when generating synthetic data. We
introduce a procedural method to include photorealistic water stains in
synthetic data. The synthetic datasets are generated to correspond to real
datasets and are further used to train an anomaly detection model and
investigate the influence of water stains. The high-resolution images used for
surface inspection lead to memory bottlenecks during anomaly detection
training. To address this, we introduce Sequential PatchCore - a method to
build coresets sequentially and make training on large images using
consumer-grade hardware tractable. This allows us to perform transfer learning
using coresets pre-trained on different dataset versions. Our results show the
benefits of using synthetic data for pre-training an explicit coreset anomaly
model and the extended performance benefits of finetuning the coreset using
real data. We observed how the impurities and labelling ambiguity lower the
model performance and have additionally reported the defect-wise recall to
provide an industrially relevant perspective on model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Spectral Convergence of Locally Linear Embedding on Manifolds
  with Boundary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lyons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the eigenvalues and eigenfunctions of a differential operator that
governs the asymptotic behavior of the unsupervised learning algorithm known as
Locally Linear Embedding when a large data set is sampled from an interval or
disc. In particular, the differential operator is of second order, mixed-type,
and degenerates near the boundary. We show that a natural regularity condition
on the eigenfunctions imposes a consistent boundary condition and use the
Frobenius method to estimate pointwise behavior. We then determine the limiting
sequence of eigenvalues analytically and compare them to numerical predictions.
Finally, we propose a variational framework for determining eigenvalues on
other compact manifolds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 7 figures; the author welcomes all comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MatrixNet: Learning over symmetry groups using learned group
  representations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Laird, Circe Hsu, Asilata Bapat, Robin Walters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group theory has been used in machine learning to provide a theoretically
grounded approach for incorporating known symmetry transformations in tasks
from robotics to protein modeling. In these applications, equivariant neural
networks use known symmetry groups with predefined representations to learn
over geometric input data. We propose MatrixNet, a neural network architecture
that learns matrix representations of group element inputs instead of using
predefined representations. MatrixNet achieves higher sample efficiency and
generalization over several standard baselines in prediction tasks over the
several finite groups and the Artin braid group. We also show that MatrixNet
respects group relations allowing generalization to group elements of greater
word length than in the training set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overshoot: Taking advantage of future gradients in momentum-based
  stochastic optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Kopal, Michal Gregor, Santiago de Leon-Martinez, Jakub Simko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overshoot is a novel, momentum-based stochastic gradient descent optimization
method designed to enhance performance beyond standard and Nesterov's momentum.
In conventional momentum methods, gradients from previous steps are aggregated
with the gradient at current model weights before taking a step and updating
the model. Rather than calculating gradient at the current model weights,
Overshoot calculates the gradient at model weights shifted in the direction of
the current momentum. This sacrifices the immediate benefit of using the
gradient w.r.t. the exact model weights now, in favor of evaluating at a point,
which will likely be more relevant for future updates. We show that
incorporating this principle into momentum-based optimizers (SGD with momentum
and Adam) results in faster convergence (saving on average at least 15% of
steps). Overshoot consistently outperforms both standard and Nesterov's
momentum across a wide range of tasks and integrates into popular
momentum-based optimizers with zero memory and small computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intra-day Solar and Power Forecast for Optimization of Intraday Market
  Participation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nelson Salazar-Peña, Adolfo Palma-Vergara, Mateo Montes, María Alejandra Vargas-Torres, Adriana Salinas, Andrés Velasco, Alejandra Tabares, Andrés González-Mancera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prediction of solar irradiance enhances reliability in photovoltaic (PV)
solar plant generation and grid integration. In Colombia, PV plants face
penalties if energy production deviates beyond governmental thresholds from
intraday market offers. This research employs Long Short-Term Memory (LSTM) and
Bidirectional-LSTM (Bi-LSTM) models, utilizing meteorological data from a PV
plant in El Paso, Cesar, Colombia, to predict solar irradiance with a 6-hour
horizon and 10-minute resolution. While Bi-LSTM showed superior performance,
the LSTM model achieved comparable results with significantly reduced training
time (6 hours versus 18 hours), making it computationally advantageous. The
LSTM predictions were averaged to create an hourly resolution model, evaluated
using Mean Absolute Error, Root-Mean-Square Error, Normalized Root-Mean-Square
Error, and Mean Absolute Percentage Error metrics. Comparison with the Global
Forecast System (GFS) revealed similar performance, with both models
effectively capturing daily solar irradiance patterns. The forecast model
integrates with an Object-Oriented power production model, enabling accurate
energy offers in the intraday market while minimizing penalty costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 37 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOGNET: A Mux-residual quantized Network leveraging Online-Generated
  weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Thien Nguyen, William Guicquero, Gilles Sicard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a compact model architecture called MOGNET, compatible
with a resource-limited hardware. MOGNET uses a streamlined Convolutional
factorization block based on a combination of 2 point-wise (1x1) convolutions
with a group-wise convolution in-between. To further limit the overall model
size and reduce the on-chip required memory, the second point-wise
convolution's parameters are on-line generated by a Cellular Automaton
structure. In addition, MOGNET enables the use of low-precision weights and
activations, by taking advantage of a Multiplexer mechanism with a proper
Bitshift rescaling for integrating residual paths without increasing the
hardware-related complexity. To efficiently train this model we also introduce
a novel weight ternarization method favoring the balance between quantized
levels. Experimental results show that given tiny memory budget (sub-2Mb),
MOGNET can achieve higher accuracy with a clear gap up to 1% at a similar or
even lower model size compared to recent state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IEEE AICAS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence Estimation for Error Detection in Text-to-SQL Systems <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Somov, Elena Tutubalina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL enables users to interact with databases through natural
language, simplifying the retrieval and synthesis of information. Despite the
success of large language models (LLMs) in converting natural language
questions into SQL queries, their broader adoption is limited by two main
challenges: achieving robust generalization across diverse queries and ensuring
interpretative confidence in their predictions. To tackle these issues, our
research investigates the integration of selective classifiers into Text-to-SQL
systems. We analyse the trade-off between coverage and risk using entropy based
confidence estimation with selective classifiers and assess its impact on the
overall performance of Text-to-SQL models. Additionally, we explore the models'
initial calibration and improve it with calibration techniques for better model
alignment between confidence and accuracy. Our experimental results show that
encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and
decoder-only Llama 3, thus the designated external entropy-based selective
classifier has better performance. The study also reveal that, in terms of
error detection, selective classifier with a higher probability detects errors
associated with irrelevant questions rather than incorrect query generations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures, to be published in AAAI 2025 Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class Incremental Fault Diagnosis under Limited Fault Data via
  Supervised Contrastive Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanrong Zhang, Yifei Yao, Zixuan Wang, Jiayuan Su, Mengxuan Li, Peng Peng, Hongwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental fault diagnosis requires a model to adapt to new fault
classes while retaining previous knowledge. However, limited research exists
for imbalanced and long-tailed data. Extracting discriminative features from
few-shot fault data is challenging, and adding new fault classes often demands
costly model retraining. Moreover, incremental training of existing methods
risks catastrophic forgetting, and severe class imbalance can bias the model's
decisions toward normal classes. To tackle these issues, we introduce a
Supervised Contrastive knowledge distiLlation for class Incremental Fault
Diagnosis (SCLIFD) framework proposing supervised contrastive knowledge
distillation for improved representation learning capability and less
forgetting, a novel prioritized exemplar selection method for sample replay to
alleviate catastrophic forgetting, and the Random Forest Classifier to address
the class imbalance. Extensive experimentation on simulated and real-world
industrial datasets across various imbalance ratios demonstrates the
superiority of SCLIFD over existing approaches. Our code can be found at
https://github.com/Zhang-Henry/SCLIFD_TII.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Merging</span> Models on the Fly Without Retraining: A Sequential Approach to
  Scalable Continual Model <span class="highlight-title">Merging</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anke Tang, Enneng Yang, Li Shen, Yong Luo, Han Hu, Bo Du, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep model merging represents an emerging research direction that combines
multiple fine-tuned models to harness their specialized capabilities across
different tasks and domains. Current model merging techniques focus on merging
all available models simultaneously, with weight interpolation-based methods
being the predominant approaches. However, these conventional approaches are
not well-suited for scenarios where models become available sequentially, and
they often suffer from high memory requirements and potential interference
between tasks. In this study, we propose a training-free projection-based
continual merging method that processes models sequentially through orthogonal
projections of weight matrices and adaptive scaling mechanisms. Our method
operates by projecting new parameter updates onto subspaces orthogonal to
existing merged parameter updates while using an adaptive scaling mechanism to
maintain stable parameter distances, enabling efficient sequential integration
of task-specific knowledge. Our approach maintains constant memory complexity
to the number of models, minimizes interference between tasks through
orthogonal projections, and retains the performance of previously merged models
through adaptive task vector scaling. Extensive experiments on CLIP-ViT models
demonstrate that our method achieves a 5-8% average accuracy improvement while
maintaining robust performance in different task orderings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-task deep-learning for sleep event detection and stage
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adriana Anido-Alonso, Diego Alvarez-Estevez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polysomnographic sleep analysis is the standard clinical method to accurately
diagnose and treat sleep disorders. It is an intricate process which involves
the manual identification, classification, and location of multiple sleep event
patterns. This is complex, for which identification of different types of
events involves focusing on different subsets of signals, resulting on an
iterative time-consuming process entailing several visual analysis passes. In
this paper we propose a multi-task deep-learning approach for the simultaneous
detection of sleep events and hypnogram construction in one single pass. Taking
as reference state-of-the-art methodology for object-detection in the field of
Computer Vision, we reformulate the problem for the analysis of multi-variate
time sequences, and more specifically for pattern detection in the sleep
analysis scenario. We investigate the performance of the resulting method in
identifying different assembly combinations of EEG arousals, respiratory events
(apneas and hypopneas) and sleep stages, also considering different input
signal montage configurations. Furthermore, we evaluate our approach using two
independent datasets, assessing true-generalization effects involving local and
external validation scenarios. Based on our results, we analyze and discuss our
method's capabilities and its potential wide-range applicability across
different settings and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIER: A Novel Metric for Evaluating What Matters in Code-Switching <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enes Yavuz Ugan, Ngoc-Quan Pham, Leonard Bärmann, Alex Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching, the alternation of languages within a single discourse,
presents a significant challenge for Automatic Speech Recognition. Despite the
unique nature of the task, performance is commonly measured with established
metrics such as Word-Error-Rate (WER). However, in this paper, we question
whether these general metrics accurately assess performance on code-switching.
Specifically, using both Connectionist-Temporal-Classification and
Encoder-Decoder models, we show fine-tuning on non-code-switched data from both
matrix and embedded language improves classical metrics on code-switching test
sets, although actual code-switched words worsen (as expected). Therefore, we
propose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only
on specific words of interest. We instantiate PIER on code-switched utterances
and show that this more accurately describes the code-switching performance,
showing huge room for improvement in future work. This focused evaluation
allows for a more precise assessment of model performance, particularly in
challenging aspects such as inter-word and intra-word code-switching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Marvels of Deep Learning in Medical Diagnosis: A
  Comprehensive <span class="highlight-title">Review</span> of COVID-19 Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Shofiqul Islama, Khondokar Fida Hasanc, Hasibul Hossain Shajeebd, Humayan Kabir Ranae, Md Saifur Rahmand, Md Munirul Hasanb, AKM Azadf, Ibrahim Abdullahg, Mohammad Ali Moni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a comprehensive review of the potential of multimodal
deep learning (DL) in medical diagnosis, using COVID-19 as a case example.
Motivated by the success of artificial intelligence applications during the
COVID-19 pandemic, this research aims to uncover the capabilities of DL in
disease screening, prediction, and classification, and to derive insights that
enhance the resilience, sustainability, and inclusiveness of science,
technology, and innovation systems. Adopting a systematic approach, we
investigate the fundamental methodologies, data sources, preprocessing steps,
and challenges encountered in various studies and implementations. We explore
the architecture of deep learning models, emphasising their data-specific
structures and underlying algorithms. Subsequently, we compare different deep
learning strategies utilised in COVID-19 analysis, evaluating them based on
methodology, data, performance, and prerequisites for future research. By
examining diverse data types and diagnostic modalities, this research
contributes to scientific understanding and knowledge of the multimodal
application of DL and its effectiveness in diagnosis. We have implemented and
analysed 11 deep learning models using COVID-19 image, text, and speech (ie,
cough) data. Our analysis revealed that the MobileNet model achieved the
highest accuracy of 99.97% for COVID-19 image data and 93.73% for speech data
(i.e., cough). However, the BiGRU model demonstrated superior performance in
COVID-19 text classification with an accuracy of 99.89%. The broader
implications of this research suggest potential benefits for other domains and
disciplines that could leverage deep learning techniques for image, text, and
speech analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonoSOWA: Scalable monocular 3D Object detector Without human
  Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Skvrna, Lukas Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting the three-dimensional position and orientation of objects using a
single RGB camera is a foundational task in computer vision with many important
applications. Traditionally, 3D object detection methods are trained in a
fully-supervised setup, requiring vast amounts of human annotations, which are
laborious, costly, and do not scale well with the ever-increasing amounts of
data being captured.
  In this paper, we present the first method to train 3D object detectors for
monocular RGB cameras without domain-specific human annotations, thus making
orders of magnitude more data available for training. Thanks to newly proposed
Canonical Object Space, the method can not only exploit data across a variety
of datasets and camera setups to train a single 3D detector, but unlike
previous work it also works out of the box in previously unseen camera setups.
All this is crucial for practical applications, where the data and cameras are
extremely heterogeneous.
  The method is evaluated on two standard autonomous driving datasets, where it
outperforms previous works, which, unlike our method, still rely on 2D human
annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing AI Language Models to Identify Prognostic Factors for Coronary
  Artery Disease: A Study in Mashhad Residents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bami Zahra, Behnampour Nasser, Doosti Hassan, Ghayour Mobarhan Majid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract: Background: Understanding cardiovascular artery disease risk
factors, the leading global cause of mortality, is crucial for influencing its
etiology, prevalence, and treatment. This study aims to evaluate prognostic
markers for coronary artery disease in Mashhad using Naive Bayes, REP Tree,
J48, CART, and CHAID algorithms. Methods:
  Using data from the 2009 MASHAD STUDY, prognostic factors for coronary artery
disease were determined with Naive Bayes, REP Tree, J48, CART, CHAID, and
Random Forest algorithms using R 3.5.3 and WEKA 3.9.4. Model efficiency was
compared by sensitivity, specificity, and accuracy. Cases were patients with
coronary artery disease; each had three controls (totally 940). Results:
Prognostic factors for coronary artery disease in Mashhad residents varied by
algorithm. CHAID identified age, myocardial infarction history, and
hypertension. CART included depression score and physical activity. REP added
education level and anxiety score. NB included diabetes and family history. J48
highlighted father's heart disease and weight loss. CHAID had the highest
accuracy (0.80).
  Conclusion:
  Key prognostic factors for coronary artery disease in CART and CHAID models
include age, myocardial infarction history, hypertension, depression score,
physical activity, and BMI. NB, REP Tree, and J48 identified numerous factors.
CHAID had the highest accuracy, sensitivity, and specificity. CART offers
simpler interpretation, aiding physician and paramedic model selection based on
specific. Keywords: RF, Na\"ive Bayes, REP, J48 algorithms, Coronary Artery
Disease (CAD).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Air Temperature from Volumetric Urban Morphology with Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berk Kıvılcım, Patrick Erik Bradley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we firstly introduce a method that converts CityGML data into
voxels which works efficiently and fast in high resolution for large scale
datasets such as cities but by sacrificing some building details to overcome
the limitations of previous voxelization methodologies that have been
computationally intensive and inefficient at transforming large-scale urban
areas into voxel representations for high resolution. Those voxelized 3D city
data from multiple cities and corresponding air temperature data are used to
develop a machine learning model. Before the model training, Gaussian blurring
is implemented on input data to consider spatial relationships, as a result the
correlation rate between air temperature and volumetric building morphology is
also increased after the Gaussian blurring. After the model training, the
prediction results are not just evaluated with Mean Square Error (MSE) but some
image similarity metrics such as Structural Similarity Index Measure (SSIM) and
Learned Perceptual Image Patch Similarity (LPIPS) that are able to detect and
consider spatial relations during the evaluation process. This trained model is
capable of predicting the spatial distribution of air temperature by using
building volume information of corresponding pixel as input. By doing so, this
research aims to assist urban planners in incorporating environmental
parameters into their planning strategies, thereby facilitating more
sustainable and inhabitable urban environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pruning for Sparse Diffusion Models based on Gradient Flow <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Wan, Tianyi Zheng, Zhaoyu Chen, Yuxiao Wang, Jia Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DMs) have impressive capabilities among generation models,
but are limited to slower inference speeds and higher computational costs.
Previous works utilize one-shot structure pruning to derive lightweight DMs
from pre-trained ones, but this approach often leads to a significant drop in
generation quality and may result in the removal of crucial weights. Thus we
propose a iterative pruning method based on gradient flow, including the
gradient flow pruning process and the gradient flow pruning criterion. We
employ a progressive soft pruning strategy to maintain the continuity of the
mask matrix and guide it along the gradient flow of the energy function based
on the pruning criterion in sparse space, thereby avoiding the sudden
information loss typically caused by one-shot pruning. Gradient-flow based
criterion prune parameters whose removal increases the gradient norm of loss
function and can enable fast convergence for a pruned model in iterative
pruning stage. Our extensive experiments on widely used datasets demonstrate
that our method achieves superior performance in efficiency and consistency
with pre-trained models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, accepted by ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Wav2Vec2 the Language of the Brain <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Fiedler, Leon Hermann, Florian Müller, Sarel Cohen, Peter Chin, Tobias Friedrich, Eilon Vaadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The decoding of continuously spoken speech from neuronal activity has the
potential to become an important clinical solution for paralyzed patients. Deep
Learning Brain Computer Interfaces (BCIs) have recently successfully mapped
neuronal activity to text contents in subjects who attempted to formulate
speech. However, only small BCI datasets are available. In contrast, labeled
data and pre-trained models for the closely related task of speech recognition
from audio are widely available. One such model is Wav2Vec2 which has been
trained in a self-supervised fashion to create meaningful representations of
speech audio data. In this study, we show that patterns learned by Wav2Vec2 are
transferable to brain data. Specifically, we replace its audio feature
extractor with an untrained Brain Feature Extractor (BFE) model. We then
execute full fine-tuning with pre-trained weights for Wav2Vec2, training ''from
scratch'' without pre-trained weights as well as freezing a pre-trained
Wav2Vec2 and training only the BFE each for 45 different BFE architectures.
Across these experiments, the best run is from full fine-tuning with
pre-trained weights, achieving a Character Error Rate (CER) of 18.54\%,
outperforming the best training from scratch run by 20.46\% and that of frozen
Wav2Vec2 training by 15.92\% percentage points. These results indicate that
knowledge transfer from audio speech recognition to brain decoding is possible
and significantly improves brain decoding performance for the same
architectures. Related source code is available at
https://github.com/tfiedlerdev/Wav2Vec2ForBrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper was submitted to ICASSP 2025 but marginally rejected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving the unsolvable: Translating case law in Hong Kong 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        King-kui Sin, Xi Xuan, Chunyu Kit, Clara Ho-yan Chan, Honic Ho-kin Ip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges translating case law under Hong Kong's
bilingual legal system. It highlights the initial success of translating all
written statutes into Chinese before the 1997 handover, a task mandated by the
Basic Law. The effort involved significant collaboration among legal,
linguistic, and translation experts, resulting in a comprehensive and
culturally appropriate bilingual legal system. However, translating case law
remains a significant challenge due to the sheer volume and continuous growth
of judicial decisions. The paper critiques the governments and judiciarys
sporadic and uncoordinated efforts to translate case law, contrasting it with
the thorough approach previously taken for statute translation. Although the
government acknowledges the importance of legal bilingualism, it lacks a
sustainable strategy for translating case law. The Judiciarys position that
translating all judgments is unnecessary, unrealistic, and not cost-effectiveis
analyzed and critiqued for its impact on legal transparency and public trust. A
proposed solution involves leveraging machine translation technology through a
human-machine interactive translation platform, which undergoes two major
transitions. Initially based on a neural model, the platform transitions to
using a large language model for improved translation accuracy. Furthermore, it
evolves from a single-agent system to a multi-agent system, incorporating
Translator, Annotator, and Proofreader agents. This multi-agent approach,
supported by a grant, aims to facilitate efficient, high-quality translation of
judicial judgments by integrating advanced artificial intelligence and
continuous feedback mechanisms, thus better meeting the needs of a bilingual
legal system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADAGE: A generic two-layer framework for adaptive agent based modelling <span class="chip">AAMAS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Patrick Evans, Sihan Zeng, Sumitra Ganesh, Leo Ardon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agent-based models (ABMs) are valuable for modelling complex, potentially
out-of-equilibria scenarios. However, ABMs have long suffered from the Lucas
critique, stating that agent behaviour should adapt to environmental changes.
Furthermore, the environment itself often adapts to these behavioural changes,
creating a complex bi-level adaptation problem. Recent progress integrating
multi-agent reinforcement learning into ABMs introduces adaptive agent
behaviour, beginning to address the first part of this critique, however, the
approaches are still relatively ad hoc, lacking a general formulation, and
furthermore, do not tackle the second aspect of simultaneously adapting
environmental level characteristics in addition to the agent behaviours. In
this work, we develop a generic two-layer framework for ADaptive AGEnt based
modelling (ADAGE) for addressing these problems. This framework formalises the
bi-level problem as a Stackelberg game with conditional behavioural policies,
providing a consolidated framework for adaptive agent-based modelling based on
solving a coupled set of non-linear equations. We demonstrate how this generic
approach encapsulates several common (previously viewed as distinct) ABM tasks,
such as policy design, calibration, scenario generation, and robust behavioural
learning under one unified framework. We provide example simulations on
multiple complex economic and financial environments, showing the strength of
the novel framework under these canonical settings, addressing long-standing
critiques of traditional ABMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2025 International Conference on Autonomous Agents
  and Multiagent Systems (AAMAS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Neural Style Transfer for Artistic Image Generation using VGG19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kapil Kashyap, Mehak Garg, Sean Fargose, Sindhu Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Throughout history, humans have created remarkable works of art, but
artificial intelligence has only recently started to make strides in generating
visually compelling art. Breakthroughs in the past few years have focused on
using convolutional neural networks (CNNs) to separate and manipulate the
content and style of images, applying texture synthesis techniques.
Nevertheless, a number of current techniques continue to encounter obstacles,
including lengthy processing times, restricted choices of style images, and the
inability to modify the weight ratio of styles. We proposed a neural style
transfer system that can add various artistic styles to a desired image to
address these constraints allowing flexible adjustments to style weight ratios
and reducing processing time. The system uses the VGG19 model for feature
extraction, ensuring high-quality, flexible stylization without compromising
content integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FASP: Fast and Accurate Structured Pruning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyu Hu, Pengxiang Zhao, Ping Li, Yi Zheng, Zhefeng Wang, Xiaoming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in the size of large language models (LLMs) has
significantly escalated their computational and memory demands, posing
challenges for efficient deployment, especially on resource-constrained
devices. Structured pruning has emerged as an effective model compression
method that can reduce these demands while preserving performance. In this
paper, we introduce FASP (Fast and Accurate Structured Pruning), a novel
structured pruning framework for LLMs that emphasizes both speed and accuracy.
FASP employs a distinctive pruning structure that interlinks sequential layers,
allowing for the removal of columns in one layer while simultaneously
eliminating corresponding rows in the preceding layer without incurring
additional performance loss. The pruning metric, inspired by Wanda, is
computationally efficient and effectively selects components to prune.
Additionally, we propose a restoration mechanism that enhances model fidelity
by adjusting the remaining weights post-pruning. We evaluate FASP on the OPT
and LLaMA model families, demonstrating superior performance in terms of
perplexity and accuracy on downstream tasks compared to state-of-the-art
methods. Our approach achieves significant speed-ups, pruning models such as
OPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090
GPU, making it a highly practical solution for optimizing LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoE$^2$: Optimizing Collaborative Inference for Edge Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lyudong Jin, Yanning Zhang, Yanhan Li, Shurong Wang, Howard H. Yang, Jian Wu, Meng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of natural language processing tasks. Exploiting the heterogeneous
capabilities of edge LLMs is crucial for diverse emerging applications, as it
enables greater cost-effectiveness and reduced latency. In this work, we
introduce \textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative
inference framework for edge LLMs. We formulate the joint gating and expert
selection problem to optimize inference performance under energy and latency
constraints. Unlike conventional MoE problems, LLM expert selection is
significantly more challenging due to the combinatorial nature and the
heterogeneity of edge LLMs across various attributes. To this end, we propose a
two-level expert selection mechanism through which we uncover an
optimality-preserving property of gating parameters across expert selections.
This property enables the decomposition of the training and selection
processes, significantly reducing complexity. Furthermore, we leverage the
objective's monotonicity and design a discrete monotonic optimization algorithm
for optimal expert selection. We implement edge servers with NVIDIA Jetson AGX
Orins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results
validate that performance improvements of various LLM models and show that our
MoE$^2$ method can achieve optimal trade-offs among different delay and energy
budgets, and outperforms baselines under various system resource constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/ACM Transactions on Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PISCO: <span class="highlight-title">Self-Supervised</span> k-Space Regularization for Improved Neural
  Implicit k-Space Representations of Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronika Spieker, Hannah Eichhorn, Wenqi Huang, Jonathan K. Stelter, Tabita Catalan, Rickmer F. Braren, Daniel Rueckert, Francisco Sahli Costabal, Kerstin Hammernik, Dimitrios C. Karampinos, Claudia Prieto, Julia A. Schnabel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit k-space representations (NIK) have shown promising results
for dynamic magnetic resonance imaging (MRI) at high temporal resolutions. Yet,
reducing acquisition time, and thereby available training data, results in
severe performance drops due to overfitting. To address this, we introduce a
novel self-supervised k-space loss function $\mathcal{L}_\mathrm{PISCO}$,
applicable for regularization of NIK-based reconstructions. The proposed loss
function is based on the concept of parallel imaging-inspired self-consistency
(PISCO), enforcing a consistent global k-space neighborhood relationship
without requiring additional data. Quantitative and qualitative evaluations on
static and dynamic MR reconstructions show that integrating PISCO significantly
improves NIK representations. Particularly for high acceleration factors
(R$\geq$54), NIK with PISCO achieves superior spatio-temporal reconstruction
quality compared to state-of-the-art methods. Furthermore, an extensive
analysis of the loss assumptions and stability shows PISCO's potential as
versatile self-supervised k-space loss function for further applications and
architectures. Code is available at:
https://github.com/compai-lab/2025-pisco-spieker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Searching of Extreme Operating Conditions for Relay Protection
  Setting Calculation Based on <span class="highlight-title">Graph</span> Neural Network and Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Jingyu Wang, Jiankang Zhang, Huaiqiang Li, Longfei Ren, Yinhong Li, Dongyuan Shi, Xianzhong Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for the Extreme Operating Conditions (EOCs) is one of the core
problems of power system relay protection setting calculation. The current
methods based on brute-force search, heuristic algorithms, and mathematical
programming can hardly meet the requirements of today's power systems in terms
of computation speed due to the drastic changes in operating conditions induced
by renewables and power electronics. This paper proposes an EOC fast search
method, named Graph Dueling Double Deep Q Network (Graph D3QN), which combines
graph neural network and deep reinforcement learning to address this challenge.
First, the EOC search problem is modeled as a Markov decision process, where
the information of the underlying power system is extracted using graph neural
networks, so that the EOC of the system can be found via deep reinforcement
learning. Then, a two-stage Guided Learning and Free Exploration (GLFE)
training framework is constructed to accelerate the convergence speed of
reinforcement learning. Finally, the proposed Graph D3QN method is validated
through case studies of searching maximum fault current for relay protection
setting calculation on the IEEE 39-bus and 118-bus systems. The experimental
results demonstrate that Graph D3QN can reduce the computation time by 10 to
1000 times while guaranteeing the accuracy of the selected EOCs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELM-DeepONets: Backpropagation-Free Training of Deep Operator Networks
  via Extreme Learning Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwijae Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Operator Networks (DeepONets) are among the most prominent frameworks
for operator learning, grounded in the universal approximation theorem for
operators. However, training DeepONets typically requires significant
computational resources. To address this limitation, we propose ELM-DeepONets,
an Extreme Learning Machine (ELM) framework for DeepONets that leverages the
backpropagation-free nature of ELM. By reformulating DeepONet training as a
least-squares problem for newly introduced parameters, the ELM-DeepONet
approach significantly reduces training complexity. Validation on benchmark
problems, including nonlinear ODEs and PDEs, demonstrates that the proposed
method not only achieves superior accuracy but also drastically reduces
computational costs. This work offers a scalable and efficient alternative for
operator learning in scientific computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum-Enhanced Transformers for Robust Acoustic Scene Classification
  in IoT Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, Pubudu N. Pathirana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of Internet of Things (IoT) devices equipped with acoustic
sensors necessitates robust acoustic scene classification (ASC) capabilities,
even in noisy and data-limited environments. Traditional machine learning
methods often struggle to generalize effectively under such conditions. To
address this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene
Classifier that leverages the power of quantum-inspired transformers. By
integrating quantum concepts like superposition and entanglement, Q-ASC
achieves superior feature learning and enhanced noise resilience compared to
classical models. Furthermore, we introduce a Quantum Variational Autoencoder
(QVAE) based data augmentation technique to mitigate the challenge of limited
labeled data in IoT deployments. Extensive evaluations on the Tampere
University of Technology (TUT) Acoustic Scenes 2016 benchmark dataset
demonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%
under challenging conditions, outperforming state-of-the-art methods by over 5%
in the best case. This research paves the way for deploying intelligent
acoustic sensing in IoT networks, with potential applications in smart homes,
industrial monitoring, and environmental surveillance, even in adverse acoustic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAL: <span class="highlight-title">Prompt</span>ing Analytic Learning with Missing Modality for Multi-Modal
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghu Yue, Yiming Chen, Xueyi Zhang, Xiaoxue Gao, Mengling Feng, Mingrui Lao, Huiping Zhuang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal class-incremental learning (MMCIL) seeks to leverage multi-modal
data, such as audio-visual and image-text pairs, thereby enabling models to
learn continuously across a sequence of tasks while mitigating forgetting.
While existing studies primarily focus on the integration and utilization of
multi-modal information for MMCIL, a critical challenge remains: the issue of
missing modalities during incremental learning phases. This oversight can
exacerbate severe forgetting and significantly impair model performance. To
bridge this gap, we propose PAL, a novel exemplar-free framework tailored to
MMCIL under missing-modality scenarios. Concretely, we devise modality-specific
prompts to compensate for missing information, facilitating the model to
maintain a holistic representation of the data. On this foundation, we
reformulate the MMCIL problem into a Recursive Least-Squares task, delivering
an analytical linear solution. Building upon these, PAL not only alleviates the
inherent under-fitting limitation in analytic learning but also preserves the
holistic representation of missing-modality data, achieving superior
performance with less forgetting across various multi-modal incremental
scenarios. Extensive experiments demonstrate that PAL significantly outperforms
competitive methods across various datasets, including UPMC-Food101 and
N24News, showcasing its robustness towards modality absence and its
anti-forgetting ability to maintain high incremental accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rational Tuning of LLM Cascades via Probabilistic Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael J. Zellinger, Matt Thomson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the reliability of large language models (LLMs) has recently
garnered significant attention. Given LLMs' propensity to hallucinate, as well
as their high sensitivity to prompt design, it is already challenging to
predict the performance of an individual LLM. However, the problem becomes more
complex for compound LLM systems such as cascades, where in addition to each
model's standalone performance, we must understand how the error rates of
different models interact. In this paper, we present a probabilistic model for
the joint performance distribution of a sequence of LLMs, which enables a
framework for rationally tuning the confidence thresholds of a LLM cascade
using continuous optimization. Compared to selecting confidence thresholds
using grid search, our parametric Markov-copula model significantly improves
runtime scaling with respect to the length of the cascade and the desired
resolution of the cost-error curve, turning them from intractable into
low-order polynomial. In addition, the optimal thresholds computed using our
continuous optimization-based algorithm increasingly outperform those found via
grid search as cascade length grows, improving the area under the cost-error
curve by 1.9% on average for cascades consisting of at least three models.
Overall, our Markov-copula model provides a rational basis for tuning LLM
cascade performance and points to the potential of probabilistic methods in
analyzing LLM systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating shared subspace with AJIVE: the power and limitation of
  multiple data matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuepeng Yang, Cong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrative data analysis often requires disentangling joint and individual
variations across multiple datasets, a challenge commonly addressed by the
Joint and Individual Variation Explained (JIVE) model. While numerous methods
have been developed to estimate the shared subspace under JIVE, the theoretical
understanding of their performance remains limited, particularly in the context
of multiple matrices and varying levels of subspace misalignment. This paper
bridges this gap by providing a systematic analysis of shared subspace
estimation in multi-matrix settings.
  We focus on the Angle-based Joint and Individual Variation Explained (AJIVE)
method, a two-stage spectral approach, and establish new performance guarantees
that uncover its strengths and limitations. Specifically, we show that in high
signal-to-noise ratio (SNR) regimes, AJIVE's estimation error decreases with
the number of matrices, demonstrating the power of multi-matrix integration.
Conversely, in low-SNR settings, AJIVE exhibits a non-diminishing error,
highlighting fundamental limitations. To complement these results, we derive
minimax lower bounds, showing that AJIVE achieves optimal rates in high-SNR
regimes. Furthermore, we analyze an oracle-aided spectral estimator to
demonstrate that the non-diminishing error in low-SNR scenarios is a
fundamental barrier. Extensive numerical experiments corroborate our
theoretical findings, providing insights into the interplay between SNR, matrix
count, and subspace misalignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Information from Observations with Uncertainty and Novelty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derek S. Prijatelj, Timothy J. Ireland, Walter J. Scheirer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A machine learning tasks from observations must encounter and process
uncertainty and novelty, especially when it is expected to maintain performance
when observing new information and to choose the best fitting hypothesis to the
currently observed information. In this context, some key questions arise: what
is information, how much information did the observations provide, how much
information is required to identify the data-generating process, how many
observations remain to get that information, and how does a predictor determine
that it has observed novel information? This paper strengthens existing answers
to these questions by formalizing the notion of "identifiable information" that
arises from the language used to express the relationship between distinct
states. Model identifiability and sample complexity are defined via computation
of an indicator function over a set of hypotheses. Their properties and
asymptotic statistics are described for data-generating processes ranging from
deterministic processes to ergodic stationary stochastic processes. This
connects the notion of identifying information in finite steps with asymptotic
statistics and PAC-learning. The indicator function's computation naturally
formalizes novel information and its identification from observations with
respect to a hypothesis set. We also proved that computable PAC-Bayes learners'
sample complexity distribution is determined by its moments in terms of the the
prior probability distribution over a fixed finite hypothesis set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 1 figure, 1 table, and 2 inline algorithms. Submitted to
  JMLR Jan. 6, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Learning Informative Trajectory Embeddings for Imitation,
  Classification and Regression <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichang Ge, Changyu Chen, Arunesh Sinha, Pradeep Varakantham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world sequential decision making tasks like autonomous driving,
robotics, and healthcare, learning from observed state-action trajectories is
critical for tasks like imitation, classification, and clustering. For example,
self-driving cars must replicate human driving behaviors, while robots and
healthcare systems benefit from modeling decision sequences, whether or not
they come from expert data. Existing trajectory encoding methods often focus on
specific tasks or rely on reward signals, limiting their ability to generalize
across domains and tasks. Inspired by the success of embedding models like CLIP
and BERT in static domains, we propose a novel method for embedding
state-action trajectories into a latent space that captures the skills and
competencies in the dynamic underlying decision-making processes. This method
operates without the need for reward labels, enabling better generalization
across diverse domains and tasks. Our contributions are threefold: (1) We
introduce a trajectory embedding approach that captures multiple abilities from
state-action data. (2) The learned embeddings exhibit strong representational
power across downstream tasks, including imitation, classification, clustering,
and regression. (3) The embeddings demonstrate unique properties, such as
controlling agent behaviors in IQ-Learn and an additive structure in the latent
space. Experimental results confirm that our method outperforms traditional
approaches, offering more flexible and powerful trajectory representations for
various applications. Our code is available at
https://github.com/Erasmo1015/vte.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAMAS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative Decentralized Backdoor Attacks on Vertical Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seohyun Lee, Wenzhi Fang, Anindya Bijoy Das, Seyyedali Hosseinalipour, David J. Love, Christopher G. Brinton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is vulnerable to backdoor attacks, where adversaries
alter model behavior on target classification labels by embedding triggers into
data samples. While these attacks have received considerable attention in
horizontal FL, they are less understood for vertical FL (VFL), where devices
hold different features of the samples, and only the server holds the labels.
In this work, we propose a novel backdoor attack on VFL which (i) does not rely
on gradient information from the server and (ii) considers potential collusion
among multiple adversaries for sample selection and trigger embedding. Our
label inference model augments variational autoencoders with metric learning,
which adversaries can train locally. A consensus process over the adversary
graph topology determines which datapoints to poison. We further propose
methods for trigger splitting across the adversaries, with an intensity-based
implantation scheme skewing the server towards the trigger. Our convergence
analysis reveals the impact of backdoor perturbations on VFL indicated by a
stationarity gap for the trained model, which we verify empirically as well. We
conduct experiments comparing our attack with recent backdoor VFL approaches,
finding that ours obtains significantly higher success rates for the same main
task performance despite not using server information. Additionally, our
results verify the impact of collusion on attack performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review in the IEEE/ACM Transactions on
  Networking Special Issue on AI and Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding the Trigger: Causal Abductive Reasoning on Video Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Minh Le, Vuong Le, Kien Do, Sunil Gupta, Svetha Venkatesh, Truyen Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new problem, Causal Abductive Reasoning on Video
Events (CARVE), which involves identifying causal relationships between events
in a video and generating hypotheses about causal chains that account for the
occurrence of a target event. To facilitate research in this direction, we
create two new benchmark datasets with both synthetic and realistic videos,
accompanied by trigger-target labels generated through a novel counterfactual
synthesis approach. To explore the challenge of solving CARVE, we present a
Causal Event Relation Network (CERN) that examines the relationships between
video events in temporal and semantic spaces to efficiently determine the
root-cause trigger events. Through extensive experiments, we demonstrate the
critical roles of event relational representation learning and interaction
modeling in solving video causal reasoning challenges. The introduction of the
CARVE task, along with the accompanying datasets and the CERN framework, will
advance future research on video causal reasoning and significantly facilitate
various applications, including video surveillance, root-cause analysis and
movie content management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed deep learning for infectious disease forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Qian, Éric Marty, Avranil Basu, Eamon B. O'Dea, Xianqiao Wang, Spencer Fox, Pejman Rohani, John M. Drake, He Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate forecasting of contagious illnesses has become increasingly
important to public health policymaking, and better prediction could prevent
the loss of millions of lives. To better prepare for future pandemics, it is
essential to improve forecasting methods and capabilities. In this work, we
propose a new infectious disease forecasting model based on physics-informed
neural networks (PINNs), an emerging area of scientific machine learning. The
proposed PINN model incorporates dynamical systems representations of disease
transmission into the loss function, thereby assimilating epidemiological
theory and data using neural networks (NNs). Our approach is designed to
prevent model overfitting, which often occurs when training deep learning
models with observation data alone. In addition, we employ an additional
sub-network to account for mobility, vaccination, and other covariates that
influence the transmission rate, a key parameter in the compartment model. To
demonstrate the capability of the proposed model, we examine the performance of
the model using state-level COVID-19 data in California. Our simulation results
show that predictions of PINN model on the number of cases, deaths, and
hospitalizations are consistent with existing benchmarks. In particular, the
PINN model outperforms the basic NN model and naive baseline forecast. We also
show that the performance of the PINN model is comparable to a sophisticated
Gaussian infection state space with time dependence (GISST) forecasting model
that integrates the compartment model with a data observation model and a
regression model for inferring parameters in the compartment model.
Nonetheless, the PINN model offers a simpler structure and is easier to
implement. Our results show that the proposed forecaster could potentially
serve as a new computational tool to enhance the current capacity of infectious
disease forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Free-Knots Kolmogorov-Arnold Network: On the Analysis of Spline Knots
  and Advancing Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangwewi Nathan Zheng, Wei Emma Zhang, Lin Yue, Miao Xu, Olaf Maennel, Weitong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold Neural Networks (KANs) have gained significant attention in
the machine learning community. However, their implementation often suffers
from poor training stability and heavy trainable parameter. Furthermore, there
is limited understanding of the behavior of the learned activation functions
derived from B-splines. In this work, we analyze the behavior of KANs through
the lens of spline knots and derive the lower and upper bound for the number of
knots in B-spline-based KANs. To address existing limitations, we propose a
novel Free Knots KAN that enhances the performance of the original KAN while
reducing the number of trainable parameters to match the trainable parameter
scale of standard Multi-Layer Perceptrons (MLPs). Additionally, we introduce
new a training strategy to ensure $C^2$ continuity of the learnable spline,
resulting in smoother activation compared to the original KAN and improve the
training stability by range expansion. The proposed method is comprehensively
evaluated on 8 datasets spanning various domains, including image, text, time
series, multimodal, and function approximation tasks. The promising results
demonstrates the feasibility of KAN-based network and the effectiveness of
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model is Secretly a Protein Sequence Optimizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinkai Wang, Jiaxing He, Yuanqi Du, Xiaohui Chen, Jianan Canal Li, Li-Ping Liu, Xiaolin Xu, Soha Hassoun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the protein sequence engineering problem, which aims to find
protein sequences with high fitness levels, starting from a given wild-type
sequence. Directed evolution has been a dominating paradigm in this field which
has an iterative process to generate variants and select via experimental
feedback. We demonstrate large language models (LLMs), despite being trained on
massive texts, are secretly protein sequence optimizers. With a directed
evolutionary method, LLM can perform protein engineering through Pareto and
experiment-budget constrained optimization, demonstrating success on both
synthetic and experimental fitness landscapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the convergence of noisy Bayesian Optimization with Expected
  Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Wang, Haowei Wang, Cosmin G. Petra, Nai-Yuan Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expected improvement (EI) is one of the most widely-used acquisition
functions in Bayesian optimization (BO). Despite its proven success in
applications for decades, important open questions remain on the theoretical
convergence behaviors and rates for EI. In this paper, we contribute to the
convergence theories of EI in three novel and critical area. First, we consider
objective functions that are under the Gaussian process (GP) prior assumption,
whereas existing works mostly focus on functions in the reproducing kernel
Hilbert space (RKHS). Second, we establish the first asymptotic error bound and
its corresponding rate for GP-EI with noisy observations under the GP prior
assumption. Third, by investigating the exploration and exploitation of the
non-convex EI function, we prove improved error bounds for both the noise-free
and noisy cases. The improved noiseless bound is extended to the RKHS
assumption as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clone-Robust AI Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key challenge in training Large Language Models (LLMs) is properly aligning
them with human preferences. Reinforcement Learning with Human Feedback (RLHF)
uses pairwise comparisons from human annotators to train reward functions and
has emerged as a popular alignment method. However, input datasets in RLHF are
not necessarily balanced in the types of questions and answers that are
included. Therefore, we want RLHF algorithms to perform well even when the set
of alternatives is not uniformly distributed. Drawing on insights from social
choice theory, we introduce robustness to approximate clones, a desirable
property of RLHF algorithms which requires that adding near-duplicate
alternatives does not significantly change the learned reward function. We
first demonstrate that the standard RLHF algorithm based on regularized maximum
likelihood estimation (MLE) fails to satisfy this property. We then propose the
weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE
by weighting alternatives based on their similarity to other alternatives. This
new algorithm guarantees robustness to approximate clones while preserving
desirable theoretical properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Vectors in In-Context Learning: E<span class="highlight-title">merge</span>nce, Formation, and Benefit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Yang, Ziqian Lin, Kangwook Lee, Dimitris Papailiopoulos, Robert Nowak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning is a remarkable capability of transformers, referring to
their ability to adapt to specific tasks based on a short history or context.
Previous research has found that task-specific information is locally encoded
within models, though their emergence and functionality remain unclear due to
opaque pre-training processes. In this work, we investigate the formation of
task vectors in a controlled setting, using models trained from scratch on
synthetic datasets. Our findings confirm that task vectors naturally emerge
under certain conditions, but the tasks may be relatively weakly and/or
non-locally encoded within the model. To promote strong task vectors encoded at
a prescribed location within the model, we propose an auxiliary training
mechanism based on a task vector prompting loss (TVP-loss). This method
eliminates the need to search for task-correlated encodings within the trained
model and demonstrably improves robustness and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mono-Forward: Backpropagation-Free Algorithm for Efficient Neural
  Network Training Harnessing Local Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Gong, Bruce Li, Waleed Abdulla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backpropagation is the standard method for achieving state-of-the-art
accuracy in neural network training, but it often imposes high memory costs and
lacks biological plausibility. In this paper, we introduce the Mono-Forward
algorithm, a purely local layerwise learning method inspired by Hinton's
Forward-Forward framework. Unlike backpropagation, Mono-Forward optimizes each
layer solely with locally available information, eliminating the reliance on
global error signals. We evaluated Mono-Forward on multi-layer perceptrons and
convolutional neural networks across multiple benchmarks, including MNIST,
Fashion-MNIST, CIFAR-10, and CIFAR-100. The test results show that Mono-Forward
consistently matches or surpasses the accuracy of backpropagation across all
tasks, with significantly reduced and more even memory usage, better
parallelizability, and a comparable convergence rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tessellated Linear Model for Age Prediction from Voice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dareen Alharthi, Mahsa Zamani, Bhiksha Raj, Rita Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice biometric tasks, such as age estimation require modeling the often
complex relationship between voice features and the biometric variable. While
deep learning models can handle such complexity, they typically require large
amounts of accurately labeled data to perform well. Such data are often scarce
for biometric tasks such as voice-based age prediction. On the other hand,
simpler models like linear regression can work with smaller datasets but often
fail to generalize to the underlying non-linear patterns present in the data.
In this paper we propose the Tessellated Linear Model (TLM), a piecewise linear
approach that combines the simplicity of linear models with the capacity of
non-linear functions. TLM tessellates the feature space into convex regions and
fits a linear model within each region. We optimize the tessellation and the
linear models using a hierarchical greedy partitioning. We evaluated TLM on the
TIMIT dataset on the task of age prediction from voice, where it outperformed
state-of-the-art deep learning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Foundation</span>s of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is a book about large language models. As indicated by the title, it
primarily focuses on foundational concepts rather than comprehensive coverage
of all cutting-edge technologies. The book is structured into four main
chapters, each exploring a key area: pre-training, generative models, prompting
techniques, and alignment methods. It is intended for college students,
professionals, and practitioners in natural language processing and related
fields, and can serve as a reference for anyone interested in large language
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Scale-aware Representations for improved
  Concept-Representation Alignment in ViTs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanchit Sinha, Guangzhi Xiong, Aidong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) are increasingly being adopted in various
sensitive vision applications - like medical diagnosis, facial recognition,
etc. To improve the interpretability of such models, many approaches attempt to
forward-align them with carefully annotated abstract, human-understandable
semantic entities - concepts. Concepts provide global rationales to the model
predictions and can be quickly understood/intervened on by domain experts. Most
current research focuses on designing model-agnostic, plug-and-play generic
concept-based explainability modules that do not incorporate the inner workings
of foundation models (e.g., inductive biases, scale invariance, etc.) during
training. To alleviate this issue for ViTs, in this paper, we propose a novel
Concept Representation Alignment Module (CRAM) which learns both scale and
position-aware representations from multi-scale feature pyramids and patch
representations respectively. CRAM further aligns these representations with
concept annotations through an attention matrix. The proposed CRAM module
improves the predictive performance of ViT architectures and also provides
accurate and robust concept explanations as demonstrated on five datasets -
including three widely used benchmarks (CUB, Pascal APY, Concept-MNIST) and 2
real-world datasets (AWA2, KITS).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Law-Based Transformation (ALT): A Lightweight Feature
  Representation for Time Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcell T. Kurbucz, Balázs Hajós, Balázs P. Halmos, Vince Á. Molnár, Antal Jakovác
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series classification (TSC) is fundamental in numerous domains,
including finance, healthcare, and environmental monitoring. However,
traditional TSC methods often struggle with the inherent complexity and
variability of time series data. Building on our previous work with the linear
law-based transformation (LLT) - which improved classification accuracy by
transforming the feature space based on key data patterns - we introduce
adaptive law-based transformation (ALT). ALT enhances LLT by incorporating
variable-length shifted time windows, enabling it to capture distinguishing
patterns of various lengths and thereby handle complex time series more
effectively. By mapping features into a linearly separable space, ALT provides
a fast, robust, and transparent solution that achieves state-of-the-art
performance with only a few hyperparameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic Collective Action in <span class="highlight-title">Recommend</span>er Systems: Promoting Songs by
  Reordering Playlists <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joachim Baumann, Celestine Mendler-Dünner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate algorithmic collective action in transformer-based recommender
systems. Our use case is a music streaming platform where a collective of fans
aims to promote the visibility of an underrepresented artist by strategically
placing one of their songs in the existing playlists they control. We introduce
two easily implementable strategies to select the position at which to insert
the song with the goal to boost recommendations at test time. The strategies
exploit statistical properties of the learner by targeting discontinuities in
the recommendations, and leveraging the long-tail nature of song distributions.
We evaluate the efficacy of our strategies using a publicly available
recommender system model released by a major music streaming platform. Our
findings reveal that through strategic placement even small collectives
(controlling less than 0.01\% of the training data) can achieve up to
$40\times$ more test time recommendations than an average song with the same
number of training set occurrences. Focusing on the externalities of the
strategy, we find that the recommendations of other songs are largely
preserved, and the newly gained recommendations are distributed across various
artists. Together, our findings demonstrate how carefully designed collective
action strategies can be effective while not necessarily being adversarial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024, camera-ready updates</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Machine Learning to Discover Parsimonious and
  Physically-Interpre<span class="highlight-title">table</span> Representations of Catchment-Scale Rainfall-Runoff
  Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Heng Wang, Hoshin V. Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the excellent real-world predictive performance of modern machine
learning (ML) methods, many scientists remain hesitant to discard traditional
physical-conceptual (PC) approaches due mainly to their relative
interpretability, which contributes to credibility during decision-making. In
this context, a currently underexplored aspect of ML is how to develop
minimally-optimal representations that can facilitate better insight regarding
system functioning. Regardless of how this is achieved, it is arguably true
that parsimonious representations better support the advancement of scientific
understanding. Our own view is that ML-based modeling of geoscientific systems
should be based in the use of computational units that are fundamentally
interpretable by design.
  This paper continues our exploration of how the strengths of ML can be
exploited in the service of better understanding via scientific investigation.
Here, we use the Mass Conserving Perceptron (MCP) as the fundamental
computational unit in a generic network architecture consisting of nodes
arranged in series and parallel to explore several generic and important issues
related to the use of observational data for constructing input-state-output
models of dynamical systems. In the context of lumped catchment modeling, we
show that physical interpretability and excellent predictive performance can
both be achieved using a relatively parsimonious distributed-state
multiple-flow-path network with context-dependent gating and information
sharing across the nodes, suggesting that MCP-based modeling can play a
significant role in application of ML to geoscientific investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>74 Pages, 4 Tables, 13 Figures, 11 Tables and 11 Figures in
  Supplementary Materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04202v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04202v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growing concerns about safety and alignment of AI systems highlight the
importance of embedding moral capabilities in artificial agents: a promising
solution is the use of learning from experience, i.e., Reinforcement Learning.
In multi-agent (social) environments, complex population-level phenomena may
emerge from interactions between individual learning agents. Many of the
existing studies rely on simulated social dilemma environments to study the
interactions of independent learning agents; however, they tend to ignore the
moral heterogeneity that is likely to be present in societies of agents in
practice. For example, at different points in time a single learning agent may
face opponents who are consequentialist (i.e., focused on maximizing outcomes
over time), norm-based (i.e., conforming to specific norms), or virtue-based
(i.e., considering a combination of different virtues). The extent to which
agents' co-development may be impacted by such moral heterogeneity in
populations is not well understood. In this paper, we present a study of the
learning dynamics of morally heterogeneous populations interacting in a social
dilemma setting. Using an Iterated Prisoner's Dilemma environment with a
partner selection mechanism, we investigate the extent to which the prevalence
of diverse moral agents in populations affects individual agents' learning
behaviors and emergent population-level outcomes. We observe several types of
non-trivial interactions between pro-social and anti-social agents, and find
that certain types of moral agents are able to steer selfish agents towards
more cooperative behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and
  Society - San Jose, CA, USA) - see
  https://ojs.aaai.org/index.php/AIES/article/view/31736</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study on Multi-task Uncertainty Quantification in Semantic
  Segmentation and Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Landgraf, Markus Hillemann, Theodor Kapler, Markus Ulrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks excel in perception tasks such as semantic segmentation
and monocular depth estimation, making them indispensable in safety-critical
applications like autonomous driving and industrial inspection. However, they
often suffer from overconfidence and poor explainability, especially for
out-of-domain data. While uncertainty quantification has emerged as a promising
solution to these challenges, multi-task settings have yet to be explored. In
an effort to shed light on this, we evaluate Monte Carlo Dropout, Deep
Sub-Ensembles, and Deep Ensembles for joint semantic segmentation and monocular
depth estimation. Thereby, we reveal that Deep Ensembles stand out as the
preferred choice, particularly in out-of-domain scenarios, and show the
potential benefit of multi-task learning with regard to the uncertainty quality
in comparison to solving both tasks separately. Additionally, we highlight the
impact of employing different uncertainty thresholds to classify pixels as
certain or uncertain, with the median uncertainty emerging as a robust default.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is an extended version of a previously published
  conference paper and is currently in review for a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Brain Activity with Advanced Transformer Models: Exploring the
  Role of Punctuation in Semantic Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenon Lamprou, Frank Polick, Yashar Moshfeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research examines the congruence between neural activity and advanced
transformer models, emphasizing the semantic significance of punctuation in
text understanding. Utilizing an innovative approach originally proposed by
Toneva and Wehbe, we evaluate four advanced transformer models RoBERTa,
DistiliBERT, ALBERT, and ELECTRA against neural activity data. Our findings
indicate that RoBERTa exhibits the closest alignment with neural activity,
surpassing BERT in accuracy. Furthermore, we investigate the impact of
punctuation removal on model performance and neural alignment, revealing that
BERT's accuracy enhances in the absence of punctuation. This study contributes
to the comprehension of how neural networks represent language and the
influence of punctuation on semantic processing within the human brain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexible task abstractions e<span class="highlight-title">merge</span> in linear networks with fast and
  bounded units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Sandbrink, Jan P. Bauer, Alexandra M. Proca, Andrew M. Saxe, Christopher Summerfield, Ali Hummos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animals survive in dynamic environments changing at arbitrary timescales, but
such data distribution shifts are a challenge to neural networks. To adapt to
change, neural systems may change a large number of parameters, which is a slow
process involving forgetting past information. In contrast, animals leverage
distribution changes to segment their stream of experience into tasks and
associate them with internal task abstracts. Animals can then respond flexibly
by selecting the appropriate task abstraction. However, how such flexible task
abstractions may arise in neural systems remains unknown. Here, we analyze a
linear gated network where the weights and gates are jointly optimized via
gradient descent, but with neuron-like constraints on the gates including a
faster timescale, nonnegativity, and bounded activity. We observe that the
weights self-organize into modules specialized for tasks or sub-tasks
encountered, while the gates layer forms unique representations that switch the
appropriate weight modules (task abstractions). We analytically reduce the
learning dynamics to an effective eigenspace, revealing a virtuous cycle: fast
adapting gates drive weight specialization by protecting previous knowledge,
while weight specialization in turn increases the update rate of the gating
layer. Task switching in the gating layer accelerates as a function of
curriculum block size and task training, mirroring key findings in cognitive
neuroscience. We show that the discovered task abstractions support
generalization through both task and subtask composition, and we extend our
findings to a non-linear network switching between two tasks. Overall, our work
offers a theory of cognitive flexibility in animals as arising from joint
gradient descent on synaptic and neural gating in a neural network
architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of <span class="highlight-title">Foundation</span> Models in Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) are large-scale deep learning models trained on
massive datasets, often using self-supervised learning techniques. These models
serve as a versatile base for a wide range of downstream tasks, including those
in medicine and healthcare. FMs have demonstrated remarkable success across
multiple healthcare domains. However, existing surveys in this field do not
comprehensively cover all areas where FMs have made significant strides. In
this survey, we present a comprehensive review of FMs in medicine, focusing on
their evolution, learning strategies, flagship models, applications, and
associated challenges. We examine how prominent FMs, such as the BERT and GPT
families, are transforming various aspects of healthcare, including clinical
large language models, medical image analysis, and omics research.
Additionally, we provide a detailed taxonomy of FM-enabled healthcare
applications, spanning clinical natural language processing, medical computer
vision, graph learning, and other biology- and omics- related tasks. Despite
the transformative potentials of FMs, they also pose unique challenges. This
survey delves into these challenges and highlights open research questions and
lessons learned to guide researchers and practitioners. Our goal is to provide
valuable insights into the capabilities of FMs in health, facilitating
responsible deployment and mitigating associated risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Approaches for Moral Value Alignment in AI Agents: a Manifesto 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01818v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01818v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasing interest in ensuring the safety of next-generation Artificial
Intelligence (AI) systems calls for novel approaches to embedding morality into
autonomous agents. This goal differs qualitatively from traditional
task-specific AI methodologies. In this paper, we provide a systematization of
existing approaches to the problem of introducing morality in machines -
modelled as a continuum. Our analysis suggests that popular techniques lie at
the extremes of this continuum - either being fully hard-coded into top-down,
explicit rules, or entirely learned in a bottom-up, implicit fashion with no
direct statement of any moral principle (this includes learning from human
feedback, as applied to the training and finetuning of large language models,
or LLMs). Given the relative strengths and weaknesses of each type of
methodology, we argue that more hybrid solutions are needed to create adaptable
and robust, yet controllable and interpretable agentic systems. To that end,
this paper discusses both the ethical foundations (including deontology,
consequentialism and virtue ethics) and implementations of morally aligned AI
systems.
  We present a series of case studies that rely on intrinsic rewards, moral
constraints or textual instructions, applied to either pure-Reinforcement
Learning or LLM-based agents. By analysing these diverse implementations under
one framework, we compare their relative strengths and shortcomings in
developing morally aligned AI systems. We then discuss strategies for
evaluating the effectiveness of moral learning agents. Finally, we present open
research questions and implications for the future of AI safety and ethics
which are emerging from this hybrid framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFactor GNNs: Revisiting Factorisation-based Models from a
  Message-Passing Perspective <span class="chip">NeurIPS
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09980v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09980v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and generalise to unseen nodes in inductive settings. Our work bridges
the gap between FMs and GNNs by proposing ReFactor GNNs. This new architecture
draws upon both modelling paradigms, which previously were largely thought of
as disjoint. Concretely, using a message-passing formalism, we show how FMs can
be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our ReFactor GNNs. Across
a multitude of well-established KGC benchmarks, our ReFactor GNNs achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36th Conference on Neural Information Processing Systems (NeurIPS
  2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear
  Contextual Bandit <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seok-Jin Kim, Min-hwan Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the performance guarantees of exploration-free greedy algorithms for
the linear contextual bandit problem. We introduce a novel condition, named the
\textit{Local Anti-Concentration} (LAC) condition, which enables a greedy
bandit algorithm to achieve provable efficiency. We show that the LAC condition
is satisfied by a broad class of distributions, including Gaussian,
exponential, uniform, Cauchy, and Student's~$t$ distributions, along with other
exponential family distributions and their truncated variants. This
significantly expands the class of distributions under which greedy algorithms
can perform efficiently. Under our proposed LAC condition, we prove that the
cumulative expected regret of the greedy algorithm for the linear contextual
bandit is bounded by $O(\operatorname{poly} \log T)$. Our results establish the
widest range of distributions known to date that allow a sublinear regret bound
for greedy algorithms, further achieving a sharp poly-logarithmic regret.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Higher-Order Topological Directionality and Directed Simplicial Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08389v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08389v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Lecha, Andrea Cavallo, Francesca Dominici, Elvin Isufi, Claudio Battiloro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological Deep Learning (TDL) has emerged as a paradigm to process and
learn from signals defined on higher-order combinatorial topological spaces,
such as simplicial or cell complexes. Although many complex systems have an
asymmetric relational structure, most TDL models forcibly symmetrize these
relationships. In this paper, we first introduce a novel notion of higher-order
directionality and we then design Directed Simplicial Neural Networks
(Dir-SNNs) based on it. Dir-SNNs are message-passing networks operating on
directed simplicial complexes able to leverage directed and possibly asymmetric
interactions among the simplices. To our knowledge, this is the first TDL model
using a notion of higher-order directionality. We theoretically and empirically
prove that Dir-SNNs are more expressive than their directed graph counterpart
in distinguishing isomorphic directed graphs. Experiments on a synthetic source
localization task demonstrate that Dir-SNNs outperform undirected SNNs when the
underlying complex is directed, and perform comparably when the underlying
complex is undirected.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid additive modeling with partial dependence for supervised
  regression and dynamical systems forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yann Claes, Vân Anh Huynh-Thu, Pierre Geurts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning processes by exploiting restricted domain knowledge is an important
task across a plethora of scientific areas, with more and more hybrid training
methods additively combining data-driven and model-based approaches. Although
the obtained models are more accurate than purely data-driven models, the
optimization process usually comes with sensitive regularization constraints.
Furthermore, while such hybrid methods have been tested in various scientific
applications, they have been mostly tested on dynamical systems, with only
limited study about the influence of each model component on global performance
and parameter identification. In this work, we introduce a new hybrid training
approach based on partial dependence, which removes the need for intricate
regularization. Moreover, we assess the performance of hybrid modeling against
traditional machine learning methods on standard regression problems. We
compare, on both synthetic and real regression problems, several approaches for
training such hybrid models. We focus on hybrid methods that additively combine
a parametric term with a machine learning term and investigate model-agnostic
training procedures. Therefore, experiments are carried out with different
types of machine learning models, including tree-based models and artificial
neural networks. We also extend our partial dependence optimization process for
dynamical systems forecasting and compare it to existing schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper entitled "Knowledge-Guided Additive
  Modeling for Supervised Regression"
  (https://link.springer.com/chapter/10.1007/978-3-031-45275-8_5), accepted for
  publication in the Machine Learning journal. The extension includes new
  experiments in the static setting, along with a dedicated section on the
  application of our method to the problem of dynamical systems forecasting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably Efficient Reinforcement Learning with Multinomial Logit
  Function Approximation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17061v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17061v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long-Fei Li, Yu-Jie Zhang, Peng Zhao, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a new class of MDPs that employs multinomial logit (MNL) function
approximation to ensure valid probability distributions over the state space.
Despite its significant benefits, incorporating the non-linear function raises
substantial challenges in both statistical and computational efficiency. The
best-known result of Hwang and Oh [2023] has achieved an
$\widetilde{\mathcal{O}}(\kappa^{-1}dH^2\sqrt{K})$ regret upper bound, where
$\kappa$ is a problem-dependent quantity, $d$ is the feature dimension, $H$ is
the episode length, and $K$ is the number of episodes. However, we observe that
$\kappa^{-1}$ exhibits polynomial dependence on the number of reachable states,
which can be as large as the state space size in the worst case and thus
undermines the motivation for function approximation. Additionally, their
method requires storing all historical data and the time complexity scales
linearly with the episode count, which is computationally expensive. In this
work, we propose a statistically efficient algorithm that achieves a regret of
$\widetilde{\mathcal{O}}(dH^2\sqrt{K} + \kappa^{-1}d^2H^2)$, eliminating the
dependence on $\kappa^{-1}$ in the dominant term for the first time. We then
address the computational challenges by introducing an enhanced algorithm that
achieves the same regret guarantee but with only constant cost. Finally, we
establish the first lower bound for this problem, justifying the optimality of
our results in $d$ and $K$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; v3 substantially improves the presentation and further
  illustrates the role of $\kappa$ in function approximation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian
  Neural Networks <span class="chip">AAAI'2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20891v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20891v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bao Gia Doan, Afshar Shamsi, Xiao-Yu Guo, Arash Mohammadi, Hamid Alinejad-Rokny, Dino Sejdinovic, Damien Teney, Damith C. Ranasinghe, Ehsan Abbasnejad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational complexity of Bayesian learning is impeding its adoption in
practical, large-scale tasks. Despite demonstrations of significant merits such
as improved robustness and resilience to unseen or out-of-distribution inputs
over their non- Bayesian counterparts, their practical use has faded to near
insignificance. In this study, we introduce an innovative framework to mitigate
the computational burden of Bayesian neural networks (BNNs). Our approach
follows the principle of Bayesian techniques based on deep ensembles, but
significantly reduces their cost via multiple low-rank perturbations of
parameters arising from a pre-trained neural network. Both vanilla version of
ensembles as well as more sophisticated schemes such as Bayesian learning with
Stein Variational Gradient Descent (SVGD), previously deemed impractical for
large models, can be seamlessly implemented within the proposed framework,
called Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a
dramatic reduction in the number of trainable parameters required to
approximate a Bayesian posterior; and ii) it not only maintains, but in some
instances, surpasses the performance of conventional Bayesian learning methods
and non-Bayesian baselines. Our results with large-scale tasks such as
ImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the
effectiveness and versatility of Bella in building highly scalable and
practical Bayesian deep models for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in AAAI'2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Space Characterization of Autoencoder Variants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anika Shrivastava, Renu Rameshan, Samar Agnihotri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the latent spaces learned by deep learning models is crucial in
exploring how they represent and generate complex data. Autoencoders (AEs) have
played a key role in the area of representation learning, with numerous
regularization techniques and training principles developed not only to enhance
their ability to learn compact and robust representations, but also to reveal
how different architectures influence the structure and smoothness of the
lower-dimensional non-linear manifold. We strive to characterize the structure
of the latent spaces learned by different autoencoders including convolutional
autoencoders (CAEs), denoising autoencoders (DAEs), and variational
autoencoders (VAEs) and how they change with the perturbations in the input. By
characterizing the matrix manifolds corresponding to the latent spaces, we
provide an explanation for the well-known observation that the latent spaces of
CAE and DAE form non-smooth manifolds, while that of VAE forms a smooth
manifold. We also map the points of the matrix manifold to a Hilbert space
using distance preserving transforms and provide an alternate view in terms of
the subspaces generated in the Hilbert space as a function of the distortion in
the input. The results show that the latent manifolds of CAE and DAE are
stratified with each stratum being a smooth product manifold, while the
manifold of VAE is a smooth product manifold of two symmetric positive definite
matrices and a symmetric positive semi-definite matrix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, and 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FSDEM: Feature Selection Dynamic Evaluation Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Rajabinasab, Anton D. Lautrup, Tobias Hyrup, Arthur Zimek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressive evaluation metrics are indispensable for informative experiments
in all areas, and while several metrics are established in some areas, in
others, such as feature selection, only indirect or otherwise limited
evaluation metrics are found. In this paper, we propose a novel evaluation
metric to address several problems of its predecessors and allow for flexible
and reliable evaluation of feature selection algorithms. The proposed metric is
a dynamic metric with two properties that can be used to evaluate both the
performance and the stability of a feature selection algorithm. We conduct
several empirical experiments to illustrate the use of the proposed metric in
the successful evaluation of feature selection algorithms. We also provide a
comparison and analysis to show the different aspects involved in the
evaluation of the feature selection algorithms. The results indicate that the
proposed metric is successful in carrying out the evaluation task for feature
selection algorithms.
  This paper is an extended version of a paper published at SISAP 2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short version of this paper is published at 17th International
  Conference on Similarity Search and Applications, SISAP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STROOBnet Optimization via GPU-Accelerated Proximal Recurrence
  Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14388v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14388v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ted Edward Holmberg, Mahdi Abdelguerfi, Elias Ioup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal networks' observational capabilities are crucial for accurate
data gathering and informed decisions across multiple sectors. This study
focuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network
(STROOBnet), linking observational nodes (e.g., surveillance cameras) to events
within defined geographical regions, enabling efficient monitoring. Using data
from Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New
Orleans, where RTCC combats rising crime amidst reduced police presence, we
address the network's initial observational imbalances. Aiming for uniform
observational efficacy, we propose the Proximal Recurrence approach. It
outperformed traditional clustering methods like k-means and DBSCAN by offering
holistic event frequency and spatial consideration, enhancing observational
coverage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures, 2023 IEEE International Conference on Big Data
  (BigData)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Consolidated Volatility Prediction with Back Propagation Neural
  Network and Genetic Algorithm <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07223v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07223v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zong Ke, Jingyu Xu, Zizhou Zhang, Yu Cheng, Wenjun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a unique approach with AI algorithms to predict emerging
stock markets volatility. Traditionally, stock volatility is derived from
historical volatility,Monte Carlo simulation and implied volatility as well. In
this paper, the writer designs a consolidated model with back-propagation
neural network and genetic algorithm to predict future volatility of emerging
stock markets and found that the results are quite accurate with low errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, 1 table, The paper will be published by IEEE on
  conference: 2024 3rd International Conference on Image Processing, Computer
  Vision and Machine Learning (ICICML 2024) (V2)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimHawNet: A Modified Hawkes Process for Temporal Network Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07260v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07260v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathilde Perez, Raphaël Romero, Bo Kang, Tijl De Bie, Jefrey Lijffijt, Charlotte Laclau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal networks allow representing connections between objects while
incorporating the temporal dimension. While static network models can capture
unchanging topological regularities, they often fail to model the effects
associated with the causal generative process of the network that occurs in
time. Hence, exploiting the temporal aspect of networks has been the focus of
many recent studies. In this context, we propose a new framework for generative
models of continuous-time temporal networks. We assume that the activation of
the edges in a temporal network is driven by a specified temporal point
process. This approach allows to directly model the waiting time between events
while incorporating time-varying history-based features as covariates in the
predictions. Coupled with a thinning algorithm designed for the simulation of
point processes, SimHawNet enables simulation of the evolution of temporal
networks in continuous time. Finally, we introduce a comprehensive evaluation
framework to assess the performance of such an approach, in which we
demonstrate that SimHawNet successfully simulates the evolution of networks
with very different generative processes and achieves performance comparable to
the state of the art, while being significantly faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking quantum machine learning kernel training for classification
  tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Alvarez-Estevez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum-enhanced machine learning is a rapidly evolving field that aims to
leverage the unique properties of quantum mechanics to enhance classical
machine learning. However, the practical applicability of these methods remains
an open question, particularly beyond the context of specifically-crafted toy
problems, and given the current limitations of quantum hardware. This study
focuses on quantum kernel methods in the context of classification tasks. In
particular, it examines the performance of Quantum Kernel Estimation (QKE) and
Quantum Kernel Training (QKT) in connection with two quantum feature mappings,
namely ZZFeatureMap and CovariantFeatureMap. Remarkably, these feature maps
have been proposed in the literature under the conjecture of possible near-term
quantum advantage and have shown promising performance in ad-hoc datasets. In
this study, we aim to evaluate their versatility and generalization
capabilities in a more general benchmark, encompassing both artificial and
established reference datasets. Classical machine learning methods,
specifically Support Vector Machines (SVMs) and logistic regression, are also
incorporated as baseline comparisons. Experimental results indicate that
quantum methods exhibit varying performance across different datasets. Despite
outperforming classical methods in ad-hoc datasets, mixed results are obtained
for the general case among standard classical benchmarks. Our experiments call
into question a general added value of applying QKT optimization, for which the
additional computational cost does not necessarily translate into improved
classification performance. Instead, it is suggested that a careful choice of
the quantum feature map in connection with proper hyperparameterization may
prove more effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures; extended experiments and datasets, fixed typos;
  in consideration for publication in IEEE TQE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Formation-Controlled Dimensionality Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeuk Jeong, Yoon Mo Jung, Euntack Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction represents the process of generating a low
dimensional representation of high dimensional data. Motivated by the formation
control of mobile agents, we propose a nonlinear dynamical system for
dimensionality reduction. The system consists of two parts; the control of
neighbor points, addressing local structures, and the control of remote points,
accounting for global structures.We also include a brief mathematical analysis
of the model and its numerical procedure. Numerical experiments are performed
on both synthetic and real datasets and comparisons with existing models
demonstrate the soundness and effectiveness of the proposed model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparsity-Aware Distributed Learning for Gaussian Processes with Linear
  Multiple Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Cornelius Suwandi, Zhidi Lin, Feng Yin, Zhiguo Wang, Sergios Theodoridis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian processes (GPs) stand as crucial tools in machine learning and
signal processing, with their effectiveness hinging on kernel design and
hyper-parameter optimization. This paper presents a novel GP linear multiple
kernel (LMK) and a generic sparsity-aware distributed learning framework to
optimize the hyper-parameters. The newly proposed grid spectral mixture product
(GSMP) kernel is tailored for multi-dimensional data, effectively reducing the
number of hyper-parameters while maintaining good approximation capability. We
further demonstrate that the associated hyper-parameter optimization of this
kernel yields sparse solutions. To exploit the inherent sparsity of the
solutions, we introduce the Sparse LInear Multiple Kernel Learning (SLIM-KL)
framework. The framework incorporates a quantized alternating direction method
of multipliers (ADMM) scheme for collaborative learning among multiple agents,
where the local optimization problem is solved using a distributed successive
convex approximation (DSCA) algorithm. SLIM-KL effectively manages large-scale
hyper-parameter optimization for the proposed kernel, simultaneously ensuring
data privacy and minimizing communication costs. Theoretical analysis
establishes convergence guarantees for the learning framework, while
experiments on diverse datasets demonstrate the superior prediction performance
and efficiency of our proposed methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AALF: Almost Always Linear Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Jakobs, Thomas Liebig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works for time-series forecasting more and more leverage the high
predictive power of Deep Learning models. With this increase in model
complexity, however, comes a lack in understanding of the underlying model
decision process, which is problematic for high-stakes application scenarios.
At the same time, simple, interpretable forecasting methods such as ARIMA still
perform very well, sometimes on-par, with Deep Learning approaches. We argue
that simple models are good enough most of the time, and that forecasting
performance could be improved by choosing a Deep Learning method only for few,
important predictions, increasing the overall interpretability of the
forecasting process. In this context, we propose a novel online model selection
framework which learns to identify these predictions. An extensive empirical
study on various real-world datasets shows that our selection methodology
performs comparable to state-of-the-art online model selections methods in most
cases while being significantly more interpretable. We find that almost always
choosing a simple autoregressive linear model for forecasting results in
competitive performance, suggesting that the need for opaque black-box models
in time-series forecasting might be smaller than recent works would suggest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating alignment between humans and neural network representations
  in image-based learning tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09377v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09377v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Demircan, Tankred Saanum, Leonardo Pettini, Marcel Binz, Blazej M Baczkowski, Christian F Doeller, Mona M Garvert, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans represent scenes and objects in rich feature spaces, carrying
information that allows us to generalise about category memberships and
abstract functions with few examples. What determines whether a neural network
model generalises like a human? We tested how well the representations of $86$
pretrained neural network models mapped to human learning trajectories across
two tasks where humans had to learn continuous relationships and categories of
natural images. In these tasks, both human participants and neural networks
successfully identified the relevant stimulus features within a few trials,
demonstrating effective generalisation. We found that while training dataset
size was a core determinant of alignment with human choices, contrastive
training with multi-modal data (text and imagery) was a common feature of
currently publicly available models that predicted human generalisation.
Intrinsic dimensionality of representations had different effects on alignment
for different model types. Lastly, we tested three sets of human-aligned
representations and found no consistent improvements in predictive accuracy
compared to the baselines. In conclusion, pretrained neural networks can serve
to extract representations for cognitive models, as they appear to capture some
fundamental aspects of cognition that are transferable across tasks. Both our
paradigms and modelling approach offer a novel way to quantify alignment
between neural networks and humans and extend cognitive science into more
naturalistic domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction-Guided Fusion of Multi-Layer Visual Features in Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Li, Yi Zheng, Haotian Chen, Xiaolei Chen, Yuxuan Liang, Chenghang Lai, Bin Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have achieved significant success in
multimodal tasks by combining pre-trained vision encoders and large language
models. However, current LVLMs mainly rely on features from the final layers of
the vision encoder, neglecting complementary information in shallower layers.
While recent methods have explored multi-layer features, they are often
task-agnostic. We investigate the contributions of visual features from
different encoder layers across 18 benchmarks and 6 task categories. Our
results show that multi-layer features provide complementary strengths with
varying task dependencies, and uniform fusion performs suboptimally. Based on
these findings, we propose an instruction-guided vision aggregator that
dynamically integrates multi-layer features based on textual instructions,
without increasing the number of visual tokens. Extensive evaluations show
superior performance, and analysis reveals the dominance of mid-to-high-level
features in semantic tasks and the critical role of low-level features in
fine-grained perception. This work provides valuable insights into the adaptive
use of hierarchical visual features in LVLMs, advancing more flexible
multimodal systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in
  Reproducing Kernel Hilbert Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Stein, Sebastian Neumayer, Nicolaj Rux, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonly used $f$-divergences of measures, e.g., the Kullback-Leibler
divergence, are subject to limitations regarding the support of the involved
measures. A remedy is regularizing the $f$-divergence by a squared maximum mean
discrepancy (MMD) associated with a characteristic kernel $K$. We use the
kernel mean embedding to show that this regularization can be rewritten as the
Moreau envelope of some function on the associated reproducing kernel Hilbert
space. Then, we exploit well-known results on Moreau envelopes in Hilbert
spaces to analyze the MMD-regularized $f$-divergences, particularly their
gradients. Subsequently, we use our findings to analyze Wasserstein gradient
flows of MMD-regularized $f$-divergences. We provide proof-of-the-concept
numerical examples for flows starting from empirical measures. Here, we cover
$f$-divergences with infinite and finite recession constants. Lastly, we extend
our results to the tight variational formulation of $f$-divergences and
numerically compare the resulting flows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 14 figures, 3 tables. Comments welcome! NEW: Incorporated
  Reviewers' suggestions, added FISTA and tight formulation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Constraint Network from Demonstrations via Positive-Unlabeled
  Learning with Memory Replay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16485v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16485v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiyu Peng, Aude Billard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning for a wide range of real-world tasks necessitates to know and write
all constraints. However, instances exist where these constraints are either
unknown or challenging to specify accurately. A possible solution is to infer
the unknown constraints from expert demonstration. The majority of prior works
limit themselves to learning simple linear constraints, or require strong
knowledge of the true constraint parameterization or environmental model. To
mitigate these problems, this paper presents a positive-unlabeled (PU) learning
approach to infer a continuous, arbitrary and possibly nonlinear, constraint
from demonstration. From a PU learning view, We treat all data in
demonstrations as positive (feasible) data, and learn a (sub)-optimal policy to
generate high-reward-winning but potentially infeasible trajectories, which
serve as unlabeled data containing both feasible and infeasible states. Under
an assumption on data distribution, a feasible-infeasible classifier (i.e.,
constraint model) is learned from the two datasets through a postprocessing PU
learning technique. The entire method employs an iterative framework
alternating between updating the policy, which generates and selects
higher-reward policies, and updating the constraint model. Additionally, a
memory buffer is introduced to record and reuse samples from previous
iterations to prevent forgetting. The effectiveness of the proposed method is
validated in two Mujoco environments, successfully inferring continuous
nonlinear constraints and outperforming a baseline method in terms of
constraint accuracy and policy safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focus On This, Not That! Steering LLMs With Adaptive Feature
  Specification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H. S. Torr, Francesco Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Instruction Tuning (IT) in training large language
models (LLMs) to perform arbitrary user-specified tasks, these models often
still leverage spurious or biased features learned from their training data,
leading to undesired behaviours when deploying them in new contexts. In this
work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to
condition their responses by focusing on specific features whilst ignoring
others, leading to different behaviours based on what features are specified.
Across several experimental settings, we show that focus-tuned models can be
adaptively steered by focusing on different features at inference-time: for
instance, robustness can be improved by focusing on task-causal features and
ignoring spurious features, and social bias can be mitigated by ignoring
demographic categories. Furthermore, FIT can steer behaviour in new contexts,
generalising under distribution shift and to new unseen features at inference
time, and thereby facilitating more robust, fair, and controllable LLM
applications in real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models in Vision: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04747v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04747v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models represent a recent emerging topic in computer
vision, demonstrating remarkable results in the area of generative modeling. A
diffusion model is a deep generative model that is based on two stages, a
forward diffusion stage and a reverse diffusion stage. In the forward diffusion
stage, the input data is gradually perturbed over several steps by adding
Gaussian noise. In the reverse stage, a model is tasked at recovering the
original input data by learning to gradually reverse the diffusion process,
step by step. Diffusion models are widely appreciated for the quality and
diversity of the generated samples, despite their known computational burdens,
i.e. low speeds due to the high number of steps involved during sampling. In
this survey, we provide a comprehensive review of articles on denoising
diffusion models applied in vision, comprising both theoretical and practical
contributions in the field. First, we identify and present three generic
diffusion modeling frameworks, which are based on denoising diffusion
probabilistic models, noise conditioned score networks, and stochastic
differential equations. We further discuss the relations between diffusion
models and other deep generative models, including variational auto-encoders,
generative adversarial networks, energy-based models, autoregressive models and
normalizing flows. Then, we introduce a multi-perspective categorization of
diffusion models applied in computer vision. Finally, we illustrate the current
limitations of diffusion models and envision some interesting directions for
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence. 25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WindsorML: High-Fidelity Computational Fluid Dynamics <span class="highlight-title">Dataset</span> For
  Automotive Aerodynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19320v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19320v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Ashton, Jordan B. Angel, Aditya S. Ghate, Gaetan K. W. Kenway, Man Long Wong, Cetin Kiris, Astrid Walle, Danielle C. Maddix, Gary Page
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new open-source high-fidelity dataset for Machine
Learning (ML) containing 355 geometric variants of the Windsor body, to help
the development and testing of ML surrogate models for external automotive
aerodynamics. Each Computational Fluid Dynamics (CFD) simulation was run with a
GPU-native high-fidelity Wall-Modeled Large-Eddy Simulations (WMLES) using a
Cartesian immersed-boundary method using more than 280M cells to ensure the
greatest possible accuracy. The dataset contains geometry variants that
exhibits a wide range of flow characteristics that are representative of those
observed on road-cars. The dataset itself contains the 3D time-averaged volume
& boundary data as well as the geometry and force & moment coefficients. This
paper discusses the validation of the underlying CFD methods as well as
contents and structure of the dataset. To the authors knowledge, this
represents the first, large-scale high-fidelity CFD dataset for the Windsor
body with a permissive open-source license (CC-BY-SA).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a
  supervised-friendly fashion <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannis Flet-Berliac, Nathan Grinsztajn, Florian Strub, Bill Wu, Eugene Choi, Chris Cremer, Arash Ahmadian, Yash Chandak, Mohammad Gheshlaghi Azar, Olivier Pietquin, Matthieu Geist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has been used to finetune Large Language Models
(LLMs) using a reward model trained from preference data, to better align with
human judgment. The recently introduced direct alignment methods, which are
often simpler, more stable, and computationally lighter, can more directly
achieve this. However, these approaches cannot optimize arbitrary rewards, and
the preference-based ones are not the only rewards of interest for LLMs (eg.,
unit tests for code generation or textual entailment for summarization, among
others). RL-finetuning is usually done with a variation of policy gradient,
which calls for on-policy or near-on-policy samples, requiring costly
generations. We introduce Contrastive Policy Gradient, or CoPG, a simple and
mathematically principled new RL algorithm that can estimate the optimal policy
even from off-policy data. It can be seen as an off-policy policy gradient
approach that does not rely on important sampling techniques and highlights the
importance of using (the right) state baseline. We show this approach to
generalize the direct alignment method IPO (identity preference optimization)
and classic policy gradient. We experiment with the proposed CoPG on a toy
bandit problem to illustrate its properties, as well as for finetuning LLMs on
a summarization task, using a learned reward function considered as ground
truth for the purpose of the experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dataset</span>-Free Weight-Initialization on Restricted Boltzmann Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07708v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07708v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muneki Yasuda, Ryosuke Maeno, Chako Takahashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In feed-forward neural networks, dataset-free weight-initialization methods
such as LeCun, Xavier (or Glorot), and He initializations have been developed.
These methods randomly determine the initial values of weight parameters based
on specific distributions (e.g., Gaussian or uniform distributions) without
using training datasets. To the best of the authors' knowledge, such a
dataset-free weight-initialization method is yet to be developed for restricted
Boltzmann machines (RBMs), which are probabilistic neural networks consisting
of two layers. In this study, we derive a dataset-free weight-initialization
method for Bernoulli--Bernoulli RBMs based on statistical mechanical analysis.
In the proposed weight-initialization method, the weight parameters are drawn
from a Gaussian distribution with zero mean. The standard deviation of the
Gaussian distribution is optimized based on our hypothesis that a standard
deviation providing a larger layer correlation (LC) between the two layers
improves the learning efficiency. The expression of the LC is derived based on
a statistical mechanical analysis. The optimal value of the standard deviation
corresponds to the maximum point of the LC. The proposed weight-initialization
method is identical to Xavier initialization in a specific case (i.e., when the
sizes of the two layers are the same, the random variables of the layers are
$\{-1,1\}$-binary, and all bias parameters are zero). The validity of the
proposed weight-initialization method is demonstrated in numerical experiments
using a toy and real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Nonadiabatic Dynamics: Eliminating Phase Freedom of
  Nonadiabatic Couplings with the State-Intraction State-Averaged
  Spin-Restricted Ensemble-Referenced Kohn-Sham Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sung Wook Moon, Soohaeng Yoo Willow, Tae Hyeon Park, Seung Kyu Min, Chang Woo Myung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Excited-state molecular dynamics (ESMD) simulations near conical
intersections (CIs) pose significant challenges when using machine learning
potentials (MLPs). Although MLPs have gained recognition for their integration
into mixed quantum-classical (MQC) methods, such as trajectory surface hopping
(TSH), and their capacity to model correlated electron-nuclear dynamics
efficiently, difficulties persist in managing nonadiabatic dynamics.
Specifically, singularities at CIs and double-valued coupling elements result
in discontinuities that disrupt the smoothness of predictive functions. Partial
solutions have been provided by learning diabatic Hamiltonians with phaseless
loss functions to these challenges. However, a definitive method for addressing
the discontinuities caused by CIs and double-valued coupling elements has yet
to be developed. Here, we introduce the phaseless coupling term, $\Delta^2$,
derived from the square of the off-diagonal elements of the diabatic
Hamiltonian in the state-interaction state-averaged spin-restricted
ensemble-referenced Kohn-Sham (SI-SA-REKS, briefly SSR)(2,2) formalism. This
approach improves the stability and accuracy of the MLP model by addressing the
issues arising from CI singularities and double-valued coupling functions. We
apply this method to the penta-2,4-dieniminium cation (PSB3), demonstrating its
effectiveness in improving MLP training for ML-based nonadiabatic dynamics. Our
results show that the $\Delta^2$ based ML-ESMD method can reproduce ab initio
ESMD simulations, underscoring its potential and efficiency for broader
applications, particularly in large-scale and long-timescale ESMD simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Positive-Unlabeled Constraint Learning for Inferring Nonlinear
  Continuous Constraints Functions from Expert Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiyu Peng, Aude Billard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning for diverse real-world robotic tasks necessitates to know and write
all constraints. However, instances exist where these constraints are either
unknown or challenging to specify accurately. A possible solution is to infer
the unknown constraints from expert demonstration. This paper presents a novel
two-step Positive-Unlabeled Constraint Learning (PUCL) algorithm to infer a
continuous constraint function from demonstrations, without requiring prior
knowledge of the true constraint parameterization or environmental model as
existing works. We treat all data in demonstrations as positive (feasible)
data, and learn a control policy to generate potentially infeasible
trajectories, which serve as unlabeled data. The proposed two-step learning
framework first identifies reliable infeasible data using a distance metric,
and secondly learns a binary feasibility classifier (i.e., constraint function)
from the feasible demonstrations and reliable infeasible data. The proposed
method is flexible to learn complex-shaped constraint boundary and will not
mistakenly classify demonstrations as infeasible as previous methods. The
effectiveness of the proposed method is verified in four constrained
environments, using a networked policy or a dynamical system policy. It
successfully infers the continuous nonlinear constraints and outperforms other
baseline methods in terms of constraint accuracy and policy safety. This work
has been published in IEEE Robotics and Automation Letters (RA-L). Please refer
to the final version at https://doi.org/10.1109/LRA.2024.3522756
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERGNN: Spectral <span class="highlight-title">Graph</span> Neural Network With Explicitly-Optimized Rational
  <span class="highlight-title">Graph</span> Filters <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoming Li, Jian Yang, Shangsong Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximation-based spectral graph neural networks, which construct graph
filters with function approximation, have shown substantial performance in
graph learning tasks. Despite their great success, existing works primarily
employ polynomial approximation to construct the filters, whereas another
superior option, namely ration approximation, remains underexplored. Although a
handful of prior works have attempted to deploy the rational approximation,
their implementations often involve intensive computational demands or still
resort to polynomial approximations, hindering full potential of the rational
graph filters. To address the issues, this paper introduces ERGNN, a novel
spectral GNN with explicitly-optimized rational filter. ERGNN adopts a unique
two-step framework that sequentially applies the numerator filter and the
denominator filter to the input signals, thus streamlining the model paradigm
while enabling explicit optimization of both numerator and denominator of the
rational filter. Extensive experiments validate the superiority of ERGNN over
state-of-the-art methods, establishing it as a practical solution for deploying
rational-based GNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 2025 IEEE International Conference on Acoustics, Speech,
  and Signal Processing, ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Adaptive Collocation Point Strategy For Physics Informed Neural
  Networks via the QR Discrete Empirical Interpolation Method <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Celaya, David Fuentes, Beatrice Riviere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) have gained significant attention
for solving forward and inverse problems related to partial differential
equations (PDEs). While advancements in loss functions and network
architectures have improved PINN accuracy, the impact of collocation point
sampling on their performance remains underexplored. Fixed sampling methods,
such as uniform random sampling and equispaced grids, can fail to capture
critical regions with high solution gradients, limiting their effectiveness for
complex PDEs. Adaptive methods, inspired by adaptive mesh refinement from
traditional numerical methods, address this by dynamically updating collocation
points during training but may overlook residual dynamics between updates,
potentially losing valuable information. To overcome this limitation, we
propose an adaptive collocation point selection strategy utilizing the QR
Discrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling
technique for efficiently approximating nonlinear functions. Our results on
benchmark PDEs, including the wave, Allen-Cahn, and Burgers' equations,
demonstrate that our QR-DEIM-based approach improves PINN accuracy compared to
existing methods, offering a promising direction for adaptive collocation point
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICML 2025. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deterministic Uncertainty Propagation for Improved Model-Based Offline
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04088v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04088v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Akgül, Manuel Haußmann, Melih Kandemir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches to model-based offline reinforcement learning often
incorporate uncertainty-based reward penalization to address the distributional
shift problem. These approaches, commonly known as pessimistic value iteration,
use Monte Carlo sampling to estimate the Bellman target to perform temporal
difference-based policy evaluation. We find out that the randomness caused by
this sampling step significantly delays convergence. We present a theoretical
result demonstrating the strong dependency of suboptimality on the number of
Monte Carlo samples taken per Bellman target calculation. Our main contribution
is a deterministic approximation to the Bellman target that uses progressive
moment matching, a method developed originally for deterministic variational
inference. The resulting algorithm, which we call Moment Matching Offline
Model-Based Policy Optimization (MOMBO), propagates the uncertainty of the next
state through a nonlinear Q-network in a deterministic fashion by approximating
the distributions of hidden layer activations by a normal distribution. We show
that it is possible to provide tighter guarantees for the suboptimality of
MOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO
to converge faster than these approaches in a large set of benchmark tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangled Interleaving Variational Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noelle Y. L. Wong, Eng Yeow Cheu, Zhonglin Chiam, Dipti Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conflicting objectives present a considerable challenge in interleaving
multi-task learning, necessitating the need for meticulous design and balance
to ensure effective learning of a representative latent data space across all
tasks without mutual negative impact. Drawing inspiration from the concept of
marginal and conditional probability distributions in probability theory, we
design a principled and well-founded approach to disentangle the original input
into marginal and conditional probability distributions in the latent space of
a variational autoencoder. Our proposed model, Deep Disentangled Interleaving
Variational Encoding (DeepDIVE) learns disentangled features from the original
input to form clusters in the embedding space and unifies these features via
the cross-attention mechanism in the fusion stage. We theoretically prove that
combining the objectives for reconstruction and forecasting fully captures the
lower bound and mathematically derive a loss function for disentanglement using
Na\"ive Bayes. Under the assumption that the prior is a mixture of log-concave
distributions, we also establish that the Kullback-Leibler divergence between
the prior and the posterior is upper bounded by a function minimized by the
minimizer of the cross entropy loss, informing our adoption of radial basis
functions (RBF) and cross entropy with interleaving training for DeepDIVE to
provide a justified basis for convergence. Experiments on two public datasets
show that DeepDIVE disentangles the original input and yields forecast
accuracies better than the original VAE and comparable to existing
state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PeFLL: Personalized Federated Learning by Learning to Learn 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05515v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05515v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Scott, Hossein Zakerinia, Christoph H. Lampert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PeFLL, a new personalized federated learning algorithm that
improves over the state-of-the-art in three aspects: 1) it produces more
accurate models, especially in the low-data regime, and not only for clients
present during its training phase, but also for any that may emerge in the
future; 2) it reduces the amount of on-client computation and client-server
communication by providing future clients with ready-to-use personalized models
that require no additional finetuning or optimization; 3) it comes with
theoretical guarantees that establish generalization from the observed clients
to future ones. At the core of PeFLL lies a learning-to-learn approach that
jointly trains an embedding network and a hypernetwork. The embedding network
is used to represent clients in a latent descriptor space in a way that
reflects their similarity to each other. The hypernetwork takes as input such
descriptors and outputs the parameters of fully personalized client models. In
combination, both networks constitute a learning algorithm that achieves
state-of-the-art performance in several personalized federated learning
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM360 K2: Building a 65B 360-Open-Source Large Language Model from
  Scratch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengzhong Liu, Bowen Tan, Hongyi Wang, Willie Neiswanger, Tianhua Tao, Haonan Li, Fajri Koto, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Liqun Ma, Liping Tang, Nikhil Ranjan, Yonghao Zhuang, Guowei He, Renxi Wang, Mingkai Deng, Robin Algayres, Yuanzhi Li, Zhiqiang Shen, Preslav Nakov, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We detail the training of the LLM360 K2-65B model, scaling up our 360-degree
OPEN SOURCE approach to the largest and most powerful models under project
LLM360. While open-source LLMs continue to advance, the answer to "How are the
largest LLMs trained?" remains unclear within the community. The implementation
details for such high-capacity models are often protected due to business
considerations associated with their high cost. This lack of transparency
prevents LLM researchers from leveraging valuable insights from prior
experience, e.g., "What are the best practices for addressing loss spikes?" The
LLM360 K2 project addresses this gap by providing full transparency and access
to resources accumulated during the training of LLMs at the largest scale. This
report highlights key elements of the K2 project, including our first model, K2
DIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals
LLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the
implementation steps and present a longitudinal analysis of K2 DIAMOND's
capabilities throughout its training process. We also outline ongoing projects
such as TXT360, setting the stage for future models in the series. By offering
previously unavailable resources, the K2 project also resonates with the
360-degree OPEN SOURCE principles of transparency, reproducibility, and
accessibility, which we believe are vital in the era of resource-intensive AI
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing User Interest based on Stream Clustering and Memory Networks
  in Large-Scale <span class="highlight-title">Recommend</span>er Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13238v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13238v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Nian Wang, Cong Xu, Ming Zhao, Bin Wang, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RSs) provide personalized recommendation service based
on user interest, which are widely used in various platforms. However, there
are lots of users with sparse interest due to lacking consumption behaviors,
which leads to poor recommendation results for them. This problem is widespread
in large-scale RSs and is particularly difficult to address. To solve this
problem, we propose a novel solution named User Interest Enhancement (UIE)
which enhances user interest including user profile and user history behavior
sequences using the enhancement vectors and personalized enhancement vector
generated based on stream clustering and memory networks from different
perspectives. UIE not only remarkably improves model performance on the users
with sparse interest but also significantly enhance model performance on other
users. UIE is an end-to-end solution which is easy to be implemented based on
ranking model. Moreover, we expand our solution and apply similar methods to
long-tail items, which also achieves excellent improvement. Furthermore, we
conduct extensive offline and online experiments in a large-scale industrial
RS. The results demonstrate that our model outperforms other models remarkably,
especially for the users with sparse interest. Until now, UIE has been fully
deployed in multiple large-scale RSs and achieved remarkable improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplified and Generalized Masked Diffusion for Discrete Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04329v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04329v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, Michalis K. Titsias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked (or absorbing) diffusion is actively explored as an alternative to
autoregressive models for generative modeling of discrete data. However,
existing work in this area has been hindered by unnecessarily complex model
formulations and unclear relationships between different perspectives, leading
to suboptimal parameterization, training objectives, and ad hoc adjustments to
counteract these issues. In this work, we aim to provide a simple and general
framework that unlocks the full potential of masked diffusion models. We show
that the continuous-time variational objective of masked diffusion models is a
simple weighted integral of cross-entropy losses. Our framework also enables
training generalized masked diffusion models with state-dependent masking
schedules. When evaluated by perplexity, our models trained on OpenWebText
surpass prior diffusion language models at GPT-2 scale and demonstrate superior
performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our
models vastly outperform previous discrete diffusion models on pixel-level
image modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet 64x64) bits per
dimension that are better than autoregressive models of similar sizes. Our code
is available at https://github.com/google-deepmind/md4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code is available at:
  https://github.com/google-deepmind/md4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing Act: Prioritization Strategies for LLM-Designed Restless
  Bandit Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12112v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12112v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shresth Verma, Niclas Boehmer, Lingkai Kong, Milind Tambe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are increasingly used to design reward functions based on human
preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards
for Restless Multi-Armed Bandits, a framework for allocating limited resources
among agents. In applications such as public health, this approach empowers
grassroots health workers to tailor automated allocation decisions to community
needs. In the presence of multiple agents, altering the reward function based
on human preferences can impact subpopulations very differently, leading to
complex tradeoffs and a multi-objective resource allocation problem. We are the
first to present a principled method termed Social Choice Language Model for
dealing with these tradeoffs for LLM-designed rewards for multiagent planners
in general and restless bandits in particular. The novel part of our model is a
transparent and configurable selection component, called an adjudicator,
external to the LLM that controls complex tradeoffs via a user-selected social
welfare function. Our experiments demonstrate that our model reliably selects
more effective, aligned, and balanced reward functions compared to purely
LLM-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Assist Humans without Inferring Rewards <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Myers, Evan Ellis, Sergey Levine, Benjamin Eysenbach, Anca Dragan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive agents should make humans' lives easier. Classically, such
assistance is studied through the lens of inverse reinforcement learning, where
an assistive agent (e.g., a chatbot, a robot) infers a human's intention and
then selects actions to help the human reach that goal. This approach requires
inferring intentions, which can be difficult in high-dimensional settings. We
build upon prior work that studies assistance through the lens of empowerment:
an assistive agent aims to maximize the influence of the human's actions such
that they exert a greater control over the environmental outcomes and can solve
tasks in fewer steps. We lift the major limitation of prior work in this
area--scalability to high-dimensional settings--with contrastive successor
representations. We formally prove that these representations estimate a
similar notion of empowerment to that studied by prior work and provide a
ready-made mechanism for optimizing it. Empirically, our proposed method
outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a
cooperative game setting. Theoretically, our work connects ideas from
information theory, neuroscience, and reinforcement learning, and charts a path
for representations to play a critical role in solving assistive problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Neural Information Processing Systems (NeurIPS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discriminative Representation learning via Attention-Enhanced
  Contrastive Learning for Short Text Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has gained significant attention in short text
clustering, yet it has an inherent drawback of mistakenly identifying samples
from the same category as negatives and then separating them in the feature
space (false negative separation), which hinders the generation of superior
representations. To generate more discriminative representations for efficient
clustering, we propose a novel short text clustering method, called
Discriminative Representation learning via \textbf{A}ttention-\textbf{E}nhanced
\textbf{C}ontrastive \textbf{L}earning for Short Text Clustering
(\textbf{AECL}). The \textbf{AECL} consists of two modules which are the
pseudo-label generation module and the contrastive learning module. Both
modules build a sample-level attention mechanism to capture similarity
relationships between samples and aggregate cross-sample features to generate
consistent representations. Then, the former module uses the more
discriminative consistent representation to produce reliable supervision
information for assist clustering, while the latter module explores similarity
relationships and consistent representations optimize the construction of
positive samples to perform similarity-guided contrastive learning, effectively
addressing the false negative separation issue. Experimental results
demonstrate that the proposed \textbf{AECL} outperforms state-of-the-art
methods. If the paper is accepted, we will open-source the code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PsyDI: Towards a Personalized and Progressively In-depth Chatbot for
  Psychological Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyan Li, Xinyan Chen, Yazhe Niu, Shuai Hu, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of psychology, traditional assessment methods, such as
standardized scales, are frequently critiqued for their static nature, lack of
personalization, and reduced participant engagement, while comprehensive
counseling evaluations are often inaccessible. The complexity of quantifying
psychological traits further limits these methods. Despite advances with large
language models (LLMs), many still depend on single-round Question-and-Answer
interactions. To bridge this gap, we introduce PsyDI, a personalized and
progressively in-depth chatbot designed for psychological measurements,
exemplified by its application in the Myers-Briggs Type Indicator (MBTI)
framework. PsyDI leverages user-related multi-modal information and engages in
customized, multi-turn interactions to provide personalized, easily accessible
measurements, while ensuring precise MBTI type determination. To address the
challenge of unquantifiable psychological traits, we introduce a novel training
paradigm that involves learning the ranking of proxy variables associated with
these traits, culminating in a robust score model for MBTI measurements. The
score model enables PsyDI to conduct comprehensive and precise measurements
through multi-turn interactions within a unified estimation context. Through
various experiments, we validate the efficacy of both the score model and the
PsyDI pipeline, demonstrating its potential to serve as a general framework for
psychological measurements. Furthermore, the online deployment of PsyDI has
garnered substantial user engagement, with over 3,000 visits, resulting in the
collection of numerous multi-turn dialogues annotated with MBTI types, which
facilitates further research. The source code for the training and web service
components is publicly available as a part of OpenDILab at:
https://github.com/opendilab/PsyDI
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Overfitting in <span class="highlight-title">Graph</span> Neural Networks via Feature and
  Hyperplane Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15081v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15081v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonhyuk Choi, Jiho Choi, Taewook Ko, Chong-Kwon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are commonly used in semi-supervised settings.
Previous research has primarily focused on finding appropriate graph filters
(e.g. aggregation methods) to perform well on both homophilic and heterophilic
graphs. While these methods are effective, they can still suffer from the
sparsity of node features, where the initial data contain few non-zero
elements. This can lead to overfitting in certain dimensions in the first
projection matrix, as training samples may not cover the entire range of graph
filters (hyperplanes). To address this, we propose a novel data augmentation
strategy. Specifically, by flipping both the initial features and hyperplane,
we create additional space for training, which leads to more precise updates of
the learnable parameters and improved robustness for unseen features during
inference. To the best of our knowledge, this is the first attempt to mitigate
the overfitting caused by the initial features. Extensive experiments on
real-world datasets show that our proposed technique increases node
classification accuracy by up to 46.5% relatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced SPS Velocity-adaptive Scheme: Access Fairness in 5G NR V2I
  Networks <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Qiong Wu, Pingyi Fan, Kezhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-Infrastructure (V2I) technology enables information exchange
between vehicles and road infrastructure. Specifically, when a vehicle
approaches a roadside unit (RSU), it can exchange information with the RSU to
obtain accurate data that assists in driving. With the release of the 3rd
Generation Partnership Project (3GPP) Release 16, which includes the 5G New
Radio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt
mode-2 communication using sensing-based semi-persistent scheduling (SPS) for
resource allocation. In this approach, vehicles identify candidate resources
within a selection window and exclude ineligible resources based on information
from a sensing window. However, vehicles often drive at different speeds,
resulting in varying amounts of data transmission with RSUs as they pass by,
which leads to unfair access. Therefore, it is essential to design an access
scheme that accounts for different vehicle speeds to achieve fair access across
the network. This paper formulates an optimization problem for vehicular
networks and proposes a multi-objective optimization scheme to address it by
adjusting the selection window in the SPS mechanism of 5G NR V2I mode-2.
Simulation results demonstrate the effectiveness of the proposed scheme
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to IEEE Journal. The source code has
  been released at:
  https://github.com/qiongwu86/Enhanced-SPS-Velocity-adaptiveScheme-Access-Fariness-in-5G-NR-V2I-Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient PAC Learning of Halfspaces with Constant Malicious Noise Rate <span class="chip">ALT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding noise tolerance of machine learning algorithms is a central
quest in learning theory. In this work, we study the problem of computationally
efficient PAC learning of halfspaces in the presence of malicious noise, where
an adversary can corrupt both instances and labels of training samples. The
best-known noise tolerance either depends on a target error rate under
distributional assumptions or on a margin parameter under large-margin
conditions. In this work, we show that when both types of conditions are
satisfied, it is possible to achieve constant noise tolerance by minimizing a
reweighted hinge loss. Our key ingredients include: 1) an efficient algorithm
that finds weights to control the gradient deterioration from corrupted
samples, and 2) a new analysis on the robustness of the hinge loss equipped
with such weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ALT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLG-CBM: Training Concept Bottleneck Models with Vision-Language
  Guidance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01432v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01432v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyansh Srivastava, Ge Yan, Tsui-Wei Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept Bottleneck Models (CBMs) provide interpretable prediction by
introducing an intermediate Concept Bottleneck Layer (CBL), which encodes
human-understandable concepts to explain models' decision. Recent works
proposed to utilize Large Language Models and pre-trained Vision-Language
Models to automate the training of CBMs, making it more scalable and automated.
However, existing approaches still fall short in two aspects: First, the
concepts predicted by CBL often mismatch the input image, raising doubts about
the faithfulness of interpretation. Second, it has been shown that concept
values encode unintended information: even a set of random concepts could
achieve comparable test accuracy to state-of-the-art CBMs. To address these
critical limitations, in this work, we propose a novel framework called
Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful
interpretability with the benefits of boosted performance. Our method leverages
off-the-shelf open-domain grounded object detectors to provide visually
grounded concept annotation, which largely enhances the faithfulness of concept
prediction while further improving the model performance. In addition, we
propose a new metric called Number of Effective Concepts (NEC) to control the
information leakage and provide better interpretability. Extensive evaluations
across five standard benchmarks show that our method, VLG-CBM, outperforms
existing methods by at least 4.27% and up to 51.09% on Accuracy at NEC=5
(denoted as ANEC-5), and by at least 0.45% and up to 29.78% on average accuracy
(denoted as ANEC-avg), while preserving both faithfulness and interpretability
of the learned concepts as demonstrated in extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Types: Exploring the Impact of Type Checking on Neural Bug
  Detection in Dynamically Typed Languages <span class="chip">ICSE'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boqi Chen, José Antonio Hernández López, Gunter Mussbacher, Dániel Varró
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivation: Automated bug detection in dynamically typed languages such as
Python is essential for maintaining code quality. The lack of mandatory type
annotations in such languages can lead to errors that are challenging to
identify early with traditional static analysis tools. Recent progress in deep
neural networks has led to increased use of neural bug detectors. In statically
typed languages, a type checker is integrated into the compiler and thus taken
into consideration when the neural bug detector is designed for these
languages.
  Problem: However, prior studies overlook this aspect during the training and
testing of neural bug detectors for dynamically typed languages. When an
optional type checker is used, assessing existing neural bug detectors on bugs
easily detectable by type checkers may impact their performance estimation.
Moreover, including these bugs in the training set of neural bug detectors can
shift their detection focus toward the wrong type of bugs.
  Contribution: We explore the impact of type checking on various neural bug
detectors for variable misuse bugs, a common type targeted by neural bug
detectors. Existing synthetic and real-world datasets are type-checked to
evaluate the prevalence of type-related bugs. Then, we investigate how
type-related bugs influence the training and testing of the neural bug
detectors.
  Findings: Our findings indicate that existing bug detection datasets contain
a significant proportion of type-related bugs. Building on this insight, we
discover integrating the neural bug detector with a type checker can be
beneficial, especially when the code is annotated with types. Further
investigation reveals neural bug detectors perform better on type-related bugs
than other bugs. Moreover, removing type-related bugs from the training data
helps improve neural bug detectors' ability to identify bugs beyond the scope
of type checkers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICSE'25 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cost-aware Bayesian Optimization via the Pandora's Box Gittins <span class="highlight-title">Index</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20062v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20062v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Xie, Raul Astudillo, Peter I. Frazier, Ziv Scully, Alexander Terenin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization is a technique for efficiently optimizing unknown
functions in a black-box manner. To handle practical settings where gathering
data requires use of finite resources, it is desirable to explicitly
incorporate function evaluation costs into Bayesian optimization policies. To
understand how to do so, we develop a previously-unexplored connection between
cost-aware Bayesian optimization and the Pandora's Box problem, a decision
problem from economics. The Pandora's Box problem admits a Bayesian-optimal
solution based on an expression called the Gittins index, which can be
reinterpreted as an acquisition function. We study the use of this acquisition
function for cost-aware Bayesian optimization, and demonstrate empirically that
it performs well, particularly in medium-high dimensions. We further show that
this performance carries over to classical Bayesian optimization without
explicit evaluation costs. Our work constitutes a first step towards
integrating techniques from Gittins index theory into Bayesian optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Graph</span> Analysis Using a GPU-based Parallel Algorithm: Quantum Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14641v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14641v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, ZhiJie He, Ding Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The article introduces a new method for applying Quantum Clustering to graph
structures. Quantum Clustering (QC) is a novel density-based unsupervised
learning method that determines cluster centers by constructing a potential
function. In this method, we use the Graph Gradient Descent algorithm to find
the centers of clusters. GPU parallelization is utilized for computing
potential values. We also conducted experiments on five widely used datasets
and evaluated using four indicators. The results show superior performance of
the method. Finally, we discuss the influence of $\sigma$ on the experimental
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The surprising efficiency of temporal difference learning for rare event
  prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17638v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17638v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoou Cheng, Jonathan Weare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We quantify the efficiency of temporal difference (TD) learning over the
direct, or Monte Carlo (MC), estimator for policy evaluation in reinforcement
learning, with an emphasis on estimation of quantities related to rare events.
Policy evaluation is complicated in the rare event setting by the long
timescale of the event and by the need for \emph{relative accuracy} in
estimates of very small values. Specifically, we focus on least-squares TD
(LSTD) prediction for finite state Markov chains, and show that LSTD can
achieve relative accuracy far more efficiently than MC. We prove a central
limit theorem for the LSTD estimator and upper bound the \emph{relative
asymptotic variance} by simple quantities characterizing the connectivity of
states relative to the transition probabilities between them. Using this bound,
we show that, even when both the timescale of the rare event and the relative
accuracy of the MC estimator are exponentially large in the number of states,
LSTD maintains a fixed level of relative accuracy with a total number of
observed transitions of the Markov chain that is only \emph{polynomially} large
in the number of states.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final camera-ready version published at NeurIPS 2024. Correct an
  assumption statement and typos, and change/add a few sentences from the last
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Based Transfer Learning for Contextual Reinforcement Learning <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04498v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04498v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jung-Hoon Cho, Vindula Jayawardana, Sirui Li, Cathy Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (RL) is a powerful approach to complex decision
making. However, one issue that limits its practical application is its
brittleness, sometimes failing to train in the presence of small changes in the
environment. Motivated by the success of zero-shot transfer-where pre-trained
models perform well on related tasks-we consider the problem of selecting a
good set of training tasks to maximize generalization performance across a
range of tasks. Given the high cost of training, it is critical to select
training tasks strategically, but not well understood how to do so. We hence
introduce Model-Based Transfer Learning (MBTL), which layers on top of existing
RL methods to effectively solve contextual RL problems. MBTL models the
generalization performance in two parts: 1) the performance set point, modeled
using Gaussian processes, and 2) performance loss (generalization gap), modeled
as a linear function of contextual similarity. MBTL combines these two pieces
of information within a Bayesian optimization (BO) framework to strategically
select training tasks. We show theoretically that the method exhibits sublinear
regret in the number of training tasks and discuss conditions to further
tighten regret bounds. We experimentally validate our methods using urban
traffic and standard continuous control benchmarks. The experimental results
suggest that MBTL can achieve up to 43x improved sample efficiency compared
with canonical independent training and multi-task training. Further
experiments demonstrate the efficacy of BO and the insensitivity to the
underlying RL algorithm and hyperparameters. This work lays the foundations for
investigating explicit modeling of generalization, thereby enabling principled
yet effective methods for contextual RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Efficiency of Distributional Temporal Difference Learning
  and Freedman's Inequality in Hilbert Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05811v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05811v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Peng, Liangyu Zhang, Zhihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributional reinforcement learning (DRL) has achieved empirical success in
various domains. One core task in DRL is distributional policy evaluation,
which involves estimating the return distribution $\eta^\pi$ for a given policy
$\pi$. Distributional temporal difference learning has been accordingly
proposed, which extends the classic temporal difference learning (TD) in RL. In
this paper, we focus on the non-asymptotic statistical rates of distributional
TD. To facilitate theoretical analysis, we propose non-parametric
distributional TD (NTD). For a $\gamma$-discounted infinite-horizon tabular
Markov decision process, we show that for NTD with a generative model, we need
$\tilde{O}(\varepsilon^{-2}\mu_{\min}^{-1}(1-\gamma)^{-3})$ interactions with
the environment to achieve an $\varepsilon$-optimal estimator with high
probability, when the estimation error is measured by the $1$-Wasserstein. This
sample complexity bound is minimax optimal up to logarithmic factors. In
addition, we revisit categorical distributional TD (CTD), showing that the same
non-asymptotic convergence bounds hold for CTD in the case of the
$1$-Wasserstein distance. We also extend our analysis to the more general
setting where the data generating process is Markovian. In the Markovian
setting, we propose variance-reduced variants of NTD and CTD, and show that
both can achieve a $\tilde{O}(\varepsilon^{-2}
\mu_{\pi,\min}^{-1}(1-\gamma)^{-3}+t_{mix}\mu_{\pi,\min}^{-1}(1-\gamma)^{-1})$
sample complexity bounds in the case of the $1$-Wasserstein distance, which
matches the state-of-the-art statistical results for classic policy evaluation.
To achieve the sharp statistical rates, we establish a novel Freedman's
inequality in Hilbert spaces. This new Freedman's inequality would be of
independent interest for statistical analysis of various infinite-dimensional
online learning problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Framework for Inference-time Scaling and Steering of Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models produce impressive results in modalities ranging from images
and video to protein design and text. However, generating samples with
user-specified properties remains a challenge. Recent research proposes
fine-tuning models to maximize rewards that capture desired properties, but
these methods require expensive training and are prone to mode collapse. In
this work, we propose Feynman Kac (FK) steering, an inference-time framework
for steering diffusion models with reward functions. FK steering works by
sampling a system of multiple interacting diffusion processes, called
particles, and resampling particles at intermediate steps based on scores
computed using functions called potentials. Potentials are defined using
rewards for intermediate states and are selected such that a high value
indicates that the particle will yield a high-reward sample. We explore various
choices of potentials, intermediate rewards, and samplers. We evaluate FK
steering on text-to-image and text diffusion models. For steering text-to-image
models with a human preference reward, we find that FK steering a 0.8B
parameter model outperforms a 2.6B parameter fine-tuned model on prompt
fidelity, with faster sampling and no training. For steering text diffusion
models with rewards for text quality and specific text attributes, we find that
FK steering generates lower perplexity, more linguistically acceptable outputs
and enables gradient-free control of attributes like toxicity. Our results
demonstrate that inference-time scaling and steering of diffusion models, even
with off-the-shelf rewards, can provide significant sample quality gains and
controllability benefits. Code is available at
https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smoothness Really Matters: A Simple Yet Effective Approach for
  Unsupervised <span class="highlight-title">Graph</span> Domain Adaptation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11654v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11654v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chen, Guo Ye, Yakun Wang, Zhao Zhang, Libang Zhang, Daixin Wang, Zhiqiang Zhang, Fuzhen Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution
shifts between domains by transferring knowledge from labeled source graphs to
given unlabeled target graphs. Existing UGDA methods primarily focus on
aligning features in the latent space learned by graph neural networks (GNNs)
across domains, often overlooking structural shifts, resulting in limited
effectiveness when addressing structurally complex transfer scenarios. Given
the sensitivity of GNNs to local structural features, even slight discrepancies
between source and target graphs could lead to significant shifts in node
embeddings, thereby reducing the effectiveness of knowledge transfer. To
address this issue, we introduce a novel approach for UGDA called Target-Domain
Structural Smoothing (TDSS). TDSS is a simple and effective method designed to
perform structural smoothing directly on the target graph, thereby mitigating
structural distribution shifts and ensuring the consistency of node
representations. Specifically, by integrating smoothing techniques with
neighborhood sampling, TDSS maintains the structural coherence of the target
graph while mitigating the risk of over-smoothing. Our theoretical analysis
shows that TDSS effectively reduces target risk by improving model smoothness.
Empirical results on three real-world datasets demonstrate that TDSS
outperforms recent state-of-the-art baselines, achieving significant
improvements across six transfer scenarios. The code is available in
https://github.com/cwei01/TDSS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, Accpected by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span>ing Attitudinal Alignment Between Large Language Models Vs. Humans
  Towards 17 Sustainable Development Goals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Wu, Ying Xu, Tingsong Xiao, Yunze Xiao, Yitong Li, Tianyang Wang, Yichi Zhang, Shanghai Zhong, Yuwei Zhang, Wei Lu, Yifan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as potent tools for advancing the
United Nations' Sustainable Development Goals (SDGs). However, the attitudinal
disparities between LLMs and humans towards these goals can pose significant
challenges. This study conducts a comprehensive review and analysis of the
existing literature on the attitudes of LLMs towards the 17 SDGs, emphasizing
the comparison between their attitudes and support for each goal and those of
humans. We examine the potential disparities, primarily focusing on aspects
such as understanding and emotions, cultural and regional differences, task
objective variations, and factors considered in the decision-making process.
These disparities arise from the underrepresentation and imbalance in LLM
training data, historical biases, quality issues, lack of contextual
understanding, and skewed ethical values reflected. The study also investigates
the risks and harms that may arise from neglecting the attitudes of LLMs
towards the SDGs, including the exacerbation of social inequalities, racial
discrimination, environmental destruction, and resource wastage. To address
these challenges, we propose strategies and recommendations to guide and
regulate the application of LLMs, ensuring their alignment with the principles
and goals of the SDGs, and therefore creating a more just, inclusive, and
sustainable future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Algorithm for Training Autonomous Vehicles with Minimal Human
  Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sang-Hyun Lee, Daehyeok Kwon, Seung-Woo Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent reinforcement learning (RL) algorithms have demonstrated impressive
results in simulated driving environments. However, autonomous vehicles trained
in simulation often struggle to work well in the real world due to the fidelity
gap between simulated and real-world environments. While directly training
real-world autonomous vehicles with RL algorithms is a promising approach to
bypass the fidelity gap problem, it presents several challenges. One critical
yet often overlooked challenge is the need to reset a driving environment
between every episode. This reset process demands significant human
intervention, leading to poor training efficiency in the real world. In this
paper, we introduce a novel autonomous algorithm that enables off-the-shelf RL
algorithms to train autonomous vehicles with minimal human intervention. Our
algorithm reduces unnecessary human intervention by aborting episodes to
prevent unsafe states and identifying informative initial states for subsequent
episodes. The key idea behind identifying informative initial states is to
estimate the expected amount of information that can be obtained from
under-explored but reachable states. Our algorithm also revisits rule-based
autonomous driving algorithms and highlights their benefits in safely returning
an autonomous vehicle to initial states. To evaluate how much human
intervention is required during training, we implement challenging urban
driving tasks that require an autonomous vehicle to reset to initial states on
its own. The experimental results show that our autonomous algorithm is
task-agnostic and achieves competitive driving performance with much less human
intervention than baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 2 tables, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Deep Subspace Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupei Zhang, Ruojia Feng, Yifei Wang, Xuequn Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FDSC, a private-protected subspace clustering (SC)
approach with federated learning (FC) schema. In each client, there is a deep
subspace clustering network accounting for grouping the isolated data, composed
of a encode network, a self-expressive layer, and a decode network. FDSC is
achieved by uploading the encode network to communicate with other clients in
the server. Besides, FDSC is also enhanced by preserving the local neighborhood
relationship in each client. With the effects of federated learning and
locality preservation, the learned data features from the encoder are boosted
so as to enhance the self-expressiveness learning and result in better
clustering performance. Experiments test FDSC on public datasets and compare
with other clustering methods, demonstrating the effectiveness of FDSC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8pages,4 figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gameplay Filters: Robust Zero-Shot Safety through Adversarial
  Imagination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00846v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00846v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy P. Nguyen, Kai-Chieh Hsu, Wenhao Yu, Jie Tan, Jaime F. Fisac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive recent advances in learning-based robot control,
ensuring robustness to out-of-distribution conditions remains an open
challenge. Safety filters can, in principle, keep arbitrary control policies
from incurring catastrophic failures by overriding unsafe actions, but existing
solutions for complex (e.g., legged) robot dynamics do not span the full motion
envelope and instead rely on local, reduced-order models. These filters tend to
overly restrict agility and can still fail when perturbed away from nominal
conditions. This paper presents the gameplay filter, a new class of predictive
safety filter that continually plays out hypothetical matches between its
simulation-trained safety strategy and a virtual adversary co-trained to invoke
worst-case events and sim-to-real error, and precludes actions that would cause
failures down the line. We demonstrate the scalability and robustness of the
approach with a first-of-its-kind full-order safety filter for (36-D)
quadrupedal dynamics. Physical experiments on two different quadruped platforms
demonstrate the superior zero-shot effectiveness of the gameplay filter under
large perturbations such as tugging and unmodeled terrain. Experiment videos
and open-source software are available online:
https://saferobotics.org/research/gameplay-filter
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing <span class="highlight-title">Graph</span> <span class="highlight-title">Self-Supervised</span> Learning with <span class="highlight-title">Graph</span> Interplay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04061v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04061v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjian Zhao, Wei Pang, Xiangru Jian, Yaoyao Xu, Chaolong Ying, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph self-supervised learning (GSSL) has emerged as a compelling framework
for extracting informative representations from graph-structured data without
extensive reliance on labeled inputs. In this study, we introduce Graph
Interplay (GIP), an innovative and versatile approach that significantly
enhances the performance equipped with various existing GSSL methods. To this
end, GIP advocates direct graph-level communications by introducing random
inter-graph edges within standard batches. Against GIP's simplicity, we further
theoretically show that \textsc{GIP} essentially performs a principled manifold
separation via combining inter-graph message passing and GSSL, bringing about
more structured embedding manifolds and thus benefits a series of downstream
tasks. Our empirical study demonstrates that GIP surpasses the performance of
prevailing GSSL methods across multiple benchmarks by significant margins,
highlighting its potential as a breakthrough approach. Besides, GIP can be
readily integrated into a series of GSSL methods and consistently offers
additional performance gain. This advancement not only amplifies the capability
of GSSL but also potentially sets the stage for a novel graph learning paradigm
in a broader sense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Due to potential implicit data leakage in our experimental setup,
  where the pretraining dataset was ordered by default labels, we withdraw this
  manuscript for further self-examination and rigorous validation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryoBench: Diverse and challenging <span class="highlight-title">dataset</span>s for the heterogeneity
  problem in cryo-EM <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05526v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05526v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkyu Jeon, Rishwanth Raghu, Miro Astore, Geoffrey Woollard, Ryan Feathers, Alkin Kaz, Sonya M. Hanson, Pilar Cossio, Ellen D. Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryo-electron microscopy (cryo-EM) is a powerful technique for determining
high-resolution 3D biomolecular structures from imaging data. Its unique
ability to capture structural variability has spurred the development of
heterogeneous reconstruction algorithms that can infer distributions of 3D
structures from noisy, unlabeled imaging data. Despite the growing number of
advanced methods, progress in the field is hindered by the lack of standardized
benchmarks with ground truth information and reliable validation metrics. Here,
we introduce CryoBench, a suite of datasets, metrics, and benchmarks for
heterogeneous reconstruction in cryo-EM. CryoBench includes five datasets
representing different sources of heterogeneity and degrees of difficulty.
These include conformational heterogeneity generated from designed motions of
antibody complexes or sampled from a molecular dynamics simulation, as well as
compositional heterogeneity from mixtures of ribosome assembly states or 100
common complexes present in cells. We then analyze state-of-the-art
heterogeneous reconstruction tools, including neural and non-neural methods,
assess their sensitivity to noise, and propose new metrics for quantitative
evaluation. We hope that CryoBench will be a foundational resource for
accelerating algorithmic development and evaluation in the cryo-EM and machine
learning communities. Project page: https://cryobench.cs.princeton.edu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An efficient likelihood-free Bayesian inference method based on
  sequential neural posterior estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12530v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12530v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Xiong, Xiliang Yang, Sanguo Zhang, Zhijian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential neural posterior estimation (SNPE) techniques have been recently
proposed for dealing with simulation-based models with intractable likelihoods.
Unlike approximate Bayesian computation, SNPE techniques learn the posterior
from sequential simulation using neural network-based conditional density
estimators by minimizing a specific loss function. The SNPE method proposed by
Lueckmann et al. (2017) used a calibration kernel to boost the sample weights
around the observed data, resulting in a concentrated loss function. However,
the use of calibration kernels may increase the variances of both the empirical
loss and its gradient, making the training inefficient. To improve the
stability of SNPE, this paper proposes to use an adaptive calibration kernel
and several variance reduction techniques. The proposed method greatly speeds
up the process of training and provides a better approximation of the posterior
than the original SNPE method and some existing competitors as confirmed by
numerical experiments. We also managed to demonstrate the superiority of the
proposed method for a high-dimensional model with a real-world dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the uncertainty principle of neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01493v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01493v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Jie Zhang, Dong-Xiao Zhang, Jian-Nan Chen, Long-Gang Pang, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we explore the inherent trade-off between accuracy and
robustness in neural networks, drawing an analogy to the uncertainty principle
in quantum mechanics. We propose that neural networks are subject to an
uncertainty relation, which manifests as a fundamental limitation in their
ability to simultaneously achieve high accuracy and robustness against
adversarial attacks. Through mathematical proofs and empirical evidence, we
demonstrate that this trade-off is a natural consequence of the sharp
boundaries formed between different class concepts during training. Our
findings reveal that the complementarity principle, a cornerstone of quantum
physics, applies to neural networks, imposing fundamental limits on their
capabilities in simultaneous learning of conjugate features. Meanwhile, our
work suggests that achieving human-level intelligence through a single network
architecture or massive datasets alone may be inherently limited. Our work
provides new insights into the theoretical foundations of neural network
vulnerability and opens up avenues for designing more robust neural network
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hidden Markov Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.06963v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.06963v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Rimella, Nick Whiteley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We define an evolving in-time Bayesian neural network called a Hidden Markov
Neural Network, which addresses the crucial challenge in time-series
forecasting and continual learning: striking a balance between adapting to new
data and appropriately forgetting outdated information. This is achieved by
modelling the weights of a neural network as the hidden states of a Hidden
Markov model, with the observed process defined by the available data. A
filtering algorithm is employed to learn a variational approximation of the
evolving-in-time posterior distribution over the weights. By leveraging a
sequential variant of Bayes by Backprop, enriched with a stronger
regularization technique called variational DropConnect, Hidden Markov Neural
Networks achieve robust regularization and scalable inference. Experiments on
MNIST, dynamic classification tasks, and next-frame forecasting in videos
demonstrate that Hidden Markov Neural Networks provide strong predictive
performance while enabling effective uncertainty quantification.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metric Learning with Progressive Self-Distillation for Audio-Visual
  Embedding Learning <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghuo Zeng, Kazushi Ikeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning projects samples into an embedded space, where similarities
and dissimilarities are quantified based on their learned representations.
However, existing methods often rely on label-guided representation learning,
where representations of different modalities, such as audio and visual data,
are aligned based on annotated labels. This approach tends to underutilize
latent complex features and potential relationships inherent in the
distributions of audio and visual data that are not directly tied to the
labels, resulting in suboptimal performance in audio-visual embedding learning.
To address this issue, we propose a novel architecture that integrates
cross-modal triplet loss with progressive self-distillation. Our method
enhances representation learning by leveraging inherent distributions and
dynamically refining soft audio-visual alignments -- probabilistic alignments
between audio and visual data that capture the inherent relationships beyond
explicit labels. Specifically, the model distills audio-visual
distribution-based knowledge from annotated labels in a subset of each batch.
This self-distilled knowledge is used t
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 2 tables. Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAL: <span class="highlight-title">Prompt</span>ing Analytic Learning with Missing Modality for Multi-Modal
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghu Yue, Yiming Chen, Xueyi Zhang, Xiaoxue Gao, Mengling Feng, Mingrui Lao, Huiping Zhuang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal class-incremental learning (MMCIL) seeks to leverage multi-modal
data, such as audio-visual and image-text pairs, thereby enabling models to
learn continuously across a sequence of tasks while mitigating forgetting.
While existing studies primarily focus on the integration and utilization of
multi-modal information for MMCIL, a critical challenge remains: the issue of
missing modalities during incremental learning phases. This oversight can
exacerbate severe forgetting and significantly impair model performance. To
bridge this gap, we propose PAL, a novel exemplar-free framework tailored to
MMCIL under missing-modality scenarios. Concretely, we devise modality-specific
prompts to compensate for missing information, facilitating the model to
maintain a holistic representation of the data. On this foundation, we
reformulate the MMCIL problem into a Recursive Least-Squares task, delivering
an analytical linear solution. Building upon these, PAL not only alleviates the
inherent under-fitting limitation in analytic learning but also preserves the
holistic representation of missing-modality data, achieving superior
performance with less forgetting across various multi-modal incremental
scenarios. Extensive experiments demonstrate that PAL significantly outperforms
competitive methods across various datasets, including UPMC-Food101 and
N24News, showcasing its robustness towards modality absence and its
anti-forgetting ability to maintain high incremental accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyeongha Rho, Hyeongkeun Lee, Valentio Iverson, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated audio captioning is a task that generates textual descriptions for
audio content, and recent studies have explored using visual information to
enhance captioning quality. However, current methods often fail to effectively
fuse audio and visual data, missing important semantic cues from each modality.
To address this, we introduce LAVCap, a large language model (LLM)-based
audio-visual captioning framework that effectively integrates visual
information with audio to improve audio captioning performance. LAVCap employs
an optimal transport-based alignment loss to bridge the modality gap between
audio and visual features, enabling more effective semantic extraction.
Additionally, we propose an optimal transport attention module that enhances
audio-visual fusion using an optimal transport assignment map. Combined with
the optimal training strategy, experimental results demonstrate that each
component of our framework is effective. LAVCap outperforms existing
state-of-the-art methods on the AudioCaps dataset, without relying on large
datasets or post-processing. Code is available at
https://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures; Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagnetDB: A Longitudinal Torrent Discovery <span class="highlight-title">Dataset</span> with IMDb-Matched
  Movies and TV Shows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Seidenberger, Noah Pursell, Anindya Maiti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  BitTorrent remains a prominent channel for illicit distribution of
copyrighted material, yet the supply side of such content remains understudied.
We introduce MagnetDB, a longitudinal dataset of torrents discovered through
the BitTorrent DHT between 2018 and 2024, containing more than 28.6 million
torrents and metadata of more than 950 million files. While our primary focus
is on enabling research based on the supply of pirated movies and TV shows, the
dataset also encompasses other legitimate and illegitimate torrents. By
applying IMDb-matching and annotation to movie and TV show torrents, MagnetDB
facilitates detailed analyses of pirated content evolution in the BitTorrent
network. Researchers can leverage MagnetDB to examine distribution trends,
subcultural practices, and the gift economy within piracy ecosystems. Through
its scale and temporal scope, MagnetDB presents a unique opportunity for
investigating the broader dynamics of BitTorrent and advancing empirical
knowledge on digital piracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frechet Music Distance: A Metric For Generative Symbolic Music
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Retkowski, Jakub Stępniak, Mateusz Modrzejewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce the Frechet Music Distance (FMD), a novel
evaluation metric for generative symbolic music models, inspired by the Frechet
Inception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in
generative audio. FMD calculates the distance between distributions of
reference and generated symbolic music embeddings, capturing abstract musical
features. We validate FMD across several datasets and models. Results indicate
that FMD effectively differentiates model quality, providing a domain-specific
metric for evaluating symbolic music generation, and establishing a
reproducible standard for future research in symbolic music modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery
  from Videos <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13397v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13397v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery (HMR) provides rich human body information for various
real-world applications. While image-based HMR methods have achieved impressive
results, they often struggle to recover humans in dynamic scenarios, leading to
temporal inconsistencies and non-smooth 3D motion predictions due to the
absence of human motion. In contrast, video-based approaches leverage temporal
information to mitigate this issue. In this paper, we present DiffMesh, an
innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh
establishes a bridge between diffusion models and human motion, efficiently
generating accurate and smooth output mesh sequences by incorporating human
motion within the forward process and reverse process in the diffusion model.
Extensive experiments are conducted on the widely used datasets (Human3.6M
\cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness
and efficiency of our DiffMesh. Visual comparisons in real-world scenarios
further highlight DiffMesh's suitability for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum-Enhanced Transformers for Robust Acoustic Scene Classification
  in IoT Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, Pubudu N. Pathirana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of Internet of Things (IoT) devices equipped with acoustic
sensors necessitates robust acoustic scene classification (ASC) capabilities,
even in noisy and data-limited environments. Traditional machine learning
methods often struggle to generalize effectively under such conditions. To
address this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene
Classifier that leverages the power of quantum-inspired transformers. By
integrating quantum concepts like superposition and entanglement, Q-ASC
achieves superior feature learning and enhanced noise resilience compared to
classical models. Furthermore, we introduce a Quantum Variational Autoencoder
(QVAE) based data augmentation technique to mitigate the challenge of limited
labeled data in IoT deployments. Extensive evaluations on the Tampere
University of Technology (TUT) Acoustic Scenes 2016 benchmark dataset
demonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%
under challenging conditions, outperforming state-of-the-art methods by over 5%
in the best case. This research paves the way for deploying intelligent
acoustic sensing in IoT networks, with potential applications in smart homes,
industrial monitoring, and environmental surveillance, even in adverse acoustic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Database <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi<span class="highlight-title">Graph</span>Match: a sub<span class="highlight-title">graph</span> matching algorithm for multi<span class="highlight-title">graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Micale, Antonio Di Maria, Roberto Grasso, Vincenzo Bonnici, Alfredo Ferro, Dennis Shasha, Rosalba Giugno, Alfredo Pulvirenti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subgraph matching is the problem of finding all the occurrences of a small
graph, called the query, in a larger graph, called the target. Although the
problem has been widely studied in simple graphs, few solutions have been
proposed for multigraphs, in which two nodes can be connected by multiple
edges, each denoting a possibly different type of relationship. In our new
algorithm MultiGraphMatch, nodes and edges can be associated with labels and
multiple properties. MultiGraphMatch introduces a novel data structure called
bit matrix to efficiently index both the query and the target and filter the
set of target edges that are matchable with each query edge. In addition, the
algorithm proposes a new technique for ordering the processing of query edges
based on the cardinalities of the sets of matchable edges. Using the CYPHER
query definition language, MultiGraphMatch can perform queries with logical
conditions on node and edge labels. We compare MultiGraphMatch with SuMGra and
graph database systems Memgraph and Neo4J, showing comparable or better
performance in all queries on a wide variety of synthetic and real-world
graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for pubblication on January 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jodes: Efficient Oblivious Join in the Distributed Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilei Wang, Xiangdong Zeng, Sheng Wang, Feifei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trusted execution environment (TEE) has provided an isolated and secure
environment for building cloud-based analytic systems, but it still suffers
from access pattern leakages caused by side-channel attacks. To better secure
the data, computation inside TEE enclave should be made oblivious, which
introduces significant overhead and severely slows down the computation. A
natural way to speed up is to build the analytic system with multiple servers
in the distributed setting. However, this setting raises a new security concern
-- the volumes of the transmissions among these servers can leak sensitive
information to a network adversary. Existing works have designed specialized
algorithms to address this concern, but their supports for equi-join, one of
the most important but non-trivial database operators, are either inefficient,
limited, or under a weak security assumption.
  In this paper, we present Jodes, an efficient oblivious join algorithm in the
distributed setting. Jodes prevents the leakage on both the network and enclave
sides, supports a general equi-join operation, and provides a high security
level protection that only publicizes the input sizes and the output size.
Meanwhile, it achieves both communication cost and computation cost
asymptotically superior to existing algorithms. To demonstrate the practicality
of Jodes, we conduct experiments in the distributed setting comprising 16
servers. Empirical results show that Jodes achieves up to a sixfold performance
improvement over state-of-the-art join algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Integration of Data Lake <span class="highlight-title">Table</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aamod Khatiwada, Roee Shraga, Renée J. Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data integration is an important step in any data science pipeline where the
objective is to unify the information available in different datasets for
comprehensive analysis. Full Disjunction, which is an associative extension of
the outer join operator, has been shown to be an effective operator for
integrating datasets. It fully preserves and combines the available
information. Existing Full Disjunction algorithms only consider the equi-join
scenario where only tuples having the same value on joining columns are
integrated. This, however, does not realistically represent an open data
scenario, where datasets come from diverse sources with inconsistent values
(e.g., synonyms, abbreviations, etc.) and with limited metadata. So, joining
just on equal values severely limits the ability of Full Disjunction to fully
combine datasets. Thus, in this work, we propose an extension of Full
Disjunction to also account for "fuzzy" matches among tuples. We present a
novel data-driven approach to enable the joining of approximate or fuzzy
matches within Full Disjunction. Experimentally, we show that fuzzy Full
Disjunction does not add significant time overhead over a state-of-the-art Full
Disjunction implementation and also that it enhances the integration
effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2KQL: From Natural Language to Kusto Query 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02933v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02933v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinye Tang, Amir H. Abdi, Jeremias Eichelbaum, Mahan Das, Alex Klein, Nihal Irmak Pakis, William Blum, Daniel L Mace, Tanvi Raja, Namrata Padmanabhan, Ye Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is growing rapidly in volume and complexity. Proficiency in database
query languages is pivotal for crafting effective queries. As coding assistants
become more prevalent, there is significant opportunity to enhance database
query languages. The Kusto Query Language (KQL) is a widely used query language
for large semi-structured data such as logs, telemetries, and time-series for
big data analytics platforms. This paper introduces NL2KQL an innovative
framework that uses large language models (LLMs) to convert natural language
queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several
key components: Schema Refiner which narrows down the schema to its most
pertinent elements; the Few-shot Selector which dynamically selects relevant
examples from a few-shot dataset; and the Query Refiner which repairs syntactic
and semantic errors in KQL queries. Additionally, this study outlines a method
for generating large datasets of synthetic NLQ-KQL pairs which are valid within
a specific database contexts. To validate NL2KQL's performance, we utilize an
array of online (based on query execution) and offline (based on query parsing)
metrics. Through ablation studies, the significance of each framework component
is examined, and the datasets used for benchmarking are made publicly
available. This work is the first of its kind and is compared with available
baselines to demonstrate its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective and General Distance Computation for Approximate Nearest
  Neighbor Search <span class="chip">ICDE2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Yang, Wentao Li, Jiabao Jin, Xiaoyao Zhong, Xiangyu Wang, Zhitao Shen, Wei Jia, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate K Nearest Neighbor (AKNN) search in high-dimensional spaces is a
critical yet challenging problem. In AKNN search, distance computation is the
core task that dominates the runtime. Existing approaches typically use
approximate distances to improve computational efficiency, often at the cost of
reduced search accuracy. To address this issue, the state-of-the-art method,
ADsampling, employs random projections to estimate approximate distances and
introduces an additional distance correction process to mitigate accuracy loss.
However, ADsampling has limitations in both effectiveness and generality,
primarily due to its heavy reliance on random projections for distance
approximation and correction. Motivated by this, we leverage data distribution
to improve distance approximation via orthogonal projection, thereby addressing
the effectiveness limitation of ADsampling; we also adopt a data-driven
approach to distance correction, decoupling the correction process from the
distance approximation process, thereby overcoming the generality limitation of
ADsampling. Extensive experiments demonstrate the superiority and effectiveness
of our method. In particular, compared to ADsampling, our method achieves a
speedup of 1.6 to 2.1 times on real-world datasets while providing higher
accuracy. In addition, our method shows superior performance in Ant Group image
search scenarios and has been integrated into their search engine VSAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDE2025 code available at:
  github.com/mingyu-hkustgz/Res-Infer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMRxRecon2024: A Multi-Modality, Multi-View K-Space <span class="highlight-title">Dataset</span> Boosting
  Universal Machine Learning for Accelerated Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Wang, Fanwen Wang, Chen Qin, Jun Lyu, Cheng Ouyang, Shuo Wang, Yan Li, Mengyao Yu, Haoyu Zhang, Kunyuan Guo, Zhang Shi, Qirong Li, Ziqiang Xu, Yajing Zhang, Hao Li, Sha Hua, Binghua Chen, Longyu Sun, Mengting Sun, Qin Li, Ying-Hua Chu, Wenjia Bai, Jing Qin, Xiahai Zhuang, Claudia Prieto, Alistair Young, Michael Markl, He Wang, Lianming Wu, Guang Yang, Xiaobo Qu, Chengyan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically
gold-standard technique for diagnosing cardiac diseases, thanks to its ability
to provide diverse information with multiple modalities and anatomical views.
Accelerated cardiac MRI is highly expected to achieve time-efficient and
patient-friendly imaging, and then advanced image reconstruction approaches are
required to recover high-quality, clinically interpretable images from
undersampled measurements. However, the lack of publicly available cardiac MRI
k-space dataset in terms of both quantity and diversity has severely hindered
substantial technological progress, particularly for data-driven artificial
intelligence. Here, we provide a standardized, diverse, and high-quality
CMRxRecon2024 dataset to facilitate the technical development, fair evaluation,
and clinical transfer of cardiac MRI reconstruction approaches, towards
promoting the universal frameworks that enable fast and robust reconstructions
across different cardiac MRI protocols in clinical practice. To the best of our
knowledge, the CMRxRecon2024 dataset is the largest and most protocal-diverse
publicly available cardiac k-space dataset. It is acquired from 330 healthy
volunteers, covering commonly used modalities, anatomical views, and
acquisition trajectories in clinical cardiac MRI workflows. Besides, an open
platform with tutorials, benchmarks, and data processing tools is provided to
facilitate data usage, advanced method development, and fair performance
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-15T00:00:00Z">2025-01-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">70</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal LLMs Can Reason about Aesthetics in Zero-Shot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Jiang, Changwen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability
shall be elicited to evaluate the aesthetics of artworks. To facilitate this
investigation, we construct MM-StyleBench, a novel high-quality dataset for
benchmarking artistic stylization. We then develop a principled method for
human preference modeling and perform a systematic correlation analysis between
MLLMs' responses and human preference. Our experiments reveal an inherent
hallucination issue of MLLMs in art evaluation, associated with response
subjectivity. ArtCoT is proposed, demonstrating that art-specific task
decomposition and the use of concrete language boost MLLMs' reasoning ability
for aesthetics. Our findings offer valuable insights into MLLMs for art and can
benefit a wide range of downstream applications, such as style transfer and
artistic image generation. Code available at
https://github.com/songrise/MLLM4Art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WIP, Homepage https://github.com/songrise/MLLM4Art</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aegis2.0: A Diverse AI Safety <span class="highlight-title">Dataset</span> and Risks Taxonomy for Alignment
  of LLM Guardrails 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian Rebedea, Jibin Rajan Varghese, Christopher Parisien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) and generative AI become increasingly
widespread, concerns about content safety have grown in parallel. Currently,
there is a clear lack of high-quality, human-annotated datasets that address
the full spectrum of LLM-related safety risks and are usable for commercial
applications. To bridge this gap, we propose a comprehensive and adaptable
taxonomy for categorizing safety risks, structured into 12 top-level hazard
categories with an extension to 9 fine-grained subcategories. This taxonomy is
designed to meet the diverse requirements of downstream users, offering more
granular and flexible tools for managing various risk types. Using a hybrid
data generation pipeline that combines human annotations with a multi-LLM
"jury" system to assess the safety of responses, we obtain Aegis 2.0, a
carefully curated collection of 34,248 samples of human-LLM interactions,
annotated according to our proposed taxonomy. To validate its effectiveness, we
demonstrate that several lightweight models, trained using parameter-efficient
techniques on Aegis 2.0, achieve performance competitive with leading safety
models fully fine-tuned on much larger, non-commercial datasets. In addition,
we introduce a novel training blend that combines safety with topic following
data.This approach enhances the adaptability of guard models, enabling them to
generalize to new risk categories defined during inference. We plan to
open-source Aegis 2.0 data and models to the research community to aid in the
safety guardrailing of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2404.05993</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personality Modeling for Persuasion of Misinformation using AI Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianmin Lou, Wentao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of misinformation on social media platforms has highlighted
the need to understand how individual personality traits influence
susceptibility to and propagation of misinformation. This study employs an
innovative agent-based modeling approach to investigate the relationship
between personality traits and misinformation dynamics. Using six AI agents
embodying different dimensions of the Big Five personality traits
(Extraversion, Agreeableness, and Neuroticism), we simulated interactions
across six diverse misinformation topics. The experiment, implemented through
the AgentScope framework using the GLM-4-Flash model, generated 90 unique
interactions, revealing complex patterns in how personality combinations affect
persuasion and resistance to misinformation. Our findings demonstrate that
analytical and critical personality traits enhance effectiveness in
evidence-based discussions, while non-aggressive persuasion strategies show
unexpected success in misinformation correction. Notably, agents with critical
traits achieved a 59.4% success rate in HIV-related misinformation discussions,
while those employing non-aggressive approaches maintained consistent
persuasion rates above 40% across different personality combinations. The study
also revealed a non-transitive pattern in persuasion effectiveness, challenging
conventional assumptions about personality-based influence. These results
provide crucial insights for developing personality-aware interventions in
digital environments and suggest that effective misinformation countermeasures
should prioritize emotional connection and trust-building over confrontational
approaches. The findings contribute to both theoretical understanding of
personality-misinformation dynamics and practical strategies for combating
misinformation in social media contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Extract Cross-Domain Aspects and Understanding Sentiments
  Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karukriti Kaushik Ghosh, Chiranjib Sur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment
analysis that aims to extract and classify sentiments based on specific aspects
or features of a product, service, or entity. Unlike traditional sentiment
analysis, which assigns a general sentiment score to entire reviews or texts,
ABSA focuses on breaking down the text into individual components or aspects
(e.g., quality, price, service) and evaluating the sentiment towards each. This
allows for a more granular level of understanding of customer opinions,
enabling businesses to pinpoint specific areas of strength and improvement. The
process involves several key steps, including aspect extraction, sentiment
classification, and aspect-level sentiment aggregation for a review paragraph
or any other form that the users have provided. ABSA has significant
applications in areas such as product reviews, social media monitoring,
customer feedback analysis, and market research. By leveraging techniques from
natural language processing (NLP) and machine learning, ABSA facilitates the
extraction of valuable insights, enabling companies to make data-driven
decisions that enhance customer satisfaction and optimize offerings. As ABSA
evolves, it holds the potential to greatly improve personalized customer
experiences by providing a deeper understanding of sentiment across various
product aspects. In this work, we have analyzed the strength of LLMs for a
complete cross-domain aspect-based sentiment analysis with the aim of defining
the framework for certain products and using it for other similar situations.
We argue that it is possible to that at an effectiveness of 92\% accuracy for
the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying General Turn-taking Models to Conversational Human-Robot
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Skantze, Bahar Irfan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Turn-taking is a fundamental aspect of conversation, but current Human-Robot
Interaction (HRI) systems often rely on simplistic, silence-based models,
leading to unnatural pauses and interruptions. This paper investigates, for the
first time, the application of general turn-taking models, specifically TurnGPT
and Voice Activity Projection (VAP), to improve conversational dynamics in HRI.
These models are trained on human-human dialogue data using self-supervised
learning objectives, without requiring domain-specific fine-tuning. We propose
methods for using these models in tandem to predict when a robot should begin
preparing responses, take turns, and handle potential interruptions. We
evaluated the proposed system in a within-subject study against a traditional
baseline system, using the Furhat robot with 39 adults in a conversational
setting, in combination with a large language model for autonomous response
generation. The results show that participants significantly prefer the
proposed system, and it significantly reduces response delays and
interruptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at HRI 2025 (the IEEE/ACM International Conference on
  Human-Robot Interaction)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Exploration of Large Language Models by Optimal
  Exploitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Grams, Patrick Betz, Christian Bartelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploration is a crucial skill for self-improvement and open-ended
problem-solving. However, it remains uncertain whether large language models
can effectively explore the state-space. Existing evaluations predominantly
focus on the trade-off between exploration and exploitation, often assessed in
multi-armed bandit problems. In contrast, this work isolates exploration as the
sole objective, tasking the agent with delivering information that enhances
future returns. For the evaluation, we propose to decompose missing rewards
into exploration and exploitation components by measuring the optimal
achievable return for the states already explored. Our experiments with various
LLMs reveal that most models struggle to sufficiently explore the state-space
and that weak exploration is insufficient. We observe a positive correlation
between model size and exploration performance, with larger models
demonstrating superior capabilities. Furthermore, we show that our
decomposition provides insights into differences in behaviors driven by agent
instructions during prompt engineering, offering a valuable tool for refining
LLM performance in exploratory tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text
  Detection Challenge <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Dugan, Andrew Zhu, Firoj Alam, Preslav Nakov, Marianna Apidianaki, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there have been many shared tasks targeting the detection of
generated text from Large Language Models (LLMs). However, these shared tasks
tend to focus either on cases where text is limited to one particular domain or
cases where text can be from many domains, some of which may not be seen during
test time. In this shared task, using the newly released RAID benchmark, we aim
to answer whether or not models can detect generated text from a large, yet
fixed, number of domains and LLMs, all of which are seen during training. Over
the course of three months, our task was attempted by 9 teams with 23 detector
submissions. We find that multiple participants were able to obtain accuracies
of over 99% on machine-generated text from RAID while maintaining a 5% False
Positive Rate -- suggesting that detectors are able to robustly detect text
from many domains and models simultaneously. We discuss potential
interpretations of this result and provide directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToMATO: Verbalizing the Mental States of Role-Playing LLMs for
  Benchmarking Theory of Mind <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazutoshi Shinoda, Nobukatsu Hojo, Kyosuke Nishida, Saki Mizuno, Keita Suzuki, Ryo Masumura, Hiroaki Sugiyama, Kuniko Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in
three aspects: 1) they assess a limited range of mental states such as beliefs,
2) false beliefs are not comprehensively explored, and 3) the diverse
personality traits of characters are overlooked. To address these challenges,
we introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over
conversations. ToMATO is generated via LLM-LLM conversations featuring
information asymmetry. By employing a prompting method that requires
role-playing LLMs to verbalize their thoughts before each utterance, we capture
both first- and second-order mental states across five categories: belief,
intention, desire, emotion, and knowledge. These verbalized thoughts serve as
answers to questions designed to assess the mental states of characters within
conversations. Furthermore, the information asymmetry introduced by hiding
thoughts from others induces the generation of false beliefs about various
mental states. Assigning distinct personality traits to LLMs further
diversifies both utterances and thoughts. ToMATO consists of 5.4k questions,
753 conversations, and 15 personality trait patterns. Our analysis shows that
this dataset construction approach frequently generates false beliefs due to
the information asymmetry between role-playing LLMs, and effectively reflects
diverse personalities. We evaluate nine LLMs on ToMATO and find that even
GPT-4o mini lags behind human performance, especially in understanding false
beliefs, and lacks robustness to various personality traits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMDocIR: Benchmarking Multi-Modal <span class="highlight-title">Retrie</span>val for Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal document retrieval is designed to identify and retrieve various
forms of multi-modal content, such as figures, tables, charts, and layout
information from extensive documents. Despite its significance, there is a
notable lack of a robust benchmark to effectively evaluate the performance of
systems in multi-modal document retrieval. To address this gap, this work
introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks:
page-level and layout-level retrieval. The former focuses on localizing the
most relevant pages within a long document, while the latter targets the
detection of specific layouts, offering a more fine-grained granularity than
whole-page analysis. A layout can refer to a variety of elements such as
textual paragraphs, equations, figures, tables, or charts. The MMDocIR
benchmark comprises a rich dataset featuring expertly annotated labels for
1,685 questions and bootstrapped labels for 173,843 questions, making it a
pivotal resource for advancing multi-modal document retrieval for both training
and evaluation. Through rigorous experiments, we reveal that (i) visual
retrievers significantly outperform their text counterparts, (ii) MMDocIR train
set can effectively benefit the training process of multi-modal document
retrieval and (iii) text retrievers leveraging on VLM-text perform much better
than those using OCR-text. These findings underscores the potential advantages
of integrating visual elements for multi-modal document retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/MMDocIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAIF: A Comprehensive Framework for Evaluating the Risks of Generative
  AI in the Public Sector <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyeongryul Lee, Heehyeon Kim, Joyce Jiyoung Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of generative AI in the public sector, encompassing
diverse applications ranging from automated public assistance to welfare
services and immigration processes, highlights its transformative potential
while underscoring the pressing need for thorough risk assessments. Despite its
growing presence, evaluations of risks associated with AI-driven systems in the
public sector remain insufficiently explored. Building upon an established
taxonomy of AI risks derived from diverse government policies and corporate
guidelines, we investigate the critical risks posed by generative AI in the
public sector while extending the scope to account for its multimodal
capabilities. In addition, we propose a Systematic dAta generatIon Framework
for evaluating the risks of generative AI (SAIF). SAIF involves four key
stages: breaking down risks, designing scenarios, applying jailbreak methods,
and exploring prompt types. It ensures the systematic and consistent generation
of prompt data, facilitating a comprehensive evaluation while providing a solid
foundation for mitigating the risks. Furthermore, SAIF is designed to
accommodate emerging jailbreak methods and evolving prompt types, thereby
enabling effective responses to unforeseen risk scenarios. We believe that this
study can play a crucial role in fostering the safe and responsible integration
of generative AI into the public sector.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 tables. AI for Public Missions (AIPM) Workshop
  at the 39th AAAI Conference on Artificial Intelligence (AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Large Language Models for Effective Screening of Depression and
  Anxiety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        June M. Liu, Mengxia Gao, Sahand Sabour, Zhuang Chen, Minlie Huang, Tatia M. C. Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depressive and anxiety disorders are widespread, necessitating timely
identification and management. Recent advances in Large Language Models (LLMs)
offer potential solutions, yet high costs and ethical concerns about training
data remain challenges. This paper introduces a pipeline for synthesizing
clinical interviews, resulting in 1,157 interactive dialogues (PsyInterview),
and presents EmoScan, an LLM-based emotional disorder screening system. EmoScan
distinguishes between coarse (e.g., anxiety or depressive disorders) and fine
disorders (e.g., major depressive disorders) and conducts high-quality
interviews. Evaluations showed that EmoScan exceeded the performance of base
models and other LLMs like GPT-4 in screening emotional disorders
(F1-score=0.7467). It also delivers superior explanations (BERTScore=0.9408)
and demonstrates robust generalizability (F1-score of 0.67 on an external
dataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as
validated by automated ratings and human evaluations. This work highlights the
importance of scalable data-generative pipelines for developing effective
mental health LLM tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding Vietnamese SentiWordNet to Improve Performance of Vietnamese
  Sentiment Analysis Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Viet Tran, Van-Tan Bui, Lam-Quan Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis is one of the most crucial tasks in Natural Language
Processing (NLP), involving the training of machine learning models to classify
text based on the polarity of opinions. Pre-trained Language Models (PLMs) can
be applied to downstream tasks through fine-tuning, eliminating the need to
train the model from scratch. Specifically, PLMs have been employed for
Sentiment Analysis, a process that involves detecting, analyzing, and
extracting the polarity of text sentiments. Numerous models have been proposed
to address this task, with pre-trained PhoBERT-V2 models standing out as the
state-of-the-art language models for Vietnamese. The PhoBERT-V2 pre-training
approach is based on RoBERTa, optimizing the BERT pre-training method for more
robust performance. In this paper, we introduce a novel approach that combines
PhoBERT-V2 and SentiWordnet for Sentiment Analysis of Vietnamese reviews. Our
proposed model utilizes PhoBERT-V2 for Vietnamese, offering a robust
optimization for the prominent BERT model in the context of Vietnamese
language, and leverages SentiWordNet, a lexical resource explicitly designed to
support sentiment classification applications. Experimental results on the VLSP
2016 and AIVIVN 2019 datasets demonstrate that our sentiment analysis system
has achieved excellent performance in comparison to other models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Inherent Limits of <span class="highlight-title">Pretrain</span>ed LLMs: The Unexpected Convergence of
  Instruction Tuning and In-Context Learning Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irina Bigoulaeva, Harish Tayyar Madabushi, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), trained on extensive web-scale corpora, have
demonstrated remarkable abilities across diverse tasks, especially as they are
scaled up. Nevertheless, even state-of-the-art models struggle in certain
cases, sometimes failing at problems solvable by young children, indicating
that traditional notions of task complexity are insufficient for explaining LLM
capabilities. However, exploring LLM capabilities is complicated by the fact
that most widely-used models are also "instruction-tuned" to respond
appropriately to prompts. With the goal of disentangling the factors
influencing LLM performance, we investigate whether instruction-tuned models
possess fundamentally different capabilities from base models that are prompted
using in-context examples. Through extensive experiments across various model
families, scales and task types, which included instruction tuning 90 different
LLMs, we demonstrate that the performance of instruction-tuned models is
significantly correlated with the in-context performance of their base
counterparts. By clarifying what instruction-tuning contributes, we extend
prior research into in-context learning, which suggests that base models use
priors from pretraining data to solve tasks. Specifically, we extend this
understanding to instruction-tuned models, suggesting that their pretraining
data similarly sets a limiting boundary on the tasks they can solve, with the
added influence of the instruction-tuning dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code for this paper is available at:
  https://github.com/UKPLab/arxiv2025-inherent-limits-plms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Feature Fusion for Emotion Analysis and Suicide Risk
  Differentiation in Chinese Psychological Support Hotlines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Jianqiang Li, Qing Zhao, Zhonglong Chen, Changwei Song, Jing Tang, Yuning Huang, Wei Zhai, Yongsheng Tong, Guanghui Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health is a critical global public health issue, and psychological
support hotlines play a pivotal role in providing mental health assistance and
identifying suicide risks at an early stage. However, the emotional expressions
conveyed during these calls remain underexplored in current research. This
study introduces a method that combines pitch acoustic features with deep
learning-based features to analyze and understand emotions expressed during
hotline interactions. Using data from China's largest psychological support
hotline, our method achieved an F1-score of 79.13% for negative binary emotion
classification.Additionally, the proposed approach was validated on an open
dataset for multi-class emotion classification,where it demonstrated better
performance compared to the state-of-the-art methods. To explore its clinical
relevance, we applied the model to analysis the frequency of negative emotions
and the rate of emotional change in the conversation, comparing 46 subjects
with suicidal behavior to those without. While the suicidal group exhibited
more frequent emotional changes than the non-suicidal group, the difference was
not statistically significant.Importantly, our findings suggest that emotional
fluctuation intensity and frequency could serve as novel features for
psychological assessment scales and suicide risk prediction.The proposed method
provides valuable insights into emotional dynamics and has the potential to
advance early intervention and improve suicide prevention strategies through
integration with clinical tools and assessments The source code is publicly
available at https://github.com/Sco-field/Speechemotionrecognition/tree/main.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge <span class="highlight-title">Graph</span>-based <span class="highlight-title">Retrie</span>val-Augmented Generation for Schema Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuangtao Ma, Sriom Chakrabarti, Arijit Khan, Bálint Molnár
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional similarity-based schema matching methods are incapable of
resolving semantic ambiguities and conflicts in domain-specific complex mapping
scenarios due to missing commonsense and domain-specific knowledge. The
hallucination problem of large language models (LLMs) also makes it challenging
for LLM-based schema matching to address the above issues. Therefore, we
propose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema
Matching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces
novel vector-based, graph traversal-based, and query-based graph retrievals, as
well as a hybrid approach and ranking schemes that identify the most relevant
subgraphs from external large knowledge graphs (KGs). We showcase that KG-based
retrieval-augmented LLMs are capable of generating more accurate results for
complex matching cases without any re-training. Our experimental results show
that KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,
Jellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the
MIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the
pre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and
21.97% in terms of precision and F1 score on the Synthea dataset, respectively.
The results also demonstrate that our approach is more efficient in end-to-end
schema matching, and scales to retrieve from large KGs. Our case studies on the
dataset from the real-world schema matching scenario exhibit that the
hallucination problem of LLMs for schema matching is well mitigated by our
solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGNET: Augmenting Generative Decoders with Representation Learning and
  Infilling Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savya Khosla, Kushal Kafle, Simon Jenni, Handong Zhao, John Collomosse, Jing Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While originally designed for unidirectional generative modeling,
decoder-only large language models (LLMs) are increasingly being adapted for
bidirectional modeling. However, unidirectional and bidirectional models are
typically trained separately with distinct objectives (generation and
representation learning, respectively). This separation overlooks the
opportunity for developing a more versatile language model and for these
objectives to complement each other. In this work, we introduce MAGNET, an
adaptation of decoder-only LLMs that enhances their ability to generate robust
representations and infill missing text spans, while preserving their knowledge
and text generation capabilities. MAGNET employs three self-supervised training
objectives and introduces an attention mechanism that combines bidirectional
and causal attention, enabling unified training across all objectives. Our
results demonstrate that LLMs adapted with MAGNET (1) surpass strong text
encoders on token-level and sentence-level representation learning tasks, (2)
generate contextually appropriate text infills by leveraging future context,
(3) retain the ability for open-ended text generation without exhibiting
repetition problem, and (4) preserve the knowledge gained by the LLM during
pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reassessing the Role of Chain-of-Thought in Sentiment Analysis: Insights
  and Limitations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyuan Zheng, Qinghua Zhao, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The relationship between language and thought remains an unresolved
philosophical issue. Existing viewpoints can be broadly categorized into two
schools: one asserting their independence, and another arguing that language
constrains thought. In the context of large language models, this debate raises
a crucial question: Does a language model's grasp of semantic meaning depend on
thought processes? To explore this issue, we investigate whether reasoning
techniques can facilitate semantic understanding. Specifically, we
conceptualize thought as reasoning, employ chain-of-thought prompting as a
reasoning technique, and examine its impact on sentiment analysis tasks. The
experiments show that chain-of-thought has a minimal impact on sentiment
analysis tasks. Both the standard and chain-of-thought prompts focus on aspect
terms rather than sentiment in the generated content. Furthermore,
counterfactual experiments reveal that the model's handling of sentiment tasks
primarily depends on information from demonstrations. The experimental results
support the first viewpoint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWSC: Shared Weight for Similar Channel in LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binrui Zeng, Yongtao Tang, Xiaodong Liu, Xiaopeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have spurred development in multiple industries.
However, the growing number of their parameters brings substantial storage and
computing burdens, making it essential to explore model compression techniques
for parameter reduction and easier deployment. We propose SWSC, an LLM
compression method based on the concept of Shared Weight for Similar Channel.
It uses the K-Means clustering algorithm to cluster model weights
channel-by-channel, generating clusters with highly similar vectors within
each. A representative vector from each cluster is selected to approximately
replace all vectors in the cluster, significantly reducing the number of model
weight parameters. However, approximate restoration will inevitably cause
damage to the performance of the model. To tackle this issue, we perform
singular value decomposition on the weight error values before and after
compression and retain the larger singular values and their corresponding
singular vectors to compensate for the accuracy. The experimental results show
that our method can effectively ensure the performance of the compressed LLM
even under low-precision conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5pages, 3 figures, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and
  Vietnamese-Lao language pair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Viet Tran, Minh-Quy Nguyen, Van-Vinh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an results of the VLSP 2022-2023 Machine Translation
Shared Tasks, focusing on Vietnamese-Chinese and Vietnamese-Lao machine
translation. The tasks were organized as part of the 9th, 10th annual workshop
on Vietnamese Language and Speech Processing (VLSP 2022, VLSP 2023). The
objective of the shared task was to build machine translation systems,
specifically targeting Vietnamese-Chinese and Vietnamese-Lao translation
(corresponding to 4 translation directions). The submission were evaluated on
1,000 pairs for testing (news and general domains) using established metrics
like BLEU [11] and SacreBLEU [12]. Additionally, system outputs also were
evaluated with human judgment provided by experts in Chinese and Lao languages.
These human assessments played a crucial role in ranking the performance of the
machine translation models, ensuring a more comprehensive evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aruna Sankaranarayanan, Dylan Hadfield-Menell, Aaron Mueller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All natural languages are structured hierarchically. In humans, this
structural restriction is neurologically coded: when two grammars are presented
with identical vocabularies, brain areas responsible for language processing
are only sensitive to hierarchical grammars. Using large language models
(LLMs), we investigate whether such functionally distinct hierarchical
processing regions can arise solely from exposure to large-scale language
distributions. We generate inputs using English, Italian, Japanese, or nonce
words, varying the underlying grammars to conform to either hierarchical or
linear/positional rules. Using these grammars, we first observe that language
models show distinct behaviors on hierarchical versus linearly structured
inputs. Then, we find that the components responsible for processing
hierarchical grammars are distinct from those that process linear grammars; we
causally verify this in ablation experiments. Finally, we observe that
hierarchy-selective components are also active on nonce grammars; this suggests
that hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fernández Fisac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI systems like foundation models (FMs) must align well with human
values to ensure their behavior is helpful and trustworthy. While Reinforcement
Learning from Human Feedback (RLHF) has shown promise for optimizing model
performance using human judgments, existing RLHF pipelines predominantly rely
on immediate feedback, which can fail to accurately reflect the downstream
impact of an interaction on users' utility. We demonstrate that feedback based
on evaluators' foresight estimates of downstream consequences systematically
induces Goodhart's Law dynamics, incentivizing misaligned behaviors like
sycophancy and deception and ultimately degrading user outcomes. To alleviate
this, we propose decoupling evaluation from prediction by refocusing RLHF on
hindsight feedback. Our theoretical analysis reveals that conditioning
evaluator feedback on downstream observations mitigates misalignment and
improves expected human utility, even when these observations are simulated by
the AI system itself. To leverage this insight in a practical alignment
algorithm, we introduce Reinforcement Learning from Hindsight Simulation
(RLHS), which first simulates plausible consequences and then elicits feedback
to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS
to two widely-employed online and offline preference optimization methods --
Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) --
and show empirically that misalignment is significantly reduced with both
methods. Through an online human user study, we show that RLHS consistently
outperforms RLHF in helping users achieve their goals and earns higher
satisfaction ratings, despite being trained solely with simulated hindsight
feedback. These results underscore the importance of focusing on long-term
consequences, even simulated ones, to mitigate misalignment in RLHF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Alignment of FOL Closeness Metrics with Human Judgement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramya Keerthy Thatikonda, Wray Buntine, Ehsan Shareghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent successful paradigm of solving logical reasoning problems with
tool-augmented large language models (LLMs) leverages translation of natural
language statements into First-Order Logic~(FOL) and external theorem provers.
However, the correctness of FOL statements, comprising operators and text
predicates, often goes unverified due to the lack of a reliable evaluation
metric for comparing generated and ground-truth FOLs. In this paper, we present
a comprehensive study of sensitivity of existing metrics and their alignment
with human judgement on FOL evaluation. Using ground-truth FOLs, we carefully
designed various perturbations on the ground-truth to assess metric
sensitivity. We sample FOL translation candidates for natural language
statements and measure the ranking alignment between automatic metrics and
human annotators. Our empirical findings highlight oversensitivity in the
n-gram metric BLEU for text perturbations, the semantic graph metric Smatch++
for structural perturbations, and FOL metric for operator perturbation. We also
observe a closer alignment between BertScore and human judgement. Additionally,
we show that combining metrics enhances both alignment and sensitivity compared
to using individual metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/RamyaKeerthy/AlignmentFOL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Knowledge Integration for Enhanced Vision-Language Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Perry, Surasakdi Siripong, Thanakorn Phonchai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have demonstrated impressive
capabilities in multimodal tasks, but their performance is often constrained by
the lack of external knowledge integration, limiting their ability to handle
knowledge-intensive tasks such as visual question answering and reasoning. To
address this challenge, we propose a novel method, Adaptive Knowledge-Guided
Pretraining for Large Vision-Language Models (AKGP-LVLM), which dynamically
incorporates structured and unstructured knowledge into LVLMs during
pretraining and fine-tuning. Our approach employs a knowledge encoder to
represent external knowledge, a retrieval mechanism to select task-relevant
information, and a dynamic adaptor to align multimodal and knowledge
representations effectively. We evaluate our method on four benchmark datasets,
demonstrating significant performance improvements over state-of-the-art
models. Furthermore, human evaluations highlight the superior correctness and
relevance of our model's outputs. Extensive analyses confirm the robustness,
efficiency, and scalability of AKGP-LVLM, making it a compelling solution for
real-world knowledge-intensive tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Hu, Jing Zhang, Xiaodong Chen, Zhe Zhao, Cuiping Li, Hong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing low-rank adaptation (LoRA) methods face challenges on sparse large
language models (LLMs) due to the inability to maintain sparsity. Recent works
introduced methods that maintain sparsity by augmenting LoRA techniques with
additional masking mechanisms. Despite these successes, such approaches suffer
from an increased memory and computation overhead, which affects efficiency of
LoRA methods. In response to this limitation, we introduce LoRS, an innovative
method designed to achieve both memory and computation efficiency when
fine-tuning sparse LLMs. To mitigate the substantial memory and computation
demands associated with preserving sparsity, our approach incorporates
strategies of weight recompute and computational graph rearrangement. In
addition, we also improve the effectiveness of LoRS through better adapter
initialization. These innovations lead to a notable reduction in memory and
computation consumption during the fine-tuning phase, all while achieving
performance levels that outperform existing LoRA approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Limits LLM-based Human Simulation: LLMs or Our Design? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Wang, Jiaying Wu, Zhenheng Tang, Bingqiao Luo, Nuo Chen, Wei Chen, Bingsheng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We argue that advancing LLM-based human simulation requires addressing both
LLM's inherent limitations and simulation framework design challenges. Recent
studies have revealed significant gaps between LLM-based human simulations and
real-world observations, highlighting these dual challenges. To address these
gaps, we present a comprehensive analysis of LLM limitations and our design
issues, proposing targeted solutions for both aspects. Furthermore, we explore
future directions that address both challenges simultaneously, particularly in
data collection, LLM generation, and evaluation. To support further research in
this field, we provide a curated collection of LLM-based human simulation
resources.\footnote{https://github.com/Persdre/llm-human-simulation}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Entropy Invariance: Enhancing Length Extrapolation in
  Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kewei Li, Yanwen Kong, Yiping Xu, Lan Huang, Ruochi Zhang, Fengfeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the length extrapolation capabilities of Large Language Models
(LLMs) remains a critical challenge in natural language processing. Many recent
efforts have focused on modifying the scaled dot-product attention mechanism,
and often introduce scaled temperatures without rigorous theoretical
justification. To fill this gap, we introduce a novel approach based on
information entropy invariance. We propose two new scaled temperatures to
enhance length extrapolation. First, a training-free method InfoScale is
designed for dot-product attention, and preserves focus on original tokens
during length extrapolation by ensuring information entropy remains consistent.
Second, we theoretically analyze the impact of scaling (CosScale) on cosine
attention. Experimental data demonstrates that combining InfoScale and CosScale
achieves state-of-the-art performance on the GAU-{\alpha} model with a context
window extended to 64 times the training length, and outperforms seven existing
methods. Our analysis reveals that significantly increasing CosScale
approximates windowed attention, and highlights the significance of attention
score dilution as a key challenge in long-range context handling. The code and
data are available at https://github.com/HT-NEKO/InfoScale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge <span class="highlight-title">prompt</span> chaining for semantic modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Pei Ding, Jingge Du, Zaiwen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of building semantics for structured data such as CSV, JSON, and XML
files is highly relevant in the knowledge representation field. Even though we
have a vast of structured data on the internet, mapping them to domain
ontologies to build semantics for them is still very challenging as it requires
the construction model to understand and learn graph-structured knowledge.
Otherwise, the task will require human beings' effort and cost. In this paper,
we proposed a novel automatic semantic modeling framework: Knowledge Prompt
Chaining. It can serialize the graph-structured knowledge and inject it into
the LLMs properly in a Prompt Chaining architecture. Through this knowledge
injection and prompting chaining, the model in our framework can learn the
structure information and latent space of the graph and generate the semantic
labels and semantic graphs following the chains' insturction naturally. Based
on experimental results, our method achieves better performance than existing
leading techniques, despite using reduced structured input data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complexity Control Facilitates Reasoning-Based Compositional
  Generalization in Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwang Zhang, Pengxiao Lin, Zhiwei Wang, Yaoyu Zhang, Zhi-Qin John Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have demonstrated impressive capabilities across various tasks,
yet their performance on compositional problems remains a subject of debate. In
this study, we investigate the internal mechanisms underlying Transformers'
behavior in compositional tasks. We find that complexity control strategies
significantly influence whether the model learns primitive-level rules that
generalize out-of-distribution (reasoning-based solutions) or relies solely on
memorized mappings (memory-based solutions). By applying masking strategies to
the model's information circuits and employing multiple complexity metrics, we
reveal distinct internal working mechanisms associated with different solution
types. Further analysis reveals that reasoning-based solutions exhibit a lower
complexity bias, which aligns with the well-studied neuron condensation
phenomenon. This lower complexity bias is hypothesized to be the key factor
enabling these solutions to learn reasoning rules. We validate these
conclusions across multiple real-world datasets, including image generation and
natural language processing tasks, confirming the broad applicability of our
findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Mistakenly submitted as a replacement to 2405.05409v4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for
  Document-level Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Guo, Yuanchang Luo, Daimeng Wei, Ling Zhang, Zongyao Li, Hengchao Shang, Zhiqiang Rao, Shaojun Li, Jinlong Yang, Zhanglin Wu, Hao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of artificial intelligence has witnessed significant advancements
in natural language processing, largely attributed to the capabilities of Large
Language Models (LLMs). These models form the backbone of Agents designed to
address long-context dependencies, particularly in Document-level Machine
Translation (DocMT). DocMT presents unique challenges, with quality,
consistency, and fluency being the key metrics for evaluation. Existing
approaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise
fluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an
incremental sentence-level forced decoding strategy \textbf{to ensure every
sentence is translated while enhancing the fluency of adjacent sentences.} Our
Agent leverages a Doc-Guided Memory, focusing solely on the summary and its
translation, which we find to be an efficient approach to maintaining
consistency. Through extensive testing across multiple languages and domains,
we demonstrate that Sent2Sent++ outperforms other methods in terms of quality,
consistency, and fluency. The results indicate that, our approach has achieved
significant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and
document-level perplexity (d-ppl). The contributions of this paper include a
detailed analysis of current DocMT research, the introduction of the
Sent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of
its effectiveness across languages and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adapting Whisper for Regional Dialects: Enhancing Public Services for
  Vulnerable Populations in the United Kingdom 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melissa Torgbi, Andrew Clayman, Jordan J. Speight, Harish Tayyar Madabushi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We collect novel data in the public service domain to evaluate the capability
of the state-of-the-art automatic speech recognition (ASR) models in capturing
regional differences in accents in the United Kingdom (UK), specifically
focusing on two accents from Scotland with distinct dialects. This study
addresses real-world problems where biased ASR models can lead to
miscommunication in public services, disadvantaging individuals with regional
accents particularly those in vulnerable populations. We first examine the
out-of-the-box performance of the Whisper large-v3 model on a baseline dataset
and our data. We then explore the impact of fine-tuning Whisper on the
performance in the two UK regions and investigate the effectiveness of existing
model evaluation techniques for our real-world application through manual
inspection of model errors. We observe that the Whisper model has a higher word
error rate (WER) on our test datasets compared to the baseline data and
fine-tuning on a given data improves performance on the test dataset with the
same domain and accent. The fine-tuned models also appear to show improved
performance when applied to the test data outside of the region it was trained
on suggesting that fine-tuned models may be transferable within parts of the
UK. Our manual analysis of model outputs reveals the benefits and drawbacks of
using WER as an evaluation metric and fine-tuning to adapt to regional
dialects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and
  Lithuanian Short Answer Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yevhen Kostiuk, Oxana Vitman, Łukasz Gagała, Artur Kiulian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the challenge of evaluating large language models
(LLMs) on the short answer matching task for Latvian and Lithuanian languages.
We introduce novel datasets consisting of 502 Latvian and 690 Lithuanian
question-answer pairs. For each question-answer pair, we generated matched and
non-matched answers using a set of alteration rules specifically designed to
introduce small but meaningful changes in the text. These generated answers
serve as test cases to assess the ability of LLMs to detect subtle differences
in matching of the original answers. A subset of the datasets was manually
verified for quality and accuracy. Our results show that while larger LLMs,
such as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance in
distinguishing matched and non-matched answers, smaller models show more
variance. For instance, LLaMa3.1 8b and EuroLLM 9b benefited from few-shot
examples, while Mistral Nemo 12b underperformed on detection of subtle text
alteration, particularly in Lithuanian, even with additional examples. QWEN2.5
7b and Mistral 7b were able to obtain a strong and comparable performance to
the larger 70b models in zero and few shot experiments. Moreover, the
performance of Mistral 7b was weaker in few shot experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy
  and Consistency for Enhanced Readability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephanie L. Day, Jacapo Cirica, Steven R. Clapp, Veronika Penkova, Amy E. Giroux, Abbey Banta, Catherine Bordeau, Poojitha Mutteneni, Ben D. Sawyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence (GenAI) holds great promise as a tool to
support personalized learning. Teachers need tools to efficiently and
effectively enhance content readability of educational texts so that they are
matched to individual students reading levels, while retaining key details.
Large Language Models (LLMs) show potential to fill this need, but previous
research notes multiple shortcomings in current approaches. In this study, we
introduced a generalized approach and metrics for the systematic evaluation of
the accuracy and consistency in which LLMs, prompting techniques, and a novel
multi-agent architecture to simplify sixty informational reading passages,
reducing each from the twelfth grade level down to the eighth, sixth, and
fourth grade levels. We calculated the degree to which each LLM and prompting
technique accurately achieved the targeted grade level for each passage,
percentage change in word count, and consistency in maintaining keywords and
key phrases (semantic similarity). One-sample t-tests and multiple regression
models revealed significant differences in the best performing LLM and prompt
technique for each of the four metrics. Both LLMs and prompting techniques
demonstrated variable utility in grade level accuracy and consistency of
keywords and key phrases when attempting to level content down to the fourth
grade reading level. These results demonstrate the promise of the application
of LLMs for efficient and precise automated text simplification, the
shortcomings of current models and prompting methods in attaining an ideal
balance across various evaluation criteria, and a generalizable method to
evaluate future systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages, 9 tables, 6 figures, and supplemental materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VCRScore: Image captioning metric based on V\&L Transformers, CLIP, and
  precision-recall 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Ruiz, Tania Ramírez, Daniela Moctezuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning has become an essential Vision & Language research task. It
is about predicting the most accurate caption given a specific image or video.
The research community has achieved impressive results by continuously
proposing new models and approaches to improve the overall model's performance.
Nevertheless, despite increasing proposals, the performance metrics used to
measure their advances have remained practically untouched through the years. A
probe of that, nowadays metrics like BLEU, METEOR, CIDEr, and ROUGE are still
very used, aside from more sophisticated metrics such as BertScore and
ClipScore.
  Hence, it is essential to adjust how are measure the advances, limitations,
and scopes of the new image captioning proposals, as well as to adapt new
metrics to these new advanced image captioning approaches.
  This work proposes a new evaluation metric for the image captioning problem.
To do that, first, it was generated a human-labeled dataset to assess to which
degree the captions correlate with the image's content. Taking these human
scores as ground truth, we propose a new metric, and compare it with several
well-known metrics, from classical to newer ones. Outperformed results were
also found, and interesting insights were presented and discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A
  study on Lithuanian History 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yevhen Kostiuk, Oxana Vitman, Łukasz Gagała, Artur Kiulian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we evaluated Lithuanian and general history knowledge of
multilingual Large Language Models (LLMs) on a multiple-choice
question-answering task. The models were tested on a dataset of Lithuanian
national and general history questions translated into Baltic, Nordic, and
other languages (English, Ukrainian, Arabic) to assess the knowledge sharing
from culturally and historically connected groups. We evaluated GPT-4o,
LLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral
7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b).
  Our results show that GPT-4o consistently outperformed all other models
across language groups, with slightly better results for Baltic and Nordic
languages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b
performed well but showed weaker alignment with Baltic languages. Smaller
models (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b)
demonstrated gaps with LT-related alignment with Baltic languages while
performing better on Nordic and other languages. The Nordic fine-tuned models
did not surpass multilingual models, indicating that shared cultural or
historical context alone does not guarantee better performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic <span class="highlight-title">Retrie</span>val-Augmented Generation: A <span class="highlight-title">Survey</span> on Agentic RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized artificial intelligence (AI)
by enabling human like text generation and natural language understanding.
However, their reliance on static training data limits their ability to respond
to dynamic, real time queries, resulting in outdated or inaccurate outputs.
Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs
by integrating real time data retrieval to provide contextually relevant and
up-to-date responses. Despite its promise, traditional RAG systems are
constrained by static workflows and lack the adaptability required for
multistep reasoning and complex task management.
  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these
limitations by embedding autonomous AI agents into the RAG pipeline. These
agents leverage agentic design patterns reflection, planning, tool use, and
multiagent collaboration to dynamically manage retrieval strategies,
iteratively refine contextual understanding, and adapt workflows to meet
complex task requirements. This integration enables Agentic RAG systems to
deliver unparalleled flexibility, scalability, and context awareness across
diverse applications.
  This survey provides a comprehensive exploration of Agentic RAG, beginning
with its foundational principles and the evolution of RAG paradigms. It
presents a detailed taxonomy of Agentic RAG architectures, highlights key
applications in industries such as healthcare, finance, and education, and
examines practical implementation strategies. Additionally, it addresses
challenges in scaling these systems, ensuring ethical decision making, and
optimizing performance for real-world applications, while providing detailed
insights into frameworks and tools for implementing Agentic RAG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Robustness of Contrastive Learning Models for Medical
  Image-Report <span class="highlight-title">Retrie</span>val <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, Gongbo Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images and reports offer invaluable insights into patient health. The
heterogeneity and complexity of these data hinder effective analysis. To bridge
this gap, we investigate contrastive learning models for cross-domain
retrieval, which associates medical images with their corresponding clinical
reports. This study benchmarks the robustness of four state-of-the-art
contrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We
introduce an occlusion retrieval task to evaluate model performance under
varying levels of image corruption. Our findings reveal that all evaluated
models are highly sensitive to out-of-distribution data, as evidenced by the
proportional decrease in performance with increasing occlusion levels. While
MedCLIP exhibits slightly more robustness, its overall performance remains
significantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a
general-purpose dataset, struggles with medical image-report retrieval,
highlighting the importance of domain-specific training data. The evaluation of
this work suggests that more effort needs to be spent on improving the
robustness of these models. By addressing these limitations, we can develop
more reliable cross-domain retrieval models for medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is accepted to AAAI 2025 Workshop -- the 9th International
  Workshop on Health Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual LLMs Struggle to Link Ortho<span class="highlight-title">graph</span>y and Semantics in
  Bilingual Word Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshaan Tanwar, Gayatri Oke, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilingual lexical processing is shaped by the complex interplay of
phonological, orthographic, and semantic features of two languages within an
integrated mental lexicon. In humans, this is evident in the ease with which
cognate words - words similar in both orthographic form and meaning (e.g.,
blind, meaning "sightless" in both English and German) - are processed,
compared to the challenges posed by interlingual homographs, which share
orthographic form but differ in meaning (e.g., gift, meaning "present" in
English but "poison" in German). We investigate how multilingual Large Language
Models (LLMs) handle such phenomena, focusing on English-Spanish,
English-French, and English-German cognates, non-cognate, and interlingual
homographs. Specifically, we evaluate their ability to disambiguate meanings
and make semantic judgments, both when these word types are presented in
isolation or within sentence contexts. Our findings reveal that while certain
LLMs demonstrate strong performance in recognizing cognates and non-cognates in
isolation, they exhibit significant difficulty in disambiguating interlingual
homographs, often performing below random baselines. This suggests LLMs tend to
rely heavily on orthographic similarities rather than semantic understanding
when interpreting interlingual homographs. Further, we find LLMs exhibit
difficulty in retrieving word meanings, with performance in isolative
disambiguation tasks having no correlation with semantic understanding.
Finally, we study how the LLM processes interlingual homographs in incongruent
sentences. We find models to opt for different strategies in understanding
English and non-English homographs, highlighting a lack of a unified approach
to handling cross-lingual ambiguities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at:
  https://github.com/EshaanT/Bilingual_processing_LLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Human-Annotated Training Data with Large Language Model
  Generation and Distillation in Open-Response Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conrad Borchers, Danielle R. Thomas, Jionghao Lin, Ralph Abboud, Kenneth R. Koedinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like GPT-4o can help automate text
classification tasks at low cost and scale. However, there are major concerns
about the validity and reliability of LLM outputs. By contrast, human coding is
generally more reliable but expensive to procure at scale. In this study, we
propose a hybrid solution to leverage the strengths of both. We combine
human-coded data and synthetic LLM-produced data to fine-tune a classical
machine learning classifier, distilling both into a smaller BERT model. We
evaluate our method on a human-coded test set as a validity measure for LLM
output quality. In three experiments, we systematically vary LLM-generated
samples' size, variety, and consistency, informed by best practices in LLM
tuning. Our findings indicate that augmenting datasets with synthetic samples
improves classifier performance, with optimal results achieved at an 80%
synthetic to 20% human-coded data ratio. Lower temperature settings of 0.3,
corresponding to less variability in LLM generations, produced more stable
improvements but also limited model learning from augmented samples. In
contrast, higher temperature settings (0.7 and above) introduced greater
variability in performance estimates and, at times, lower performance. Hence,
LLMs may produce more uniform output that classifiers overfit to earlier or
produce more diverse output that runs the risk of deteriorating model
performance through information irrelevant to the prediction task. Filtering
out inconsistent synthetic samples did not enhance performance. We conclude
that integrating human and LLM-generated data to improve text classification
models in assessment offers a scalable solution that leverages both the
accuracy of human coding and the variety of LLM outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript accepted to the Second Workshop on Generative AI for
  Learning Analytics (GenAI-LA) at LAK25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteLLA: A Structured Grading System Using LLMs with RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hefei Qiu, Brian White, Ashley Ding, Reinaldo Costa, Ali Hachem, Wei Ding, Ping Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown strong general capabilities in many
applications. However, how to make them reliable tools for some specific tasks
such as automated short answer grading (ASAG) remains a challenge. We present
SteLLA (Structured Grading System Using LLMs with RAG) in which a) Retrieval
Augmented Generation (RAG) approach is used to empower LLMs specifically on the
ASAG task by extracting structured information from the highly relevant and
reliable external knowledge based on the instructor-provided reference answer
and rubric, b) an LLM performs a structured and question-answering-based
evaluation of student answers to provide analytical grades and feedback. A
real-world dataset that contains students' answers in an exam was collected
from a college-level Biology course. Experiments show that our proposed system
can achieve substantial agreement with the human grader while providing
break-down grades and feedback on all the knowledge points examined in the
problem. A qualitative and error analysis of the feedback generated by GPT4
shows that GPT4 is good at capturing facts while may be prone to inferring too
much implication from the given text in the grading task which provides
insights into the usage of LLMs in the ASAG system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language
  Models through Simulation and Task Decomposition <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sneheel Sarangi, Maha Elgarf, Hanan Salam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theory of Mind (ToM) is the ability to understand and reflect on the mental
states of others. Although this capability is crucial for human interaction,
testing on Large Language Models (LLMs) reveals that they possess only a
rudimentary understanding of it. Although the most capable closed-source LLMs
have come close to human performance on some ToM tasks, they still perform
poorly on complex variations of the task that involve more structured
reasoning. In this work, we utilize the concept of "pretend-play", or
``Simulation Theory'' from cognitive psychology to propose ``Decompose-ToM'':
an LLM-based inference algorithm that improves model performance on complex ToM
tasks. We recursively simulate user perspectives and decompose the ToM task
into a simpler set of functions: subject identification, question-reframing,
world model updation, and knowledge availability. We test the algorithm on
higher-order ToM tasks and a task testing for ToM capabilities in a
conversational setting, demonstrating that our approach shows significant
improvement across models compared to baseline methods while requiring minimal
prompt tuning across tasks and no additional model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Visual Commonsense Answering and Explaining with Generative
  Scene <span class="highlight-title">Graph</span> Constructing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yuan, Xiaoyuan Fang, Rong Quan, Jing Li, Wei Bi, Xiaogang Xu, Piji Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Commonsense Reasoning, which is regarded as one challenging task to
pursue advanced visual scene comprehension, has been used to diagnose the
reasoning ability of AI systems. However, reliable reasoning requires a good
grasp of the scene's details. Existing work fails to effectively exploit the
real-world object relationship information present within the scene, and
instead overly relies on knowledge from training memory. Based on these
observations, we propose a novel scene-graph-enhanced visual commonsense
reasoning generation method named \textit{\textbf{G2}}, which first utilizes
the image patches and LLMs to construct a location-free scene graph, and then
answer and explain based on the scene graph's information. We also propose
automatic scene graph filtering and selection strategies to absorb valuable
scene graph information during training. Extensive experiments are conducted on
the tasks and datasets of scene graph constructing and visual commonsense
answering and explaining, respectively. Experimental results and ablation
analysis demonstrate the effectiveness of our proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Framework for Inference-time Scaling and Steering of Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models produce impressive results in modalities ranging from images
and video to protein design and text. However, generating samples with
user-specified properties remains a challenge. Recent research proposes
fine-tuning models to maximize rewards that capture desired properties, but
these methods require expensive training and are prone to mode collapse. In
this work, we propose Feynman Kac (FK) steering, an inference-time framework
for steering diffusion models with reward functions. FK steering works by
sampling a system of multiple interacting diffusion processes, called
particles, and resampling particles at intermediate steps based on scores
computed using functions called potentials. Potentials are defined using
rewards for intermediate states and are selected such that a high value
indicates that the particle will yield a high-reward sample. We explore various
choices of potentials, intermediate rewards, and samplers. We evaluate FK
steering on text-to-image and text diffusion models. For steering text-to-image
models with a human preference reward, we find that FK steering a 0.8B
parameter model outperforms a 2.6B parameter fine-tuned model on prompt
fidelity, with faster sampling and no training. For steering text diffusion
models with rewards for text quality and specific text attributes, we find that
FK steering generates lower perplexity, more linguistically acceptable outputs
and enables gradient-free control of attributes like toxicity. Our results
demonstrate that inference-time scaling and steering of diffusion models, even
with off-the-shelf rewards, can provide significant sample quality gains and
controllability benefits. Code is available at
https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency of Responses and Continuations Generated by Large Language
  Models on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate remarkable capabilities in text
generation, yet their emotional consistency and semantic coherence in social
media contexts remain insufficiently understood. This study investigates how
LLMs handle emotional content and maintain semantic relationships through
continuation and response tasks using two open-source models: Gemma and Llama.
By analyzing climate change discussions from Twitter and Reddit, we examine
emotional transitions, intensity patterns, and semantic similarity between
human-authored and LLM-generated content. Our findings reveal that while both
models maintain high semantic coherence, they exhibit distinct emotional
patterns: Gemma shows a tendency toward negative emotion amplification,
particularly anger, while maintaining certain positive emotions like optimism.
Llama demonstrates superior emotional preservation across a broader spectrum of
affects. Both models systematically generate responses with attenuated
emotional intensity compared to human-authored content and show a bias toward
positive emotions in response tasks. Additionally, both models maintain strong
semantic similarity with original texts, though performance varies between
continuation and response tasks. These findings provide insights into LLMs'
emotional and semantic processing capabilities, with implications for their
deployment in social media contexts and human-AI interaction design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSA-ASR: Efficient Multilingual Speaker Attribution with frozen ASR
  Models <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18152v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18152v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thai-Binh Nguyen, Alexander Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speaker-attributed automatic speech recognition (SA-ASR) aims to transcribe
speech while assigning transcripts to the corresponding speakers accurately.
Existing methods often rely on complex modular systems or require extensive
fine-tuning of joint modules, limiting their adaptability and general
efficiency. This paper introduces a novel approach, leveraging a frozen
multilingual ASR model to incorporate speaker attribution into the
transcriptions, using only standard monolingual ASR datasets. Our method
involves training a speaker module to predict speaker embeddings based on weak
labels without requiring additional ASR model modifications. Despite being
trained exclusively with non-overlapping monolingual data, our approach
effectively extracts speaker attributes across diverse multilingual datasets,
including those with overlapping speech. Experimental results demonstrate
competitive performance compared to strong baselines, highlighting the model's
robustness and potential for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Boundaries: Learning a Universal Entity Taxonomy across <span class="highlight-title">Dataset</span>s
  and Languages for Open Named Entity Recognition <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Yang, Wantong Zhao, Caishuang Huang, Junjie Ye, Xiao Wang, Huiyuan Zheng, Yang Nan, Yuran Wang, Xueying Xu, Kaixin Huang, Yunke Zhang, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Named Entity Recognition (NER), which involves identifying arbitrary
types of entities from arbitrary domains, remains challenging for Large
Language Models (LLMs). Recent studies suggest that fine-tuning LLMs on
extensive NER data can boost their performance. However, training directly on
existing datasets neglects their inconsistent entity definitions and redundant
data, limiting LLMs to dataset-specific learning and hindering out-of-domain
adaptation. To address this, we present B2NERD, a compact dataset designed to
guide LLMs' generalization in Open NER under a universal entity taxonomy.
B2NERD is refined from 54 existing English and Chinese datasets using a
two-step process. First, we detect inconsistent entity definitions across
datasets and clarify them by distinguishable label names to construct a
universal taxonomy of 400+ entity types. Second, we address redundancy using a
data pruning strategy that selects fewer samples with greater category and
semantic diversity. Comprehensive evaluation shows that B2NERD significantly
enhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,
outperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3
out-of-domain benchmarks across 15 datasets and 6 languages. The data, models,
and code are publicly available at https://github.com/UmeanNever/B2NER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025. Camera-ready version updated. Project page:
  https://github.com/UmeanNever/B2NER</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IndoNLP 2025: Shared Task on Real-Time Reverse Transliteration for
  Romanized Indo-Aryan languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deshan Sumanathilaka, Isuri Anuradha, Ruvan Weerasinghe, Nicholas Micallef, Julian Hough
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper overviews the shared task on Real-Time Reverse Transliteration for
Romanized Indo-Aryan languages. It focuses on the reverse transliteration of
low-resourced languages in the Indo-Aryan family to their native scripts.
Typing Romanized Indo-Aryan languages using ad-hoc transliterals and achieving
accurate native scripts are complex and often inaccurate processes with the
current keyboard systems. This task aims to introduce and evaluate a real-time
reverse transliterator that converts Romanized Indo-Aryan languages to their
native scripts, improving the typing experience for users. Out of 11 registered
teams, four teams participated in the final evaluation phase with
transliteration models for Sinhala, Hindi and Malayalam. These proposed
solutions not only solve the issue of ad-hoc transliteration but also empower
low-resource language usability in the digital arena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, 1 Figure, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind the Error! Detection and Localization of Instruction Errors in
  Vision-and-Language Navigation <span class="chip">IROS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Yiming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of
the most intuitive yet challenging embodied AI tasks. Agents are tasked to
navigate towards a target goal by executing a set of low-level actions,
following a series of natural language instructions. All VLN-CE methods in the
literature assume that language instructions are exact. However, in practice,
instructions given by humans can contain errors when describing a spatial
environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do
not address this scenario, making the state-of-the-art methods in VLN-CE
fragile in the presence of erroneous instructions from human users. For the
first time, we propose a novel benchmark dataset that introduces various types
of instruction errors considering potential human causes. This benchmark
provides valuable insight into the robustness of VLN systems in continuous
environments. We observe a noticeable performance drop (up to -25%) in Success
Rate when evaluating the state-of-the-art VLN-CE methods on our benchmark.
Moreover, we formally define the task of Instruction Error Detection and
Localization, and establish an evaluation protocol on top of our benchmark
dataset. We also propose an effective method, based on a cross-modal
transformer architecture, that achieves the best performance in error detection
and localization, compared to baselines. Surprisingly, our proposed method has
revealed errors in the validation set of the two commonly used datasets for
VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in
other tasks. Code and dataset available at
https://intelligolabs.github.io/R2RIE-CE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 figures, 8 pages. Accepted at IROS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge
  <span class="highlight-title">Graph</span>-Enhanced <span class="highlight-title">Retrie</span>val-Augmented Generation (KG-RAG) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17696v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17696v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Dong, Yimin Yuan, Kan Chen, Shupei Cheng, Chujie Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel framework for adaptable AI tutors using
Knowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG). This approach
addresses the critical challenges of information hallucination and limited
course-specific adaptation prevalent in Large Language Model (LLM)-based
tutoring systems. By integrating Knowledge Graphs (KGs) with RAG, we provide a
structured representation of course concepts and their interrelationships,
grounding the AI tutor's responses in relevant, validated material. We leverage
Qwen2.5, a powerful and cost-effective LLM, within our KG-RAG framework. A user
study (n=50) demonstrated positive student feedback regarding answer relevance,
ease of use, and overall satisfaction. This KG-RAG framework offers a promising
pathway towards personalized learning experiences and broader access to
high-quality education.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallelizing Linear Transformers with the Delta Rule over Sequence
  Length 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06484v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06484v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers with linear attention (i.e., linear transformers) and
state-space models have recently been suggested as a viable linear-time
alternative to transformers with softmax attention. However, these models still
underperform transformers especially on tasks that require in-context
retrieval. While more expressive variants of linear transformers which replace
the additive update in linear transformers with the delta rule (DeltaNet) have
been found to be more effective at associative recall, existing algorithms for
training such models do not parallelize over sequence length and are thus
inefficient to train on modern hardware. This work describes a
hardware-efficient algorithm for training linear transformers with the delta
rule, which exploits a memory-efficient representation for computing products
of Householder matrices. This algorithm allows us to scale up DeltaNet to
standard language modeling settings. We train a 1.3B model for 100B tokens and
find that it outperforms recent linear-time baselines such as Mamba and GLA in
terms of perplexity and zero-shot performance on downstream tasks. We also
experiment with two hybrid models which combine DeltaNet layers with (1)
sliding-window attention layers every other layer or (2) two global attention
layers, and find that these hybrids outperform strong transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Automated Simulation Research Workflow through LLM <span class="highlight-title">Prompt</span>
  Engineering Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Liu, Yubo Chai, Jianfeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has created new opportunities for
the automation of scientific research spanning both experimental processes and
computational simulations. This study explores the feasibility of constructing
an autonomous simulation agent (ASA) powered by LLMs through prompt engineering
and automated program design to automate the entire simulation research process
according to a human-provided research plan. This process includes experimental
design, remote upload and simulation execution, data analysis, and report
compilation. Using a well-studied simulation problem of polymer chain
conformations as a test case, we assessed the long-task completion and
reliability of ASAs powered by different LLMs, including GPT-4o, Claude-3.5,
etc. Our findings revealed that ASA-GPT-4o achieved near-flawless execution on
designated research missions, underscoring the potential of methods like ASA to
achieve automation in simulation research processes to enhance research
efficiency. The outlined automation can be iteratively performed for up to 20
cycles without human intervention, illustrating the potential of ASA for
long-task workflow automation. Additionally, we discussed the intrinsic traits
of ASA in managing extensive tasks, focusing on self-validation mechanisms, and
the balance between local attention and global oversight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code and example results of ASA can be found at
  https://github.com/zokaraa/autonomous_simulation_agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AfriHate: A Multilingual Collection of Hate Speech and Abusive Language
  <span class="highlight-title">Dataset</span>s for African Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Saminu Mohammad Aliyu, Nelson Odhiambo Onyango, Lilian D. A. Wanzare, Samuel Rutunda, Lukman Jibril Aliyu, Esubalew Alemneh, Oumaima Hourrane, Hagos Tesfahun Gebremichael, Elyas Abdi Ismail, Meriem Beloucif, Ebrahim Chekol Jibril, Andiswa Bukula, Rooweither Mabuya, Salomey Osei, Abigail Oppong, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw, Chiamaka Ijeoma Chukwuneke, Paul Röttger, Seid Muhie Yimam, Nedjma Ousidhoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech and abusive language are global phenomena that need
socio-cultural background knowledge to be understood, identified, and
moderated. However, in many regions of the Global South, there have been
several documented occurrences of (1) absence of moderation and (2) censorship
due to the reliance on keyword spotting out of context. Further, high-profile
individuals have frequently been at the center of the moderation process, while
large and targeted hate speech campaigns against minorities have been
overlooked. These limitations are mainly due to the lack of high-quality data
in the local languages and the failure to include local communities in the
collection, annotation, and moderation processes. To address this issue, we
present AfriHate: a multilingual collection of hate speech and abusive language
datasets in 15 African languages. Each instance in AfriHate is annotated by
native speakers familiar with the local culture. We report the challenges
related to the construction of the datasets and present various classification
baseline results with and without using LLMs. The datasets, individual
annotations, and hate speech and offensive language lexicons are available on
https://github.com/AfriHate/AfriHate
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware
  Self-Reflection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning (IT) is crucial to tailoring large language models (LLMs)
towards human-centric interactions. Recent advancements have shown that the
careful selection of a small, high-quality subset of IT data can significantly
enhance the performance of LLMs. Despite this, common approaches often rely on
additional models or data, which increases costs and limits widespread
adoption. In this work, we propose a novel approach, termed SelectIT, that
capitalizes on the foundational capabilities of the LLM itself. Specifically,
we exploit the intrinsic uncertainty present in LLMs to more effectively select
high-quality IT data, without the need for extra resources. Furthermore, we
introduce a curated IT dataset, the Selective Alpaca, created by applying
SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT
using Selective Alpaca leads to substantial model ability enhancement. The
robustness of SelectIT has also been corroborated in various foundation models
and domain-specific tasks. Our findings suggest that longer and more
computationally intensive IT data may serve as superior sources of IT, offering
valuable insights for future research in this area. Data, code, and scripts are
freely available at https://github.com/Blue-Raincoat/SelectIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Knowledge Conflicts in Language Model-Driven Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Cao, Zhaoyang Zhang, Xiangtian Li, Chufan Wu, Hansong Zhang, Wenqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of knowledge-driven seq-to-seq generation tasks, such as
document-based question answering and document summarization systems, two
fundamental knowledge sources play crucial roles: the inherent knowledge
embedded within model parameters and the external knowledge obtained through
context. Recent studies revealed a significant challenge: when there exists a
misalignment between the model's inherent knowledge and the ground truth
answers in training data, the system may exhibit problematic behaviors during
inference, such as ignoring input context, or generating unfaithful content.
Our investigation proposes a strategy to minimize hallucination by building
explicit connection between source inputs and generated outputs. We
specifically target a common hallucination pattern in question answering,
examining how the correspondence between entities and their contexts during
model training influences the system's performance at inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>revised version, more figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TANQ: An open domain <span class="highlight-title">dataset</span> of <span class="highlight-title">table</span> answered questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashara Akhtar, Chenxi Pang, Andreea Marzoca, Yasemin Altun, Julian Martin Eisenschlos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models, potentially augmented with tool usage such as retrieval are
becoming the go-to means of answering questions. Understanding and answering
questions in real-world settings often requires retrieving information from
different sources, processing and aggregating data to extract insights, and
presenting complex findings in form of structured artifacts such as novel
tables, charts, or infographics. In this paper, we introduce TANQ, the first
open domain question answering dataset where the answers require building
tables from information across multiple sources. We release the full source
attribution for every cell in the resulting table and benchmark
state-of-the-art language models in open, oracle, and closed book setups. Our
best-performing baseline, GPT4 reaches an overall F1 score of 29.1, lagging
behind human performance by 19.7 points. We analyse baselines' performance
across different dataset attributes such as different skills required for this
task, including multi-hop reasoning, math operations, and unit conversions. We
further discuss common failures in model-generated answers, suggesting that
TANQ is a complex task with many challenges ahead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Salmon: A Suite for Acoustic Language Model Evaluation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07437v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07437v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gallil Maimon, Amit Roth, Yossi Adi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech language models have recently demonstrated great potential as
universal speech processing systems. Such models have the ability to model the
rich acoustic information existing in audio signals, beyond spoken content,
such as emotion, background noise, etc. Despite this, evaluation benchmarks
which evaluate awareness to a wide range of acoustic aspects, are lacking. To
help bridge this gap, we introduce SALMon, a novel evaluation suite
encompassing background noise, emotion, speaker identity and room impulse
response. The proposed benchmarks both evaluate the consistency of the
inspected element and how much it matches the spoken text. We follow a
modelling based approach, measuring whether a model gives correct samples
higher scores than incorrect ones. This approach makes the benchmark fast to
compute even for large models. We evaluated several speech language models on
SALMon, thus highlighting the strengths and weaknesses of each evaluated
method. We make the code and data publicly available at
https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2025, project page -
  https://pages.cs.huji.ac.il/adiyoss-lab/salmon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noise-powered Multi-modal Knowledge <span class="highlight-title">Graph</span> Representation Framework <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06832v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06832v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Yin Fang, Yichi Zhang, Lingbing Guo, Jiaoyan Chen, Jeff Z. Pan, Huajun Chen, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Multi-modal Pre-training highlights the necessity for a unified
Multi-Modal Knowledge Graph (MMKG) representation learning framework. Such a
framework is essential for embedding structured knowledge into multi-modal
Large Language Models effectively, alleviating issues like knowledge
misconceptions and multi-modal hallucinations. In this work, we explore the
efficacy of models in accurately embedding entities within MMKGs through two
pivotal tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal
Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG
method that utilizes a Transformer-based architecture equipped with
modality-level noise masking to robustly integrate multi-modal entity features
in KGs. By incorporating specific training objectives for both MKGC and MMEA,
our approach achieves SOTA performance across a total of ten datasets,
demonstrating its versatility. Moreover, SNAG can not only function as a
standalone model but also enhance other existing methods, providing stable
performance improvements. Code and data are available at
https://github.com/zjukg/SNAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Accepted, Repo is available at
  https://github.com/zjukg/SNAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative Analysis of Listwise Reranking with Large Language Models in
  Limited-Resource Language Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxin Shen, Lun Wang, Chuanqi Shi, Shaoshuai Du, Yiyi Tao, Yixian Shen, Hang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated significant effectiveness
across various NLP tasks, including text ranking. This study assesses the
performance of large language models (LLMs) in listwise reranking for
limited-resource African languages. We compare proprietary models RankGPT3.5,
Rank4o-mini, RankGPTo1-mini and RankClaude-sonnet in cross-lingual contexts.
Results indicate that these LLMs significantly outperform traditional baseline
methods such as BM25-DT in most evaluation metrics, particularly in nDCG@10 and
MRR@100. These findings highlight the potential of LLMs in enhancing reranking
tasks for low-resource languages and offer insights into cost-effective
solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Mirror Cognitive Language Processing? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18023v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18023v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Ren, Renren Jin, Tongxuan Zhang, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable abilities in text
comprehension and logical reasoning, indicating that the text representations
learned by LLMs can facilitate their language processing capabilities. In
neuroscience, brain cognitive processing signals are typically utilized to
study human language processing. Therefore, it is natural to ask how well the
text embeddings from LLMs align with the brain cognitive processing signals,
and how training strategies affect the LLM-brain alignment? In this paper, we
employ Representational Similarity Analysis (RSA) to measure the alignment
between 23 mainstream LLMs and fMRI signals of the brain to evaluate how
effectively LLMs simulate cognitive language processing. We empirically
investigate the impact of various factors (e.g., pre-training data size, model
scaling, alignment training, and prompts) on such LLM-brain alignment.
Experimental results indicate that pre-training data size and model scaling are
positively correlated with LLM-brain similarity, and alignment training can
significantly improve LLM-brain similarity. Explicit prompts contribute to the
consistency of LLMs with brain cognitive language processing, while nonsensical
noisy prompts may attenuate such alignment. Additionally, the performance of a
wide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated
with the LLM-brain similarity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Outlines for Code: Literate Programming in the LLM Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kensen Shi, Deniz Altınbüken, Saswat Anand, Mihai Christodorescu, Katja Grünwedel, Alexa Koenings, Sai Naidu, Anurag Pathak, Marc Rasi, Fredde Ribeiro, Brandon Ruffin, Siddhant Sanyam, Maxim Tabachnyk, Sara Toth, Roy Tu, Tobias Welp, Pengcheng Yin, Manzil Zaheer, Satish Chandra, Charles Sutton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose using natural language outlines as a novel modality and
interaction surface for providing AI assistance to developers throughout the
software development process. An NL outline for a code function comprises
multiple statements written in concise prose, which partition the code and
summarize its main ideas in the style of literate programming. Crucially, we
find that modern LLMs can generate accurate and high-quality NL outlines in
practice. Moreover, NL outlines enable a bidirectional sync between code and
NL, allowing changes in one to be automatically reflected in the other. We
discuss many use cases for NL outlines: they can accelerate understanding and
navigation of code and diffs, simplify code maintenance, augment code search,
steer code generation, and more. We then propose and compare multiple LLM
prompting techniques for generating outlines and ask professional developers to
judge outline quality. Finally, we present two case studies applying NL
outlines toward code review and malware detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Debating with Preset Stances for Hallucination
  Elimination of LLMs <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Fang, Moxin Li, Wenjie Wang, Hui Lin, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in various natural language processing
tasks but struggle with hallucination issues. Existing solutions have
considered utilizing LLMs' inherent reasoning abilities to alleviate
hallucination, such as self-correction and diverse sampling methods. However,
these methods often overtrust LLMs' initial answers due to inherent biases. The
key to alleviating this issue lies in overriding LLMs' inherent biases for
answer inspection. To this end, we propose a CounterFactual Multi-Agent Debate
(CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent
biases by compelling LLMs to generate justifications for a predetermined
answer's correctness. The LLMs with different predetermined stances are engaged
with a skeptical critic for counterfactual debate on the rationality of
generated justifications. Finally, the debate process is evaluated by a
third-party judge to determine the final answer. Extensive experiments on four
datasets of three tasks demonstrate the superiority of CFMAD over existing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01028v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01028v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, Haofen Wang, Jun Yu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As retrieval-augmented generation prevails in large language models,
embedding models are becoming increasingly crucial. Despite the growing number
of general embedding models, prior work often overlooks the critical role of
training data quality. In this work, we introduce KaLM-Embedding, a general
multilingual embedding model that leverages a large quantity of cleaner, more
diverse, and domain-specific training data. Our model has been trained with key
techniques proven to enhance performance: (1) persona-based synthetic data to
create diversified examples distilled from LLMs, (2) ranking consistency
filtering to remove less informative samples, and (3) semi-homogeneous task
batch sampling to improve training efficacy. Departing from traditional
BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,
facilitating the adaptation of auto-regressive language models for general
embedding tasks. Extensive evaluations of the MTEB benchmark across multiple
languages show that our model outperforms others of comparable size, setting a
new standard for multilingual embedding models with <1B parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. 23 pages, 6 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction
  Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Fang, Xinle Deng, Kangwei Liu, Ningyu Zhang, Jingyang Qian, Penghui Yang, Xiaohui Fan, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models excel at interpreting complex natural language
instructions, enabling them to perform a wide range of tasks. In the life
sciences, single-cell RNA sequencing (scRNA-seq) data serves as the "language
of cellular biology", capturing intricate gene expression patterns at the
single-cell level. However, interacting with this "language" through
conventional tools is often inefficient and unintuitive, posing challenges for
researchers. To address these limitations, we present InstructCell, a
multi-modal AI copilot that leverages natural language as a medium for more
direct and flexible single-cell analysis. We construct a comprehensive
multi-modal instruction dataset that pairs text-based instructions with
scRNA-seq profiles from diverse tissues and species. Building on this, we
develop a multi-modal cell language architecture capable of simultaneously
interpreting and processing both modalities. InstructCell empowers researchers
to accomplish critical tasks-such as cell type annotation, conditional
pseudo-cell generation, and drug sensitivity prediction-using straightforward
natural language commands. Extensive evaluations demonstrate that InstructCell
consistently meets or exceeds the performance of existing single-cell
foundation models, while adapting to diverse experimental conditions. More
importantly, InstructCell provides an accessible and intuitive tool for
exploring complex single-cell data, lowering technical barriers and enabling
deeper biological insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell,
  Models: https://huggingface.co/zjunlp/Instructcell-chat,
  https://huggingface.co/zjunlp/InstructCell-instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding E<span class="highlight-title">merge</span>nt Abilities of Language Models from the Loss
  Perspective <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15796v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15796v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiao Du, Aohan Zeng, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have put into question the belief that emergent abilities in
language models are exclusive to large models. This skepticism arises from two
observations: 1) smaller models can also exhibit high performance on emergent
abilities and 2) there is doubt on the discontinuous metrics used to measure
these abilities. In this paper, we propose to study emergent abilities in the
lens of pre-training loss, instead of model size or training compute. We
demonstrate that the Transformer models with the same pre-training loss, but
different model and data sizes, generate the same performance on various
downstream tasks, with a fixed data corpus, tokenization, and model
architecture. We also discover that a model exhibits emergent abilities on
certain tasks -- regardless of the continuity of metrics -- when its
pre-training loss falls below a specific threshold. Before reaching this
threshold, its performance remains at the level of random guessing. This
inspires us to redefine emergent abilities as those that manifest in models
with lower pre-training losses, highlighting that these abilities cannot be
predicted by merely extrapolating the performance trends of models with higher
pre-training losses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures. Accepted in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Con-ReCall: Detecting <span class="highlight-title">Pre-train</span>ing Data in LLMs via Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03363v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03363v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Wang, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training data in large language models is key to their success, but it
also presents privacy and security risks, as it may contain sensitive
information. Detecting pre-training data is crucial for mitigating these
concerns. Existing methods typically analyze target text in isolation or solely
with non-member contexts, overlooking potential insights from simultaneously
considering both member and non-member contexts. While previous work suggested
that member contexts provide little information due to the minor distributional
shift they induce, our analysis reveals that these subtle shifts can be
effectively leveraged when contrasted with non-member contexts. In this paper,
we propose Con-ReCall, a novel approach that leverages the asymmetric
distributional shifts induced by member and non-member contexts through
contrastive decoding, amplifying subtle differences to enhance membership
inference. Extensive empirical evaluations demonstrate that Con-ReCall achieves
state-of-the-art performance on the WikiMIA benchmark and is robust against
various text manipulation techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Automata Embeddings for Goal-Conditioned Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beyazit Yalcinkaya, Niklas Lauffer, Marcell Vazquez-Chanlatte, Sanjit A. Seshia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-conditioned reinforcement learning is a powerful way to control an AI
agent's behavior at runtime. That said, popular goal representations, e.g.,
target states or natural language, are either limited to Markovian tasks or
rely on ambiguous task semantics. We propose representing temporal goals using
compositions of deterministic finite automata (cDFAs) and use cDFAs to guide RL
agents. cDFAs balance the need for formal temporal semantics with ease of
interpretation: if one can understand a flow chart, one can understand a cDFA.
On the other hand, cDFAs form a countably infinite concept class with Boolean
semantics, and subtle changes to the automaton can result in very different
tasks, making them difficult to condition agent behavior on. To address this,
we observe that all paths through a DFA correspond to a series of reach-avoid
tasks and propose pre-training graph neural network embeddings on "reach-avoid
derived" DFAs. Through empirical evaluation, we demonstrate that the proposed
pre-training method enables zero-shot generalization to various cDFA task
classes and accelerated policy specialization without the myopic suboptimality
of hierarchical methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 2 OLMo 2 Furious 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present OLMo 2, the next generation of our fully open language models.
OLMo 2 includes dense autoregressive models with improved architecture and
training recipe, pretraining data mixtures, and instruction tuning recipes. Our
modified model architecture and training recipe achieve both better training
stability and improved per-token efficiency. Our updated pretraining data
mixture introduces a new, specialized data mix called Dolmino Mix 1124, which
significantly improves model capabilities across many downstream task
benchmarks when introduced via late-stage curriculum training (i.e. specialized
data during the annealing phase of pretraining). Finally, we incorporate best
practices from T\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data
and extending our final-stage reinforcement learning with verifiable rewards
(RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to
compute, often matching or outperforming open-weight only models like Llama 3.1
and Qwen 2.5 while using fewer FLOPs and with fully transparent training data,
code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or
surpassing open-weight only models of comparable size, including Qwen 2.5,
Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B
and 13B scales, both pretrained and post-trained, including their full training
data, training code and recipes, training logs and thousands of intermediate
checkpoints. The final instruction model is available on the Ai2 Playground as
a free research demo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Model demo available at playground.allenai.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated <span class="highlight-title">Review</span> Generation Method Based on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20906v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20906v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Changying Du, Zhi-Jian Zhao, Jinlong Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Literature research, vital for scientific work, faces the challenge of
surging information volumes exceeding researchers' processing capabilities. We
present an automated review generation method based on large language models
(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our
statistically validated evaluation framework demonstrates that the generated
reviews match or exceed manual quality, offering broad applicability across
research fields without requiring users' domain knowledge. Applied to propane
dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,
averaging seconds per article per LLM account, producing comprehensive reviews
spanning 35 topics, with extended analysis of 1041 articles providing insights
into catalysts' properties. Through multi-layered quality control, we
effectively mitigated LLMs' hallucinations, with expert verification confirming
accuracy and citation integrity while demonstrating hallucination risks reduced
to below 0.5\% with 95\% confidence. Released Windows application enables
one-click review generation, enhancing research productivity and literature
recommendation efficiency while setting the stage for broader scientific
explorations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, 1 tables Code:
  https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration Data:
  https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData This research
  has been invited for a Short Oral presentation at the 18th ICC -
  International Congress on Catalysis, taking place in Lyon, France from July
  14-19, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking the Imposters: How Censorship and Domain Adaptation Affect the
  Detection of Machine-Generated Tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan E. Tuck, Rakesh M. Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has significantly
improved the generation of fluent and convincing text, raising concerns about
their potential misuse on social media platforms. We present a comprehensive
methodology for creating nine Twitter datasets to examine the generative
capabilities of four prominent LLMs: Llama 3, Mistral, Qwen2, and GPT4o. These
datasets encompass four censored and five uncensored model configurations,
including 7B and 8B parameter base-instruction models of the three open-source
LLMs. Additionally, we perform a data quality analysis to assess the
characteristics of textual outputs from human, "censored," and "uncensored"
models, employing semantic meaning, lexical richness, structural patterns,
content characteristics, and detector performance metrics to identify
differences and similarities. Our evaluation demonstrates that "uncensored"
models significantly undermine the effectiveness of automated detection
methods. This study addresses a critical gap by exploring smaller open-source
models and the ramifications of "uncensoring," providing valuable insights into
how domain adaptation and content moderation strategies influence both the
detectability and structural characteristics of machine-generated text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PASS: Presentation Automation for Slide Generation and Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Aggarwal, Aarohi Bhand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's fast-paced world, effective presentations have become an essential
tool for communication in both online and offline meetings. The crafting of a
compelling presentation requires significant time and effort, from gathering
key insights to designing slides that convey information clearly and concisely.
However, despite the wealth of resources available, people often find
themselves manually extracting crucial points, analyzing data, and organizing
content in a way that ensures clarity and impact. Furthermore, a successful
presentation goes beyond just the slides; it demands rehearsal and the ability
to weave a captivating narrative to fully engage the audience. Although there
has been some exploration of automating document-to-slide generation, existing
research is largely centered on converting research papers. In addition,
automation of the delivery of these presentations has yet to be addressed. We
introduce PASS, a pipeline used to generate slides from general Word documents,
going beyond just research papers, which also automates the oral delivery of
the generated slides. PASS analyzes user documents to create a dynamic,
engaging presentation with an AI-generated voice. Additionally, we developed an
LLM-based evaluation metric to assess our pipeline across three critical
dimensions of presentations: relevance, coherence, and redundancy. The data and
codes are available at https://github.com/AggarwalTushar/PASS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Evaluation of Large Language Models for Classifying Tropical
  and Infectious Diseases <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mercy Asiedu, Nenad Tomasev, Chintan Ghate, Tiya Tiyasirichokchai, Awa Dieng, Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, Eric Ndombi, Katherine Heller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have shown promise for medical question
answering, there is limited work focused on tropical and infectious
disease-specific exploration. We build on an opensource tropical and infectious
diseases (TRINDs) dataset, expanding it to include demographic and semantic
clinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM
performance on these, comparing generalist and medical LLMs, as well as LLM
outcomes to human experts. We demonstrate through systematic experimentation,
the benefit of contextual information such as demographics, location, gender,
risk factors for optimal LLM response. Finally we develop a prototype of
TRINDs-LM, a research tool that provides a playground to navigate how context
impacts LLM outputs for health.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2 NeurIPS 2024 workshops: Generative AI for Health
  Workshop and Workshop on Advancements In Medical Foundation Models:
  Explainability, Robustness, Security, and Beyond</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">125</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ouroboros-Diffusion: Exploring Consistent Content Generation in
  Tuning-free Long Video Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyuan Chen, Fuchen Long, Jie An, Zhaofan Qiu, Ting Yao, Jiebo Luo, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The first-in-first-out (FIFO) video diffusion, built on a pre-trained
text-to-video model, has recently emerged as an effective approach for
tuning-free long video generation. This technique maintains a queue of video
frames with progressively increasing noise, continuously producing clean frames
at the queue's head while Gaussian noise is enqueued at the tail. However,
FIFO-Diffusion often struggles to keep long-range temporal consistency in the
generated videos due to the lack of correspondence modeling across frames. In
this paper, we propose Ouroboros-Diffusion, a novel video denoising framework
designed to enhance structural and content (subject) consistency, enabling the
generation of consistent videos of arbitrary length. Specifically, we introduce
a new latent sampling technique at the queue tail to improve structural
consistency, ensuring perceptually smooth transitions among frames. To enhance
subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA)
mechanism, which aligns subjects across frames within short segments to achieve
better visual coherence. Furthermore, we introduce self-recurrent guidance.
This technique leverages information from all previous cleaner frames at the
front of the queue to guide the denoising of noisier frames at the end,
fostering rich and contextual global information interaction. Extensive
experiments of long video generation on the VBench benchmark demonstrate the
superiority of our Ouroboros-Diffusion, particularly in terms of subject
consistency, motion smoothness, and temporal consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal LLMs Can Reason about Aesthetics in Zero-Shot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Jiang, Changwen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability
shall be elicited to evaluate the aesthetics of artworks. To facilitate this
investigation, we construct MM-StyleBench, a novel high-quality dataset for
benchmarking artistic stylization. We then develop a principled method for
human preference modeling and perform a systematic correlation analysis between
MLLMs' responses and human preference. Our experiments reveal an inherent
hallucination issue of MLLMs in art evaluation, associated with response
subjectivity. ArtCoT is proposed, demonstrating that art-specific task
decomposition and the use of concrete language boost MLLMs' reasoning ability
for aesthetics. Our findings offer valuable insights into MLLMs for art and can
benefit a wide range of downstream applications, such as style transfer and
artistic image generation. Code available at
https://github.com/songrise/MLLM4Art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WIP, Homepage https://github.com/songrise/MLLM4Art</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and
  Segmentation Mask Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Bhat, Rupak Bose, Chinedu Innocent Nwoye, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring and annotating surgical data is often resource-intensive, ethical
constraining, and requiring significant expert involvement. While generative AI
models like text-to-image can alleviate data scarcity, incorporating spatial
annotations, such as segmentation masks, is crucial for precision-driven
surgical applications, simulation, and education. This study introduces both a
novel task and method, SimGen, for Simultaneous Image and Mask Generation.
SimGen is a diffusion model based on the DDPM framework and Residual U-Net,
designed to jointly generate high-fidelity surgical images and their
corresponding segmentation masks. The model leverages cross-correlation priors
to capture dependencies between continuous image and discrete mask
distributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed to
enhance class separability and uniformity in the RGB space of the masks. SimGen
delivers high-fidelity images and accurate segmentation masks, outperforming
baselines across six public datasets assessed on image and semantic inception
distance metrics. Ablation study shows that the CFL improves mask quality and
spatial separation. Downstream experiments suggest generated image-mask pairs
are usable if regulations limit human data release for research. This work
offers a cost-effective solution for generating paired surgical images and
complex labels, advancing surgical AI development by reducing the need for
expensive manual annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 17 figures, 4 tables, project page at
  https://camma-public.github.io/endogen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision <span class="highlight-title">Foundation</span> Models for Computed Tomo<span class="highlight-title">graph</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Pai, Ibrahim Hadzic, Dennis Bontempi, Keno Bressem, Benjamin H. Kann, Andriy Fedorov, Raymond H. Mak, Hugo J. W. L. Aerts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) have shown transformative potential in radiology by
performing diverse, complex tasks across imaging modalities. Here, we developed
CT-FM, a large-scale 3D image-based pre-trained model designed explicitly for
various radiological tasks. CT-FM was pre-trained using 148,000 computed
tomography (CT) scans from the Imaging Data Commons through label-agnostic
contrastive learning. We evaluated CT-FM across four categories of tasks,
namely, whole-body and tumor segmentation, head CT triage, medical image
retrieval, and semantic understanding, showing superior performance against
state-of-the-art models. Beyond quantitative success, CT-FM demonstrated the
ability to cluster regions anatomically and identify similar anatomical and
structural concepts across scans. Furthermore, it remained robust across
test-retest settings and indicated reasonable salient regions attached to its
embeddings. This study demonstrates the value of large-scale medical imaging
foundation models and by open-sourcing the model weights, code, and data, aims
to support more adaptable, reliable, and interpretable AI solutions in
radiology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 figures, followed by 9 Extended Data Figures and a Supplementary
  Information document</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RepVideo: Rethinking Cross-Layer Representation for Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Si, Weichen Fan, Zhengyao Lv, Ziqi Huang, Yu Qiao, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation has achieved remarkable progress with the introduction of
diffusion models, which have significantly improved the quality of generated
videos. However, recent research has primarily focused on scaling up model
training, while offering limited insights into the direct impact of
representations on the video generation process. In this paper, we initially
investigate the characteristics of features in intermediate layers, finding
substantial variations in attention maps across different layers. These
variations lead to unstable semantic representations and contribute to
cumulative differences between features, which ultimately reduce the similarity
between adjacent frames and negatively affect temporal coherence. To address
this, we propose RepVideo, an enhanced representation framework for
text-to-video diffusion models. By accumulating features from neighboring
layers to form enriched representations, this approach captures more stable
semantic information. These enhanced representations are then used as inputs to
the attention mechanism, thereby improving semantic expressiveness while
ensuring feature consistency across adjacent frames. Extensive experiments
demonstrate that our RepVideo not only significantly enhances the ability to
generate accurate spatial appearances, such as capturing complex spatial
relationships between multiple objects, but also improves temporal consistency
in video generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vchitect.github.io/RepVid-Webpage</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D scene generation has garnered growing attention in recent years and has
made significant progress. Generating 4D cities is more challenging than 3D
scenes due to the presence of structurally complex, visually diverse objects
like buildings and vehicles, and heightened human sensitivity to distortions in
urban environments. To tackle these issues, we propose CityDreamer4D, a
compositional generative model specifically tailored for generating unbounded
4D cities. Our main insights are 1) 4D city generation should separate dynamic
objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2)
all objects in the 4D scene should be composed of different types of neural
fields for buildings, vehicles, and background stuff. Specifically, we propose
Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic
traffic scenarios and static city layouts using a highly compact BEV
representation. Objects in 4D cities are generated by combining stuff-oriented
and instance-oriented neural fields for background stuff, buildings, and
vehicles. To suit the distinct characteristics of background stuff and
instances, the neural fields employ customized generative hash grids and
periodic positional embeddings as scene parameterizations. Furthermore, we
offer a comprehensive suite of datasets for city generation, including OSM,
GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world
city layouts, while the Google Earth and CityTopia datasets deliver
large-scale, high-quality city imagery complete with 3D instance annotations.
Leveraging its compositional design, CityDreamer4D supports a range of
downstream applications, such as instance editing, city stylization, and urban
simulation, while delivering state-of-the-art performance in generating
realistic 4D cities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes
  with Gaussian Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Ma, Runyi Yang, Bin Ren, Ender Konukoglu, Luc Van Gool, Danda Pani Paudel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localizing text descriptions in large-scale 3D scenes is inherently an
ambiguous task. This nonetheless arises while describing general concepts, e.g.
all traffic lights in a city.
  To facilitate reasoning based on such concepts, text localization in the form
of distribution is required. In this paper, we generate the distribution of the
camera poses conditioned upon the textual description.
  To facilitate such generation, we propose a diffusion-based architecture that
conditionally diffuses the noisy 6DoF camera poses to their plausible
locations.
  The conditional signals are derived from the text descriptions, using the
pre-trained text encoders. The connection between text descriptions and pose
distribution is established through pretrained Vision-Language-Model, i.e.
CLIP. Furthermore, we demonstrate that the candidate poses for the distribution
can be further refined by rendering potential poses using 3D Gaussian
splatting, guiding incorrectly posed samples towards locations that better
align with the textual description, through visual reasoning.
  We demonstrate the effectiveness of our method by comparing it with both
standard retrieval methods and learning-based approaches. Our proposed method
consistently outperforms these baselines across all five large-scale datasets.
Our source code and dataset will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An analysis of data variation and bias in image-based dermatological
  <span class="highlight-title">dataset</span>s for machine learning classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mauro, Emanoel Thyago, Othon Vinicius, Rodrigo Abreu, Kelvin Cunha, José Gabriel, Rafael Barros, Thales Bezerra, Manoel Henriques, Natalia Lopes, Érico Moutinho, Jéssica Guido, Tsang Ing Ren, Paulo Borba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI algorithms have become valuable in aiding professionals in healthcare. The
increasing confidence obtained by these models is helpful in critical decision
demands. In clinical dermatology, classification models can detect malignant
lesions on patients' skin using only RGB images as input. However, most
learning-based methods employ data acquired from dermoscopic datasets on
training, which are large and validated by a gold standard. Clinical models aim
to deal with classification on users' smartphone cameras that do not contain
the corresponding resolution provided by dermoscopy. Also, clinical
applications bring new challenges. It can contain captures from uncontrolled
environments, skin tone variations, viewpoint changes, noises in data and
labels, and unbalanced classes. A possible alternative would be to use transfer
learning to deal with the clinical images. However, as the number of samples is
low, it can cause degradations on the model's performance; the source
distribution used in training differs from the test set. This work aims to
evaluate the gap between dermoscopic and clinical samples and understand how
the dataset variations impact training. It assesses the main differences
between distributions that disturb the model's prediction. Finally, from
experiments on different architectures, we argue how to combine the data from
divergent distributions, decreasing the impact on the model's final accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual WetlandBirds <span class="highlight-title">Dataset</span>: Bird Species Identification and Behavior
  Recognition in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Rodriguez-Juan, David Ortiz-Perez, Manuel Benavent-Lledo, David Mulero-Pérez, Pablo Ruiz-Ponce, Adrian Orihuela-Torres, Jose Garcia-Rodriguez, Esther Sebastián-González
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current biodiversity loss crisis makes animal monitoring a relevant field
of study. In light of this, data collected through monitoring can provide
essential insights, and information for decision-making aimed at preserving
global biodiversity. Despite the importance of such data, there is a notable
scarcity of datasets featuring videos of birds, and none of the existing
datasets offer detailed annotations of bird behaviors in video format. In
response to this gap, our study introduces the first fine-grained video dataset
specifically designed for bird behavior detection and species classification.
This dataset addresses the need for comprehensive bird video datasets and
provides detailed data on bird actions, facilitating the development of deep
learning models to recognize these, similar to the advancements made in human
action recognition. The proposed dataset comprises 178 videos recorded in
Spanish wetlands, capturing 13 different bird species performing 7 distinct
behavior classes. In addition, we also present baseline results using state of
the art models on two tasks: bird behavior recognition and species
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Joint Denoising, Demosaicing, and Compression from the Raw
  Natural Image Noise <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoit Brummer, Christophe De Vleeschouwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Raw Natural Image Noise Dataset (RawNIND), a
diverse collection of paired raw images designed to support the development of
denoising models that generalize across sensors, image development workflows,
and styles. Two denoising methods are proposed: one operates directly on raw
Bayer data, leveraging computational efficiency, while the other processes
linear RGB images for improved generalization to different sensors, with both
preserving flexibility for subsequent development. Both methods outperform
traditional approaches which rely on developed images. Additionally, the
integration of denoising and compression at the raw data level significantly
enhances rate-distortion performance and computational efficiency. These
findings suggest a paradigm shift toward raw data workflows for efficient and
flexible image processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Agricultural Insights: RiceLeafBD - A Novel <span class="highlight-title">Dataset</span> and
  Optimal Model Selection for Rice Leaf Disease Diagnosis through Transfer
  Learning Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadia Afrin Rimi, Md. Jalal Uddin Chowdhury, Rifat Abdullah, Iftekhar Ahmed, Mahrima Akter Mim, Mohammad Shoaib Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of people living in this agricultural nation of ours, which is
surrounded by lush greenery, is growing on a daily basis. As a result of this,
the level of arable land is decreasing, as well as residential houses and
industrial factories. The food crisis is becoming the main threat for us in the
upcoming days. Because on the one hand, the population is increasing, and on
the other hand, the amount of food crop production is decreasing due to the
attack of diseases. Rice is one of the most significant cultivated crops since
it provides food for more than half of the world's population. Bangladesh is
dependent on rice (Oryza sativa) as a vital crop for its agriculture, but it
faces a significant problem as a result of the ongoing decline in rice yield
brought on by common diseases. Early disease detection is the main difficulty
in rice crop cultivation. In this paper, we proposed our own dataset, which was
collected from the Bangladesh field, and also applied deep learning and
transfer learning models for the evaluation of the datasets. We elaborately
explain our dataset and also give direction for further research work to serve
society using this dataset. We applied a light CNN model and pre-trained
InceptionNet-V2, EfficientNet-V2, and MobileNet-V2 models, which achieved 91.5%
performance for the EfficientNet-V2 model of this work. The results obtained
assaulted other models and even exceeded approaches that are considered to be
part of the state of the art. It has been demonstrated by this study that it is
possible to precisely and effectively identify diseases that affect rice leaves
using this unbiased datasets. After analysis of the performance of different
models, the proposed datasets are significant for the society for research work
to provide solutions for decreasing rice leaf disease.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lights, Camera, Matching: The Role of Image Illumination in Fair Face
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriella Pangelinan, Grace Bezold, Haiyu Wu, Michael C. King, Kevin W. Bowyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial brightness is a key image quality factor impacting face recognition
accuracy differentials across demographic groups. In this work, we aim to
decrease the accuracy gap between the similarity score distributions for
Caucasian and African American female mated image pairs, as measured by d'
between distributions. To balance brightness across demographic groups, we
conduct three experiments, interpreting brightness in the face skin region
either as median pixel value or as the distribution of pixel values. Balancing
based on median brightness alone yields up to a 46.8% decrease in d', while
balancing based on brightness distribution yields up to a 57.6% decrease. In
all three cases, the similarity scores of the individual distributions improve,
with mean scores maximally improving 5.9% for Caucasian females and 3.7% for
African American females.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures, Conference submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT
  Scans: The C4R Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sneha N. Naik, Elsa D. Angelini, Eric A. Hoffman, Elizabeth C. Oelsner, R. Graham Barr, Benjamin M. Smith, Andrew F. Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ratio of airway tree lumen to lung size (ALR), assessed at full
inspiration on high resolution full-lung computed tomography (CT), is a major
risk factor for chronic obstructive pulmonary disease (COPD). There is growing
interest to infer ALR from cardiac CT images, which are widely available in
epidemiological cohorts, to investigate the relationship of ALR to severe
COVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously,
cardiac scans included approximately 2/3 of the total lung volume with 5-6x
greater slice thickness than high-resolution (HR) full-lung (FL) CT. In this
study, we present a novel attention-based Multi-view Swin Transformer to infer
FL ALR values from segmented cardiac CT scans. For the supervised training we
exploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of
Atherosclerosis (MESA). Our network significantly outperforms a proxy direct
ALR inference on segmented cardiac CT scans and achieves accuracy and
reproducibility comparable with a scan-rescan reproducibility of the FL ALR
ground-truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in Proceedings of International Symposium on
  Biomedical Imaging (ISBI), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Multi-Scale Cross-Attention for Person Image Generation <span class="chip">ECCV2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Tang, Ling Shao, Nicu Sebe, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel cross-attention-based generative
adversarial network (GAN) for the challenging person image generation task.
Cross-attention is a novel and intuitive multi-modal fusion method in which an
attention/correlation matrix is calculated between two feature maps of
different modalities. Specifically, we propose the novel XingGAN (or
CrossingGAN), which consists of two generation branches that capture the
person's appearance and shape, respectively. Moreover, we propose two novel
cross-attention blocks to effectively transfer and update the person's shape
and appearance embeddings for mutual improvement. This has not been considered
by any other existing GAN-based image generation work. To further learn the
long-range correlations between different person poses at different scales and
sub-regions, we propose two novel multi-scale cross-attention blocks. To tackle
the issue of independent correlation computations within the cross-attention
mechanism leading to noisy and ambiguous attention weights, which hinder
performance improvements, we propose a module called enhanced attention (EA).
Lastly, we introduce a novel densely connected co-attention module to fuse
appearance and shape features at different stages effectively. Extensive
experiments on two public datasets demonstrate that the proposed method
outperforms current GAN-based methods and performs on par with diffusion-based
methods. However, our method is significantly faster than diffusion-based
methods in both training and inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TPAMI, an extended version of a paper published in
  ECCV2020. arXiv admin note: substantial text overlap with arXiv:2007.09278</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-based One-For-All: A Universal Framework for Heterogeneous
  Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jhe-Hao Lin, Yi Yao, Chan-Feng Hsu, Hongxia Xie, Hong-Han Shuai, Wen-Huang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) involves transferring knowledge from a
pre-trained heavy teacher model to a lighter student model, thereby reducing
the inference cost while maintaining comparable effectiveness. Prior KD
techniques typically assume homogeneity between the teacher and student models.
However, as technology advances, a wide variety of architectures have emerged,
ranging from initial Convolutional Neural Networks (CNNs) to Vision
Transformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,
developing a universal KD framework compatible with any architecture has become
an important research topic. In this paper, we introduce a feature-based
one-for-all (FOFA) KD framework to enable feature distillation across diverse
architecture. Our framework comprises two key components. First, we design
prompt tuning blocks that incorporate student feedback, allowing teacher
features to adapt to the student model's learning process. Second, we propose
region-aware attention to mitigate the view mismatch problem between
heterogeneous architecture. By leveraging these two modules, effective
distillation of intermediate features can be achieved across heterogeneous
architectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate
the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Planning with 3D-vision Language <span class="highlight-title">Pre-train</span>ing for End-to-End
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengpeng Li, Hanli Wang, Xianfei Li, Wenlong Liao, Tao He, Pai Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving is a challenging task that requires perceiving and
understanding the surrounding environment for safe trajectory planning. While
existing vision-based end-to-end models have achieved promising results, these
methods are still facing the challenges of vision understanding, decision
reasoning and scene generalization. To solve these issues, a generative
planning with 3D-vision language pre-training model named GPVL is proposed for
end-to-end autonomous driving. The proposed paradigm has two significant
aspects. On one hand, a 3D-vision language pre-training module is designed to
bridge the gap between visual perception and linguistic understanding in the
bird's eye view. On the other hand, a cross-modal language model is introduced
to generate holistic driving decisions and fine-grained trajectories with
perception and navigation information in an auto-regressive manner. Experiments
on the challenging nuScenes dataset demonstrate that the proposed scheme
achieves excellent performances compared with state-of-the-art methods.
Besides, the proposed GPVL presents strong generalization ability and real-time
potential when handling high-level commands in various scenarios. It is
believed that the effective, robust and efficient performance of GPVL is
crucial for the practical application of future autonomous driving systems.
Code is available at https://github.com/ltp1995/GPVL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Task-Level Optimal <span class="highlight-title">Prompt</span>s for Visual In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhu, Huan Ma, Changqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of Vision Foundation Models (VFMs) in recent years,
Visual In-Context Learning (VICL) has become a better choice compared to
modifying models in most scenarios. Different from retraining or fine-tuning
model, VICL does not require modifications to the model's weights or
architecture, and only needs a prompt with demonstrations to teach VFM how to
solve tasks. Currently, significant computational cost for finding optimal
prompts for every test sample hinders the deployment of VICL, as determining
which demonstrations to use for constructing prompts is very costly. In this
paper, however, we find a counterintuitive phenomenon that most test samples
actually achieve optimal performance under the same prompts, and searching for
sample-level prompts only costs more time but results in completely identical
prompts. Therefore, we propose task-level prompting to reduce the cost of
searching for prompts during the inference stage and introduce two time-saving
yet effective task-level prompt search strategies. Extensive experimental
results show that our proposed method can identify near-optimal prompts and
reach the best VICL performance with a minimal cost that prior work has never
achieved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term
  Dense Anticipation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Zatsarynna, Emad Bahrami, Yazan Abu Farha, Gianpiero Francesca, Juergen Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our work addresses the problem of stochastic long-term dense anticipation.
The goal of this task is to predict actions and their durations several minutes
into the future based on provided video observations. Anticipation over
extended horizons introduces high uncertainty, as a single observation can lead
to multiple plausible future outcomes. To address this uncertainty, stochastic
models are designed to predict several potential future action sequences.
Recent work has further proposed to incorporate uncertainty modelling for
observed frames by simultaneously predicting per-frame past and future actions
in a unified manner. While such joint modelling of actions is beneficial, it
requires long-range temporal capabilities to connect events across distant past
and future time points. However, the previous work struggles to achieve such a
long-range understanding due to its limited and/or sparse receptive field. To
alleviate this issue, we propose a novel MANTA (MAmba for ANTicipation)
network. Our model enables effective long-term temporal modelling even for very
long sequences while maintaining linear complexity in sequence length. We
demonstrate that our approach achieves state-of-the-art results on three
datasets - Breakfast, 50Salads, and Assembly101 - while also significantly
improving computational and memory efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMDocIR: Benchmarking Multi-Modal <span class="highlight-title">Retrie</span>val for Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal document retrieval is designed to identify and retrieve various
forms of multi-modal content, such as figures, tables, charts, and layout
information from extensive documents. Despite its significance, there is a
notable lack of a robust benchmark to effectively evaluate the performance of
systems in multi-modal document retrieval. To address this gap, this work
introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks:
page-level and layout-level retrieval. The former focuses on localizing the
most relevant pages within a long document, while the latter targets the
detection of specific layouts, offering a more fine-grained granularity than
whole-page analysis. A layout can refer to a variety of elements such as
textual paragraphs, equations, figures, tables, or charts. The MMDocIR
benchmark comprises a rich dataset featuring expertly annotated labels for
1,685 questions and bootstrapped labels for 173,843 questions, making it a
pivotal resource for advancing multi-modal document retrieval for both training
and evaluation. Through rigorous experiments, we reveal that (i) visual
retrievers significantly outperform their text counterparts, (ii) MMDocIR train
set can effectively benefit the training process of multi-modal document
retrieval and (iii) text retrievers leveraging on VLM-text perform much better
than those using OCR-text. These findings underscores the potential advantages
of integrating visual elements for multi-modal document retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/MMDocIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Diffusion Guidance via Learning Degradation-Aware Models for
  Blind Super Resolution <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao-Hao Lu, Ren Wang, Ching-Chun Huang, Wei-Chen Chiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion-based blind super-resolution (SR) methods have shown
great ability to generate high-resolution images with abundant high-frequency
detail, but the detail is often achieved at the expense of fidelity. Meanwhile,
another line of research focusing on rectifying the reverse process of
diffusion models (i.e., diffusion guidance), has demonstrated the power to
generate high-fidelity results for non-blind SR. However, these methods rely on
known degradation kernels, making them difficult to apply to blind SR. To
address these issues, we introduce degradation-aware models that can be
integrated into the diffusion guidance framework, eliminating the need to know
degradation kernels. Additionally, we propose two novel techniques input
perturbation and guidance scalar to further improve our performance. Extensive
experimental results show that our proposed method has superior performance
over state-of-the-art methods on blind SR benchmarks
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in WACV 2025. Code is available at:
  https://github.com/ryanlu2240/Boosting-Diffusion-Guidance-via-Learning-Degradation-Aware-Models-for-Blind-Super-Resolution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDEA: Image Description Enhanced CLIP-Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Ye, Feng Jiang, Qiufeng Wang, Kaizhu Huang, Jiaqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP (Contrastive Language-Image Pre-training) has attained great success in
pattern recognition and computer vision. Transferring CLIP to downstream tasks
(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.
However, current studies primarily focus on either prompt learning for text or
adapter tuning for vision, without fully exploiting the complementary
information and correlations among image-text pairs. In this paper, we propose
an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to
few-shot image classification tasks. This method captures fine-grained features
by leveraging both visual features and textual descriptions of images. IDEA is
a training-free method for CLIP, and it can be comparable to or even exceeds
state-of-the-art models on multiple tasks. Furthermore, we introduce
Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable
components (i.e., a projector and a learnable latent space), further enhancing
the model's performance and achieving SOTA results on 11 datasets. As one
important contribution, we employ the Llama model and design a comprehensive
pipeline to generate textual descriptions for images of 11 datasets, resulting
in a total of 1,637,795 image-text pairs, named "IMD-11". Our code and data are
released at https://github.com/FourierAI/IDEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Pose-Constrained UV Map Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matej Suchanek, Miroslav Purkrabek, Jiri Matas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  UV map estimation is used in computer vision for detailed analysis of human
posture or activity. Previous methods assign pixels to body model vertices by
comparing pixel descriptors independently, without enforcing global coherence
or plausibility in the UV map. We propose Pose-Constrained Continuous Surface
Embeddings (PC-CSE), which integrates estimated 2D human pose into the
pixel-to-vertex assignment process. The pose provides global anatomical
constraints, ensuring that UV maps remain coherent while preserving local
precision. Evaluation on DensePose COCO demonstrates consistent improvement,
regardless of the chosen 2D human pose model. Whole-body poses offer better
constraints by incorporating additional details about the hands and feet.
Conditioning UV maps with human pose reduces invalid mappings and enhances
anatomical plausibility. In addition, we highlight inconsistencies in the
ground-truth annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-visual modality micro drone-based structural damage detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Osei Agyemanga, Liaoyuan Zeng, Jianwen Chena, Isaac Adjei-Mensah, Daniel Acheampong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate detection and resilience of object detectors in structural damage
detection are important in ensuring the continuous use of civil infrastructure.
However, achieving robustness in object detectors remains a persistent
challenge, impacting their ability to generalize effectively. This study
proposes DetectorX, a robust framework for structural damage detection coupled
with a micro drone. DetectorX addresses the challenges of object detector
robustness by incorporating two innovative modules: a stem block and a spiral
pooling technique. The stem block introduces a dynamic visual modality by
leveraging the outputs of two Deep Convolutional Neural Network (DCNN) models.
The framework employs the proposed event-based reward reinforcement learning to
constrain the actions of a parent and child DCNN model leading to a reward.
This results in the induction of two dynamic visual modalities alongside the
Red, Green, and Blue (RGB) data. This enhancement significantly augments
DetectorX's perception and adaptability in diverse environmental situations.
Further, a spiral pooling technique, an online image augmentation method,
strengthens the framework by increasing feature representations by
concatenating spiraled and average/max pooled features. In three extensive
experiments: (1) comparative and (2) robustness, which use the Pacific
Earthquake Engineering Research Hub ImageNet dataset, and (3) field-experiment,
DetectorX performed satisfactorily across varying metrics, including precision
(0.88), recall (0.84), average precision (0.91), mean average precision (0.76),
and mean average recall (0.73), compared to the competing detectors including
You Only Look Once X-medium (YOLOX-m) and others. The study's findings indicate
that DetectorX can provide satisfactory results and demonstrate resilience in
challenging environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring ChatGPT for Face Presentation Attack Detection in Zero and
  Few-Shot in-Context Learning <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alain Komaty, Hatef Otroshi Shahreza, Anjith George, Sebastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study highlights the potential of ChatGPT (specifically GPT-4o) as a
competitive alternative for Face Presentation Attack Detection (PAD),
outperforming several PAD models, including commercial solutions, in specific
scenarios. Our results show that GPT-4o demonstrates high consistency,
particularly in few-shot in-context learning, where its performance improves as
more examples are provided (reference data). We also observe that detailed
prompts enable the model to provide scores reliably, a behavior not observed
with concise prompts. Additionally, explanation-seeking prompts slightly
enhance the model's performance by improving its interpretability. Remarkably,
the model exhibits emergent reasoning capabilities, correctly predicting the
attack type (print or replay) with high accuracy in few-shot scenarios, despite
not being explicitly instructed to classify attack types. Despite these
strengths, GPT-4o faces challenges in zero-shot tasks, where its performance is
limited compared to specialized PAD systems. Experiments were conducted on a
subset of the SOTERIA dataset, ensuring compliance with data privacy
regulations by using only data from consenting individuals. These findings
underscore GPT-4o's promise in PAD applications, laying the groundwork for
future research to address broader data privacy concerns and improve
cross-dataset generalization. Code available here:
https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WACV workshop 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Admitting Ignorance Helps the Video Question Answering Models to Answer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Li, Tom Drummond, Mingming Gong, Mohammed Bennamoun, Qiuhong Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant progress has been made in the field of video question answering
(VideoQA) thanks to deep learning and large-scale pretraining. Despite the
presence of sophisticated model structures and powerful video-text foundation
models, most existing methods focus solely on maximizing the correlation
between answers and video-question pairs during training. We argue that these
models often establish shortcuts, resulting in spurious correlations between
questions and answers, especially when the alignment between video and text
data is suboptimal. To address these spurious correlations, we propose a novel
training framework in which the model is compelled to acknowledge its ignorance
when presented with an intervened question, rather than making guesses solely
based on superficial question-answer correlations. We introduce methodologies
for intervening in questions, utilizing techniques such as displacement and
perturbation, and design frameworks for the model to admit its lack of
knowledge in both multi-choice VideoQA and open-ended settings. In practice, we
integrate a state-of-the-art model into our framework to validate its
effectiveness. The results clearly demonstrate that our framework can
significantly enhance the performance of VideoQA models with minimal structural
modifications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Learner Generalizes Across AI-Generated Image Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Wu, Jing Liu, Jing Li, Yequan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current fake image detectors trained on large synthetic image datasets
perform satisfactorily on limited studied generative models. However, they
suffer a notable performance decline over unseen models. Besides, collecting
adequate training data from online generative models is often expensive or
infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a
novel AI-generated image detector which learns a specialized metric space to
effectively distinguish unseen fake images by utilizing very few samples.
Experiments show FSD achieves state-of-the-art performance by $+7.4\%$ average
ACC on GenImage dataset. More importantly, our method is better capable of
capturing the intra-category common features in unseen images without further
training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding
  and Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianru Zhang, Li Ju, Prashant Singh, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing large-scale datasets, especially involving complex and
high-dimensional data like images, is particularly challenging. While
self-supervised learning (SSL) has proven effective for learning
representations from unlabelled data, it typically focuses on flat,
non-hierarchical structures, missing the multi-level relationships present in
many real-world datasets. Hierarchical clustering (HC) can uncover these
relationships by organizing data into a tree-like structure, but it often
relies on rigid similarity metrics that struggle to capture the complexity of
diverse data types. To address these we envision $\texttt{InfoHier}$, a
framework that combines SSL with HC to jointly learn robust latent
representations and hierarchical structures. This approach leverages SSL to
provide adaptive representations, enhancing HC's ability to capture complex
patterns. Simultaneously, it integrates HC loss to refine SSL training,
resulting in representations that are more attuned to the underlying
information hierarchy. $\texttt{InfoHier}$ has the potential to improve the
expressiveness and performance of both clustering and representation learning,
offering significant benefits for data analysis, management, and information
retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Transformation Learning for Equivariant Representations <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemyung Yu, Jaehyun Choi, Dong-Jae Lee, HyeongGwon Hong, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised representation learning has significantly advanced various
machine learning tasks. In the computer vision domain, state-of-the-art
approaches utilize transformations like random crop and color jitter to achieve
invariant representations, embedding semantically the same inputs despite
transformations. However, this can degrade performance in tasks requiring
precise features, such as localization or flower classification. To address
this, recent research incorporates equivariant representation learning, which
captures transformation-sensitive information. However, current methods depend
on transformation labels and thus struggle with interdependency and complex
transformations. We propose Self-supervised Transformation Learning (STL),
replacing transformation labels with transformation representations derived
from image pairs. The proposed method ensures transformation representation is
image-invariant and learns corresponding equivariant transformations, enhancing
performance without increased batch complexity. We demonstrate the approach's
effectiveness across diverse classification and detection tasks, outperforming
existing methods in 7 out of 11 benchmarks and excelling in detection. By
integrating complex transformations like AugMix, unusable by prior equivariant
methods, this approach enhances performance across tasks, underscoring its
adaptability and resilience. Additionally, its compatibility with various base
models highlights its flexibility and broad applicability. The code is
available at https://github.com/jaemyung-u/stl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RealVVT: Towards Photorealistic Video Virtual Try-on via Spatio-Temporal
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Li, Zhengkai Jiang, Jiawei Zhou, Zhihong Liu, Xiaowei Chi, Haoqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual try-on has emerged as a pivotal task at the intersection of computer
vision and fashion, aimed at digitally simulating how clothing items fit on the
human body. Despite notable progress in single-image virtual try-on (VTO),
current methodologies often struggle to preserve a consistent and authentic
appearance of clothing across extended video sequences. This challenge arises
from the complexities of capturing dynamic human pose and maintaining target
clothing characteristics. We leverage pre-existing video foundation models to
introduce RealVVT, a photoRealistic Video Virtual Try-on framework tailored to
bolster stability and realism within dynamic video contexts. Our methodology
encompasses a Clothing & Temporal Consistency strategy, an Agnostic-guided
Attention Focus Loss mechanism to ensure spatial consistency, and a Pose-guided
Long Video VTO technique adept at handling extended video sequences.Extensive
experiments across various datasets confirms that our approach outperforms
existing state-of-the-art models in both single-image and video VTO tasks,
offering a viable solution for practical applications within the realms of
fashion e-commerce and virtual fitting environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages (8 pages main text, 2 pages references), 5 figures in the
  main text, and 4 pages supplementary materials with 3 additional figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexiClip: Locality-Preserving Free-Form Character Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anant Khandelwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animating clipart images with seamless motion while maintaining visual
fidelity and temporal coherence presents significant challenges. Existing
methods, such as AniClipart, effectively model spatial deformations but often
fail to ensure smooth temporal transitions, resulting in artifacts like abrupt
motions and geometric distortions. Similarly, text-to-video (T2V) and
image-to-video (I2V) models struggle to handle clipart due to the mismatch in
statistical properties between natural video and clipart styles. This paper
introduces FlexiClip, a novel approach designed to overcome these limitations
by addressing the intertwined challenges of temporal consistency and geometric
integrity. FlexiClip extends traditional B\'ezier curve-based trajectory
modeling with key innovations: temporal Jacobians to correct motion dynamics
incrementally, continuous-time modeling via probability flow ODEs (pfODEs) to
mitigate temporal noise, and a flow matching loss inspired by GFlowNet
principles to optimize smooth motion transitions. These enhancements ensure
coherent animations across complex scenarios involving rapid movements and
non-rigid deformations. Extensive experiments validate the effectiveness of
FlexiClip in generating animations that are not only smooth and natural but
also structurally consistent across diverse clipart types, including humans and
animals. By integrating spatial and temporal modeling with pre-trained video
diffusion models, FlexiClip sets a new standard for high-quality clipart
animation, offering robust performance across a wide range of visual content.
Project Page: https://creative-gen.github.io/flexiclip.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused
  Odometry with Gaussian Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Hong, Chunran Zheng, Yishu Shen, Changze Li, Fu Zhang, Tong Qin, Shaojie Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene
representation approach. However, existing vision-only 3D-GS methods often rely
on hand-crafted heuristics for point-cloud densification and face challenges in
handling occlusions and high GPU memory and computation consumption.
LiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior
performance in localization and dense mapping by leveraging complementary
sensing characteristics: rich texture information from cameras, precise
geometric measurements from LiDAR, and high-frequency motion data from IMU.
Inspired by this, we propose a novel real-time Gaussian-based simultaneous
localization and mapping (SLAM) system. Our map system comprises a global
Gaussian map and a sliding window of Gaussians, along with an IESKF-based
odometry. The global Gaussian map consists of hash-indexed voxels organized in
a recursive octree, effectively covering sparse spatial volumes while adapting
to different levels of detail and scales. The Gaussian map is initialized
through multi-sensor fusion and optimized with photometric gradients. Our
system incrementally maintains a sliding window of Gaussians, significantly
reducing GPU computation and memory consumption by only optimizing the map
within the sliding window. Moreover, we implement a tightly coupled
multi-sensor fusion odometry with an iterative error state Kalman filter
(IESKF), leveraging real-time updating and rendering of the Gaussian map. Our
system represents the first real-time Gaussian-based SLAM framework deployable
on resource-constrained embedded systems, demonstrated on the NVIDIA Jetson
Orin NX platform. The framework achieves real-time performance while
maintaining robust multi-sensor fusion capabilities. All implementation
algorithms, hardware designs, and CAD models will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeFlow: Longitudinal Brain Image Registration and Aging Progression
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bailiang Jian, Jiazhen Pan, Yitong Li, Fabian Bongratz, Ruochen Li, Daniel Rueckert, Benedikt Wiestler, Christian Wachinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting future brain states is crucial for understanding healthy aging and
neurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone
for such analyses, has long been limited by its inability to forecast future
developments, reliance on extensive, dense longitudinal data, and the need to
balance registration accuracy with temporal smoothness. In this work, we
present \emph{TimeFlow}, a novel framework for longitudinal brain MRI
registration that overcomes all these challenges. Leveraging a U-Net
architecture with temporal conditioning inspired by diffusion models, TimeFlow
enables accurate longitudinal registration and facilitates prospective analyses
through future image prediction. Unlike traditional methods that depend on
explicit smoothness regularizers and dense sequential data, TimeFlow achieves
temporal consistency and continuity without these constraints. Experimental
results highlight its superior performance in both future timepoint prediction
and registration accuracy compared to state-of-the-art methods. Additionally,
TimeFlow supports novel biological brain aging analyses, effectively
differentiating neurodegenerative conditions from healthy aging. It eliminates
the need for segmentation, thereby avoiding the challenges of non-trivial
annotation and inconsistent segmentation errors. TimeFlow paves the way for
accurate, data-efficient, and annotation-free prospective analyses of brain
aging and chronic diseases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Facial Image Privacy Preservation in Cloud-Based Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Mengyuan Sun, Xueluan Gong, Yanjiao Chen, Qian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial recognition models are increasingly employed by commercial
enterprises, government agencies, and cloud service providers for identity
verification, consumer services, and surveillance. These models are often
trained using vast amounts of facial data processed and stored in cloud-based
platforms, raising significant privacy concerns. Users' facial images may be
exploited without their consent, leading to potential data breaches and misuse.
This survey presents a comprehensive review of current methods aimed at
preserving facial image privacy in cloud-based services. We categorize these
methods into two primary approaches: image obfuscation-based protection and
adversarial perturbation-based protection. We provide an in-depth analysis of
both categories, offering qualitative and quantitative comparisons of their
effectiveness. Additionally, we highlight unresolved challenges and propose
future research directions to improve privacy preservation in cloud computing
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Product of Gaussian Mixture Diffusion Model for non-linear MRI Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurenz Nagler, Martin Zach, Thomas Pock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently shown remarkable results in magnetic resonance
imaging reconstruction. However, the employed networks typically are black-box
estimators of the (smoothed) prior score with tens of millions of parameters,
restricting interpretability and increasing reconstruction time. Furthermore,
parallel imaging reconstruction algorithms either rely on off-line coil
sensitivity estimation, which is prone to misalignment and restricting sampling
trajectories, or perform per-coil reconstruction, making the computational cost
proportional to the number of coils. To overcome this, we jointly reconstruct
the image and the coil sensitivities using the lightweight,
parameter-efficient, and interpretable product of Gaussian mixture diffusion
model as an image prior and a classical smoothness priors on the coil
sensitivities. The proposed method delivers promising results while allowing
for fast inference and demonstrating robustness to contrast out-of-distribution
data and sampling trajectories, comparable to classical variational penalties
such as total variation. Finally, the probabilistic formulation allows the
calculation of the posterior expectation and pixel-wise variance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with
  Multi-modality Refinement Module 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongzhihan Wang, Yang Yang, Liang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry (VO) plays a crucial role in autonomous driving, robotic
navigation, and other related tasks by estimating the position and orientation
of a camera based on visual input. Significant progress has been made in
data-driven VO methods, particularly those leveraging deep learning techniques
to extract image features and estimate camera poses. However, these methods
often struggle in low-light conditions because of the reduced visibility of
features and the increased difficulty of matching keypoints. To address this
limitation, we introduce BrightVO, a novel VO model based on Transformer
architecture, which not only performs front-end visual feature extraction, but
also incorporates a multi-modality refinement module in the back-end that
integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,
this module iteratively refines pose estimates to reduce errors and improve
both accuracy and robustness. Furthermore, we create a synthetic low-light
dataset, KiC4R, which includes a variety of lighting conditions to facilitate
the training and evaluation of VO frameworks in challenging environments.
Experimental results demonstrate that BrightVO achieves state-of-the-art
performance on both the KiC4R dataset and the KITTI benchmarks. Specifically,
it provides an average improvement of 20% in pose estimation accuracy in normal
outdoor environments and 259% in low-light conditions, outperforming existing
methods. For widespread use and further development, the research work is fully
open-source at https://github.com/Anastasiawd/BrightVO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StereoGen: High-quality Stereo Image Generation from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianqi Wang, Hao Yang, Gangwei Xu, Junda Cheng, Min Lin, Yong Deng, Jinliang Zang, Yurui Chen, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art supervised stereo matching methods have achieved amazing
results on various benchmarks. However, these data-driven methods suffer from
generalization to real-world scenarios due to the lack of real-world annotated
data. In this paper, we propose StereoGen, a novel pipeline for high-quality
stereo image generation. This pipeline utilizes arbitrary single images as left
images and pseudo disparities generated by a monocular depth estimation model
to synthesize high-quality corresponding right images. Unlike previous methods
that fill the occluded area in warped right images using random backgrounds or
using convolutions to take nearby pixels selectively, we fine-tune a diffusion
inpainting model to recover the background. Images generated by our model
possess better details and undamaged semantic structures. Besides, we propose
Training-free Confidence Generation and Adaptive Disparity Selection. The
former suppresses the negative effect of harmful pseudo ground truth during
stereo training, while the latter helps generate a wider disparity distribution
and better synthetic images. Experiments show that models trained under our
pipeline achieve state-of-the-art zero-shot generalization results among all
published methods. The code will be available upon publication of the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Learning of Depth and Appearance for Portrait Image Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinya Ji, Gaspard Zoss, Prashanth Chandran, Lingchen Yang, Xun Cao, Barbara Solenthaler, Derek Bradley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D portrait animation has experienced significant advancements in recent
years. Much research has utilized the prior knowledge embedded in large
generative diffusion models to enhance high-quality image manipulation.
However, most methods only focus on generating RGB images as output, and the
co-generation of consistent visual plus 3D output remains largely
under-explored. In our work, we propose to jointly learn the visual appearance
and depth simultaneously in a diffusion-based portrait image generator. Our
method embraces the end-to-end diffusion paradigm and introduces a new
architecture suitable for learning this conditional joint distribution,
consisting of a reference network and a channel-expanded diffusion backbone.
Once trained, our framework can be efficiently adapted to various downstream
applications, such as facial depth-to-image and image-to-depth generation,
portrait relighting, and audio-driven talking head animation with consistent 3D
output.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonSter: Marry Monodepth to Stereo Unleashes Power 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Cheng, Longliang Liu, Gangwei Xu, Xianqi Wang, Zhaoxing Zhang, Yong Deng, Jinliang Zang, Yurui Chen, Zhipeng Cai, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching recovers depth from image correspondences. Existing methods
struggle to handle ill-posed regions with limited matching cues, such as
occlusions and textureless areas. To address this, we propose MonSter, a novel
method that leverages the complementary strengths of monocular depth estimation
and stereo matching. MonSter integrates monocular depth and stereo matching
into a dual-branch architecture to iteratively improve each other.
Confidence-based guidance adaptively selects reliable stereo cues for monodepth
scale-shift recovery. The refined monodepth is in turn guides stereo
effectively at ill-posed regions. Such iterative mutual enhancement enables
MonSter to evolve monodepth priors from coarse object-level structures to
pixel-level geometry, fully unlocking the potential of stereo matching. As
shown in Fig.1, MonSter ranks 1st across five most commonly used leaderboards
-- SceneFlow, KITTI 2012, KITTI 2015, Middlebury, and ETH3D. Achieving up to
49.5% improvements (Bad 1.0 on ETH3D) over the previous best method.
Comprehensive analysis verifies the effectiveness of MonSter in ill-posed
regions. In terms of zero-shot generalization, MonSter significantly and
consistently outperforms state-of-the-art across the board. The code is
publicly available at: https://github.com/Junda24/MonSter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Wildfire Flame and Smoke through Edge Computing using Transfer
  Learning Enhanced Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanny Vazquez, Shengjie Zhai, Mei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous unmanned aerial vehicles (UAVs) integrated with edge computing
capabilities empower real-time data processing directly on the device,
dramatically reducing latency in critical scenarios such as wildfire detection.
This study underscores Transfer Learning's (TL) significance in boosting the
performance of object detectors for identifying wildfire smoke and flames,
especially when trained on limited datasets, and investigates the impact TL has
on edge computing metrics. With the latter focusing how TL-enhanced You Only
Look Once (YOLO) models perform in terms of inference time, power usage, and
energy consumption when using edge computing devices. This study utilizes the
Aerial Fire and Smoke Essential (AFSE) dataset as the target, with the Flame
and Smoke Detection Dataset (FASDD) and the Microsoft Common Objects in Context
(COCO) dataset serving as source datasets. We explore a two-stage cascaded TL
method, utilizing D-Fire or FASDD as initial stage target datasets and AFSE as
the subsequent stage. Through fine-tuning, TL significantly enhances detection
precision, achieving up to 79.2% mean Average Precision (mAP@0.5), reduces
training time, and increases model generalizability across the AFSE dataset.
However, cascaded TL yielded no notable improvements and TL alone did not
benefit the edge computing metrics evaluated. Lastly, this work found that
YOLOv5n remains a powerful model when lacking hardware acceleration, finding
that YOLOv5n can process images nearly twice as fast as its newer counterpart,
YOLO11n. Overall, the results affirm TL's role in augmenting the accuracy of
object detectors while also illustrating that additional enhancements are
needed to improve edge computing performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Organizing Edge Computing Distribution Framework for Visual SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jussi Kalliola, Lauri Suomela, Sergio Moreschini, David Hästbacka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localization within a known environment is a crucial capability for mobile
robots. Simultaneous Localization and Mapping (SLAM) is a prominent solution to
this problem. SLAM is a framework that consists of a diverse set of
computational tasks ranging from real-time tracking to computation-intensive
map optimization. This combination can present a challenge for resource-limited
mobile robots. Previously, edge-assisted SLAM methods have demonstrated
promising real-time execution capabilities by offloading heavy computations
while performing real-time tracking onboard. However, the common approach of
utilizing a client-server architecture for offloading is sensitive to server
and network failures. In this article, we propose a novel edge-assisted SLAM
framework capable of self-organizing fully distributed SLAM execution across a
network of devices or functioning on a single device without connectivity. The
architecture consists of three layers and is designed to be device-agnostic,
resilient to network failures, and minimally invasive to the core SLAM system.
We have implemented and demonstrated the framework for monocular ORB SLAM3 and
evaluated it in both fully distributed and standalone SLAM configurations
against the ORB SLAM3. The experiment results demonstrate that the proposed
design matches the accuracy and resource utilization of the monolithic approach
while enabling collaborative execution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computerized Assessment of Motor Imitation for Distinguishing Autism in
  Video (CAMI-2DNet) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaleab A. Kinfu, Carolina Pacheco, Alice D. Sperry, Deana Crocetti, Bahar Tunçgenç, Stewart H. Mostofsky, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motor imitation impairments are commonly reported in individuals with autism
spectrum conditions (ASCs), suggesting that motor imitation could be used as a
phenotype for addressing autism heterogeneity. Traditional methods for
assessing motor imitation are subjective, labor-intensive, and require
extensive human training. Modern Computerized Assessment of Motor Imitation
(CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video
data, are less subjective. However, they rely on labor-intensive data
normalization and cleaning techniques, and human annotations for algorithm
training. To address these challenges, we propose CAMI-2DNet, a scalable and
interpretable deep learning-based approach to motor imitation assessment in
video data, which eliminates the need for data normalization, cleaning and
annotation. CAMI-2DNet uses an encoder-decoder architecture to map a video to a
motion encoding that is disentangled from nuisance factors such as body shape
and camera views. To learn a disentangled representation, we employ synthetic
data generated by motion retargeting of virtual characters through the
reshuffling of motion, body shape, and camera views, as well as real
participant data. To automatically assess how well an individual imitates an
actor, we compute a similarity score between their motion encodings, and use it
to discriminate individuals with ASCs from neurotypical (NT) individuals. Our
comparative analysis demonstrates that CAMI-2DNet has a strong correlation with
human scores while outperforming CAMI-2D in discriminating ASC vs NT children.
Moreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater
practicality by operating directly on video data and without the need for
ad-hoc data normalization and human annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PACF: Prototype Augmented Compact Features for Improving Domain Adaptive
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Liu, Yongchao Feng, Yanan Zhang, Qingjie Liu, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been significant advancement in object detection.
However, applying off-the-shelf detectors to a new domain leads to significant
performance drop, caused by the domain gap. These detectors exhibit
higher-variance class-conditional distributions in the target domain than that
in the source domain, along with mean shift. To address this problem, we
propose the Prototype Augmented Compact Features (PACF) framework to regularize
the distribution of intra-class features. Specifically, we provide an in-depth
theoretical analysis on the lower bound of the target features-related
likelihood and derive the prototype cross entropy loss to further calibrate the
distribution of target RoI features. Furthermore, a mutual regularization
strategy is designed to enable the linear and prototype-based classifiers to
learn from each other, promoting feature compactness while enhancing
discriminability. Thanks to this PACF framework, we have obtained a more
compact cross-domain feature space, within which the variance of the target
features' class-conditional distributions has significantly decreased, and the
class-mean shift between the two domains has also been further reduced. The
results on different adaptation settings are state-of-the-art, which
demonstrate the board applicability and effectiveness of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Watermarking in Diffusion Model: Gaussian Shading with Exact Diffusion
  Inversion via Coupled Transformations (EDICT) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Panthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel approach to enhance the performance of Gaussian
Shading, a prevalent watermarking technique, by integrating the Exact Diffusion
Inversion via Coupled Transformations (EDICT) framework. While Gaussian Shading
traditionally embeds watermarks in a noise latent space, followed by iterative
denoising for image generation and noise addition for watermark recovery, its
inversion process is not exact, leading to potential watermark distortion. We
propose to leverage EDICT's ability to derive exact inverse mappings to refine
this process. Our method involves duplicating the watermark-infused noisy
latent and employing a reciprocal, alternating denoising and noising scheme
between the two latents, facilitated by EDICT. This allows for a more precise
reconstruction of both the image and the embedded watermark. Empirical
evaluation on standard datasets demonstrates that our integrated approach
yields a slight, yet statistically significant improvement in watermark
recovery fidelity. These results highlight the potential of EDICT to enhance
existing diffusion-based watermarking techniques by providing a more accurate
and robust inversion mechanism. To the best of our knowledge, this is the first
work to explore the synergy between EDICT and Gaussian Shading for digital
watermarking, opening new avenues for research in robust and high-fidelity
watermark embedding and extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-to-Force Estimation for Soft Tissue Interaction in
  Robotic-Assisted Surgery Using Structured Light 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayin Wang, Mingfeng Yao, Yanran Wei, Xiaoyu Guo, Ayong Zheng, Weidong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For Minimally Invasive Surgical (MIS) robots, accurate haptic interaction
force feedback is essential for ensuring the safety of interacting with soft
tissue. However, most existing MIS robotic systems cannot facilitate direct
measurement of the interaction force with hardware sensors due to space
limitations. This letter introduces an effective vision-based scheme that
utilizes a One-Shot structured light projection with a designed pattern on soft
tissue coupled with haptic information processing through a trained
image-to-force neural network. The images captured from the endoscopic stereo
camera are analyzed to reconstruct high-resolution 3D point clouds for soft
tissue deformation. Based on this, a modified PointNet-based force estimation
method is proposed, which excels in representing the complex mechanical
properties of soft tissue. Numerical force interaction experiments are
conducted on three silicon materials with different stiffness. The results
validate the effectiveness of the proposed scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Densely Connected Parameter-Efficient Tuning for Referring Image
  Segmentation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Huang, Zunnan Xu, Ting Liu, Yong Liu, Haonan Han, Kehong Yuan, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of computer vision, Parameter-Efficient Tuning (PET) is
increasingly replacing the traditional paradigm of pre-training followed by
full fine-tuning. PET is particularly favored for its effectiveness in large
foundation models, as it streamlines transfer learning costs and optimizes
hardware utilization. However, the current PET methods are mainly designed for
single-modal optimization. While some pioneering studies have undertaken
preliminary explorations, they still remain at the level of aligned encoders
(e.g., CLIP) and lack exploration of misaligned encoders. These methods show
sub-optimal performance with misaligned encoders, as they fail to effectively
align the multimodal features during fine-tuning. In this paper, we introduce
DETRIS, a parameter-efficient tuning framework designed to enhance low-rank
visual feature propagation by establishing dense interconnections between each
layer and all preceding layers, which enables effective cross-modal feature
interaction and adaptation to misaligned encoders. We also suggest using text
adapters to improve textual features. Our simple yet efficient approach greatly
surpasses state-of-the-art methods with 0.9% to 1.8% backbone parameter
updates, evaluated on challenging benchmarks. Our project is available at
\url{https://github.com/jiaqihuang01/DETRIS}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and High-Quality Neural Implicit Representation for 3D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyuan Yang, Bailin Deng, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various SDF-based neural implicit surface reconstruction methods have been
proposed recently, and have demonstrated remarkable modeling capabilities.
However, due to the global nature and limited representation ability of a
single network, existing methods still suffer from many drawbacks, such as
limited accuracy and scale of the reconstruction. In this paper, we propose a
versatile, scalable and high-quality neural implicit representation to address
these issues. We integrate a divide-and-conquer approach into the neural
SDF-based reconstruction. Specifically, we model the object or scene as a
fusion of multiple independent local neural SDFs with overlapping regions. The
construction of our representation involves three key steps: (1) constructing
the distribution and overlap relationship of the local radiance fields based on
object structure or data distribution, (2) relative pose registration for
adjacent local SDFs, and (3) SDF blending. Thanks to the independent
representation of each local region, our approach can not only achieve
high-fidelity surface reconstruction, but also enable scalable scene
reconstruction. Extensive experimental results demonstrate the effectiveness
and practicality of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOTLoc: General Outdoor Text-based Localization Using Scene <span class="highlight-title">Graph</span>
  <span class="highlight-title">Retrie</span>val with OpenStreetMap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghwi Jung, Keonwoo Kim, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose GOTLoc, a robust localization method capable of operating even in
outdoor environments where GPS signals are unavailable. The method achieves
this robust localization by leveraging comparisons between scene graphs
generated from text descriptions and maps. Existing text-based localization
studies typically represent maps as point clouds and identify the most similar
scenes by comparing embeddings of text and point cloud data. However, point
cloud maps have limited scalability as it is impractical to pre-generate maps
for all outdoor spaces. Furthermore, their large data size makes it challenging
to store and utilize them directly on actual robots. To address these issues,
GOTLoc leverages compact data structures, such as scene graphs, to store
spatial information, enabling individual robots to carry and utilize large
amounts of map data. Additionally, by utilizing publicly available map data,
such as OpenStreetMap, which provides global information on outdoor spaces, we
eliminate the need for additional effort to create custom map data. For
performance evaluation, we utilized the KITTI360Pose dataset in conjunction
with corresponding OpenStreetMap data to compare the proposed method with
existing approaches. Our results demonstrate that the proposed method achieves
accuracy comparable to algorithms relying on point cloud maps. Moreover, in
city-scale tests, GOTLoc required significantly less storage compared to point
cloud-based methods and completed overall processing within a few seconds,
validating its applicability to real-world robotics. Our code is available at
https://github.com/donghwijung/GOTLoc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIAFEx: An Attention-based Feature Extraction Method for Medical Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Ramos-Soto, Jorge Ramos-Frutos, Ezequiel Perez-Zarate, Diego Oliva, Sandra E. Balderas-Mata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature extraction techniques are crucial in medical image classification;
however, classical feature extractors in addition to traditional machine
learning classifiers often exhibit significant limitations in providing
sufficient discriminative information for complex image sets. While
Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown
promise in feature extraction, they are prone to overfitting due to the
inherent characteristics of medical imaging data, including small sample sizes
or high intra-class variance. In this work, the Medical Image Attention-based
Feature Extractor (MIAFEx) is proposed, a novel method that employs a learnable
refinement mechanism to enhance the classification token within the Transformer
encoder architecture. This mechanism adjusts the token based on learned
weights, improving the extraction of salient features and enhancing the model's
adaptability to the challenges presented by medical imaging data. The MIAFEx
output features quality is compared against classical feature extractors using
traditional and hybrid classifiers. Also, the performance of these features is
compared against modern CNN and ViT models in classification tasks,
demonstrating its superiority in accuracy and robustness across multiple
complex classification medical imaging datasets. This advantage is particularly
pronounced in scenarios with limited training data, where traditional and
modern models often struggle to generalize effectively. The source code of this
proposal can be found at
https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In preparation for Journal Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynamicFace: High-Quality and Consistent Video Face Swapping using
  Composable 3D Facial Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runqi Wang, Sijie Xu, Tianyao He, Yang Chen, Wei Zhu, Dejia Song, Nemo Chen, Xu Tang, Yao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face swapping transfers the identity of a source face to a target face while
retaining the attributes like expression, pose, hair, and background of the
target face. Advanced face swapping methods have achieved attractive results.
However, these methods often inadvertently transfer identity information from
the target face, compromising expression-related details and accurate identity.
We propose a novel method DynamicFace that leverages the power of diffusion
model and plug-and-play temporal layers for video face swapping. First, we
introduce four fine-grained face conditions using 3D facial priors. All
conditions are designed to be disentangled from each other for precise and
unique control. Then, we adopt Face Former and ReferenceNet for high-level and
detailed identity injection. Through experiments on the FF++ dataset, we
demonstrate that our method achieves state-of-the-art results in face swapping,
showcasing superior image quality, identity preservation, and expression
accuracy. Besides, our method could be easily transferred to video domain with
temporal attention layer. Our code and results will be available on the project
page: https://dynamic-face.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Devil is in Temporal Token: High Quality Video Reasoning
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitong Gong, Yunzhi Zhuge, Lu Zhang, Zongxin Yang, Pingping Zhang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for Video Reasoning Segmentation rely heavily on a single
special token to represent the object in the keyframe or the entire video,
inadequately capturing spatial complexity and inter-frame motion. To overcome
these challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation
approach that leverages Multimodal Large Language Models (MLLMs) to inject rich
spatiotemporal features into hierarchical tokens.Our key innovations include a
Temporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS).
Specifically, we design frame-level <SEG> and temporal-level <TAK> tokens that
utilize MLLM's autoregressive learning to effectively capture both local and
global information. Subsequently, we apply a similarity-based weighted fusion
and frame selection strategy, then utilize SAM2 to perform keyframe
segmentation and propagation. To enhance keyframe localization accuracy, the
TKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ
achieves state-of-the-art performance on ReVOS, surpassing VISA by
5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight
the strong temporal reasoning and segmentation capabilities of our method. Code
and model weights will be released at VRS-HQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Subjective and Objective Evaluation Method for
  Text-generated Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelu Qi, Ping Shi, Shuqi Wang, Zhaoyang Zhang, Zefeng Ying, Da Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-video (T2V) technology advancements, as demonstrated by models
such as Gen3, Pika, and Sora, have significantly broadened its applicability
and popularity. This progress has created a growing demand for accurate quality
assessment metrics to evaluate the perceptual quality of text-generated videos
and optimize video generation models. However, assessing the quality of
text-generated videos remains challenging due to the presence of highly complex
distortions, such as unnatural actions and phenomena that defy human cognition.
To address these challenges, we constructed a large-scale benchmark dataset for
\textbf{T}ext-generated \textbf{V}ideo \textbf{eval}uation,
\textbf{T2VEval-Bench}, comprising 148 textual words and 1,783 videos generated
by 12 models. During the subjective evaluation, we collected five key scores:
overall impression, video quality, aesthetic quality, realness, and text-video
consistency. For objective evaluation, we developed the \textbf{T2VEval} model,
which assesses videos across three branches: quality, authenticity, and
consistency. Using an attention-based fusion module, T2VEval effectively
integrates features from each branch and predicts scores with the aid of a
large oracle model. Additionally, we implemented a progressive training
strategy, enabling each branch to learn targeted knowledge while maintaining
synergy with the others. Experimental results demonstrate that T2VEval achieves
state-of-the-art performance across multiple metrics. The dataset and code will
be open-sourced upon completion of the follow-up work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fake News Video Explanation Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhi Chen, Zhong Qian, Peifeng Li, Qiaoming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal explanation involves the assessment of the veracity of a variety
of different content, and relies on multiple information modalities to
comprehensively consider the relevance and consistency between modalities. Most
existing fake news video detection methods focus on improving accuracy while
ignoring the importance of providing explanations. In this paper, we propose a
novel problem - Fake News Video Explanation (FNVE) - Given a multimodal news
containing both video and caption text, we aim to generate natural language
explanations to reveal the truth of predictions. To this end, we develop
FakeNVE, a new dataset of explanations for truthfully multimodal posts, where
each explanation is a natural language (English) sentence describing the
attribution of a news thread. We benchmark FakeNVE by using a multimodal
transformer-based architecture. Subsequently, a BART-based autoregressive
decoder is used as the generator. Empirical results show compelling results for
various baselines (applicable to FNVE) across multiple evaluation metrics. We
also perform human evaluation on explanation generation, achieving high scores
for both adequacy and fluency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Efficacy of Meta-Learning: Unveiling Superior Data
  Diversity Utilization of MAML Over <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavita Selva, Satita Vittayaareekul, Brando Miranda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, data and model size dominate the narrative in the training of
super-large, powerful models. However, there has been a lack of exploration on
the effect of other attributes of the training dataset on model performance. We
hypothesize that dataset diversity can impact the performance of vision models.
Our study shows positive correlations between test set accuracy and data
diversity, providing an argument for furthering the research of dataset
attributes beyond size. We analyzed pre-training and model-agnostic
meta-learning methods on twelve popular visual datasets (e.g., Omniglot,
CIFAR-FS, Aircraft) and five model configurations, including MAML variants with
different numbers of inner gradient steps and supervised learning. We show
moderate to strong positive correlations (R-squared: 0.15-0.42) between
accuracy and data diversity and weaker but significant correlations (R-squared:
~0.2) between loss and diversity. These findings support our hypothesis and
demonstrate a promising way for a deeper exploration of how formal data
diversity influences model performance. This initial study highlights the
potential of (Task2Vec) data diversity as a valuable measure in the rapidly
evolving field of large-scale learning and emphasizes that understanding the
dataset is key to building more powerful and generalizable models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yuan: Yielding Unblemished Aesthetics Through A Unified Network for
  Visual Imperfections Removal in Generated Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Yu, Chee Seng Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI presents transformative potential across various domains, from
creative arts to scientific visualization. However, the utility of AI-generated
imagery is often compromised by visual flaws, including anatomical
inaccuracies, improper object placements, and misplaced textual elements. These
imperfections pose significant challenges for practical applications. To
overcome these limitations, we introduce \textit{Yuan}, a novel framework that
autonomously corrects visual imperfections in text-to-image synthesis.
\textit{Yuan} uniquely conditions on both the textual prompt and the segmented
image, generating precise masks that identify areas in need of refinement
without requiring manual intervention -- a common constraint in previous
methodologies. Following the automated masking process, an advanced inpainting
module seamlessly integrates contextually coherent content into the identified
regions, preserving the integrity and fidelity of the original image and
associated text prompts. Through extensive experimentation on publicly
available datasets such as ImageNet100 and Stanford Dogs, along with a
custom-generated dataset, \textit{Yuan} demonstrated superior performance in
eliminating visual imperfections. Our approach consistently achieved higher
scores in quantitative metrics, including NIQE, BRISQUE, and PI, alongside
favorable qualitative evaluations. These results underscore \textit{Yuan}'s
potential to significantly enhance the quality and applicability of
AI-generated images across diverse fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and
  Unstructured Parameter Prioritization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waqwoya Abebe, Sadegh Jafari, Sixing Yu, Akash Dutta, Jan Strube, Nathan R. Tallent, Luanzheng Guo, Pablo Munoz, Ali Jannesari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) is a powerful approach of automating the
design of efficient neural architectures. In contrast to traditional NAS
methods, recently proposed one-shot NAS methods prove to be more efficient in
performing NAS. One-shot NAS works by generating a singular weight-sharing
supernetwork that acts as a search space (container) of subnetworks. Despite
its achievements, designing the one-shot search space remains a major
challenge. In this work we propose a search space design strategy for Vision
Transformer (ViT)-based architectures. In particular, we convert the Segment
Anything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our
approach involves automating the search space design via layer-wise structured
pruning and parameter prioritization. While the structured pruning applies
probabilistic removal of certain transformer layers, parameter prioritization
performs weight reordering and slicing of MLP-blocks in the remaining layers.
We train supernetworks on several datasets using the sandwich rule. For
deployment, we enhance subnetwork discovery by utilizing a program autotuner to
identify efficient subnetworks within the search space. The resulting
subnetworks are 30-70% smaller in size compared to the original pre-trained SAM
ViT-B, yet outperform the pretrained model. Our work introduces a new and
effective method for ViT NAS search-space design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Few-shot Crack Segmentation and its Precise 3D Automatic
  Measurement in Concrete Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengru Deng, Jiapeng Yao, Chun Li, Su Wang, Xinrun Li, Varun Ojha, Xuhui He, Takashi Matsumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-Spatial Systems has become increasingly essential in concrete crack
inspection. However, existing methods often lacks adaptability to diverse
scenarios, exhibits limited robustness in image-based approaches, and struggles
with curved or complex geometries. To address these limitations, an innovative
framework for two-dimensional (2D) crack detection, three-dimensional (3D)
reconstruction, and 3D automatic crack measurement was proposed by integrating
computer vision technologies and multi-modal Simultaneous localization and
mapping (SLAM) in this study. Firstly, building on a base DeepLabv3+
segmentation model, and incorporating specific refinements utilizing foundation
model Segment Anything Model (SAM), we developed a crack segmentation method
with strong generalization across unfamiliar scenarios, enabling the generation
of precise 2D crack masks. To enhance the accuracy and robustness of 3D
reconstruction, Light Detection and Ranging (LiDAR) point clouds were utilized
together with image data and segmentation masks. By leveraging both image- and
LiDAR-SLAM, we developed a multi-frame and multi-modal fusion framework that
produces dense, colorized point clouds, effectively capturing crack semantics
at a 3D real-world scale. Furthermore, the crack geometric attributions were
measured automatically and directly within 3D dense point cloud space,
surpassing the limitations of conventional 2D image-based measurements. This
advancement makes the method suitable for structural components with curved and
complex 3D geometries. Experimental results across various concrete structures
highlight the significant improvements and unique advantages of the proposed
method, demonstrating its effectiveness, accuracy, and robustness in real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Text-To-Image Diffusion Models For Controlled High-Quality
  Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Süleyman, Göksel Biricik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image (T2I) diffusion models have demonstrated an
outstanding performance in synthesizing diverse high-quality visuals from
natural language text captions. Multiple layout-to-image models have been
developed to control the generation process by utilizing a broad array of
layouts such as segmentation maps, edges, and human keypoints. In this work, we
present ObjectDiffusion, a model that takes inspirations from the top
cutting-edge image generative frameworks to seamlessly condition T2I models
with new bounding boxes capabilities. Specifically, we make substantial
modifications to the network architecture introduced in ContorlNet to integrate
it with the condition processing and injection techniques proposed in GLIGEN.
ObjectDiffusion is initialized with pretraining parameters to leverage the
generation knowledge obtained from training on large-scale datasets. We
fine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on
the COCO2017 validation dataset. Our model achieves an AP$_{50}$ of 46.6, an AR
of 44.5, and a FID of 19.8 outperforming the current SOTA model trained on
open-source datasets in all of the three metrics. ObjectDiffusion demonstrates
a distinctive capability in synthesizing diverse, high-quality, high-fidelity
images that seamlessly conform to the semantic and spatial control layout.
Evaluated in qualitative and quantitative tests, ObjectDiffusion exhibits
remarkable grounding abilities on closed-set and open-set settings across a
wide variety of contexts. The qualitative assessment verifies the ability of
ObjectDiffusion to generate multiple objects of different sizes and locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual
  Defect Detection <span class="chip">ICTAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qisen Cheng, Shuhui Qu, Janghwan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised visual defect detection is critical in industrial applications,
requiring a representation space that captures normal data features while
detecting deviations. Achieving a balance between expressiveness and
compactness is challenging; an overly expressive space risks inefficiency and
mode collapse, impairing detection accuracy. We propose a novel approach using
an enhanced VQ-VAE framework optimized for unsupervised defect detection. Our
model introduces a patch-aware dynamic code assignment scheme, enabling
context-sensitive code allocation to optimize spatial representation. This
strategy enhances normal-defect distinction and improves detection accuracy
during inference. Experiments on MVTecAD, BTAD, and MTSD datasets show our
method achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, Accepted to 36th IEEE ICTAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cancer-Net PCa-Seg: Benchmarking Deep Learning Models for Prostate
  Cancer Segmentation Using Synthetic Correlated Diffusion Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jarett Dewbury, Chi-en Amy Tai, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer (PCa) is the most prevalent cancer among men in the United
States, accounting for nearly 300,000 cases, 29% of all diagnoses and 35,000
total deaths in 2024. Traditional screening methods such as prostate-specific
antigen (PSA) testing and magnetic resonance imaging (MRI) have been pivotal in
diagnosis, but have faced limitations in specificity and generalizability. In
this paper, we explore the potential of enhancing PCa lesion segmentation using
a novel MRI modality called synthetic correlated diffusion imaging (CDI$^s$).
We employ several state-of-the-art deep learning models, including U-Net,
SegResNet, Swin UNETR, Attention U-Net, and LightM-UNet, to segment PCa lesions
from a 200 CDI$^s$ patient cohort. We find that SegResNet achieved superior
segmentation performance with a Dice-Sorensen coefficient (DSC) of $76.68 \pm
0.8$. Notably, the Attention U-Net, while slightly less accurate (DSC $74.82
\pm 2.0$), offered a favorable balance between accuracy and computational
efficiency. Our findings demonstrate the potential of deep learning models in
improving PCa lesion segmentation using CDI$^s$ to enhance PCa management and
clinical support.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, to be published in Studies in Computational
  Intelligence. This paper introduces Cancer-Net PCa-Seg, a comprehensive
  evaluation of deep learning models for prostate cancer segmentation using
  synthetic correlated diffusion imaging (CDI$^s$). We benchmark five
  state-of-the-art architectures: U-Net, SegResNet, Swin UNETR, Attention
  U-Net, and LightM-UNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied Scene Understanding for Vision Language Models via MetaVQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhen Wang, Chenda Duan, Zhenghao Peng, Yuxin Liu, Bolei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) demonstrate significant potential as embodied
AI agents for various mobility applications. However, a standardized,
closed-loop benchmark for evaluating their spatial reasoning and sequential
decision-making capabilities is lacking. To address this, we present MetaVQA: a
comprehensive benchmark designed to assess and enhance VLMs' understanding of
spatial relationships and scene dynamics through Visual Question Answering
(VQA) and closed-loop simulations. MetaVQA leverages Set-of-Mark prompting and
top-down view ground-truth annotations from nuScenes and Waymo datasets to
automatically generate extensive question-answer pairs based on diverse
real-world traffic scenarios, ensuring object-centric and context-rich
instructions. Our experiments show that fine-tuning VLMs with the MetaVQA
dataset significantly improves their spatial reasoning and embodied scene
comprehension in safety-critical simulations, evident not only in improved VQA
accuracies but also in emerging safety-aware driving maneuvers. In addition,
the learning demonstrates strong transferability from simulation to real-world
observation. Code and data will be publicly available at
https://metadriverse.github.io/metavqa .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for the project webpage, see https://metadriverse.github.io/metavqa</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vessel Bifurcation Landmark Pair <span class="highlight-title">Dataset</span> for Abdominal CT Deformable
  Image Registration (DIR) Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward R Criscuolo, Yao Hao, Zhendong Zhang, Trevor McKeown, Deshan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable image registration (DIR) is an enabling technology in many
diagnostic and therapeutic tasks. Despite this, DIR algorithms have limited
clinical use, largely due to a lack of benchmark datasets for quality assurance
during development. To support future algorithm development, here we introduce
our first-of-its-kind abdominal CT DIR benchmark dataset, comprising large
numbers of highly accurate landmark pairs on matching blood vessel
bifurcations. Abdominal CT image pairs of 30 patients were acquired from
several public repositories as well as the authors' institution with IRB
approval. The two CTs of each pair were originally acquired for the same
patient on different days. An image processing workflow was developed and
applied to each image pair: 1) Abdominal organs were segmented with a deep
learning model, and image intensity within organ masks was overwritten. 2)
Matching image patches were manually identified between two CTs of each image
pair 3) Vessel bifurcation landmarks were labeled on one image of each image
patch pair. 4) Image patches were deformably registered, and landmarks were
projected onto the second image. 5) Landmark pair locations were refined
manually or with an automated process. This workflow resulted in 1895 total
landmark pairs, or 63 per case on average. Estimates of the landmark pair
accuracy using digital phantoms were 0.7+/-1.2mm. The data is published in
Zenodo at https://doi.org/10.5281/zenodo.14362785. Instructions for use can be
found at https://github.com/deshanyang/Abdominal-DIR-QA. This dataset is a
first-of-its-kind for abdominal DIR validation. The number, accuracy, and
distribution of landmark pairs will allow for robust validation of DIR
algorithms with precision beyond what is currently available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VCRScore: Image captioning metric based on V\&L Transformers, CLIP, and
  precision-recall 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Ruiz, Tania Ramírez, Daniela Moctezuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning has become an essential Vision & Language research task. It
is about predicting the most accurate caption given a specific image or video.
The research community has achieved impressive results by continuously
proposing new models and approaches to improve the overall model's performance.
Nevertheless, despite increasing proposals, the performance metrics used to
measure their advances have remained practically untouched through the years. A
probe of that, nowadays metrics like BLEU, METEOR, CIDEr, and ROUGE are still
very used, aside from more sophisticated metrics such as BertScore and
ClipScore.
  Hence, it is essential to adjust how are measure the advances, limitations,
and scopes of the new image captioning proposals, as well as to adapt new
metrics to these new advanced image captioning approaches.
  This work proposes a new evaluation metric for the image captioning problem.
To do that, first, it was generated a human-labeled dataset to assess to which
degree the captions correlate with the image's content. Taking these human
scores as ground truth, we propose a new metric, and compare it with several
well-known metrics, from classical to newer ones. Outperformed results were
also found, and interesting insights were presented and discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Adaptation of Training-Free <span class="highlight-title">Foundation</span> Model for 3D Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingxin He, Yifan Hu, Zhaoye Zhou, Mohamed Jarraya, Fang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision foundation models have achieved remarkable progress across various
image analysis tasks. In the image segmentation task, foundation models like
the Segment Anything Model (SAM) enable generalizable zero-shot segmentation
through user-provided prompts. However, SAM primarily trained on natural
images, lacks the domain-specific expertise of medical imaging. This limitation
poses challenges when applying SAM to medical image segmentation, including the
need for extensive fine-tuning on specialized medical datasets and a dependency
on manual prompts, which are both labor-intensive and require intervention from
medical experts.
  This work introduces the Few-shot Adaptation of Training-frEe SAM (FATE-SAM),
a novel method designed to adapt the advanced Segment Anything Model 2 (SAM2)
for 3D medical image segmentation. FATE-SAM reassembles pre-trained modules of
SAM2 to enable few-shot adaptation, leveraging a small number of support
examples to capture anatomical knowledge and perform prompt-free segmentation,
without requiring model fine-tuning. To handle the volumetric nature of medical
images, we incorporate a Volumetric Consistency mechanism that enhances spatial
coherence across 3D slices. We evaluate FATE-SAM on multiple medical imaging
datasets and compare it with supervised learning methods, zero-shot SAM
approaches, and fine-tuned medical SAM methods. Results show that FATE-SAM
delivers robust and accurate segmentation while eliminating the need for large
annotated datasets and expert intervention. FATE-SAM provides a practical,
efficient solution for medical image segmentation, making it more accessible
for clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Robustness of Contrastive Learning Models for Medical
  Image-Report <span class="highlight-title">Retrie</span>val <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, Gongbo Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images and reports offer invaluable insights into patient health. The
heterogeneity and complexity of these data hinder effective analysis. To bridge
this gap, we investigate contrastive learning models for cross-domain
retrieval, which associates medical images with their corresponding clinical
reports. This study benchmarks the robustness of four state-of-the-art
contrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We
introduce an occlusion retrieval task to evaluate model performance under
varying levels of image corruption. Our findings reveal that all evaluated
models are highly sensitive to out-of-distribution data, as evidenced by the
proportional decrease in performance with increasing occlusion levels. While
MedCLIP exhibits slightly more robustness, its overall performance remains
significantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a
general-purpose dataset, struggles with medical image-report retrieval,
highlighting the importance of domain-specific training data. The evaluation of
this work suggests that more effort needs to be spent on improving the
robustness of these models. By addressing these limitations, we can develop
more reliable cross-domain retrieval models for medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is accepted to AAAI 2025 Workshop -- the 9th International
  Workshop on Health Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep <span class="highlight-title">Self-Supervised</span> Disturbance Mapping with the OPERA Sentinel-1
  Radiometric Terrain Corrected SAR Backscatter Product 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harris Hardiman-Mostow, Charles Marshak, Alexander L. Handwerger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping land surface disturbances supports disaster response, resource and
ecosystem management, and climate adaptation efforts. Synthetic aperture radar
(SAR) is an invaluable tool for disturbance mapping, providing consistent
time-series images of the ground regardless of weather or illumination
conditions. Despite SAR's potential for disturbance mapping, processing SAR
data to an analysis-ready format requires expertise and significant compute
resources, particularly for large-scale global analysis. In October 2023,
NASA's Observational Products for End-Users from Remote Sensing Analysis
(OPERA) project released the near-global Radiometric Terrain Corrected SAR
backscatter from Sentinel-1 (RTC-S1) dataset, providing publicly available,
analysis-ready SAR imagery. In this work, we utilize this new dataset to
systematically analyze land surface disturbances. As labeling SAR data is often
prohibitively time-consuming, we train a self-supervised vision transformer -
which requires no labels to train - on OPERA RTC-S1 data to estimate a
per-pixel distribution from the set of baseline imagery and assess disturbances
when there is significant deviation from the modeled distribution. To test our
model's capability and generality, we evaluate three different natural
disasters - which represent high-intensity, abrupt disturbances - from three
different regions of the world. Across events, our approach yields high quality
delineations: F1 scores exceeding 0.6 and Areas Under the Precision-Recall
Curve exceeding 0.65, consistently outperforming existing SAR disturbance
methods. Our findings suggest that a self-supervised vision transformer is
well-suited for global disturbance mapping and can be a valuable tool for
operational, near-global disturbance monitoring, particularly when labeled data
does not exist.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 18 figures, 5 tables. Preprint. Submitted to JSTARS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Distance Map Regression Network with Shape-aware Loss for
  Imbalanced Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyu Li, Xiabi Liu, Said Boumaraf, Xiaopeng Gong, Donghai Liao, Xiaohong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small object segmentation, like tumor segmentation, is a difficult and
critical task in the field of medical image analysis. Although deep learning
based methods have achieved promising performance, they are restricted to the
use of binary segmentation mask. Inspired by the rigorous mapping between
binary segmentation mask and distance map, we adopt distance map as a novel
ground truth and employ a network to fulfill the computation of distance map.
Specially, we propose a new segmentation framework that incorporates the
existing binary segmentation network and a light weight regression network
(dubbed as LR-Net). Thus, the LR-Net can convert the distance map computation
into a regression task and leverage the rich information of distance maps.
Additionally, we derive a shape-aware loss by employing distance maps as
penalty map to infer the complete shape of an object. We evaluated our approach
on MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge dataset and a clinical
dataset. Experimental results show that our approach outperforms the
classification-based methods as well as other existing state-of-the-arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Medical Image Anonymization Based on Latent Code Projection
  and Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyu Li, Nicholas Ayache, Hervé Delingette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image anonymization aims to protect patient privacy by removing
identifying information, while preserving the data utility to solve downstream
tasks. In this paper, we address the medical image anonymization problem with a
two-stage solution: latent code projection and optimization. In the projection
stage, we design a streamlined encoder to project input images into a latent
space and propose a co-training scheme to enhance the projection process. In
the optimization stage, we refine the latent code using two deep loss functions
designed to address the trade-off between identity protection and data utility
dedicated to medical images. Through a comprehensive set of qualitative and
quantitative experiments, we showcase the effectiveness of our approach on the
MIMIC-CXR chest X-ray dataset by generating anonymized synthetic images that
can serve as training set for detecting lung pathologies. Source codes are
available at https://github.com/Huiyu-Li/GMIA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relation U-Net 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng He, Rina Bao, P. Ellen Grant, Yangming Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Towards clinical interpretations, this paper presents a new
''output-with-confidence'' segmentation neural network with multiple input
images and multiple output segmentation maps and their pairwise relations. A
confidence score of the test image without ground-truth can be estimated from
the difference among the estimated relation maps. We evaluate the method based
on the widely used vanilla U-Net for segmentation and our new model is named
Relation U-Net which can output segmentation maps of the input images as well
as an estimated confidence score of the test image without ground-truth.
Experimental results on four public datasets show that Relation U-Net can not
only provide better accuracy than vanilla U-Net but also estimate a confidence
score which is linearly correlated to the segmentation accuracy on test images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISIB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self <span class="highlight-title">Pre-train</span>ing with Adaptive Mask Autoencoders for Variable-Contrast
  3D Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badhan Kumar Das, Gengyan Zhao, Han Liu, Thomas J. Re, Dorin Comaniciu, Eli Gibson, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Masked Autoencoder (MAE) has recently demonstrated effectiveness in
pre-training Vision Transformers (ViT) for analyzing natural images. By
reconstructing complete images from partially masked inputs, the ViT encoder
gathers contextual information to predict the missing regions. This capability
to aggregate context is especially important in medical imaging, where
anatomical structures are functionally and mechanically linked to surrounding
regions. However, current methods do not consider variations in the number of
input images, which is typically the case in real-world Magnetic Resonance (MR)
studies. To address this limitation, we propose a 3D Adaptive Masked
Autoencoders (AMAE) architecture that accommodates a variable number of 3D
input contrasts per subject. A magnetic resonance imaging (MRI) dataset of
45,364 subjects was used for pretraining and a subset of 1648 training, 193
validation and 215 test subjects were used for finetuning. The performance
demonstrates that self pre-training of this adaptive masked autoencoders can
enhance the infarct segmentation performance by 2.8%-3.7% for ViT-based
segmentation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, ISBI 2025 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Salient Information Preserving Adversarial Training Improves Clean and
  Robust Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Redgrave, Adam Czajka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we introduce Salient Information Preserving Adversarial Training
(SIP-AT), an intuitive method for relieving the robustness-accuracy trade-off
incurred by traditional adversarial training. SIP-AT uses salient image regions
to guide the adversarial training process in such a way that fragile features
deemed meaningful by an annotator remain unperturbed during training, allowing
models to learn highly predictive non-robust features without sacrificing
overall robustness. This technique is compatible with both human-based and
automatically generated salience estimates, allowing SIP-AT to be used as a
part of human-driven model development without forcing SIP-AT to be reliant
upon additional human data. We perform experiments across multiple datasets and
architectures and demonstrate that SIP-AT is able to boost the clean accuracy
of models while maintaining a high degree of robustness against attacks at
multiple epsilon levels. We complement our central experiments with an
observational study measuring the rate at which human subjects successfully
identify perturbed images. This study helps build a more intuitive
understanding of adversarial attack strength and demonstrates the heightened
importance of low-epsilon robustness. Our results demonstrate the efficacy of
SIP-AT and provide valuable insight into the risks posed by adversarial samples
of various strengths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHYI: Action Support for Contrastive Learning in High-Fidelity
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiang Xia, Lin Xiao, Yannick Montorfani, Francesco Pavia, Enis Simsar, Thomas Hofmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this project, we address the issue of infidelity in text-to-image
generation, particularly for actions involving multiple objects. For this we
build on top of the CONFORM framework which uses Contrastive Learning to
improve the accuracy of the generated image for multiple objects. However the
depiction of actions which involves multiple different object has still large
room for improvement. To improve, we employ semantically hypergraphic
contrastive adjacency learning, a comprehension of enhanced contrastive
structure and "contrast but link" technique. We further amend Stable
Diffusion's understanding of actions by InteractDiffusion. As evaluation
metrics we use image-text similarity CLIP and TIFA. In addition, we conducted a
user study.
  Our method shows promising results even with verbs that Stable Diffusion
understands mediocrely. We then provide future directions by analyzing the
results.
  Our codebase can be found on polybox under the link:
https://polybox.ethz.ch/index.php/s/dJm3SWyRohUrFxn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main content 4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video (T2V) generative models have advanced significantly, yet their
ability to compose different objects, attributes, actions, and motions into a
video remains unexplored. Previous text-to-video benchmarks also neglect this
important ability for evaluation. In this work, we conduct the first systematic
study on compositional text-to-video generation. We propose T2V-CompBench, the
first benchmark tailored for compositional text-to-video generation.
T2V-CompBench encompasses diverse aspects of compositionality, including
consistent attribute binding, dynamic attribute binding, spatial relationships,
motion binding, action binding, object interactions, and generative numeracy.
We further carefully design evaluation metrics of multimodal large language
model (MLLM)-based, detection-based, and tracking-based metrics, which can
better reflect the compositional text-to-video generation quality of seven
proposed categories with 1400 text prompts. The effectiveness of the proposed
metrics is verified by correlation with human evaluations. We also benchmark
various text-to-video generative models and conduct in-depth analysis across
different models and various compositional categories. We find that
compositional text-to-video generation is highly challenging for current
models, and we hope our attempt could shed light on future research in this
direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://t2v-compbench-2025.github.io/ Code:
  https://github.com/KaiyueSun98/T2V-CompBench/tree/V2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeblurDiNAT: A Compact Model with Exceptional Generalization and Visual
  Fidelity on Unseen Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13163v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13163v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhou Liu, Binghan Li, Chengkai Liu, Mi Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deblurring networks have effectively restored clear images from the
blurred ones. However, they often struggle with generalization to unknown
domains. Moreover, these models typically focus on distortion metrics such as
PSNR and SSIM, neglecting the critical aspect of metrics aligned with human
perception. To address these limitations, we propose DeblurDiNAT, a deblurring
Transformer based on Dilated Neighborhood Attention. First, DeblurDiNAT employs
an alternating dilation factor paradigm to capture both local and global
blurred patterns, enhancing generalization and perceptual clarity. Second, a
local cross-channel learner aids the Transformer block to understand the
short-range relationships between adjacent channels. Additionally, we present a
linear feed-forward network with a simple while effective design. Finally, a
dual-stage feature fusion module is introduced as an alternative to the
existing approach, which efficiently process multi-scale visual information
across network levels. Compared to state-of-the-art models, our compact
DeblurDiNAT demonstrates superior generalization capabilities and achieves
remarkable performance in perceptual metrics, while maintaining a favorable
model size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Click-Calib: A Robust Extrinsic Calibration Method for Surround-View
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surround-View System (SVS) is an essential component in Advanced Driver
Assistance System (ADAS) and requires precise calibrations. However,
conventional offline extrinsic calibration methods are cumbersome and
time-consuming as they rely heavily on physical patterns. Additionally, these
methods primarily focus on short-range areas surrounding the vehicle, resulting
in lower calibration quality in more distant zones. To address these
limitations, we propose Click-Calib, a pattern-free approach for offline SVS
extrinsic calibration. Without requiring any special setup, the user only needs
to click a few keypoints on the ground in natural scenes. Unlike other offline
calibration approaches, Click-Calib optimizes camera poses over a wide range by
minimizing reprojection distance errors of keypoints, thereby achieving
accurate calibrations at both short and long distances. Furthermore,
Click-Calib supports both single-frame and multiple-frame modes, with the
latter offering even better results. Evaluations on our in-house dataset and
the public WoodScape dataset demonstrate its superior accuracy and robustness
compared to baseline methods. Code is available at
https://github.com/lwangvaleo/click_calib.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Framework for Inference-time Scaling and Steering of Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models produce impressive results in modalities ranging from images
and video to protein design and text. However, generating samples with
user-specified properties remains a challenge. Recent research proposes
fine-tuning models to maximize rewards that capture desired properties, but
these methods require expensive training and are prone to mode collapse. In
this work, we propose Feynman Kac (FK) steering, an inference-time framework
for steering diffusion models with reward functions. FK steering works by
sampling a system of multiple interacting diffusion processes, called
particles, and resampling particles at intermediate steps based on scores
computed using functions called potentials. Potentials are defined using
rewards for intermediate states and are selected such that a high value
indicates that the particle will yield a high-reward sample. We explore various
choices of potentials, intermediate rewards, and samplers. We evaluate FK
steering on text-to-image and text diffusion models. For steering text-to-image
models with a human preference reward, we find that FK steering a 0.8B
parameter model outperforms a 2.6B parameter fine-tuned model on prompt
fidelity, with faster sampling and no training. For steering text diffusion
models with rewards for text quality and specific text attributes, we find that
FK steering generates lower perplexity, more linguistically acceptable outputs
and enables gradient-free control of attributes like toxicity. Our results
demonstrate that inference-time scaling and steering of diffusion models, even
with off-the-shelf rewards, can provide significant sample quality gains and
controllability benefits. Code is available at
https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SA-MLP: A Low-Power Multiplication-Free Deep Network for 3D Point Cloud
  Classification in Resource-Constrained Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zheng, Chao Zhang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud classification plays a crucial role in the processing and
analysis of data from 3D sensors such as LiDAR, which are commonly used in
applications like autonomous vehicles, robotics, and environmental monitoring.
However, traditional neural networks, which rely heavily on multiplication
operations, often face challenges in terms of high computational costs and
energy consumption. This study presents a novel family of efficient MLP-based
architectures designed to improve the computational efficiency of point cloud
classification tasks in sensor systems. The baseline model, Mul-MLP, utilizes
conventional multiplication operations, while Add-MLP and Shift-MLP replace
multiplications with addition and shift operations, respectively. These
replacements leverage more sensor-friendly operations that can significantly
reduce computational overhead, making them particularly suitable for
resource-constrained sensor platforms. To further enhance performance, we
propose SA-MLP, a hybrid architecture that alternates between shift and adder
layers, preserving the network depth while optimizing computational efficiency.
Unlike previous approaches such as ShiftAddNet, which increase the layer count
and limit representational capacity by freezing shift weights, SA-MLP fully
exploits the complementary advantages of shift and adder layers by employing
distinct learning rates and optimizers. Experimental results show that Add-MLP
and Shift-MLP achieve competitive performance compared to Mul-MLP, while SA-MLP
surpasses the baseline, delivering results comparable to state-of-the-art MLP
models in terms of both classification accuracy and computational efficiency.
This work offers a promising, energy-efficient solution for sensor-driven
applications requiring real-time point cloud classification, particularly in
environments with limited computational resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A design of Convolutional Neural Network model for the Diagnosis of the
  COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06394v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06394v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the spread of COVID-19 around the globe over the past year, the usage of
artificial intelligence (AI) algorithms and image processing methods to analyze
the X-ray images of patients' chest with COVID-19 has become essential. The
COVID-19 virus recognition in the lung area of a patient is one of the basic
and essential needs of clicical centers and hospitals. Most research in this
field has been devoted to papers on the basis of deep learning methods
utilizing CNNs (Convolutional Neural Network), which mainly deal with the
screening of sick and healthy people.In this study, a new structure of a
19-layer CNN has been recommended for accurately recognition of the COVID-19
from the X-ray pictures of chest. The offered CNN is developed to serve as a
precise diagnosis system for a three class (viral pneumonia, Normal, COVID) and
a four classclassification (Lung opacity, Normal, COVID-19, and pneumonia). A
comparison is conducted among the outcomes of the offered procedure and some
popular pretrained networks, including Inception, Alexnet, ResNet50,
Squeezenet, and VGG19 and based on Specificity, Accuracy, Precision,
Sensitivity, Confusion Matrix, and F1-score. The experimental results of the
offered CNN method specify its dominance over the existing published
procedures. This method can be a useful tool for clinicians in deciding
properly about COVID-19.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Important mistakes found. There's no new version currently. Also
  contradiction with authorship</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compression with Global Guidance: Towards Training-free High-Resolution
  MLLMs Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05179v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05179v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Liu, Ziming Wang, Yuhang Han, Yingyao Wang, Jiale Yuan, Jun Song, Bo Zheng, Linfeng Zhang, Siteng Huang, Honggang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have attracted considerable
attention due to their exceptional performance in visual content understanding
and reasoning. However, their inference efficiency has been a notable concern,
as the increasing length of multimodal contexts leads to quadratic complexity.
Token compression techniques, which reduce the number of visual tokens, have
demonstrated their effectiveness in reducing computational costs. Yet, these
approaches have struggled to keep pace with the rapid advancements in MLLMs,
especially the AnyRes strategy in the context of high-resolution image
understanding. In this paper, we propose a novel token compression method,
GlobalCom$^2$, tailored for high-resolution MLLMs that receive both the
thumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the
thumbnail as the "commander" of the entire token compression process, directing
the allocation of retention ratios and the specific compression for each crop.
In this way, redundant tokens are eliminated while important local details are
adaptively preserved to the highest extent feasible. Empirical results across
10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between
performance and efficiency, and consistently outperforms state-of-the-art token
compression methods with LLaVA-NeXT-7B/13B models. Our code is released at
https://github.com/xuyang-liu16/GlobalCom2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is released at
  \url{https://github.com/xuyang-liu16/GlobalCom2}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Spurious Correlations using Counterfactual Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Paul Cohen, Louis Blankemeier, Akshay Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models driven by spurious correlations often yield poor generalization
performance. We propose the counterfactual (CF) alignment method to detect and
quantify spurious correlations of black box classifiers. Our methodology is
based on counterfactual images generated with respect to one classifier being
input into other classifiers to see if they also induce changes in the outputs
of these classifiers. The relationship between these responses can be
quantified and used to identify specific instances where a spurious correlation
exists. This is validated by observing intuitive trends in face-attribute and
waterbird classifiers, as well as by fabricating spurious correlations and
detecting their presence, both visually and quantitatively. Furthermore,
utilizing the CF alignment method, we demonstrate that we can evaluate robust
optimization methods (GroupDRO, JTT, and FLAC) by detecting a reduction in
spurious correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR), Code:
  https://github.com/ieee8023/latentshift</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PACE: Marrying generalization in PArameter-efficient fine-tuning with
  Consistency rEgularization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17137v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17137v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Ni, Shan Zhang, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained
transformers to downstream tasks. However, the optimization of tasks
performance often comes at the cost of generalizability in fine-tuned models.
To address this issue, we theoretically connect smaller weight gradient norms
during training and larger datasets to the improvements in model
generalization. Motivated by this connection, we propose reducing gradient
norms for enhanced generalization and aligning fine-tuned model with the
pre-trained counterpart to retain knowledge from large-scale pre-training data.
Yet, naive alignment does not guarantee gradient reduction and can potentially
cause gradient explosion, complicating efforts to manage gradients. To address
such an issue, we propose PACE, marrying generalization of PArameter-efficient
fine-tuning with Consistency rEgularization. We perturb features learned from
the adapter with the multiplicative noise and ensure the fine-tuned model
remains consistent for same sample under different perturbations. Theoretical
analysis shows that PACE not only implicitly regularizes gradients for enhanced
generalization, but also implicitly aligns the fine-tuned and pre-trained
models to retain knowledge. Experimental evidence supports our theories. PACE
surpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC,
few-shot learning, domain adaptation) showcasing its potential for
resource-efficient fine-tuning. It also improves LoRA in text classification
(GLUE) and mathematical reasoning (GSM-8K). The code is available at
https://github.com/MaxwellYaoNi/PACE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 as a spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TextSleuth: Towards Explainable Tampered Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14816v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14816v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenfan Qu, Jian Liu, Haoxing Chen, Baihan Yu, Jingjing Liu, Weiqiang Wang, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, tampered text detection has attracted increasing attention due to
its essential role in information security. Although existing methods can
detect the tampered text region, the interpretation of such detection remains
unclear, making the prediction unreliable. To address this problem, we propose
to explain the basis of tampered text detection with natural language via large
multimodal models. To fill the data gap for this task, we propose a
large-scale, comprehensive dataset, ETTD, which contains both pixel-level
annotations for tampered text region and natural language annotations
describing the anomaly of the tampered text. Multiple methods are employed to
improve the quality of the proposed data. For example, elaborate queries are
introduced to generate high-quality anomaly descriptions with GPT4o. A fused
mask prompt is proposed to reduce confusion when querying GPT4o to generate
anomaly descriptions. To automatically filter out low-quality annotations, we
also propose to prompt GPT4o to recognize tampered texts before describing the
anomaly, and to filter out the responses with low OCR accuracy. To further
improve explainable tampered text detection, we propose a simple yet effective
model called TextSleuth, which achieves improved fine-grained perception and
cross-domain generalization by focusing on the suspected region, with a
two-stage analysis paradigm and an auxiliary grounding prompt. Extensive
experiments on both the ETTD dataset and the public dataset have verified the
effectiveness of the proposed methods. In-depth analysis is also provided to
inspire further research. Our dataset and code will be open-source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first work for explainable tampered text detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Foundation</span> Language-Image Model of the Retina (FLAIR): Encoding Expert
  Knowledge in Text Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julio Silva-Rodríguez, Hadi Chakor, Riadh Kobbi, Jose Dolz, Ismail Ben Ayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation vision-language models are currently transforming computer vision,
and are on the rise in medical imaging fueled by their very promising
generalization capabilities. However, the initial attempts to transfer this new
paradigm to medical imaging have shown less impressive performances than those
observed in other domains, due to the significant domain shift and the complex,
expert domain knowledge inherent to medical-imaging tasks. Motivated by the
need for domain-expert foundation models, we present FLAIR, a pre-trained
vision-language model for universal retinal fundus image understanding. To this
end, we compiled 38 open-access, mostly categorical fundus imaging datasets
from various sources, with up to 101 different target conditions and 288,307
images. We integrate the expert's domain knowledge in the form of descriptive
textual prompts, during both pre-training and zero-shot inference, enhancing
the less-informative categorical supervision of the data. Such a textual
expert's knowledge, which we compiled from the relevant clinical literature and
community standards, describes the fine-grained features of the pathologies as
well as the hierarchies and dependencies between them. We report comprehensive
evaluations, which illustrate the benefit of integrating expert knowledge and
the strong generalization capabilities of FLAIR under difficult scenarios with
domain shifts or unseen categories. When adapted with a lightweight linear
probe, FLAIR outperforms fully-trained, dataset-focused models, more so in the
few-shot regimes. Interestingly, FLAIR outperforms by a wide margin
larger-scale generalist image-language models and retina domain-specific
self-supervised networks, which emphasizes the potential of embedding experts'
domain knowledge and the limitations of generalist models in medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Medical Image Analysis. The pre-trained model is
  available at: https://github.com/jusiro/FLAIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MADiff: Text-Guided Fashion Image Editing with Mask Prediction and
  Attention-Enhanced Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechao Zhan, Dehong Gao, Jinxia Zhang, Jiale Huang, Yang Hu, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided image editing model has achieved great success in general domain.
However, directly applying these models to the fashion domain may encounter two
issues: (1) Inaccurate localization of editing region; (2) Weak editing
magnitude. To address these issues, the MADiff model is proposed. Specifically,
to more accurately identify editing region, the MaskNet is proposed, in which
the foreground region, densepose and mask prompts from large language model are
fed into a lightweight UNet to predict the mask for editing region. To
strengthen the editing magnitude, the Attention-Enhanced Diffusion Model is
proposed, where the noise map, attention map, and the mask from MaskNet are fed
into the proposed Attention Processor to produce a refined noise map. By
integrating the refined noise map into the diffusion model, the edited image
can better align with the target prompt. Given the absence of benchmarks in
fashion image editing, we constructed a dataset named Fashion-E, comprising
28390 image-text pairs in the training set, and 2639 image-text pairs for four
types of fashion tasks in the evaluation set. Extensive experiments on
Fashion-E demonstrate that our proposed method can accurately predict the mask
of editing region and significantly enhance editing magnitude in fashion image
editing compared to the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Industrial Anomaly Detection and Localization Using Weakly-Supervised
  Residual Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03492v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03492v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxi Li, Jingqi Wu, Deyin Liu, Lin Wu, Hao Chen, Mingwen Wang, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in industrial anomaly detection (AD) have demonstrated
that incorporating a small number of anomalous samples during training can
significantly enhance accuracy. However, this improvement often comes at the
cost of extensive annotation efforts, which are impractical for many real-world
applications. In this paper, we introduce a novel framework, Weak}ly-supervised
RESidual Transformer (WeakREST), designed to achieve high anomaly detection
accuracy while minimizing the reliance on manual annotations. First, we
reformulate the pixel-wise anomaly localization task into a block-wise
classification problem. Second, we introduce a residual-based feature
representation called Positional Fast Anomaly Residuals (PosFAR) which captures
anomalous patterns more effectively. To leverage this feature, we adapt the
Swin Transformer for enhanced anomaly detection and localization. Additionally,
we propose a weak annotation approach, utilizing bounding boxes and image tags
to define anomalous regions. This approach establishes a semi-supervised
learning context that reduces the dependency on precise pixel-level labels. To
further improve the learning process, we develop a novel ResMixMatch algorithm,
capable of handling the interplay between weak labels and residual-based
representations.
  On the benchmark dataset MVTec-AD, our method achieves an Average Precision
(AP) of $83.0\%$, surpassing the previous best result of $82.7\%$ in the
unsupervised setting. In the supervised AD setting, WeakREST attains an AP of
$87.6\%$, outperforming the previous best of $86.0\%$. Notably, even when using
weaker annotations such as bounding boxes, WeakREST exceeds the performance of
leading methods relying on pixel-wise supervision, achieving an AP of $87.1\%$
compared to the prior best of $86.0\%$ on MVTec-AD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Surprising Ineffectiveness of <span class="highlight-title">Pre-Train</span>ed Visual Representations for
  Model-Based Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Schneider, Robert Krug, Narunas Vaskevicius, Luigi Palmieri, Joschka Boedecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Reinforcement Learning (RL) methods often require extensive amounts of
data. As opposed to model-free RL, model-based RL (MBRL) offers a potential
solution with efficient data utilization through planning. Additionally, RL
lacks generalization capabilities for real-world tasks. Prior work has shown
that incorporating pre-trained visual representations (PVRs) enhances sample
efficiency and generalization. While PVRs have been extensively studied in the
context of model-free RL, their potential in MBRL remains largely unexplored.
In this paper, we benchmark a set of PVRs on challenging control tasks in a
model-based RL setting. We investigate the data efficiency, generalization
capabilities, and the impact of different properties of PVRs on the performance
of model-based agents. Our results, perhaps surprisingly, reveal that for MBRL
current PVRs are not more sample efficient than learning representations from
scratch, and that they do not generalize better to out-of-distribution (OOD)
settings. To explain this, we analyze the quality of the trained dynamics
model. Furthermore, we show that data diversity and network architecture are
the most important contributors to OOD generalization performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024). Project page: https://schneimo.com/pvr4mbrl/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CGCOD: Class-Guided Camouflaged Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Zhang, Qing Zhang, Jiayun Wu, Youwei Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflaged Object Detection (COD) aims to identify objects that blend
seamlessly into their surroundings. The inherent visual complexity of
camouflaged objects, including their low contrast with the background, diverse
textures, and subtle appearance variations, often obscures semantic cues,
making accurate segmentation highly challenging. Existing methods primarily
rely on visual features, which are insufficient to handle the variability and
intricacy of camouflaged objects, leading to unstable object perception and
ambiguous segmentation results. To tackle these limitations, we introduce a
novel task, class-guided camouflaged object detection (CGCOD), which extends
traditional COD task by incorporating object-specific class knowledge to
enhance detection robustness and accuracy. To facilitate this task, we present
a new dataset, CamoClass, comprising real-world camouflaged objects with class
annotations. Furthermore, we propose a multi-stage framework, CGNet, which
incorporates a plug-and-play class prompt generator and a simple yet effective
class-guided detector. This establishes a new paradigm for COD, bridging the
gap between contextual understanding and class-guided detection. Extensive
experimental results demonstrate the effectiveness of our flexible framework in
improving the performance of proposed and existing detectors by leveraging
class-level textual information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of radiomic feature harmonization techniques for benign and
  malignant pulmonary nodules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire Huchthausen, Menglin Shi, Gabriel L. A. de Sousa, Jonathan Colen, Emery Shelley, James Larner, Einsley Janowski, Krishni Wijesooriya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  BACKGROUND: Radiomics provides quantitative features of pulmonary nodules
(PNs) which could aid lung cancer diagnosis, but medical image acquisition
variability is an obstacle to clinical application. Acquisition effects may
differ between radiomic features from benign vs. malignant PNs. PURPOSE: We
evaluated how to account for differences between benign and malignant PNs when
correcting radiomic features' acquisition dependency. METHODS: We used 567
chest CT scans grouped as benign, malignant, or lung cancer screening (mixed
benign, malignant). ComBat harmonization was applied to extracted features for
variation in 4 acquisition parameters. We compared: harmonizing without
distinction, harmonizing with a covariate to preserve distinctions between
subgroups, and harmonizing subgroups separately. Significant ($p\le0.05$)
Kruskal-Wallis tests showed whether harmonization removed acquisition
dependency. A LASSO-SVM pipeline was trained on successfully harmonized
features to predict malignancy. To evaluate predictive information in these
features, the trained harmonization estimators and predictive model were
applied to unseen test sets. Harmonization and predictive performance were
assessed for 10 trials of 5-fold cross-validation. RESULTS: An average 2.1% of
features (95% CI:1.9-2.4%) were acquisition-independent when harmonized without
distinction, 27.3% (95% CI:25.7-28.9%) when harmonized with a covariate, and
90.9% (95% CI:90.4-91.5%) when harmonized separately. Data harmonized
separately or with a covariate trained models with higher ROC-AUC for screening
scans than data harmonized without distinction between benign and malignant PNs
(Delong test, adjusted $p\le0.05$). CONCLUSIONS: Radiomic features of benign
and malignant PNs need different corrective transformations to recover
acquisition-independent distributions. This can be done by harmonizing
separately or with a covariate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, plus supplemental material; updated author list,
  corrected result in paragraph 3 of Discussion, updated Figure S1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural damage detection via hierarchical damage information with
  volumetric assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Osei Agyemang, Isaac Adjei-Mensah, Daniel Acheampong, Gordon Owusu Boateng, Adu Asare Baffour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural health monitoring (SHM) is essential for ensuring the safety and
longevity of infrastructure, but complex image environments, noisy labels, and
reliance on manual damage assessments often hinder its effectiveness. This
study introduces the Guided Detection Network (Guided-DetNet), a framework
designed to address these challenges. Guided-DetNet is characterized by a
Generative Attention Module (GAM), Hierarchical Elimination Algorithm (HEA),
and Volumetric Contour Visual Assessment (VCVA). GAM leverages cross-horizontal
and cross-vertical patch merging and cross-foreground-background feature fusion
to generate varied features to mitigate complex image environments. HEA
addresses noisy labeling using hierarchical relationships among classes to
refine instances given an image by eliminating unlikely class instances. VCVA
assesses the severity of detected damages via volumetric representation and
quantification leveraging the Dirac delta distribution. A comprehensive
quantitative study and two robustness tests were conducted using the PEER Hub
dataset, and a drone-based application, which involved a field experiment, was
conducted to substantiate Guided-DetNet's promising performances. In triple
classification tasks, the framework achieved 96% accuracy, surpassing
state-of-the-art classifiers by up to 3%. In dual detection tasks, it
outperformed competitive detectors with a precision of 94% and a mean average
precision (mAP) of 79% while maintaining a frame rate of 57.04fps, suitable for
real-time applications. Additionally, robustness tests demonstrated resilience
under adverse conditions, with precision scores ranging from 79% to 91%.
Guided-DetNet is established as a robust and efficient framework for SHM,
offering advancements in automation and precision, with the potential for
widespread application in drone-based infrastructure inspections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic
  Emphasis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Ziqiang Dang, Jianqiang Ren, Liefeng Bo, Zhigang Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A good co-speech motion generation cannot be achieved without a careful
integration of common rhythmic motion and rare yet essential semantic motion.
In this work, we propose SemTalk for holistic co-speech motion generation with
frame-level semantic emphasis. Our key insight is to separately learn general
motions and sparse motions, and then adaptively fuse them. In particular,
rhythmic consistency learning is explored to establish rhythm-related base
motion, ensuring a coherent foundation that synchronizes gestures with the
speech rhythm. Subsequently, textit{semantic emphasis learning is designed to
generate semantic-aware sparse motion, focusing on frame-level semantic cues.
Finally, to integrate sparse motion into the base motion and generate
semantic-emphasized co-speech gestures, we further leverage a learned semantic
score for adaptive synthesis. Qualitative and quantitative comparisons on two
public datasets demonstrate that our method outperforms the state-of-the-art,
delivering high-quality co-speech motion with enhanced semantic richness over a
stable base motion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACE++: Instruction-Based Image Creation and Editing via Context-Aware
  Content Filling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report ACE++, an instruction-based diffusion framework that tackles
various image generation and editing tasks. Inspired by the input format for
the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context
Condition Unit (LCU) introduced in ACE and extend this input paradigm to any
editing and generation tasks. To take full advantage of image generative
priors, we develop a two-stage training scheme to minimize the efforts of
finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the
first stage, we pre-train the model using task data with the 0-ref tasks from
the text-to-image model. There are many models in the community based on the
post-training of text-to-image foundational models that meet this training
paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with
painting tasks and can be used as an initialization to accelerate the training
process. In the second stage, we finetune the above model to support the
general instructions using all tasks defined in ACE. To promote the widespread
application of ACE++ in different scenarios, we provide a comprehensive set of
models that cover both full finetuning and lightweight finetuning, while
considering general applicability and applicability in vertical scenarios. The
qualitative analysis showcases the superiority of ACE++ in terms of generating
image quality and prompt following ability. Code and models will be available
on the project page: https://ali-vilab. github.io/ACE_plus_page/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Energy-Independent Density for CT Metal Artifact Reduction via
  Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Wu, Xu Guo, Lixuan Chen, Yanyan Liu, Dongming He, Xudong Wang, Xueli Chen, Yifeng Zhang, S. Kevin Zhou, Jingyi Yu, Yuyao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray CT often suffers from shadowing and streaking artifacts in the presence
of metallic materials, which severely degrade imaging quality. Physically, the
linear attenuation coefficients (LACs) of metals vary significantly with X-ray
energy, causing a nonlinear beam hardening effect (BHE) in CT measurements.
Reconstructing CT images from metal-corrupted measurements consequently becomes
a challenging nonlinear inverse problem. Existing state-of-the-art (SOTA) metal
artifact reduction (MAR) algorithms rely on supervised learning with numerous
paired CT samples. While promising, these supervised methods often assume that
the unknown LACs are energy-independent, ignoring the energy-induced BHE, which
results in limited generalization. Moreover, the requirement for large datasets
also limits their applications in real-world scenarios. In this work, we
propose Density neural representation (Diner), a novel unsupervised MAR method.
Our key innovation lies in formulating MAR as an energy-independent density
reconstruction problem that strictly adheres to the photon-tissue absorption
physical model. This model is inherently nonlinear and complex, making it a
rarely considered approach in inverse imaging problems. By introducing the
water-equivalent tissues approximation and a new polychromatic model to
characterize the nonlinear CT acquisition process, we directly learn the neural
representation of the density map from raw measurements without using external
training data. This energy-independent density reconstruction framework
fundamentally resolves the nonlinear BHE, enabling superior MAR performance
across a wide range of scanning scenarios. Extensive experiments on both
simulated and real-world datasets demonstrate the superiority of our
unsupervised Diner over popular supervised methods in terms of MAR performance
and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3VL: Using Trees to Improve Vision-Language Models' Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nir Yellinek, Leonid Karlinsky, Raja Giryes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language models (VLMs) have proven to be effective at aligning image
and text representations, producing superior zero-shot results when transferred
to many downstream tasks. However, these representations suffer from some key
shortcomings in understanding Compositional Language Concepts (CLC), such as
recognizing objects' attributes, states, and relations between different
objects. Moreover, VLMs typically have poor interpretability, making it
challenging to debug and mitigate compositional-understanding failures. In this
work, we introduce the architecture and training technique of Tree-augmented
Vision-Language (3VL) model accompanied by our proposed Anchor inference method
and Differential Relevance (DiRe) interpretability tool. By expanding the text
of an arbitrary image-text pair into a hierarchical tree structure using
language analysis tools, 3VL allows the induction of this structure into the
visual representation learned by the model, enhancing its interpretability and
compositional reasoning. Additionally, we show how Anchor, a simple technique
for text unification, can be used to filter nuisance factors while increasing
CLC understanding performance, e.g., on the fundamental VL-Checklist benchmark.
We also show how DiRe, which performs a differential comparison between VLM
relevancy maps, enables us to generate compelling visualizations of the reasons
for a model's success or failure. Our code is available at:
https://github.com/niryellinek/3VL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to IEEE TIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When No-Reference Image Quality Models Meet MAP Estimation in Diffusion
  Latents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixia Zhang, Dingquan Li, Guangtao Zhai, Xiaokang Yang, Kede Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary no-reference image quality assessment (NR-IQA) models can
effectively quantify perceived image quality, often achieving strong
correlations with human perceptual scores on standard IQA benchmarks. Yet,
limited efforts have been devoted to treating NR-IQA models as natural image
priors for real-world image enhancement, and consequently comparing them from a
perceptual optimization standpoint. In this work, we show -- for the first time
-- that NR-IQA models can be plugged into the maximum a posteriori (MAP)
estimation framework for image enhancement. This is achieved by performing
gradient ascent in the diffusion latent space rather than in the raw pixel
domain, leveraging a pretrained differentiable and bijective diffusion process.
Likely, different NR-IQA models lead to different enhanced outputs, which in
turn provides a new computational means of comparing them. Unlike conventional
correlation-based measures, our comparison method offers complementary insights
into the respective strengths and weaknesses of the competing NR-IQA models in
perceptual optimization scenarios. Additionally, we aim to improve the
best-performing NR-IQA model in diffusion latent MAP estimation by
incorporating the advantages of other top-performing methods. The resulting
model delivers noticeably better results in enhancing real-world images
afflicted by unknown and complex distortions, all preserving a high degree of
image fidelity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex
  and Professional Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01505v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01505v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Li, Andong Deng, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Mohammed Bennamoun, Qiuhong Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning over sports videos for question answering is an important task with
numerous applications, such as player training and information retrieval.
However, this task has not been explored due to the lack of relevant datasets
and the challenging nature it presents. Most datasets for video question
answering (VideoQA) focus mainly on general and coarse-grained understanding of
daily-life videos, which is not applicable to sports scenarios requiring
professional action understanding and fine-grained motion analysis. In this
paper, we introduce the first dataset, named Sports-QA, specifically designed
for the sports VideoQA task. The Sports-QA dataset includes various types of
questions, such as descriptions, chronologies, causalities, and counterfactual
conditions, covering multiple sports. Furthermore, to address the
characteristics of the sports VideoQA task, we propose a new Auto-Focus
Transformer (AFT) capable of automatically focusing on particular scales of
temporal information for question answering. We conduct extensive experiments
on Sports-QA, including baseline studies and the evaluation of different
methods. The results demonstrate that our AFT achieves state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximizing Uncertainty for Federated learning via Bayesian
  Optimisation-based Model Poisoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marios Aristodemou, Xiaolan Liu, Yuan Wang, Konstantinos G. Kyriakopoulos, Sangarapillai Lambotharan, Qingsong Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we transition from Narrow Artificial Intelligence towards Artificial Super
Intelligence, users are increasingly concerned about their privacy and the
trustworthiness of machine learning (ML) technology. A common denominator for
the metrics of trustworthiness is the quantification of uncertainty inherent in
DL algorithms, and specifically in the model parameters, input data, and model
predictions. One of the common approaches to address privacy-related issues in
DL is to adopt distributed learning such as federated learning (FL), where
private raw data is not shared among users. Despite the privacy-preserving
mechanisms in FL, it still faces challenges in trustworthiness. Specifically,
the malicious users, during training, can systematically create malicious model
parameters to compromise the models predictive and generative capabilities,
resulting in high uncertainty about their reliability. To demonstrate malicious
behaviour, we propose a novel model poisoning attack method named Delphi which
aims to maximise the uncertainty of the global model output. We achieve this by
taking advantage of the relationship between the uncertainty and the model
parameters of the first hidden layer of the local model. Delphi employs two
types of optimisation , Bayesian Optimisation and Least Squares Trust Region,
to search for the optimal poisoned model parameters, named as Delphi-BO and
Delphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise
the distance of the predictive probability distribution towards an uncertain
distribution of model output. Furthermore, we establish a mathematical proof
for the attack effectiveness demonstrated in FL. Numerical results demonstrate
that Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR
highlighting vulnerability of FL systems to model poisoning attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MGF: Mixed Gaussian Flow for Diverse Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahe Chen, Jinkun Cao, Dahua Lin, Kris Kitani, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To predict future trajectories, the normalizing flow with a standard Gaussian
prior suffers from weak diversity. The ineffectiveness comes from the conflict
between the fact of asymmetric and multi-modal distribution of likely outcomes
and symmetric and single-modal original distribution and supervision losses.
Instead, we propose constructing a mixed Gaussian prior for a normalizing flow
model for trajectory prediction. The prior is constructed by analyzing the
trajectory patterns in the training samples without requiring extra annotations
while showing better expressiveness and being multi-modal and asymmetric.
Besides diversity, it also provides better controllability for probabilistic
trajectory generation. We name our method Mixed Gaussian Flow (MGF). It
achieves state-of-the-art performance in the evaluation of both trajectory
alignment and diversity on the popular UCY/ETH and SDD datasets. Code is
available at https://github.com/mulplue/MGF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurips 2024. Code: https://github.com/mulplue/MGF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask-guided cross-image attention for zero-shot in-silico
  histopathologic image generation with a diffusion model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11664v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11664v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Winter, Nicolas Triltsch, Marco Rosati, Anatoliy Shumilov, Ziya Kokaragac, Yuri Popov, Thomas Padel, Laura Sebastian Monasor, Ross Hill, Markus Schick, Nicolas Brieu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating in-silico data with generative AI promises a cost-effective
alternative to staining, imaging, and annotating whole slide images in
computational pathology. Diffusion models are the state-of-the-art solution for
generating in-silico images, offering unparalleled fidelity and realism. Using
appearance transfer diffusion models allows for zero-shot image generation,
facilitating fast application and making model training unnecessary. However
current appearance transfer diffusion models are designed for natural images,
where the main task is to transfer the foreground object from an origin to a
target domain, while the background is of insignificant importance. In
computational pathology, specifically in oncology, it is however not
straightforward to define which objects in an image should be classified as
foreground and background, as all objects in an image may be of critical
importance for the detailed understanding the tumor micro-environment. We
contribute to the applicability of appearance transfer diffusion models to
immunohistochemistry-stained images by modifying the appearance transfer
guidance to alternate between class-specific AdaIN feature statistics matchings
using existing segmentation masks. The performance of the proposed method is
demonstrated on the downstream task of supervised epithelium segmentation,
showing that the number of manual annotations required for model training can
be reduced by 75%, outperforming the baseline approach. Additionally, we
consulted with a certified pathologist to investigate future improvements. We
anticipate this work to inspire the application of zero-shot diffusion models
in computational pathology, providing an efficient method to generate in-silico
images with unmatched fidelity and realism, which prove meaningful for
downstream tasks, such as training existing deep learning models or finetuning
foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoHan: Robust Hand Detection in Operation Room 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roi Papo, Sapir Gershov, Tom Friedman, Itay Or, Gil Bolotin, Shlomi Laufer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-specific localization has garnered significant interest within the
computer vision community. Although there are numerous datasets with hand
annotations from various angles and settings, domain transfer techniques
frequently struggle in surgical environments. This is mainly due to the limited
availability of gloved hand instances and the unique challenges of operating
rooms (ORs). Thus, hand-detection models tailored to OR settings require
extensive training and expensive annotation processes. To overcome these
challenges, we present "RoHan" - a novel approach for robust hand detection in
the OR, leveraging advanced semi-supervised domain adaptation techniques to
tackle the challenges of varying recording conditions, diverse glove colors,
and occlusions common in surgical settings. Our methodology encompasses two
main stages: (1) data augmentation strategy that utilizes "Artificial Gloves,"
a method for augmenting publicly available hand datasets with synthetic images
of hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that
improves detection performance in real-world OR settings through iterative
prediction refinement and efficient frame filtering. We evaluate our method
using two datasets: simulated enterotomy repair and saphenous vein graft
harvesting. "RoHan" substantially reduces the need for extensive labeling and
model training, paving the way for the practical implementation of hand
detection technologies in medical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-based Unsupervised Audio-visual Speech Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Eudes Ayilo, Mostafa Sadeghi, Romain Serizel, Xavier Alameda-Pineda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new unsupervised audio-visual speech enhancement (AVSE)
approach that combines a diffusion-based audio-visual speech generative model
with a non-negative matrix factorization (NMF) noise model. First, the
diffusion model is pre-trained on clean speech conditioned on corresponding
video data to simulate the speech generative distribution. This pre-trained
model is then paired with the NMF-based noise model to estimate clean speech
iteratively. Specifically, a diffusion-based posterior sampling approach is
implemented within the reverse diffusion process, where after each iteration, a
speech estimate is obtained and used to update the noise parameters.
Experimental results confirm that the proposed AVSE approach not only
outperforms its audio-only counterpart but also generalizes better than a
recent supervised-generative AVSE method. Additionally, the new inference
algorithm offers a better balance between inference speed and performance
compared to the previous diffusion-based method. Code and demo available at:
https://jeaneudesayilo.github.io/fast_UdiffSE
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Pain Classification using Spatio-Temporal Deep Learning
  Approaches with Facial Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aafaf Ridouan, Amine Bohi, Youssef Mourchid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pain management and severity detection are crucial for effective treatment,
yet traditional self-reporting methods are subjective and may be unsuitable for
non-verbal individuals (people with limited speaking skills). To address this
limitation, we explore automated pain detection using facial expressions. Our
study leverages deep learning techniques to improve pain assessment by
analyzing facial images from the Pain Emotion Faces Database (PEMF). We propose
two novel approaches1: (1) a hybrid ConvNeXt model combined with Long
Short-Term Memory (LSTM) blocks to analyze video frames and predict pain
presence, and (2) a Spatio-Temporal Graph Convolution Network (STGCN)
integrated with LSTM to process landmarks from facial images for pain
detection. Our work represents the first use of the PEMF dataset for binary
pain classification and demonstrates the effectiveness of these models through
extensive experimentation. The results highlight the potential of combining
spatial and temporal features for enhanced pain detection, offering a promising
advancement in objective pain assessment methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 3 tables. Accepted and presented at the 18th
  International Conference on Machine Vision (ICMV 2024), Edinburgh, UK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multispectral Pedestrian Detection with Sparsely Annotated Label <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan Lee, Seungho Shin, Gyeong-Moon Park, Jung Uk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although existing Sparsely Annotated Object Detection (SAOD) approches have
made progress in handling sparsely annotated environments in multispectral
domain, where only some pedestrians are annotated, they still have the
following limitations: (i) they lack considerations for improving the quality
of pseudo-labels for missing annotations, and (ii) they rely on fixed ground
truth annotations, which leads to learning only a limited range of pedestrian
visual appearances in the multispectral domain. To address these issues, we
propose a novel framework called Sparsely Annotated Multispectral Pedestrian
Detection (SAMPD). For limitation (i), we introduce Multispectral
Pedestrian-aware Adaptive Weight (MPAW) and Positive Pseudo-label Enhancement
(PPE) module. Utilizing multispectral knowledge, these modules ensure the
generation of high-quality pseudo-labels and enable effective learning by
increasing weights for high-quality pseudo-labels based on modality
characteristics. To address limitation (ii), we propose an Adaptive Pedestrian
Retrieval Augmentation (APRA) module, which adaptively incorporates pedestrian
patches from ground-truth and dynamically integrates high-quality pseudo-labels
with the ground-truth, facilitating a more diverse learning pool of
pedestrians. Extensive experimental results demonstrate that our SAMPD
significantly enhances performance in sparsely annotated environments within
the multispectral domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximation properties relative to continuous scale space for hybrid
  discretizations of Gaussian derivative operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05095v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05095v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Lindeberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an analysis of properties of two hybrid discretization
methods for Gaussian derivatives, based on convolutions with either the
normalized sampled Gaussian kernel or the integrated Gaussian kernel followed
by central differences. The motivation for studying these discretization
methods is that in situations when multiple spatial derivatives of different
order are needed at the same scale level, they can be computed significantly
more efficiently compared to more direct derivative approximations based on
explicit convolutions with either sampled Gaussian kernels or integrated
Gaussian kernels.
  While these computational benefits do also hold for the genuinely discrete
approach for computing discrete analogues of Gaussian derivatives, based on
convolution with the discrete analogue of the Gaussian kernel followed by
central differences, the underlying mathematical primitives for the discrete
analogue of the Gaussian kernel, in terms of modified Bessel functions of
integer order, may not be available in certain frameworks for image processing,
such as when performing deep learning based on scale-parameterized filters in
terms of Gaussian derivatives, with learning of the scale levels.
  In this paper, we present a characterization of the properties of these
hybrid discretization methods, in terms of quantitative performance measures
concerning the amount of spatial smoothing that they imply, as well as the
relative consistency of scale estimates obtained from scale-invariant feature
detectors with automatic scale selection, with an emphasis on the behaviour for
very small values of the scale parameter, which may differ significantly from
corresponding results obtained from the fully continuous scale-space theory, as
well as between different types of discretization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:2311.11317</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OminiControl: Minimal and Universal Control for Diffusion Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15098v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15098v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce OminiControl, a highly versatile and
parameter-efficient framework that integrates image conditions into pre-trained
Diffusion Transformer (DiT) models. At its core, OminiControl leverages a
parameter reuse mechanism, enabling the DiT to encode image conditions using
itself as a powerful backbone and process them with its flexible multi-modal
attention processors. Unlike existing methods, which rely heavily on additional
encoder modules with complex architectures, OminiControl (1) effectively and
efficiently incorporates injected image conditions with only ~0.1% additional
parameters, and (2) addresses a wide range of image conditioning tasks in a
unified manner, including subject-driven generation and spatially-aligned
conditions such as edges, depth, and more. Remarkably, these capabilities are
achieved by training on images generated by the DiT itself, which is
particularly beneficial for subject-driven generation. Extensive evaluations
demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted
models in both subject-driven and spatially-aligned conditional generation.
Additionally, we release our training dataset, Subjects200K, a diverse
collection of over 200,000 identity-consistent images, along with an efficient
data synthesis pipeline to advance research in subject-consistent generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhao, Tingwei Chen, Zhijie Cai, Xiaoyang Li, Hang Li, Qimei Chen, Guangxu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Wi-Fi sensing has garnered significant attention due to its
numerous benefits, such as privacy protection, low cost, and penetration
ability. Extensive research has been conducted in this field, focusing on areas
such as gesture recognition, people identification, and fall detection.
However, many data-driven methods encounter challenges related to domain shift,
where the model fails to perform well in environments different from the
training data. One major factor contributing to this issue is the limited
availability of Wi-Fi sensing datasets, which makes models learn excessive
irrelevant information and over-fit to the training set. Unfortunately,
collecting large-scale Wi-Fi sensing datasets across diverse scenarios is a
challenging task. To address this problem, we propose CrossFi, a siamese
network-based approach that excels in both in-domain scenario and cross-domain
scenario, including few-shot, zero-shot scenarios, and even works in few-shot
new-class scenario where testing set contains new categories. The core
component of CrossFi is a sample-similarity calculation network called CSi-Net,
which improves the structure of the siamese network by using an attention
mechanism to capture similarity information, instead of simply calculating the
distance or cosine similarity. Based on it, we develop an extra Weight-Net that
can generate a template for each class, so that our CrossFi can work in
different scenarios. Experimental results demonstrate that our CrossFi achieves
state-of-the-art performance across various scenarios. In gesture recognition
task, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72%
in one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario,
and 84.75% in one-shot new-class scenario. The code for our model is publicly
available at https://github.com/RS2002/CrossFi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple Information <span class="highlight-title">Prompt</span> Learning for Cloth-Changing Person
  Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengxun Wei, Zan Gao, Chunjie Ma, Yibo Zhao, Weili Guan, Shengyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-changing person re-identification is a subject closer to the real
world, which focuses on solving the problem of person re-identification after
pedestrians change clothes. The primary challenge in this field is to overcome
the complex interplay between intra-class and inter-class variations and to
identify features that remain unaffected by changes in appearance. Sufficient
data collection for model training would significantly aid in addressing this
problem. However, it is challenging to gather diverse datasets in practice.
Current methods focus on implicitly learning identity information from the
original image or introducing additional auxiliary models, which are largely
limited by the quality of the image and the performance of the additional
model. To address these issues, inspired by prompt learning, we propose a novel
multiple information prompt learning (MIPL) scheme for cloth-changing person
ReID, which learns identity robust features through the common prompt guidance
of multiple messages. Specifically, the clothing information stripping (CIS)
module is designed to decouple the clothing information from the original RGB
image features to counteract the influence of clothing appearance. The
Bio-guided attention (BGA) module is proposed to increase the learning
intensity of the model for key information. A dual-length hybrid patch (DHP)
module is employed to make the features have diverse coverage to minimize the
impact of feature bias. Extensive experiments demonstrate that the proposed
method outperforms all state-of-the-art methods on the LTCC, Celeb-reID,
Celeb-reID-light, and CSCC datasets, achieving rank-1 scores of 74.8%, 73.3%,
66.0%, and 88.1%, respectively. When compared to AIM (CVPR23), ACID (TIP23),
and SCNet (MM23), MIPL achieves rank-1 improvements of 11.3%, 13.8%, and 7.9%,
respectively, on the PRCC dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Silent Majority: Demystifying Memorization Effect in the Presence of
  Spurious Correlations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu You, Haocheng Dai, Yifei Min, Jasjeet S. Sekhon, Sarang Joshi, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models often rely on simple spurious features -- patterns in
training data that correlate with targets but are not causally related to them,
like image backgrounds in foreground classification. This reliance typically
leads to imbalanced test performance across minority and majority groups. In
this work, we take a closer look at the fundamental cause of such imbalanced
performance through the lens of memorization, which refers to the ability to
predict accurately on \textit{atypical} examples (minority groups) in the
training set but failing in achieving the same accuracy in the testing set.
This paper systematically shows the ubiquitous existence of spurious features
in a small set of neurons within the network, providing the first-ever evidence
that memorization may contribute to imbalanced group performance. Through three
experimental sources of converging empirical evidence, we find the property of
a small subset of neurons or channels in memorizing minority group information.
Inspired by these findings, we articulate the hypothesis: the imbalanced group
performance is a byproduct of ``noisy'' spurious memorization confined to a
small set of neurons. To further substantiate this hypothesis, we show that
eliminating these unnecessary spurious memorization patterns via a novel
framework during training can significantly affect the model performance on
minority groups. Our experimental results across various architectures and
benchmarks offer new insights on how neural networks encode core and spurious
knowledge, laying the groundwork for future research in demystifying robustness
to spurious correlation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DATransNet: Dynamic Attention Transformer Network for Infrared Small
  Target Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19599v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19599v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Hu, Yian Huang, Kexuan Li, Luping Zhang, Chang Long, Yiming Zhu, Tian Pu, Zhenming Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target detection (ISTD) is widely used in civilian and
military applications. However, ISTD encounters several challenges, including
the tendency for small and dim targets to be obscured by complex backgrounds.To
address this issue, we propose the Dynamic Attention Transformer Network
(DATransNet), which aims to extract and preserve edge information of small
targets.DATransNet employs the Dynamic Attention Transformer (DATrans),
simulating central difference convolutions (CDC) to extract and integrate
gradient features with deeper features.Furthermore, we propose a global feature
extraction module (GFEM) that offers a comprehensive perspective to prevent the
network from focusing solely on details while neglecting the background
information. We compare the network with state-of-the-art (SOTA) approaches,
and the results demonstrate that our method performs effectively. Our source
code is available at https://github.com/greekinRoma/DATransNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ultra-High-Definition Image Deblurring via Multi-scale Cubic-Mixer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchi Chen, Xiuyi Jia, Zhuoran Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, transformer-based algorithms are making a splash in the domain of
image deblurring. Their achievement depends on the self-attention mechanism
with CNN stem to model long range dependencies between tokens. Unfortunately,
this ear-pleasing pipeline introduces high computational complexity and makes
it difficult to run an ultra-high-definition image on a single GPU in real
time. To trade-off accuracy and efficiency, the input degraded image is
computed cyclically over three dimensional ($C$, $W$, and $H$) signals without
a self-attention mechanism. We term this deep network as Multi-scale
Cubic-Mixer, which is acted on both the real and imaginary components after
fast Fourier transform to estimate the Fourier coefficients and thus obtain a
deblurred image. Furthermore, we combine the multi-scale cubic-mixer with a
slicing strategy to generate high-quality results at a much lower computational
cost. Experimental results demonstrate that the proposed algorithm performs
favorably against the state-of-the-art deblurring approaches on the several
benchmarks and a new ultra-high-definition dataset in terms of accuracy and
speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot Video Restoration and Enhancement Using <span class="highlight-title">Pre-Train</span>ed Image
  Diffusion Model <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Cao, Huanjing Yue, Xin Liu, Jingyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based zero-shot image restoration and enhancement models have
achieved great success in various tasks of image restoration and enhancement.
However, directly applying them to video restoration and enhancement results in
severe temporal flickering artifacts. In this paper, we propose the first
framework for zero-shot video restoration and enhancement based on the
pre-trained image diffusion model. By replacing the spatial self-attention
layer with the proposed short-long-range (SLR) temporal attention layer, the
pre-trained image diffusion model can take advantage of the temporal
correlation between frames. We further propose temporal consistency guidance,
spatial-temporal noise sharing, and an early stopping sampling strategy to
improve temporally consistent sampling. Our method is a plug-and-play module
that can be inserted into any diffusion-based image restoration or enhancement
methods to further improve their performance. Experimental results demonstrate
the superiority of our proposed method. Our code is available at
https://github.com/cao-cong/ZVRD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous Concepts Removal in Text-to-image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingxu Han, Weisong Sun, Yanrong Hu, Chunrong Fang, Yonglong Zhang, Shiqing Ma, Tao Zheng, Zhenyu Chen, Zhenting Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have shown an impressive ability to generate
high-quality images from input textual descriptions. However, concerns have
been raised about the potential for these models to create content that
infringes on copyrights or depicts disturbing subject matter. Removing specific
concepts from these models is a promising potential solution to this problem.
However, existing methods for concept removal do not work well in practical but
challenging scenarios where concepts need to be continuously removed.
Specifically, these methods lead to poor alignment between the text prompts and
the generated image after the continuous removal process. To address this
issue, we propose a novel approach called CCRT that includes a designed
knowledge distillation paradigm. It constrains the text-image alignment
behavior during the continuous concept removal process by using a set of text
prompts generated through our genetic algorithm, which employs a designed
fuzzing strategy. We conduct extensive experiments involving the removal of
various concepts. The results evaluated through both algorithmic metrics and
human studies demonstrate that our CCRT can effectively remove the targeted
concepts in a continuous manner while maintaining the high generation quality
(e.g., text-image alignment) of the model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal-in-the-Loop for Learning with Imbalanced Noisy Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Brandon Graham-Knight, Jamil Fayyad, Nourhan Bayasi, Patricia Lasserre, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance and label noise are pervasive in large-scale datasets, yet
much of machine learning research assumes well-labeled, balanced data, which
rarely reflects real world conditions. Existing approaches typically address
either label noise or class imbalance in isolation, leading to suboptimal
results when both issues coexist. In this work, we propose
Conformal-in-the-Loop (CitL), a novel training framework that addresses both
challenges with a conformal prediction-based approach. CitL evaluates sample
uncertainty to adjust weights and prune unreliable examples, enhancing model
resilience and accuracy with minimal computational cost. Our extensive
experiments include a detailed analysis showing how CitL effectively emphasizes
impactful data in noisy, imbalanced datasets. Our results show that CitL
consistently boosts model performance, achieving up to a 6.1% increase in
classification accuracy and a 5.0 mIoU improvement in segmentation. Our code is
publicly available: CitL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the Effect of Network Pruning on Performance and
  Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan von Rad, Florian Seuffert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) are often over-parameterized for their tasks and
can be compressed quite drastically by removing weights, a process called
pruning. We investigate the impact of different pruning techniques on the
classification performance and interpretability of GoogLeNet. We systematically
apply unstructured and structured pruning, as well as connection sparsity
(pruning of input weights) methods to the network and analyze the outcomes
regarding the network's performance on the validation set of ImageNet. We also
compare different retraining strategies, such as iterative pruning and one-shot
pruning. We find that with sufficient retraining epochs, the performance of the
networks can approximate the performance of the default GoogLeNet - and even
surpass it in some cases. To assess interpretability, we employ the Mechanistic
Interpretability Score (MIS) developed by Zimmermann et al. . Our experiments
reveal that there is no significant relationship between interpretability and
pruning rate when using MIS as a measure. Additionally, we observe that
networks with extremely low accuracy can still achieve high MIS scores,
suggesting that the MIS may not always align with intuitive notions of
interpretability, such as understanding the basis of correct decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make-A-Character 2: Anima<span class="highlight-title">table</span> 3D Character Generation From a Single
  Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Liu, Yutong Wang, Jiahao Chen, Jianfang Li, Tangli Xue, Longlong Li, Jianqiang Ren, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report introduces Make-A-Character 2, an advanced system for generating
high-quality 3D characters from single portrait photographs, ideal for game
development and digital human applications. Make-A-Character 2 builds upon its
predecessor by incorporating several significant improvements for image-based
head generation. We utilize the IC-Light method to correct non-ideal
illumination in input photos and apply neural network-based color correction to
harmonize skin tones between the photos and game engine renders. We also employ
the Hierarchical Representation Network to capture high-frequency facial
structures and conduct adaptive skeleton calibration for accurate and
expressive facial animations. The entire image-to-3D-character generation
process takes less than 2 minutes. Furthermore, we leverage transformer
architecture to generate co-speech facial and gesture actions, enabling
real-time conversation with the generated character. These technologies have
been integrated into our conversational AI avatar products.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal and Multi-scale Spatial Environment Understanding for
  Immersive Visual Text-to-Speech <span class="chip">AAAI'2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11409v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11409v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Liu, Shuwei He, Yifan Hu, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Text-to-Speech (VTTS) aims to take the environmental image as the
prompt to synthesize the reverberant speech for the spoken content. The
challenge of this task lies in understanding the spatial environment from the
image. Many attempts have been made to extract global spatial visual
information from the RGB space of an spatial image. However, local and depth
image information are crucial for understanding the spatial environment, which
previous works have ignored. To address the issues, we propose a novel
multi-modal and multi-scale spatial environment understanding scheme to achieve
immersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and
Depth spaces of the spatial image to learn more comprehensive spatial
information, and the multi-scale seeks to model the local and global spatial
knowledge simultaneously. Specifically, we first split the RGB and Depth images
into patches and adopt the Gemini-generated environment captions to guide the
local spatial understanding. After that, the multi-modal and multi-scale
features are integrated by the local-aware global spatial understanding. In
this way, M2SE-VTTS effectively models the interactions between local and
global spatial contexts in the multi-modal spatial environment. Objective and
subjective evaluations suggest that our model outperforms the advanced
baselines in environmental speech generation. The code and audio samples are
available at: https://github.com/AI-S2-Lab/M2SE-VTTS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,2 figures, Accepted by AAAI'2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Context Temporal Consistent Modeling for Referring Video Object
  Segmentation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sun-Hyuk Choi, Hayoung Jo, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring video object segmentation aims to segment objects within a video
corresponding to a given text description. Existing transformer-based temporal
modeling approaches face challenges related to query inconsistency and the
limited consideration of context. Query inconsistency produces unstable masks
of different objects in the middle of the video. The limited consideration of
context leads to the segmentation of incorrect objects by failing to adequately
account for the relationship between the given text and instances. To address
these issues, we propose the Multi-context Temporal Consistency Module (MTCM),
which consists of an Aligner and a Multi-Context Enhancer (MCE). The Aligner
removes noise from queries and aligns them to achieve query consistency. The
MCE predicts text-relevant queries by considering multi-context. We applied
MTCM to four different models, increasing performance across all of them,
particularly achieving 47.6 J&F on the MeViS. Code is available at
https://github.com/Choi58/MTCM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comment: Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Noise-Tolerant Network for Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike image classification and annotation, for which deep network models
have achieved dominating superior performances compared to traditional computer
vision algorithms, deep learning for automatic image segmentation still faces
critical challenges. One of such hurdles is to obtain ground-truth
segmentations as the training labels for deep network training. Especially when
we study biomedical images, such as histopathological images (histo-images), it
is unrealistic to ask for manual segmentation labels as the ground truth for
training due to the fine image resolution as well as the large image size and
complexity. In this paper, instead of relying on clean segmentation labels, we
study whether and how integrating imperfect or noisy segmentation results from
off-the-shelf segmentation algorithms may help achieve better segmentation
results through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend
the noisy label deep learning to image segmentation with two novel aspects: (1)
multiple noisy labels can be integrated into one deep learning model; (2) noisy
segmentation modeling, including probabilistic parameters, is adaptive,
depending on the given testing image appearance. Implementation of the new ANTN
model on both the synthetic data and real-world histo-images demonstrates its
effectiveness and superiority over off-the-shelf and other existing
deep-learning-based image segmentation algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Long Video Tokenization via Coordinate-based Patch
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiwon Jang, Sihyun Yu, Jinwoo Shin, Pieter Abbeel, Younggyo Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient tokenization of videos remains a challenge in training vision
models that can process long videos. One promising direction is to develop a
tokenizer that can encode long video clips, as it would enable the tokenizer to
leverage the temporal coherence of videos better for tokenization. However,
training existing tokenizers on long videos often incurs a huge training cost
as they are trained to reconstruct all the frames at once. In this paper, we
introduce CoordTok, a video tokenizer that learns a mapping from
coordinate-based representations to the corresponding patches of input videos,
inspired by recent advances in 3D generative models. In particular, CoordTok
encodes a video into factorized triplane representations and reconstructs
patches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows
for training large tokenizer models directly on long videos without requiring
excessive training resources. Our experiments show that CoordTok can
drastically reduce the number of tokens for encoding long video clips. For
instance, CoordTok can encode a 128-frame video with 128$\times$128 resolution
into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar
reconstruction quality. We further show that this efficient video tokenization
enables memory-efficient training of a diffusion transformer that can generate
128 frames at once.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available on the project webpage:
  https://huiwon-jang.github.io/coordtok/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unifying Information-theoretic Perspective on Evaluating Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Fox, Samarth Swarup, Abhijin Adiga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering the difficulty of interpreting generative model output, there is
significant current research focused on determining meaningful evaluation
metrics. Several recent approaches utilize "precision" and "recall," borrowed
from the classification domain, to individually quantify the output fidelity
(realism) and output diversity (representation of the real data variation),
respectively. With the increase in metric proposals, there is a need for a
unifying perspective, allowing for easier comparison and clearer explanation of
their benefits and drawbacks. To this end, we unify a class of
kth-nearest-neighbors (kNN)-based metrics under an information-theoretic lens
using approaches from kNN density estimation. Additionally, we propose a
tri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall
Cross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity
and two distinct aspects of diversity, inter- and intra-class. Our
domain-agnostic metric, derived from the information-theoretic concepts of
entropy and cross-entropy, can be dissected for both sample- and mode-level
analysis. Our detailed experimental results demonstrate the sensitivity of our
metric components to their respective qualities and reveal undesirable
behaviors of other metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Skin Disease Diagnosis: Interpre<span class="highlight-title">table</span> Visual Concept Discovery
  with SAM <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Hu, Janet Wang, Jihun Hamm, Rie R Yotsu, Zhengming Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current AI-assisted skin image diagnosis has achieved dermatologist-level
performance in classifying skin cancer, driven by rapid advancements in deep
learning architectures. However, unlike traditional vision tasks, skin images
in general present unique challenges due to the limited availability of
well-annotated datasets, complex variations in conditions, and the necessity
for detailed interpretations to ensure patient safety. Previous segmentation
methods have sought to reduce image noise and enhance diagnostic performance,
but these techniques require fine-grained, pixel-level ground truth masks for
training. In contrast, with the rise of foundation models, the Segment Anything
Model (SAM) has been introduced to facilitate promptable segmentation, enabling
the automation of the segmentation process with simple yet effective prompts.
Efforts applying SAM predominantly focus on dermatoscopy images, which present
more easily identifiable lesion boundaries than clinical photos taken with
smartphones. This limitation constrains the practicality of these approaches to
real-world applications. To overcome the challenges posed by noisy clinical
photos acquired via non-standardized protocols and to improve diagnostic
accessibility, we propose a novel Cross-Attentive Fusion framework for
interpretable skin lesion diagnosis. Our method leverages SAM to generate
visual concepts for skin diseases using prompts, integrating local visual
concepts with global image features to enhance model performance. Extensive
evaluation on two skin disease datasets demonstrates our proposed method's
effectiveness on lesion diagnosis and interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View
  Synthesis <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11458v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11458v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method that achieves state-of-the-art rendering quality and
efficiency on monocular dynamic scene reconstruction using deformable 3D
Gaussians. Implicit deformable representations commonly model motion with a
canonical space and time-dependent backward-warping deformation field. Our
method, GauFRe, uses a forward-warping deformation to explicitly model
non-rigid transformations of scene geometry. Specifically, we propose a
template set of 3D Gaussians residing in a canonical space, and a
time-dependent forward-warping deformation field to model dynamic objects.
Additionally, we tailor a 3D Gaussian-specific static component supported by an
inductive bias-aware initialization approach which allows the deformation field
to focus on moving scene regions, improving the rendering of complex real-world
motion. The differentiable pipeline is optimized end-to-end with a
self-supervised rendering loss. Experiments show our method achieves
competitive results and higher efficiency than both previous state-of-the-art
NeRF and Gaussian-based methods. For real-world scenes, GauFRe can train in ~20
mins and offer 96 FPS real-time rendering on an RTX 3090 GPU. Project website:
https://lynl7130.github.io/gaufre/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025. 11 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early
  Knee Osteoarthritis Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Aladine Chetouani, Mohamed Jarraya, Yung Hsin Chen, Yuhua Ru, Fang Chen, Fabian Bauer, Liping Zhang, Didier Hans, Rachid Jennane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knee Osteoarthritis (KOA) is a common musculoskeletal condition that
significantly affects mobility and quality of life, particularly in elderly
populations. However, training deep learning models for early KOA
classification is often hampered by the limited availability of annotated
medical datasets, owing to the high costs and labour-intensive nature of data
labelling. Traditional data augmentation techniques, while useful, rely on
simple transformations and fail to introduce sufficient diversity into the
dataset. To address these challenges, we propose the Key-Exchange Convolutional
Auto-Encoder (KECAE) as an innovative Artificial Intelligence (AI)-based data
augmentation strategy for early KOA classification. Our model employs a
convolutional autoencoder with a novel key-exchange mechanism that generates
synthetic images by selectively exchanging key pathological features between
X-ray images, which not only diversifies the dataset but also ensures the
clinical validity of the augmented data. A hybrid loss function is introduced
to supervise feature learning and reconstruction, integrating multiple
components, including reconstruction, supervision, and feature separation
losses. Experimental results demonstrate that the KECAE-generated data
significantly improve the performance of KOA classification models, with
accuracy gains of up to 1.98% across various standard and state-of-the-art
architectures. Furthermore, a clinical validation study involving expert
radiologists confirms the anatomical plausibility and diagnostic realism of the
synthetic outputs. These findings highlight the potential of KECAE as a robust
tool for augmenting medical datasets in early KOA detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Activity Recognition in an Open World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derek S. Prijatelj, Samuel Grieggs, Jin Huang, Dawei Du, Ameya Shringi, Christopher Funk, Adam Kaufman, Eric Robertson, Walter J. Scheirer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Managing novelty in perception-based human activity recognition (HAR) is
critical in realistic settings to improve task performance over time and ensure
solution generalization outside of prior seen samples. Novelty manifests in HAR
as unseen samples, activities, objects, environments, and sensor changes, among
other ways. Novelty may be task-relevant, such as a new class or new features,
or task-irrelevant resulting in nuisance novelty, such as never before seen
noise, blur, or distorted video recordings. To perform HAR optimally,
algorithmic solutions must be tolerant to nuisance novelty, and learn over time
in the face of novelty. This paper 1) formalizes the definition of novelty in
HAR building upon the prior definition of novelty in classification tasks, 2)
proposes an incremental open world learning (OWL) protocol and applies it to
the Kinetics datasets to generate a new benchmark KOWL-718, 3) analyzes the
performance of current state-of-the-art HAR models when novelty is introduced
over time, 4) provides a containerized and packaged pipeline for reproducing
the OWL protocol and for modifying for any future updates to Kinetics. The
experimental analysis includes an ablation study of how the different models
perform under various conditions as annotated by Kinetics-AVA. The protocol as
an algorithm for reproducing experiments using the KOWL-718 benchmark will be
publicly released with code and containers at
https://github.com/prijatelj/human-activity-recognition-in-an-open-world. The
code may be used to analyze different annotations and subsets of the Kinetics
datasets in an incremental open world fashion, as well as be extended as
further updates to Kinetics are released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 16 figures, 3 tables. Published in JAIR 81 on Dec 20, 2024.
  All author affiliations are from during the paper's original funded work.
  Updated info and current emails are provided in this version's first page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence-Driven Deep Learning Framework for Early Detection of Knee
  Osteoarthritis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Aladine Chetouani, Yung Hsin Chen, Yuhua Ru, Fang Chen, Mohamed Jarraya, Fabian Bauer, Liping Zhang, Didier Hans, Rachid Jennane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knee Osteoarthritis (KOA) is a prevalent musculoskeletal disorder that
severely impacts mobility and quality of life, particularly among older adults.
Its diagnosis often relies on subjective assessments using the
Kellgren-Lawrence (KL) grading system, leading to variability in clinical
evaluations. To address these challenges, we propose a confidence-driven deep
learning framework for early KOA detection, focusing on distinguishing KL-0 and
KL-2 stages. The Siamese-based framework integrates a novel multi-level feature
extraction architecture with a hybrid loss strategy. Specifically, multi-level
Global Average Pooling (GAP) layers are employed to extract features from
varying network depths, ensuring comprehensive feature representation, while
the hybrid loss strategy partitions training samples into high-, medium-, and
low-confidence subsets. Tailored loss functions are applied to improve model
robustness and effectively handle uncertainty in annotations. Experimental
results on the Osteoarthritis Initiative (OAI) dataset demonstrate that the
proposed framework achieves competitive accuracy, sensitivity, and specificity,
comparable to those of expert radiologists. Cohen's kappa values (k > 0.85))
confirm substantial agreement, while McNemar's test (p > 0.05) indicates no
statistically significant differences between the model and radiologists.
Additionally, Confidence distribution analysis reveals that the model emulates
radiologists' decision-making patterns. These findings highlight the potential
of the proposed approach to serve as an auxiliary diagnostic tool, enhancing
early KOA detection and reducing clinical workload.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Novel Object Detection via Cooperative <span class="highlight-title">Foundation</span>al Models <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12068v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12068v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Bharadwaj, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at https://rohit901.github.io/coop-foundation-models/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MVTamperBench: Evaluating Robustness of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19794v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19794v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Agarwal, Srikant Panda, Angeline Charles, Bhargava Kumar, Hitesh Patel, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Vision-Language Models (VLMs) have enabled significant
progress in complex video understanding tasks. However, their robustness to
real-world manipulations remains underexplored, limiting their reliability in
critical applications. To address this gap, we introduce MVTamperBench, a
comprehensive benchmark designed to evaluate VLM's resilience to video
tampering effects, including rotation, dropping, masking, substitution, and
repetition. By systematically assessing state-of-the-art models, MVTamperBench
reveals substantial variability in robustness, with models like InternVL2-8B
achieving high performance, while others, such as Llama-VILA1.5-8B, exhibit
severe vulnerabilities. To foster broader adoption and reproducibility,
MVTamperBench is integrated into VLMEvalKit, a modular evaluation toolkit,
enabling streamlined testing and facilitating advancements in model robustness.
Our benchmark represents a critical step towards developing tamper-resilient
VLMs, ensuring their dependability in real-world scenarios.
  Project Page: https://amitbcp.github.io/MVTamperBench/
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Approach to Phase (Norm) <span class="highlight-title">Retrie</span>val Frames 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramin Farshchian, Rajab Ali Kamyabi-Gol, Fahimeh Arabyani-Neyshaburi, Fatemeh Esmaeelzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the properties of continuous frames, with a
particular focus on phase retrieval and norm retrieval in the context of
Hilbert spaces. We introduce the concept of continuous near-Riesz bases and
prove their invariance under invertible operators. Some equivalent conditions
for phase and norm retrieval property of continuous frames are presented. We
study the stability of phase retrieval under perturbations. Furthermore, tensor
product frames for separable Hilbert spaces are studied, and we establish the
equivalence of phase retrieval and norm retrieval properties between components
and their tensor products.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMDocIR: Benchmarking Multi-Modal <span class="highlight-title">Retrie</span>val for Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal document retrieval is designed to identify and retrieve various
forms of multi-modal content, such as figures, tables, charts, and layout
information from extensive documents. Despite its significance, there is a
notable lack of a robust benchmark to effectively evaluate the performance of
systems in multi-modal document retrieval. To address this gap, this work
introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks:
page-level and layout-level retrieval. The former focuses on localizing the
most relevant pages within a long document, while the latter targets the
detection of specific layouts, offering a more fine-grained granularity than
whole-page analysis. A layout can refer to a variety of elements such as
textual paragraphs, equations, figures, tables, or charts. The MMDocIR
benchmark comprises a rich dataset featuring expertly annotated labels for
1,685 questions and bootstrapped labels for 173,843 questions, making it a
pivotal resource for advancing multi-modal document retrieval for both training
and evaluation. Through rigorous experiments, we reveal that (i) visual
retrievers significantly outperform their text counterparts, (ii) MMDocIR train
set can effectively benefit the training process of multi-modal document
retrieval and (iii) text retrievers leveraging on VLM-text perform much better
than those using OCR-text. These findings underscores the potential advantages
of integrating visual elements for multi-modal document retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/MMDocIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding
  and Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianru Zhang, Li Ju, Prashant Singh, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing large-scale datasets, especially involving complex and
high-dimensional data like images, is particularly challenging. While
self-supervised learning (SSL) has proven effective for learning
representations from unlabelled data, it typically focuses on flat,
non-hierarchical structures, missing the multi-level relationships present in
many real-world datasets. Hierarchical clustering (HC) can uncover these
relationships by organizing data into a tree-like structure, but it often
relies on rigid similarity metrics that struggle to capture the complexity of
diverse data types. To address these we envision $\texttt{InfoHier}$, a
framework that combines SSL with HC to jointly learn robust latent
representations and hierarchical structures. This approach leverages SSL to
provide adaptive representations, enhancing HC's ability to capture complex
patterns. Simultaneously, it integrates HC loss to refine SSL training,
resulting in representations that are more attuned to the underlying
information hierarchy. $\texttt{InfoHier}$ has the potential to improve the
expressiveness and performance of both clustering and representation learning,
offering significant benefits for data analysis, management, and information
retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time <span class="highlight-title">Index</span>ing for Large-scale <span class="highlight-title">Recommend</span>ation by Streaming Vector
  Quantization <span class="highlight-title">Retrie</span>ver 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyan Bin, Jianfei Cui, Wujie Yan, Zhichen Zhao, Xintian Han, Chongyang Yan, Feng Zhang, Xun Zhou, Qi Wu, Zuotao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrievers, which form one of the most important recommendation stages, are
responsible for efficiently selecting possible positive samples to the later
stages under strict latency limitations. Because of this, large-scale systems
always rely on approximate calculations and indexes to roughly shrink candidate
scale, with a simple ranking model. Considering simple models lack the ability
to produce precise predictions, most of the existing methods mainly focus on
incorporating complicated ranking models. However, another fundamental problem
of index effectiveness remains unresolved, which also bottlenecks complication.
In this paper, we propose a novel index structure: streaming Vector
Quantization model, as a new generation of retrieval paradigm. Streaming VQ
attaches items with indexes in real time, granting it immediacy. Moreover,
through meticulous verification of possible variants, it achieves additional
benefits like index balancing and reparability, enabling it to support
complicated ranking models as existing approaches. As a lightweight and
implementation-friendly architecture, streaming VQ has been deployed and
replaced all major retrievers in Douyin and Douyin Lite, resulting in
remarkable user engagement gain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge <span class="highlight-title">Graph</span>-based <span class="highlight-title">Retrie</span>val-Augmented Generation for Schema Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuangtao Ma, Sriom Chakrabarti, Arijit Khan, Bálint Molnár
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional similarity-based schema matching methods are incapable of
resolving semantic ambiguities and conflicts in domain-specific complex mapping
scenarios due to missing commonsense and domain-specific knowledge. The
hallucination problem of large language models (LLMs) also makes it challenging
for LLM-based schema matching to address the above issues. Therefore, we
propose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema
Matching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces
novel vector-based, graph traversal-based, and query-based graph retrievals, as
well as a hybrid approach and ranking schemes that identify the most relevant
subgraphs from external large knowledge graphs (KGs). We showcase that KG-based
retrieval-augmented LLMs are capable of generating more accurate results for
complex matching cases without any re-training. Our experimental results show
that KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,
Jellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the
MIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the
pre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and
21.97% in terms of precision and F1 score on the Synthea dataset, respectively.
The results also demonstrate that our approach is more efficient in end-to-end
schema matching, and scales to retrieve from large KGs. Our case studies on the
dataset from the real-world schema matching scenario exhibit that the
hallucination problem of LLMs for schema matching is well mitigated by our
solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNMDR: Dynamic Networks and Multi-view Drug Representations for Safe
  Medication <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanlin Liu, Xiaomei Yu, Zihao Liu, Xue Li, Xingxu Fan, Xiangwei Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medication Recommendation (MR) is a promising research topic which booms
diverse applications in the healthcare and clinical domains. However, existing
methods mainly rely on sequential modeling and static graphs for representation
learning, which ignore the dynamic correlations in diverse medical events of a
patient's temporal visits, leading to insufficient global structural
exploration on nodes. Additionally, mitigating drug-drug interactions (DDIs) is
another issue determining the utility of the MR systems. To address the
challenges mentioned above, this paper proposes a novel MR method with the
integration of dynamic networks and multi-view drug representations (DNMDR).
Specifically, weighted snapshot sequences for dynamic heterogeneous networks
are constructed based on discrete visits in temporal EHRs, and all the dynamic
networks are jointly trained to gain both structural correlations in diverse
medical events and temporal dependency in historical health conditions, for
achieving comprehensive patient representations with both semantic features and
structural relationships. Moreover, combining the drug co-occurrences and
adverse drug-drug interactions (DDIs) in internal view of drug molecule
structure and interactive view of drug pairs, the safe drug representations are
available to obtain high-quality medication combination recommendation.
Finally, extensive experiments on real world datasets are conducted for
performance evaluation, and the experimental results demonstrate that the
proposed DNMDR method outperforms the state-of-the-art baseline models with a
large margin on various metrics such as PRAUC, Jaccard, DDI rates and so on.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding <span class="highlight-title">Retrie</span>val using LLM-based Listwise Rankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mandeep Rathee, Sean MacAvaney, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown strong promise as rerankers,
especially in ``listwise'' settings where an LLM is prompted to rerank several
search results at once. However, this ``cascading'' retrieve-and-rerank
approach is limited by the bounded recall problem: relevant documents not
retrieved initially are permanently excluded from the final ranking. Adaptive
retrieval techniques address this problem, but do not work with listwise
rerankers because they assume a document's score is computed independently from
other documents. In this paper, we propose an adaptation of an existing
adaptive retrieval method that supports the listwise setting and helps guide
the retrieval process itself (thereby overcoming the bounded recall problem for
LLM rerankers). Specifically, our proposed algorithm merges results both from
the initial ranking and feedback documents provided by the most relevant
documents seen up to that point. Through extensive experiments across diverse
LLM rerankers, first stage retrievers, and feedback sources, we demonstrate
that our method can improve nDCG@10 by up to 13.23% and recall by 28.02%--all
while keeping the total number of LLM inferences constant and overheads due to
the adaptive process minimal. The work opens the door to leveraging LLM-based
search in settings where the initial pool of results is limited, e.g., by
legacy systems, or by the cost of deploying a semantic first-stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic <span class="highlight-title">Retrie</span>val-Augmented Generation: A <span class="highlight-title">Survey</span> on Agentic RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized artificial intelligence (AI)
by enabling human like text generation and natural language understanding.
However, their reliance on static training data limits their ability to respond
to dynamic, real time queries, resulting in outdated or inaccurate outputs.
Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs
by integrating real time data retrieval to provide contextually relevant and
up-to-date responses. Despite its promise, traditional RAG systems are
constrained by static workflows and lack the adaptability required for
multistep reasoning and complex task management.
  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these
limitations by embedding autonomous AI agents into the RAG pipeline. These
agents leverage agentic design patterns reflection, planning, tool use, and
multiagent collaboration to dynamically manage retrieval strategies,
iteratively refine contextual understanding, and adapt workflows to meet
complex task requirements. This integration enables Agentic RAG systems to
deliver unparalleled flexibility, scalability, and context awareness across
diverse applications.
  This survey provides a comprehensive exploration of Agentic RAG, beginning
with its foundational principles and the evolution of RAG paradigms. It
presents a detailed taxonomy of Agentic RAG architectures, highlights key
applications in industries such as healthcare, finance, and education, and
examines practical implementation strategies. Additionally, it addresses
challenges in scaling these systems, ensuring ethical decision making, and
optimizing performance for real-world applications, while providing detailed
insights into frameworks and tools for implementing Agentic RAG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Robustness of Contrastive Learning Models for Medical
  Image-Report <span class="highlight-title">Retrie</span>val <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, Gongbo Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images and reports offer invaluable insights into patient health. The
heterogeneity and complexity of these data hinder effective analysis. To bridge
this gap, we investigate contrastive learning models for cross-domain
retrieval, which associates medical images with their corresponding clinical
reports. This study benchmarks the robustness of four state-of-the-art
contrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We
introduce an occlusion retrieval task to evaluate model performance under
varying levels of image corruption. Our findings reveal that all evaluated
models are highly sensitive to out-of-distribution data, as evidenced by the
proportional decrease in performance with increasing occlusion levels. While
MedCLIP exhibits slightly more robustness, its overall performance remains
significantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a
general-purpose dataset, struggles with medical image-report retrieval,
highlighting the importance of domain-specific training data. The evaluation of
this work suggests that more effort needs to be spent on improving the
robustness of these models. By addressing these limitations, we can develop
more reliable cross-domain retrieval models for medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is accepted to AAAI 2025 Workshop -- the 9th International
  Workshop on Health Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supply<span class="highlight-title">Graph</span>: A Benchmark <span class="highlight-title">Dataset</span> for Supply Chain Planning using <span class="highlight-title">Graph</span>
  Neural Networks <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15299v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15299v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, MD Shafikul Islam, Adipto Raihan Akib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained traction across different domains
such as transportation, bio-informatics, language processing, and computer
vision. However, there is a noticeable absence of research on applying GNNs to
supply chain networks. Supply chain networks are inherently graph-like in
structure, making them prime candidates for applying GNN methodologies. This
opens up a world of possibilities for optimizing, predicting, and solving even
the most complex supply chain problems. A major setback in this approach lies
in the absence of real-world benchmark datasets to facilitate the research and
resolution of supply chain problems using GNNs. To address the issue, we
present a real-world benchmark dataset for temporal tasks, obtained from one of
the leading FMCG companies in Bangladesh, focusing on supply chain planning for
production purposes. The dataset includes temporal data as node features to
enable sales predictions, production planning, and the identification of
factory issues. By utilizing this dataset, researchers can employ GNNs to
address numerous supply chain problems, thereby advancing the field of supply
chain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 4th workshop on Graphs and more Complex structures for
  Learning and Reasoning, colocated with AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusion <span class="highlight-title">Self-supervised</span> Learning for <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19692v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19692v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Lei Sang, Yi Zhang, Yiwen Zhang, Yun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely deployed in various web environments, and
self-supervised learning (SSL) has recently attracted significant attention in
this field. Contrastive learning (CL) stands out as a major SSL paradigm due to
its robust ability to generate self-supervised signals. Mainstream graph
contrastive learning (GCL)-based methods typically implement CL by creating
contrastive views through various data augmentation techniques. Despite these
methods are effective, we argue that there still exist several challenges. i)
Data augmentation ($e.g.,$ discarding edges or adding noise) necessitates
additional graph convolution (GCN) or modeling operations, which are highly
time-consuming and potentially harm the embedding quality. ii) Existing
CL-based methods use traditional CL objectives to capture self-supervised
signals. However, few studies have explored obtaining CL objectives from more
perspectives and have attempted to fuse the varying signals from these CL
objectives to enhance recommendation performance.
  To overcome these challenges, we propose a High-order Fusion Graph
Contrastive Learning (HFGCL) framework for recommendation. Specifically,
instead of facilitating data augmentations, we use high-order information from
GCN process to create contrastive views. Additionally, to integrate
self-supervised signals from various CL objectives, we propose an advanced CL
objective. By ensuring that positive pairs are distanced from negative samples
derived from both contrastive views, we effectively fuse self-supervised
signals from distinct CL objectives, thereby enhancing the mutual information
between positive pairs. Experimental results on three public datasets
demonstrate the superior recommendation performance and efficiency of HFGCL
compared to the state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">155</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Fast, Specialized Machine Learning Force Fields: Distilling
  <span class="highlight-title">Foundation</span> Models via Energy Hessians <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishan Amin, Sanjeev Raja, Aditi Krishnapriyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The foundation model (FM) paradigm is transforming Machine Learning Force
Fields (MLFFs), leveraging general-purpose representations and scalable
training to perform a variety of computational chemistry tasks. Although MLFF
FMs have begun to close the accuracy gap relative to first-principles methods,
there is still a strong need for faster inference speed. Additionally, while
research is increasingly focused on general-purpose models which transfer
across chemical space, practitioners typically only study a small subset of
systems at a given time. This underscores the need for fast, specialized MLFFs
relevant to specific downstream applications, which preserve test-time physical
soundness while maintaining train-time scalability. In this work, we introduce
a method for transferring general-purpose representations from MLFF foundation
models to smaller, faster MLFFs specialized to specific regions of chemical
space. We formulate our approach as a knowledge distillation procedure, where
the smaller "student" MLFF is trained to match the Hessians of the energy
predictions of the "teacher" foundation model. Our specialized MLFFs can be up
to 20 $\times$ faster than the original foundation model, while retaining, and
in some cases exceeding, its performance and that of undistilled models. We
also show that distilling from a teacher model with a direct force
parameterization into a student model trained with conservative forces (i.e.,
computed as derivatives of the potential energy) successfully leverages the
representations from the large-scale teacher for improved accuracy, while
maintaining energy conservation during test-time molecular dynamics
simulations. More broadly, our work suggests a new paradigm for MLFF
development, in which foundation models are released along with smaller,
specialized simulation "engines" for common chemical subsets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Stability Estimates in Adversarial Explainable AI through
  Alternate Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Burger, Charles Walter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in the effectiveness of machine learning models have come at the
cost of enormous complexity resulting in a poor understanding of how they
function. Local surrogate methods have been used to approximate the workings of
these complex models, but recent work has revealed their vulnerability to
adversarial attacks where the explanation produced is appreciably different
while the meaning and structure of the complex model's output remains similar.
This prior work has focused on the existence of these weaknesses but not on
their magnitude. Here we explore using an alternate search method with the goal
of finding minimum viable perturbations, the fewest perturbations necessary to
achieve a fixed similarity value between the original and altered text's
explanation. Intuitively, a method that requires fewer perturbations to expose
a given level of instability is inferior to one which requires more. This
nuance allows for superior comparisons of the stability of explainability
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 5 tables. arXiv admin note: text overlap with
  arXiv:2406.15839</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrystalGRW: Generative Modeling of Crystal Structures with Targeted
  Properties via Geodesic Random Walks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krit Tangsongcharoen, Teerachote Pakornchote, Chayanon Atthapak, Natthaphon Choomphon-anomakhun, Annop Ektarawong, Björn Alling, Christopher Sutton, Thiti Bovornratanaraks, Thiparat Chotibut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining whether a candidate crystalline material is thermodynamically
stable depends on identifying its true ground-state structure, a central
challenge in computational materials science. We introduce CrystalGRW, a
diffusion-based generative model on Riemannian manifolds that proposes novel
crystal configurations and can predict stable phases validated by density
functional theory. The crystal properties, such as fractional coordinates,
atomic types, and lattice matrices, are represented on suitable Riemannian
manifolds, ensuring that new predictions generated through the diffusion
process preserve the periodicity of crystal structures. We incorporate an
equivariant graph neural network to also account for rotational and
translational symmetries during the generation process. CrystalGRW demonstrates
the ability to generate realistic crystal structures that are close to their
ground states with accuracy comparable to existing models, while also enabling
conditional control, such as specifying a desired crystallographic point group.
These features help accelerate materials discovery and inverse design by
offering stable, symmetry-consistent crystal candidates for experimental
validation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10+12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VECT-GAN: A variationally encoded generative model for overcoming data
  scarcity in pharmaceutical science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Abdalla, Marrisa Taub, Eleanor Hilton, Priya Akkaraju, Alexander Milanovic, Mine Orlu, Abdul W. Basit, Michael T Cook, Tapabrata Chakraborty, David Shorthouse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in pharmaceutical research has led to reliance on
labour-intensive trial and error approaches for development rather than data
driven methods. While Machine Learning offers a solution, existing datasets are
often small and noisy, limiting their utility. To address this, we developed a
Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT
GAN), a novel generative model specifically designed for augmenting small,
noisy datasets. We introduce a pipeline where data is augmented before
regression model development and demonstrate that this consistently and
significantly improves performance over other state of the art tabular
generative models. We apply this pipeline across six pharmaceutical datasets,
and highlight its real-world applicability by developing novel polymers with
medically desirable mucoadhesive properties, which we made and experimentally
characterised. Additionally, we pre-train the model on the ChEMBL database of
drug-like molecules, leveraging knowledge distillation to enhance its
generalisability, making it readily available for use on pharmaceutical
datasets containing small molecules, which is an extremely common
pharmaceutical task. We demonstrate the power of synthetic data for
regularising small tabular datasets, highlighting its potential to become
standard practice in pharmaceutical model development, and make our method,
including VECT GAN pretrained on ChEMBL available as a pip package.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 primary figures, 3 supplementary figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trusted Machine Learning Models Unlock Private Inference for Problems
  Currently Infeasible with Crypto<span class="highlight-title">graph</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilia Shumailov, Daniel Ramage, Sarah Meiklejohn, Peter Kairouz, Florian Hartmann, Borja Balle, Eugene Bagdasarian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We often interact with untrusted parties. Prioritization of privacy can limit
the effectiveness of these interactions, as achieving certain goals
necessitates sharing private data. Traditionally, addressing this challenge has
involved either seeking trusted intermediaries or constructing cryptographic
protocols that restrict how much data is revealed, such as multi-party
computations or zero-knowledge proofs. While significant advances have been
made in scaling cryptographic approaches, they remain limited in terms of the
size and complexity of applications they can be used for. In this paper, we
argue that capable machine learning models can fulfill the role of a trusted
third party, thus enabling secure computations for applications that were
previously infeasible. In particular, we describe Trusted Capable Model
Environments (TCMEs) as an alternative approach for scaling secure computation,
where capable machine learning model(s) interact under input/output
constraints, with explicit information flow control and explicit statelessness.
This approach aims to achieve a balance between privacy and computational
efficiency, enabling private inference where classical cryptographic solutions
are currently infeasible. We describe a number of use cases that are enabled by
TCME, and show that even some simple classic cryptographic problems can already
be solved with TCME. Finally, we outline current limitations and discuss the
path forward in implementing them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Aware Risk Control for Intensity Modulated Radiation Therapies
  Quality Assurance with Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin He, David Adam, Sarah Han-Oh, Anqi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measurement quality assurance (QA) practices play a key role in the safe use
of Intensity Modulated Radiation Therapies (IMRT) for cancer treatment. These
practices have reduced measurement-based IMRT QA failure below 1%. However,
these practices are time and labor intensive which can lead to delays in
patient care. In this study, we examine how conformal prediction methodologies
can be used to robustly triage plans. We propose a new training-aware conformal
risk control method by combining the benefit of conformal risk control and
conformal training. We incorporate the decision making thresholds based on the
gamma passing rate, along with the risk functions used in clinical evaluation,
into the design of the risk control framework. Our method achieves high
sensitivity and specificity and significantly reduces the number of plans
needing measurement without generating a huge confidence interval. Our results
demonstrate the validity and applicability of conformal prediction methods for
improving efficiency and reducing the workload of the IMRT QA process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 Machine Learning for Health Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kolmogorov-Arnold Networks for Time Series Granger Causality Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiliang Liu, Yunfang Xu, Zijin Li, Zhengye Si, Xiaoxiao Yang, Xinyue Yang, Zhiwen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an
innovative architecture that extends the recently proposed Kolmogorov-Arnold
Networks (KAN) to the domain of causal inference. By extracting base weights
from KAN layers and incorporating the sparsity-inducing penalty along with
ridge regularization, GCKAN infers the Granger causality from time series while
enabling automatic time lag selection. Additionally, we propose an algorithm
leveraging time-reversed Granger causality to enhance inference accuracy. The
algorithm compares prediction and sparse-inducing losses derived from the
original and time-reversed series, automatically selecting the casual
relationship with the higher score or integrating the results to mitigate
spurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene
regulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the
proposed model achieves competitive performance to state-of-the-art methods in
inferring Granger causality from nonlinear, high-dimensional, and
limited-sample time series.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing Approximated Fixpoints via Dampened Mann Iteration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Baldan, Sebastian Gurke, Barbara König, Tommaso Padoan, Florian Wittbold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fixpoints are ubiquitous in computer science and when dealing with
quantitative semantics and verification one is commonly led to consider least
fixpoints of (higher-dimensional) functions over the nonnegative reals. We show
how to approximate the least fixpoint of such functions, focusing on the case
in which they are not known precisely, but represented by a sequence of
approximating functions that converge to them. We concentrate on monotone and
non-expansive functions, for which uniqueness of fixpoints is not guaranteed
and standard fixpoint iteration schemes might get stuck at a fixpoint that is
not the least. Our main contribution is the identification of an iteration
scheme, a variation of Mann iteration with a dampening factor, which, under
suitable conditions, is shown to guarantee convergence to the least fixpoint of
the function of interest. We then argue that these results are relevant in the
context of model-based reinforcement learning for Markov decision processes
(MDPs), showing that the proposed iteration scheme instantiates to MDPs and
allows us to derive convergence to the optimal expected return. More generally,
we show that our results can be used to iterate to the least fixpoint almost
surely for systems where the function of interest can be approximated with
given probabilistic error bounds, as it happens for probabilistic systems, such
as simple stochastic games, that can be explored via sampling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Reinforcement Learning Approach to Quiet and Safe UAM Traffic
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surya Murthy, John-Paul Clarke, Ufuk Topcu, Zhenyu Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban air mobility (UAM) is a transformative system that operates various
small aerial vehicles in urban environments to reshape urban transportation.
However, integrating UAM into existing urban environments presents a variety of
complex challenges. Recent analyses of UAM's operational constraints highlight
aircraft noise and system safety as key hurdles to UAM system implementation.
Future UAM air traffic management schemes must ensure that the system is both
quiet and safe. We propose a multi-agent reinforcement learning approach to
manage UAM traffic, aiming at both vertical separation assurance and noise
mitigation. Through extensive training, the reinforcement learning agent learns
to balance the two primary objectives by employing altitude adjustments in a
multi-layer UAM network. The results reveal the tradeoffs among noise impact,
traffic congestion, and separation. Overall, our findings demonstrate the
potential of reinforcement learning in mitigating UAM's noise impact while
maintaining safe separation using altitude adjustments
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper presented at SciTech 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Exploration of Large Language Models by Optimal
  Exploitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Grams, Patrick Betz, Christian Bartelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploration is a crucial skill for self-improvement and open-ended
problem-solving. However, it remains uncertain whether large language models
can effectively explore the state-space. Existing evaluations predominantly
focus on the trade-off between exploration and exploitation, often assessed in
multi-armed bandit problems. In contrast, this work isolates exploration as the
sole objective, tasking the agent with delivering information that enhances
future returns. For the evaluation, we propose to decompose missing rewards
into exploration and exploitation components by measuring the optimal
achievable return for the states already explored. Our experiments with various
LLMs reveal that most models struggle to sufficiently explore the state-space
and that weak exploration is insufficient. We observe a positive correlation
between model size and exploration performance, with larger models
demonstrating superior capabilities. Furthermore, we show that our
decomposition provides insights into differences in behaviors driven by agent
instructions during prompt engineering, offering a valuable tool for refining
LLM performance in exploratory tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Melt Pool Features and Spatter Using Symbolic Regression and
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olabode T. Ajenifujah, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Additive manufacturing (AM) is a rapidly evolving technology that has
attracted applications across a wide range of fields due to its ability to
fabricate complex geometries. However, one of the key challenges in AM is
achieving consistent print quality. This inconsistency is often attributed to
uncontrolled melt pool dynamics, partly caused by spatter which can lead to
defects. Therefore, capturing and controlling the evolution of the melt pool is
crucial for enhancing process stability and part quality. In this study, we
developed a framework to support decision-making in AM operations, facilitating
quality control and minimizing defects via machine learning (ML) and polynomial
symbolic regression models. We implemented experimentally validated
computational tools as a cost-effective approach to collect large datasets from
laser powder bed fusion (LPBF) processes. For a dataset consisting of 281
process conditions, parameters such as melt pool dimensions (length, width,
depth), melt pool geometry (area, volume), and volume indicated as spatter were
extracted. Using machine learning (ML) and polynomial symbolic regression
models, a high R2 of over 95 % was achieved in predicting the melt pool
dimensions and geometry features for both the training and testing datasets,
with either process conditions (power and velocity) or melt pool dimensions as
the model inputs. In the case of volume indicated as spatter, R2 improved after
logarithmic transforming the model inputs, which was either the process
conditions or the melt pool dimensions. Among the investigated ML models, the
ExtraTree model achieved the highest R2 values of 96.7 % and 87.5 %.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text
  Detection Challenge <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Dugan, Andrew Zhu, Firoj Alam, Preslav Nakov, Marianna Apidianaki, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there have been many shared tasks targeting the detection of
generated text from Large Language Models (LLMs). However, these shared tasks
tend to focus either on cases where text is limited to one particular domain or
cases where text can be from many domains, some of which may not be seen during
test time. In this shared task, using the newly released RAID benchmark, we aim
to answer whether or not models can detect generated text from a large, yet
fixed, number of domains and LLMs, all of which are seen during training. Over
the course of three months, our task was attempted by 9 teams with 23 detector
submissions. We find that multiple participants were able to obtain accuracies
of over 99% on machine-generated text from RAID while maintaining a 5% False
Positive Rate -- suggesting that detectors are able to robustly detect text
from many domains and models simultaneously. We discuss potential
interpretations of this result and provide directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projection Implicit Q-Learning with Support Constraint for Offline
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinchen Han, Hossam Afifi, Michel Marot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) faces a critical challenge of
extrapolation errors caused by out-of-distribution (OOD) actions. Implicit
Q-Learning (IQL) algorithm employs expectile regression to achieve in-sample
learning, effectively mitigating the risks associated with OOD actions.
However, the fixed hyperparameter in policy evaluation and density-based policy
improvement method limit its overall efficiency. In this paper, we propose
Proj-IQL, a projective IQL algorithm enhanced with the support constraint. In
the policy evaluation phase, Proj-IQL generalizes the one-step approach to a
multi-step approach through vector projection, while maintaining in-sample
learning and expectile regression framework. In the policy improvement phase,
Proj-IQL introduces support constraint that is more aligned with the policy
evaluation approach. Furthermore, we theoretically demonstrate that Proj-IQL
guarantees monotonic policy improvement and enjoys a progressively more
rigorous criterion for superior actions. Empirical results demonstrate the
Proj-IQL achieves state-of-the-art performance on D4RL benchmarks, especially
in challenging navigation domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT
  Scans: The C4R Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sneha N. Naik, Elsa D. Angelini, Eric A. Hoffman, Elizabeth C. Oelsner, R. Graham Barr, Benjamin M. Smith, Andrew F. Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ratio of airway tree lumen to lung size (ALR), assessed at full
inspiration on high resolution full-lung computed tomography (CT), is a major
risk factor for chronic obstructive pulmonary disease (COPD). There is growing
interest to infer ALR from cardiac CT images, which are widely available in
epidemiological cohorts, to investigate the relationship of ALR to severe
COVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously,
cardiac scans included approximately 2/3 of the total lung volume with 5-6x
greater slice thickness than high-resolution (HR) full-lung (FL) CT. In this
study, we present a novel attention-based Multi-view Swin Transformer to infer
FL ALR values from segmented cardiac CT scans. For the supervised training we
exploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of
Atherosclerosis (MESA). Our network significantly outperforms a proxy direct
ALR inference on segmented cardiac CT scans and achieves accuracy and
reproducibility comparable with a scan-rescan reproducibility of the FL ALR
ground-truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in Proceedings of International Symposium on
  Biomedical Imaging (ISBI), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Two-Stage <span class="highlight-title">Pretrain</span>ing-Finetuning Framework for Treatment Effect
  Estimation with Unmeasured Confounding <span class="chip">KDD 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Zhou, Yaxuan Li, Chunyuan Zheng, Haiteng Zhang, Min Zhang, Haoxuan Li, Mingming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the conditional average treatment effect (CATE) from observational
data plays a crucial role in areas such as e-commerce, healthcare, and
economics. Existing studies mainly rely on the strong ignorability assumption
that there are no unmeasured confounders, whose presence cannot be tested from
observational data and can invalidate any causal conclusion. In contrast, data
collected from randomized controlled trials (RCT) do not suffer from
confounding, but are usually limited by a small sample size. In this paper, we
propose a two-stage pretraining-finetuning (TSPF) framework using both
large-scale observational data and small-scale RCT data to estimate the CATE in
the presence of unmeasured confounding. In the first stage, a foundational
representation of covariates is trained to estimate counterfactual outcomes
through large-scale observational data. In the second stage, we propose to
train an augmented representation of the covariates, which is concatenated to
the foundational representation obtained in the first stage to adjust for the
unmeasured confounding. To avoid overfitting caused by the small-scale RCT data
in the second stage, we further propose a partial parameter initialization
approach, rather than training a separate network. The superiority of our
approach is validated on two public datasets with extensive experiments. The
code is available at https://github.com/zhouchuanCN/KDD25-TSPF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 25 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAC Learnability of Scenario Decision-Making Algorithms: Necessary and
  Sufficient Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume O. Berger, Raphaël M. Jungers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the PAC property of scenario decision-making algorithms, that is,
the ability to make a decision that has an arbitrarily low risk of violating an
unknown safety constraint, provided sufficiently many realizations (called
scenarios) of the safety constraint are sampled. Sufficient conditions for
scenario decision-making algorithms to be PAC are available in the literature,
such as finiteness of the VC dimension of its associated classifier and
existence of a compression scheme. We study the question of whether these
sufficient conditions are also necessary. We show with counterexamples that
this is not the case in general. This contrasts with binary classification
learning, for which the analogous conditions are sufficient and necessary.
Popular scenario decision-making algorithms, such as scenario optimization,
enjoy additional properties, such as stability and consistency. We show that
even under these additional assumptions the above conclusions hold. Finally, we
derive a necessary condition for scenario decision-making algorithms to be PAC,
inspired by the VC dimension and the so-called no-free-lunch theorem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Compression Bounds for Scenario Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume O. Berger, Raphaël M. Jungers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scenario decision making offers a flexible way of making decision in an
uncertain environment while obtaining probabilistic guarantees on the risk of
failure of the decision. The idea of this approach is to draw samples of the
uncertainty and make a decision based on the samples, called "scenarios". The
probabilistic guarantees take the form of a bound on the probability of
sampling a set of scenarios that will lead to a decision whose risk of failure
is above a given maximum tolerance. This bound can be expressed as a function
of the number of sampled scenarios, the maximum tolerated risk, and some
intrinsic property of the problem called the "compression size". Several such
bounds have been proposed in the literature under various assumptions on the
problem. We propose new bounds that improve upon the existing ones without
requiring stronger assumptions on the problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Increasing Batch Size Improves Convergence of Stochastic Gradient
  Descent with Momentum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keisuke Kamo, Hideaki Iiduka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient descent with momentum (SGDM), which is defined by adding
a momentum term to SGD, has been well studied in both theory and practice.
Theoretically investigated results showed that the settings of the learning
rate and momentum weight affect the convergence of SGDM. Meanwhile, practical
results showed that the setting of batch size strongly depends on the
performance of SGDM. In this paper, we focus on mini-batch SGDM with constant
learning rate and constant momentum weight, which is frequently used to train
deep neural networks in practice. The contribution of this paper is showing
theoretically that using a constant batch size does not always minimize the
expectation of the full gradient norm of the empirical loss in training a deep
neural network, whereas using an increasing batch size definitely minimizes it,
that is, increasing batch size improves convergence of mini-batch SGDM. We also
provide numerical results supporting our analyses, indicating specifically that
mini-batch SGDM with an increasing batch size converges to stationary points
faster than with a constant batch size. Python implementations of the
optimizers used in the numerical experiments are available at
https://anonymous.4open.science/r/momentum-increasing-batch-size-888C/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incrementally Learning Multiple Diverse Data Domains via Multi-Source
  Dynamic Expansion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runqing Wu, Fei Ye, Qihe Liu, Guoxi Huang, Jinyu Guo, Rongyao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning seeks to develop a model capable of incrementally
assimilating new information while retaining prior knowledge. However, current
research predominantly addresses a straightforward learning context, wherein
all data samples originate from a singular data domain. This paper shifts focus
to a more complex and realistic learning environment, characterized by data
samples sourced from multiple distinct domains. We tackle this intricate
learning challenge by introducing a novel methodology, termed the Multi-Source
Dynamic Expansion Model (MSDEM), which leverages various pre-trained models as
backbones and progressively establishes new experts based on them to adapt to
emerging tasks. Additionally, we propose an innovative dynamic expandable
attention mechanism designed to selectively harness knowledge from multiple
backbones, thereby accelerating the new task learning. Moreover, we introduce a
dynamic graph weight router that strategically reuses all previously acquired
parameters and representations for new task learning, maximizing the positive
knowledge transfer effect, which further improves generalization performance.
We conduct a comprehensive series of experiments, and the empirical findings
indicate that our proposed approach achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARMOR: Shielding Unlearnable Examples against Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueluan Gong, Yuji Wang, Yanjiao Chen, Haocheng Dong, Yiming Li, Mengyuan Sun, Shuaike Li, Qian Wang, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Private data, when published online, may be collected by unauthorized parties
to train deep neural networks (DNNs). To protect privacy, defensive noises can
be added to original samples to degrade their learnability by DNNs. Recently,
unlearnable examples are proposed to minimize the training loss such that the
model learns almost nothing. However, raw data are often pre-processed before
being used for training, which may restore the private information of protected
data. In this paper, we reveal the data privacy violation induced by data
augmentation, a commonly used data pre-processing technique to improve model
generalization capability, which is the first of its kind as far as we are
concerned. We demonstrate that data augmentation can significantly raise the
accuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To
address this issue, we propose a defense framework, dubbed ARMOR, to protect
data privacy from potential breaches of data augmentation. To overcome the
difficulty of having no access to the model training process, we design a
non-local module-assisted surrogate model that better captures the effect of
data augmentation. In addition, we design a surrogate augmentation selection
strategy that maximizes distribution alignment between augmented and
non-augmented samples, to choose the optimal augmentation strategy for each
class. We also use a dynamic step size adjustment algorithm to enhance the
defensive noise generation process. Extensive experiments are conducted on 4
datasets and 5 data augmentation methods to verify the performance of ARMOR.
Comparisons with 6 state-of-the-art defense methods have demonstrated that
ARMOR can preserve the unlearnability of protected private data under data
augmentation. ARMOR reduces the test accuracy of the model trained on augmented
protected samples by as much as 60% more than baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Phenotyping for Adolescent Mental Health: A Feasibility Study
  Employing Machine Learning to Predict Mental Health Risk From Active and
  Passive Smartphone Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Balasundaram Kadirvelu, Teresa Bellido Bel, Aglaia Freccero, Martina Di Simplicio, Dasha Nicholls, A Aldo Faisal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Adolescents are particularly vulnerable to mental disorders, with
over 75% of cases manifesting before the age of 25. Research indicates that
only 18 to 34% of young people experiencing high levels of depression or
anxiety symptoms seek support. Digital tools leveraging smartphones offer
scalable and early intervention opportunities. Objective: Using a novel machine
learning framework, this study evaluated the feasibility of integrating active
and passive smartphone data to predict mental disorders in non-clinical
adolescents. Specifically, we investigated the utility of the Mindcraft app in
predicting risks for internalising and externalising disorders, eating
disorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean
age 16.1 years) were recruited from three London schools. Participants
completed the Strengths and Difficulties Questionnaire, the Eating Disorders-15
Questionnaire, Sleep Condition Indicator Questionnaire and indicated the
presence/absence of suicidal ideation. They used the Mindcraft app for 14 days,
contributing active data via self-reports and passive data from smartphone
sensors. A contrastive pretraining phase was applied to enhance user-specific
feature stability, followed by supervised fine-tuning. The model evaluation
employed leave-one-subject-out cross-validation using balanced accuracy as the
primary metric. Results: The integration of active and passive data achieved
superior performance compared to individual data sources, with mean balanced
accuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal
ideation and 0.70 for eating disorders. The contrastive learning framework
stabilised daily behavioural representations, enhancing predictive robustness.
This study demonstrates the potential of integrating active and passive
smartphone data with advanced machine-learning techniques for predicting mental
health risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Graph</span> Counterfactual Explainable AI via Latent Space Traversal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Abildtrup Hansen, Paraskevas Pegios, Anna Calissano, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explaining the predictions of a deep neural network is a nontrivial task, yet
high-quality explanations for predictions are often a prerequisite for
practitioners to trust these models. Counterfactual explanations aim to explain
predictions by finding the ''nearest'' in-distribution alternative input whose
prediction changes in a pre-specified way. However, it remains an open question
how to define this nearest alternative input, whose solution depends on both
the domain (e.g. images, graphs, tabular data, etc.) and the specific
application considered. For graphs, this problem is complicated i) by their
discrete nature, as opposed to the continuous nature of state-of-the-art graph
classifiers; and ii) by the node permutation group acting on the graphs. We
propose a method to generate counterfactual explanations for any differentiable
black-box graph classifier, utilizing a case-specific permutation equivariant
graph variational autoencoder. We generate counterfactual explanations in a
continuous fashion by traversing the latent space of the autoencoder across the
classification boundary of the classifier, allowing for seamless integration of
discrete graph structure and continuous graph attributes. We empirically
validate the approach on three graph datasets, showing that our model is
consistently high-performing and more robust than the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Northern Lights Deep Learning Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Güemes-Palau, Miquel Ferriol-Galmés, Jordi Paillisse-Vilanova, Albert López-Brescó, Pere Barlet-Ros, Albert Cabellos-Aparicio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network simulation is pivotal in network modeling, assisting with tasks
ranging from capacity planning to performance estimation. Traditional
approaches such as Discrete Event Simulation (DES) face limitations in terms of
computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel
integration of a testbed network with a Machine Learning (ML) model to address
these challenges. By using the testbed as a hardware accelerator,
RouteNet-Gauss generates training datasets rapidly and simulates network
scenarios with high fidelity to real-world conditions. Experimental results
show that RouteNet-Gauss significantly reduces prediction errors by up to 95%
and achieves a 488x speedup in inference time compared to state-of-the-art
DES-based methods. RouteNet-Gauss's modular architecture is dynamically
constructed based on the specific characteristics of the network scenario, such
as topology and routing. This enables it to understand and generalize to
different network configurations beyond those seen during training, including
networks up to 10x larger. Additionally, it supports Temporal Aggregated
Performance Estimation (TAPE), providing configurable temporal granularity and
maintaining high accuracy in flow performance metrics. This approach shows
promise in improving both simulation efficiency and accuracy, offering a
valuable tool for network operators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Meets Queue-Reactive: A Framework for Realistic Limit
  Order Book Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Bodor, Laurent Carlier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Queue-Reactive model introduced by Huang et al. (2015) has become a
standard tool for limit order book modeling, widely adopted by both researchers
and practitioners for its simplicity and effectiveness. We present the
Multidimensional Deep Queue-Reactive (MDQR) model, which extends this framework
in three ways: it relaxes the assumption of queue independence, enriches the
state space with market features, and models the distribution of order sizes.
Through a neural network architecture, the model learns complex dependencies
between different price levels and adapts to varying market conditions, while
preserving the interpretable point-process foundation of the original
framework. Using data from the Bund futures market, we show that MDQR captures
key market properties including the square-root law of market impact,
cross-queue correlations, and realistic order size patterns. The model
demonstrates particular strength in reproducing both conditional and stationary
distributions of order sizes, as well as various stylized facts of market
microstructure. The model achieves this while maintaining the computational
efficiency needed for practical applications such as strategy development
through reinforcement learning or realistic backtesting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Garov, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning algorithms often encounter different or
"out-of-distribution" (OOD) data at deployment time, and OOD detection is
frequently employed to detect these examples. While it works reasonably well in
practice, existing theoretical results on OOD detection are highly pessimistic.
In this work, we take a closer look at this problem, and make a distinction
between uniform and non-uniform learnability, following PAC learning theory. We
characterize under what conditions OOD detection is uniformly and non-uniformly
learnable, and we show that in several cases, non-uniform learnability turns a
number of negative results into positive. In all cases where OOD detection is
learnable, we provide concrete learning algorithms and a sample-complexity
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDEA: Image Description Enhanced CLIP-Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Ye, Feng Jiang, Qiufeng Wang, Kaizhu Huang, Jiaqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP (Contrastive Language-Image Pre-training) has attained great success in
pattern recognition and computer vision. Transferring CLIP to downstream tasks
(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.
However, current studies primarily focus on either prompt learning for text or
adapter tuning for vision, without fully exploiting the complementary
information and correlations among image-text pairs. In this paper, we propose
an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to
few-shot image classification tasks. This method captures fine-grained features
by leveraging both visual features and textual descriptions of images. IDEA is
a training-free method for CLIP, and it can be comparable to or even exceeds
state-of-the-art models on multiple tasks. Furthermore, we introduce
Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable
components (i.e., a projector and a learnable latent space), further enhancing
the model's performance and achieving SOTA results on 11 datasets. As one
important contribution, we employ the Llama model and design a comprehensive
pipeline to generate textual descriptions for images of 11 datasets, resulting
in a total of 1,637,795 image-text pairs, named "IMD-11". Our code and data are
released at https://github.com/FourierAI/IDEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning for temporal super-resolution 4D Flow MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pia Callmer, Mia Bonini, Edward Ferdian, David Nordsletten, Daniel Giese, Alistair A. Young, Alexander Fyrdahl, David Marlevi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive technique
for volumetric, time-resolved blood flow quantification. However, apparent
trade-offs between acquisition time, image noise, and resolution limit clinical
applicability. In particular, in regions of highly transient flow, coarse
temporal resolution can hinder accurate capture of physiologically relevant
flow variations. To overcome these issues, post-processing techniques using
deep learning have shown promising results to enhance resolution post-scan
using so-called super-resolution networks. However, while super-resolution has
been focusing on spatial upsampling, temporal super-resolution remains largely
unexplored. The aim of this study was therefore to implement and evaluate a
residual network for temporal super-resolution 4D Flow MRI. To achieve this, an
existing spatial network (4DFlowNet) was re-designed for temporal upsampling,
adapting input dimensions, and optimizing internal layer structures. Training
and testing were performed using synthetic 4D Flow MRI data originating from
patient-specific in-silico models, as well as using in-vivo datasets. Overall,
excellent performance was achieved with input velocities effectively denoised
and temporally upsampled, with a mean absolute error (MAE) of 1.0 cm/s in an
unseen in-silico setting, outperforming deterministic alternatives (linear
interpolation MAE = 2.3 cm/s, sinc interpolation MAE = 2.6 cm/s). Further, the
network synthesized high-resolution temporal information from unseen
low-resolution in-vivo data, with strong correlation observed at peak flow
frames. As such, our results highlight the potential of utilizing data-driven
neural networks for temporal super-resolution 4D Flow MRI, enabling
high-frame-rate flow quantification without extending acquisition times beyond
clinically acceptable limits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nesterov Acceleration for Ensemble Kalman Inversion and Variants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sydney Vernon, Eviatar Bach, Oliver R. A. Dunbar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble Kalman inversion (EKI) is a derivative-free, particle-based
optimization method for solving inverse problems. It can be shown that EKI
approximates a gradient flow, which allows the application of methods for
accelerating gradient descent. Here, we show that Nesterov acceleration is
effective in speeding up the reduction of the EKI cost function on a variety of
inverse problems. We also implement Nesterov acceleration for two EKI variants,
unscented Kalman inversion and ensemble transform Kalman inversion. Our
specific implementation takes the form of a particle-level nudge that is
demonstrably simple to couple in a black-box fashion with any existing EKI
variant algorithms, comes with no additional computational expense, and with no
additional tuning hyperparameters. This work shows a pathway for future
research to translate advances in gradient-based optimization into advances in
gradient-free Kalman optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Networked Agents in the Dark: Team Value Learning under Partial
  Observability <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel cooperative multi-agent reinforcement learning (MARL)
approach for networked agents. In contrast to previous methods that rely on
complete state information or joint observations, our agents must learn how to
reach shared objectives under partial observability. During training, they
collect individual rewards and approximate a team value function through local
communication, resulting in cooperative behavior. To describe our problem, we
introduce the networked dynamic partially observable Markov game framework,
where agents communicate over a switching topology communication network. Our
distributed method, DNA-MARL, uses a consensus mechanism for local
communication and gradient descent for local computation. DNA-MARL increases
the range of the possible applications of networked agents, being well-suited
for real world domains that impose privacy and where the messages may not reach
their recipients. We evaluate DNA-MARL across benchmark MARL scenarios. Our
results highlight the superior performance of DNA-MARL over previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 5 tables. Accepted as supplemental material at
  Proceedings of the 24th International Conference on Autonomous Agents and
  Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May 19 - 23, 2025,
  IFAAMAS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging LLM Agents for Translating Network Configurations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Wei, Xiaohui Xie, Yiwei Zuo, Tianshuo Hu, Xinyi Chen, Kaiwen Chi, Yong Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Configuration translation is a critical and frequent task in network
operations. When a network device is damaged or outdated, administrators need
to replace it to maintain service continuity. The replacement devices may
originate from different vendors, necessitating configuration translation to
ensure seamless network operation. However, translating configurations manually
is a labor-intensive and error-prone process. In this paper, we propose an
intent-based framework for translating network configuration with Large
Language Model (LLM) Agents. The core of our approach is an Intent-based
Retrieval Augmented Generation (IRAG) module that systematically splits a
configuration file into fragments, extracts intents, and generates accurate
translations. We also design a two-stage verification method to validate the
syntax and semantics correctness of the translated configurations. We implement
and evaluate the proposed method on real-world network configurations.
Experimental results show that our method achieves 97.74% syntax correctness,
outperforming state-of-the-art methods in translation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeshMask: Physics-Based Simulations with Masked <span class="highlight-title">Graph</span> Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Garnier, Vincent Lannelongue, Jonathan Viquerat, Elie Hachem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel masked pre-training technique for graph neural networks
(GNNs) applied to computational fluid dynamics (CFD) problems. By randomly
masking up to 40\% of input mesh nodes during pre-training, we force the model
to learn robust representations of complex fluid dynamics. We pair this masking
strategy with an asymmetric encoder-decoder architecture and gated multi-layer
perceptrons to further enhance performance. The proposed method achieves
state-of-the-art results on seven CFD datasets, including a new challenging
dataset of 3D intracranial aneurysm simulations with over 250,000 nodes per
mesh. Moreover, it significantly improves model performance and training
efficiency across such diverse range of fluid simulation tasks. We demonstrate
improvements of up to 60\% in long-term prediction accuracy compared to
previous best models, while maintaining similar computational costs. Notably,
our approach enables effective pre-training on multiple datasets
simultaneously, significantly reducing the time and data required to achieve
high performance on new tasks. Through extensive ablation studies, we provide
insights into the optimal masking ratio, architectural choices, and training
strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-Constrained Federated Continual Learning: What Does Matter? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Li, Yuying Wang, Jiahua Dong, Haozhao Wang, Yining Qi, Rui Zhang, Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Continual Learning (FCL) aims to enable sequentially
privacy-preserving model training on streams of incoming data that vary in edge
devices by preserving previous knowledge while adapting to new data. Current
FCL literature focuses on restricted data privacy and access to previously seen
data while imposing no constraints on the training overhead. This is
unreasonable for FCL applications in real-world scenarios, where edge devices
are primarily constrained by resources such as storage, computational budget,
and label rate. We revisit this problem with a large-scale benchmark and
analyze the performance of state-of-the-art FCL approaches under different
resource-constrained settings. Various typical FCL techniques and six datasets
in two incremental learning scenarios (Class-IL and Domain-IL) are involved in
our experiments. Through extensive experiments amounting to a total of over
1,000+ GPU hours, we find that, under limited resource-constrained settings,
existing FCL approaches, with no exception, fail to achieve the expected
performance. Our conclusions are consistent in the sensitivity analysis. This
suggests that most existing FCL methods are particularly too resource-dependent
for real-world deployment. Moreover, we study the performance of typical FCL
techniques with resource constraints and shed light on future research
directions in FCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2303.11165 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAPPA - A Hybrid <span class="highlight-title">Graph</span> Neural Network for Predicting Pure Component
  Vapor Pressures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Hoffmann, Hans Hasse, Fabian Jirasek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the pure component vapor pressure is one of the most important
properties for designing chemical processes, no broadly applicable,
sufficiently accurate, and open-source prediction method has been available. To
overcome this, we have developed GRAPPA - a hybrid graph neural network for
predicting vapor pressures of pure components. GRAPPA enables the prediction of
the vapor pressure curve of basically any organic molecule, requiring only the
molecular structure as input. The new model consists of three parts: A graph
attention network for the message passing step, a pooling function that
captures long-range interactions, and a prediction head that yields the
component-specific parameters of the Antoine equation, from which the vapor
pressure can readily and consistently be calculated for any temperature. We
have trained and evaluated GRAPPA on experimental vapor pressure data of almost
25,000 pure components. We found excellent prediction accuracy for unseen
components, outperforming state-of-the-art group contribution methods and other
machine learning approaches in applicability and accuracy. The trained model
and its code are fully disclosed, and GRAPPA is directly applicable via the
interactive website ml-prop.mv.rptu.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformed Low-rank Adaptation via Tensor Decomposition and Its
  Applications to Text-to-image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Tao, Yuhta Takida, Naoki Murata, Qibin Zhao, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an
increasingly popular technique with many applications. Among the various PEFT
methods, Low-Rank Adaptation (LoRA) and its variants have gained significant
attention due to their effectiveness, enabling users to fine-tune models with
limited computational resources. However, the approximation gap between the
low-rank assumption and desired fine-tuning weights prevents the simultaneous
acquisition of ultra-parameter-efficiency and better performance. To reduce
this gap and further improve the power of LoRA, we propose a new PEFT method
that combines two classes of adaptations, namely, transform and residual
adaptations. In specific, we first apply a full-rank and dense transform to the
pre-trained weight. This learnable transform is expected to align the
pre-trained weight as closely as possible to the desired weight, thereby
reducing the rank of the residual weight. Then, the residual part can be
effectively approximated by more compact and parameter-efficient structures,
with a smaller approximation error. To achieve ultra-parameter-efficiency in
practice, we design highly flexible and effective tensor decompositions for
both the transform and residual adaptations. Additionally, popular PEFT methods
such as DoRA can be summarized under this transform plus residual adaptation
scheme. Experiments are conducted on fine-tuning Stable Diffusion models in
subject-driven and controllable generation. The results manifest that our
method can achieve better performances and parameter efficiency compared to
LoRA and several baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding
  and Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianru Zhang, Li Ju, Prashant Singh, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing large-scale datasets, especially involving complex and
high-dimensional data like images, is particularly challenging. While
self-supervised learning (SSL) has proven effective for learning
representations from unlabelled data, it typically focuses on flat,
non-hierarchical structures, missing the multi-level relationships present in
many real-world datasets. Hierarchical clustering (HC) can uncover these
relationships by organizing data into a tree-like structure, but it often
relies on rigid similarity metrics that struggle to capture the complexity of
diverse data types. To address these we envision $\texttt{InfoHier}$, a
framework that combines SSL with HC to jointly learn robust latent
representations and hierarchical structures. This approach leverages SSL to
provide adaptive representations, enhancing HC's ability to capture complex
patterns. Simultaneously, it integrates HC loss to refine SSL training,
resulting in representations that are more attuned to the underlying
information hierarchy. $\texttt{InfoHier}$ has the potential to improve the
expressiveness and performance of both clustering and representation learning,
offering significant benefits for data analysis, management, and information
retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Transformation Learning for Equivariant Representations <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemyung Yu, Jaehyun Choi, Dong-Jae Lee, HyeongGwon Hong, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised representation learning has significantly advanced various
machine learning tasks. In the computer vision domain, state-of-the-art
approaches utilize transformations like random crop and color jitter to achieve
invariant representations, embedding semantically the same inputs despite
transformations. However, this can degrade performance in tasks requiring
precise features, such as localization or flower classification. To address
this, recent research incorporates equivariant representation learning, which
captures transformation-sensitive information. However, current methods depend
on transformation labels and thus struggle with interdependency and complex
transformations. We propose Self-supervised Transformation Learning (STL),
replacing transformation labels with transformation representations derived
from image pairs. The proposed method ensures transformation representation is
image-invariant and learns corresponding equivariant transformations, enhancing
performance without increased batch complexity. We demonstrate the approach's
effectiveness across diverse classification and detection tasks, outperforming
existing methods in 7 out of 11 benchmarks and excelling in detection. By
integrating complex transformations like AugMix, unusable by prior equivariant
methods, this approach enhances performance across tasks, underscoring its
adaptability and resilience. Additionally, its compatibility with various base
models highlights its flexibility and broad applicability. The code is
available at https://github.com/jaemyung-u/stl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled Interleaving Variational Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noelle Y. L. Wong, Eng Yeow Cheu, Zhonglin Chiam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conflicting objectives present a considerable challenge in interleaving
multi-task learning, necessitating the need for meticulous design and balance
to ensure effective learning of a representative latent data space across all
tasks without mutual negative impact. Drawing inspiration from the concept of
marginal and conditional probability distributions in probability theory, we
design a principled and well-founded approach to disentangle the original input
into marginal and conditional probability distributions in the latent space of
a variational autoencoder. Our proposed model, Deep Disentangled Interleaving
Variational Encoding (DeepDIVE) learns disentangled features from the original
input to form clusters in the embedding space and unifies these features via
the cross-attention mechanism in the fusion stage. We theoretically prove that
combining the objectives for reconstruction and forecasting fully captures the
lower bound and mathematically derive a loss function for disentanglement using
Na\"ive Bayes. Under the assumption that the prior is a mixture of log-concave
distributions, we also establish that the Kullback-Leibler divergence between
the prior and the posterior is upper bounded by a function minimized by the
minimizer of the cross entropy loss, informing our adoption of radial basis
functions (RBF) and cross entropy with interleaving training for DeepDIVE to
provide a justified basis for convergence. Experiments on two public datasets
show that DeepDIVE disentangles the original input and yields forecast
accuracies better than the original VAE and comparable to existing
state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagonal Over-parameterization in Reproducing Kernel Hilbert Spaces as
  an Adaptive Feature Model: Generalization and Adaptivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Li, Qian Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a diagonal adaptive kernel model that dynamically
learns kernel eigenvalues and output coefficients simultaneously during
training. Unlike fixed-kernel methods tied to the neural tangent kernel theory,
the diagonal adaptive kernel model adapts to the structure of the truth
function, significantly improving generalization over fixed-kernel methods,
especially when the initial kernel is misaligned with the target. Moreover, we
show that the adaptivity comes from learning the right eigenvalues during
training, showing a feature learning behavior. By extending to deeper
parameterization, we further show how extra depth enhances adaptability and
generalization. This study combines the insights from feature learning and
implicit regularization and provides new perspective into the adaptivity and
generalization potential of neural networks beyond the kernel regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2409.00894</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric
  Properties of Generated Sea Route <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Rohe, Florian Burger, Michael Kölle, Sebastian Wölckert, Maximilian Zorn, Claudia Linnhoff-Popien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The demand for artificially generated data for the development, training and
testing of new algorithms is omnipresent. Quantum computing (QC), does offer
the hope that its inherent probabilistic functionality can be utilised in this
field of generative artificial intelligence. In this study, we use
quantum-classical hybrid generative adversarial networks (QuGANs) to
artificially generate graphs of shipping routes. We create a training dataset
based on real shipping data and investigate to what extent QuGANs are able to
learn and reproduce inherent distributions and geometric features of this data.
We compare hybrid QuGANs with classical Generative Adversarial Networks (GANs),
with a special focus on their parameter efficiency. Our results indicate that
QuGANs are indeed able to quickly learn and represent underlying geometric
properties and distributions, although they seem to have difficulties in
introducing variance into the sampled data. Compared to classical GANs of
greater size, measured in the number of parameters used, some QuGANs show
similar result quality. Our reference to concrete use cases, such as the
generation of shipping data, provides an illustrative example and demonstrate
the potential and diversity in which QC can be used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPEQ: Stabilization Phases for Efficient Q-Learning in High
  Update-To-Data Ratio Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Romeo, Girolamo Macaluso, Alessandro Sestini, Andrew D. Bagdanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key challenge in Deep Reinforcement Learning is sample efficiency,
especially in real-world applications where collecting environment interactions
is expensive or risky. Recent off-policy algorithms improve sample efficiency
by increasing the Update-To-Data (UTD) ratio and performing more gradient
updates per environment interaction. While this improves sample efficiency, it
significantly increases computational cost due to the higher number of gradient
updates required. In this paper we propose a sample-efficient method to improve
computational efficiency by separating training into distinct learning phases
in order to exploit gradient updates more effectively. Our approach builds on
top of the Dropout Q-Functions (DroQ) algorithm and alternates between an
online, low UTD ratio training phase, and an offline stabilization phase.
During the stabilization phase, we fine-tune the Q-functions without collecting
new environment interactions. This process improves the effectiveness of the
replay buffer and reduces computational overhead. Our experimental results on
continuous control problems show that our method achieves results comparable to
state-of-the-art, high UTD ratio algorithms while requiring 56\% fewer gradient
updates and 50\% less training time than DroQ. Our approach offers an effective
and computationally economical solution while maintaining the same sample
efficiency as the more costly, high UTD ratio state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Product of Gaussian Mixture Diffusion Model for non-linear MRI Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurenz Nagler, Martin Zach, Thomas Pock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently shown remarkable results in magnetic resonance
imaging reconstruction. However, the employed networks typically are black-box
estimators of the (smoothed) prior score with tens of millions of parameters,
restricting interpretability and increasing reconstruction time. Furthermore,
parallel imaging reconstruction algorithms either rely on off-line coil
sensitivity estimation, which is prone to misalignment and restricting sampling
trajectories, or perform per-coil reconstruction, making the computational cost
proportional to the number of coils. To overcome this, we jointly reconstruct
the image and the coil sensitivities using the lightweight,
parameter-efficient, and interpretable product of Gaussian mixture diffusion
model as an image prior and a classical smoothness priors on the coil
sensitivities. The proposed method delivers promising results while allowing
for fast inference and demonstrating robustness to contrast out-of-distribution
data and sampling trajectories, comparable to classical variational penalties
such as total variation. Finally, the probabilistic formulation allows the
calculation of the posterior expectation and pixel-wise variance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-grained Spatio-temporal Event Prediction with Self-adaptive Anchor
  <span class="highlight-title">Graph</span> <span class="chip">SDM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang-Tao Zhou, Zhao Kang, Sicong Liu, Lizong Zhang, Ling Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event prediction tasks often handle spatio-temporal data distributed in a
large spatial area. Different regions in the area exhibit different
characteristics while having latent correlations. This spatial heterogeneity
and correlations greatly affect the spatio-temporal distributions of event
occurrences, which has not been addressed by state-of-the-art models. Learning
spatial dependencies of events in a continuous space is challenging due to its
fine granularity and a lack of prior knowledge. In this work, we propose a
novel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event
prediction. It adopts an encoder-decoder architecture that jointly models the
state dynamics of spatially localized regions using neural Ordinary
Differential Equations (ODEs). The state evolution is built on the foundation
of a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial
dependencies. By adaptively localizing the anchor nodes in the space and
jointly constructing the correlation edges between them, the SAAG enhances the
model's ability of learning complex spatial event patterns. The proposed GSTPP
model greatly improves the accuracy of fine-grained event prediction. Extensive
experimental results show that our method greatly improves the prediction
accuracy over existing spatio-temporal event prediction approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIAM International Conference on Data Mining 2025
  (SDM'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Learning of Depth and Appearance for Portrait Image Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinya Ji, Gaspard Zoss, Prashanth Chandran, Lingchen Yang, Xun Cao, Barbara Solenthaler, Derek Bradley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D portrait animation has experienced significant advancements in recent
years. Much research has utilized the prior knowledge embedded in large
generative diffusion models to enhance high-quality image manipulation.
However, most methods only focus on generating RGB images as output, and the
co-generation of consistent visual plus 3D output remains largely
under-explored. In our work, we propose to jointly learn the visual appearance
and depth simultaneously in a diffusion-based portrait image generator. Our
method embraces the end-to-end diffusion paradigm and introduces a new
architecture suitable for learning this conditional joint distribution,
consisting of a reference network and a channel-expanded diffusion backbone.
Once trained, our framework can be efficiently adapted to various downstream
applications, such as facial depth-to-image and image-to-depth generation,
portrait relighting, and audio-driven talking head animation with consistent 3D
output.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Reservoir Computing and Risk Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naomi Mona Chmielewski, Nina Amini, Joseph Mikael
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a way to bound the generalisation errors of several classes of
quantum reservoirs using the Rademacher complexity. We give specific,
parameter-dependent bounds for two particular quantum reservoir classes. We
analyse how the generalisation bounds scale with growing numbers of qubits.
Applying our results to classes with polynomial readout functions, we find that
the risk bounds converge in the number of training samples. The explicit
dependence on the quantum reservoir and readout parameters in our bounds can be
used to control the generalisation error to a certain extent. It should be
noted that the bounds scale exponentially with the number of qubits $n$. The
upper bounds on the Rademacher complexity can be applied to other reservoir
classes that fulfill a few hypotheses on the quantum dynamics and the readout
function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWSC: Shared Weight for Similar Channel in LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binrui Zeng, Yongtao Tang, Xiaodong Liu, Xiaopeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have spurred development in multiple industries.
However, the growing number of their parameters brings substantial storage and
computing burdens, making it essential to explore model compression techniques
for parameter reduction and easier deployment. We propose SWSC, an LLM
compression method based on the concept of Shared Weight for Similar Channel.
It uses the K-Means clustering algorithm to cluster model weights
channel-by-channel, generating clusters with highly similar vectors within
each. A representative vector from each cluster is selected to approximately
replace all vectors in the cluster, significantly reducing the number of model
weight parameters. However, approximate restoration will inevitably cause
damage to the performance of the model. To tackle this issue, we perform
singular value decomposition on the weight error values before and after
compression and retain the larger singular values and their corresponding
singular vectors to compensate for the accuracy. The experimental results show
that our method can effectively ensure the performance of the compressed LLM
even under low-precision conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5pages, 3 figures, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer-based Multivariate Time Series Anomaly Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charalampos Shimillas, Kleanthis Malialis, Konstantinos Fokianos, Marios M. Polycarpou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing complexity of Cyber-Physical Systems (CPS) and the
integration of Internet of Things (IoT), the use of sensors for online
monitoring generates large volume of multivariate time series (MTS) data.
Consequently, the need for robust anomaly diagnosis in MTS is paramount to
maintaining system reliability and safety. While significant advancements have
been made in anomaly detection, localization remains a largely underexplored
area, though crucial for intelligent decision-making. This paper introduces a
novel transformer-based model for unsupervised anomaly diagnosis in MTS, with a
focus on improving localization performance, through an in-depth analysis of
the self-attention mechanism's learning behavior under both normal and
anomalous conditions. We formulate the anomaly localization problem as a
three-stage process: time-step, window, and segment-based. This leads to the
development of the Space-Time Anomaly Score (STAS), a new metric inspired by
the connection between transformer latent representations and space-time
statistical models. STAS is designed to capture individual anomaly behaviors
and inter-series dependencies, delivering enhanced localization performance.
Additionally, the Statistical Feature Anomaly Score (SFAS) complements STAS by
analyzing statistical features around anomalies, with their combination helping
to reduce false alarms. Experiments on real world and synthetic datasets
illustrate the model's superiority over state-of-the-art methods in both
detection and localization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Learning Algorithm That Attains the Human Optimum in a Repeated
  Human-Machine Interaction Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason T. Isa, Lillian J. Ratliff, Samuel A. Burden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When humans interact with learning-based control systems, a common goal is to
minimize a cost function known only to the human. For instance, an exoskeleton
may adapt its assistance in an effort to minimize the human's metabolic
cost-of-transport. Conventional approaches to synthesizing the learning
algorithm solve an inverse problem to infer the human's cost. However, these
problems can be ill-posed, hard to solve, or sensitive to problem data. Here we
show a game-theoretic learning algorithm that works solely by observing human
actions to find the cost minimum, avoiding the need to solve an inverse
problem. We evaluate the performance of our algorithm in an extensive set of
human subjects experiments, demonstrating consistent convergence to the minimum
of a prescribed human cost function in scalar and multidimensional
instantiations of the game. We conclude by outlining future directions for
theoretical and empirical extensions of our results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT-PatchTST: Channel-Time Patch Time-Series Transformer for Long-Term
  Renewable Energy Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menghao Huo, Kuan Lu, Yuxiao Li, Qiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting renewable energy output is crucial for the efficient
integration of solar and wind power into modern energy systems. This study
develops and evaluates an advanced deep learning model, Channel-Time Patch
Time-Series Transformer (CT-PatchTST), to forecast the power output of
photovoltaic and wind energy systems using annual offshore wind power, onshore
wind power, and solar power generation data from Denmark. While the original
Patch Time-Series Transformer(PatchTST) model employs a channel-independent
(CI) approach, it tends to overlook inter-channel relationships during
training, potentially leading to a loss of critical information. To address
this limitation and further leverage the benefits of increased data granularity
brought by CI, we propose CT-PatchTST. This enhanced model improves the
processing of inter-channel information while maintaining the advantages of the
channel-independent approach. The predictive performance of CT-PatchTST is
rigorously analyzed, demonstrating its ability to provide precise and reliable
energy forecasts. This work contributes to improving the predictability of
renewable energy systems, supporting their broader adoption and integration
into energy grids.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fernández Fisac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI systems like foundation models (FMs) must align well with human
values to ensure their behavior is helpful and trustworthy. While Reinforcement
Learning from Human Feedback (RLHF) has shown promise for optimizing model
performance using human judgments, existing RLHF pipelines predominantly rely
on immediate feedback, which can fail to accurately reflect the downstream
impact of an interaction on users' utility. We demonstrate that feedback based
on evaluators' foresight estimates of downstream consequences systematically
induces Goodhart's Law dynamics, incentivizing misaligned behaviors like
sycophancy and deception and ultimately degrading user outcomes. To alleviate
this, we propose decoupling evaluation from prediction by refocusing RLHF on
hindsight feedback. Our theoretical analysis reveals that conditioning
evaluator feedback on downstream observations mitigates misalignment and
improves expected human utility, even when these observations are simulated by
the AI system itself. To leverage this insight in a practical alignment
algorithm, we introduce Reinforcement Learning from Hindsight Simulation
(RLHS), which first simulates plausible consequences and then elicits feedback
to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS
to two widely-employed online and offline preference optimization methods --
Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) --
and show empirically that misalignment is significantly reduced with both
methods. Through an online human user study, we show that RLHS consistently
outperforms RLHF in helping users achieve their goals and earns higher
satisfaction ratings, despite being trained solely with simulated hindsight
feedback. These results underscore the importance of focusing on long-term
consequences, even simulated ones, to mitigate misalignment in RLHF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Aligned Data Forgetting via Twin Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxing Niu, Haoxuan Ji, Yuyao Sun, Zheng Lin, Fei Gao, Yuhang Wang, Haichao Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern privacy regulations have spurred the evolution of machine unlearning,
a technique enabling a trained model to efficiently forget specific training
data. In prior unlearning methods, the concept of "data forgetting" is often
interpreted and implemented as achieving zero classification accuracy on such
data. Nevertheless, the authentic aim of machine unlearning is to achieve
alignment between the unlearned model and the gold model, i.e., encouraging
them to have identical classification accuracy. On the other hand, the gold
model often exhibits non-zero classification accuracy due to its generalization
ability. To achieve aligned data forgetting, we propose a Twin Machine
Unlearning (TMU) approach, where a twin unlearning problem is defined
corresponding to the original unlearning problem. Consequently, the
generalization-label predictor trained on the twin problem can be transferred
to the original problem, facilitating aligned data forgetting. Comprehensive
empirical experiments illustrate that our approach significantly enhances the
alignment between the unlearned model and the gold model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2408.11433</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Risk-sensitive Satisficing in Contextual Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shogo Ito, Tatsuji Takahashi, Yu Kono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The contextual bandit problem, which is a type of reinforcement learning
tasks, provides an effective framework for solving challenges in recommendation
systems, such as satisfying real-time requirements, enabling personalization,
addressing cold-start problems. However, contextual bandit algorithms face
challenges since they need to handle large state-action spaces sequentially.
These challenges include the high costs for learning and balancing exploration
and exploitation, as well as large variations in performance that depend on the
domain of application. To address these challenges, Tsuboya et~al. proposed the
Regional Linear Risk-sensitive Satisficing (RegLinRS) algorithm. RegLinRS
switches between exploration and exploitation based on how well the agent has
achieved the target. However, the reward expectations in RegLinRS are linearly
approximated based on features, which limits its applicability when the
relationship between features and reward expectations is non-linear. To handle
more complex environments, we proposed Neural Risk-sensitive Satisficing
(NeuralRS), which incorporates neural networks into RegLinRS, and demonstrated
its utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AROB-ISBC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenMLDB: A Real-Time <span class="highlight-title">Relational</span> Data Feature Computation System for
  Online ML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhe Zhou, Wei Zhou, Liguo Qi, Hao Zhang, Dihao Chen, Bingsheng He, Mian Lu, Guoliang Li, Fan Wu, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and consistent feature computation is crucial for a wide range of
online ML applications. Typically, feature computation is divided into two
distinct phases, i.e., offline stage for model training and online stage for
model serving. These phases often rely on execution engines with different
interface languages and function implementations, causing significant
inconsistencies. Moreover, many online ML features involve complex time-series
computations (e.g., functions over varied-length table windows) that differ
from standard streaming and analytical queries. Existing data processing
systems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for
these computations, making them unsuitable for real-time online ML applications
that demand timely feature updates.
  This paper presents OpenMLDB, a feature computation system deployed in
4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB
first employs a unified query plan generator for consistent computation results
across the offline and online stages, significantly reducing feature deployment
overhead. Second, OpenMLDB provides an online execution engine that resolves
performance bottlenecks caused by long window computations (via
pre-aggregation) and multi-table window unions (via data self-adjusting). It
also provides a high-performance offline execution engine with window parallel
optimization and time-aware data skew resolving. Third, OpenMLDB features a
compact data format and stream-focused indexing to maximize memory usage and
accelerate data access. Evaluations in testing and real workloads reveal
significant performance improvements and resource savings compared to the
baseline systems. The open community of OpenMLDB now has over 150 contributors
and gained 1.6k stars on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular <span class="highlight-title">Graph</span> Contrastive Learning with Line <span class="highlight-title">Graph</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyuan Chen, Shangzhe Li, Ruomei Liu, Bowen Shi, Jiaheng Liu, Junran Wu, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trapped by the label scarcity in molecular property prediction and drug
design, graph contrastive learning (GCL) came forward. Leading contrastive
learning works show two kinds of view generators, that is, random or learnable
data corruption and domain knowledge incorporation. While effective, the two
ways also lead to molecular semantics altering and limited generalization
capability, respectively. To this end, we relate the \textbf{L}in\textbf{E}
graph with \textbf{MO}lecular graph co\textbf{N}trastive learning and propose a
novel method termed \textit{LEMON}. Specifically, by contrasting the given
graph with the corresponding line graph, the graph encoder can freely encode
the molecular semantics without omission. Furthermore, we present a new patch
with edge attribute fusion and two local contrastive losses enhance information
transmission and tackle hard negative samples. Compared with state-of-the-art
(SOTA) methods for view generation, superior performance on molecular property
prediction suggests the effectiveness of our proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normalize Then Propagate: Efficient Homophilous Regularization for
  Few-shot Semi-Supervised Node Classification <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baoming Zhang, MingCai Chen, Jianqing Song, Shuangjie Li, Jie Zhang, Chongjun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated remarkable ability in
semi-supervised node classification. However, most existing GNNs rely heavily
on a large amount of labeled data for training, which is labor-intensive and
requires extensive domain knowledge. In this paper, we first analyze the
restrictions of GNNs generalization from the perspective of supervision signals
in the context of few-shot semi-supervised node classification. To address
these challenges, we propose a novel algorithm named NormProp, which utilizes
the homophily assumption of unlabeled nodes to generate additional supervision
signals, thereby enhancing the generalization against label scarcity. The key
idea is to efficiently capture both the class information and the consistency
of aggregation during message passing, via decoupling the direction and
Euclidean norm of node representations. Moreover, we conduct a theoretical
analysis to determine the upper bound of Euclidean norm, and then propose
homophilous regularization to constraint the consistency of unlabeled nodes.
Extensive experiments demonstrate that NormProp achieve state-of-the-art
performance under low-label rate scenarios with low computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNMDR: Dynamic Networks and Multi-view Drug Representations for Safe
  Medication <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanlin Liu, Xiaomei Yu, Zihao Liu, Xue Li, Xingxu Fan, Xiangwei Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medication Recommendation (MR) is a promising research topic which booms
diverse applications in the healthcare and clinical domains. However, existing
methods mainly rely on sequential modeling and static graphs for representation
learning, which ignore the dynamic correlations in diverse medical events of a
patient's temporal visits, leading to insufficient global structural
exploration on nodes. Additionally, mitigating drug-drug interactions (DDIs) is
another issue determining the utility of the MR systems. To address the
challenges mentioned above, this paper proposes a novel MR method with the
integration of dynamic networks and multi-view drug representations (DNMDR).
Specifically, weighted snapshot sequences for dynamic heterogeneous networks
are constructed based on discrete visits in temporal EHRs, and all the dynamic
networks are jointly trained to gain both structural correlations in diverse
medical events and temporal dependency in historical health conditions, for
achieving comprehensive patient representations with both semantic features and
structural relationships. Moreover, combining the drug co-occurrences and
adverse drug-drug interactions (DDIs) in internal view of drug molecule
structure and interactive view of drug pairs, the safe drug representations are
available to obtain high-quality medication combination recommendation.
Finally, extensive experiments on real world datasets are conducted for
performance evaluation, and the experimental results demonstrate that the
proposed DNMDR method outperforms the state-of-the-art baseline models with a
large margin on various metrics such as PRAUC, Jaccard, DDI rates and so on.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Sampled Softmax with Inverted Multi-<span class="highlight-title">Index</span>: Methods, Theory and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Chen, Jin Zhang, Xu huang, Yi Yang, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The softmax function is a cornerstone of multi-class classification, integral
to a wide range of machine learning applications, from large-scale retrieval
and ranking models to advanced large language models. However, its
computational cost grows linearly with the number of classes, which becomes
prohibitively expensive in scenarios with millions or even billions of classes.
The sampled softmax, which relies on self-normalized importance sampling, has
emerged as a powerful alternative, significantly reducing computational
complexity. Yet, its estimator remains unbiased only when the sampling
distribution matches the true softmax distribution. To improve both
approximation accuracy and sampling efficiency, we propose the MIDX Sampler, a
novel adaptive sampling strategy based on an inverted multi-index approach.
Concretely, we decompose the softmax probability into several multinomial
probabilities, each associated with a specific set of codewords and the last
associated with the residual score of queries, thus reducing time complexity to
the number of codewords instead of the number of classes. To further boost
efficiency, we replace the query-specific residual probability with a simple
uniform distribution, simplifying the computation while retaining high
performance. Our method is backed by rigorous theoretical analysis, addressing
key concerns such as sampling bias, gradient bias, convergence rates, and
generalization error bounds. The results demonstrate that a smaller divergence
from the ideal softmax distribution leads to faster convergence and improved
generalization. Extensive experiments on large-scale language models,
sequential recommenders, and extreme multi-class classification tasks confirm
that the MIDX-Sampler delivers superior effectiveness and efficiency compared
to existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIAFEx: An Attention-based Feature Extraction Method for Medical Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Ramos-Soto, Jorge Ramos-Frutos, Ezequiel Perez-Zarate, Diego Oliva, Sandra E. Balderas-Mata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature extraction techniques are crucial in medical image classification;
however, classical feature extractors in addition to traditional machine
learning classifiers often exhibit significant limitations in providing
sufficient discriminative information for complex image sets. While
Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) have shown
promise in feature extraction, they are prone to overfitting due to the
inherent characteristics of medical imaging data, including small sample sizes
or high intra-class variance. In this work, the Medical Image Attention-based
Feature Extractor (MIAFEx) is proposed, a novel method that employs a learnable
refinement mechanism to enhance the classification token within the Transformer
encoder architecture. This mechanism adjusts the token based on learned
weights, improving the extraction of salient features and enhancing the model's
adaptability to the challenges presented by medical imaging data. The MIAFEx
output features quality is compared against classical feature extractors using
traditional and hybrid classifiers. Also, the performance of these features is
compared against modern CNN and ViT models in classification tasks,
demonstrating its superiority in accuracy and robustness across multiple
complex classification medical imaging datasets. This advantage is particularly
pronounced in scenarios with limited training data, where traditional and
modern models often struggle to generalize effectively. The source code of this
proposal can be found at
https://github.com/Oscar-RamosS/Medical-Image-Attention-based-Feature-Extractor-MIAFEx
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In preparation for Journal Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for
  Digital Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Houbing Herbert Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an Adaptive Neuro-Symbolic Learning Framework for
digital twin technology called ``ANSR-DT." Our approach combines pattern
recognition algorithms with reinforcement learning and symbolic reasoning to
enable real-time learning and adaptive intelligence. This integration enhances
the understanding of the environment and promotes continuous learning, leading
to better and more effective decision-making in real-time for applications that
require human-machine collaboration. We evaluated the \textit{ANSR-DT}
framework for its ability to learn and adapt to dynamic patterns, observing
significant improvements in decision accuracy, reliability, and
interpretability when compared to existing state-of-the-art methods. However,
challenges still exist in extracting and integrating symbolic rules in complex
environments, which limits the full potential of our framework in heterogeneous
settings. Moreover, our ongoing research aims to address this issue in the
future by ensuring seamless integration of neural models at large. In addition,
our open-source implementation promotes reproducibility and encourages future
research to build on our foundational work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Tao, Jehan Yang, Dan Ding, Zackory Erickson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF
controllers like joysticks often requires frequent switching between control
modes, where each mode maps controller movements to specific robot actions.
Manually performing this frequent switching can make teleoperation cumbersome
and inefficient. On the other hand, existing automatic mode-switching
solutions, such as heuristic-based or learning-based methods, are often
task-specific and lack generalizability. In this paper, we introduce LLM-Driven
Automatic Mode Switching (LAMS), a novel approach that leverages Large Language
Models (LLMs) to automatically switch control modes based on task context.
Unlike existing methods, LAMS requires no prior task demonstrations and
incrementally improves by integrating user-generated mode-switching examples.
We validate LAMS through an ablation study and a user study with 10
participants on complex, long-horizon tasks, demonstrating that LAMS
effectively reduces manual mode switches, is preferred over alternative
methods, and improves performance over time. The project website with
supplementary materials is at https://lams-assistance.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning-Enhanced Procedural Generation for Dynamic
  Narrative-Driven AR Experiences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddha Srinivas Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural Content Generation (PCG) is widely used to create scalable and
diverse environments in games. However, existing methods, such as the Wave
Function Collapse (WFC) algorithm, are often limited to static scenarios and
lack the adaptability required for dynamic, narrative-driven applications,
particularly in augmented reality (AR) games. This paper presents a
reinforcement learning-enhanced WFC framework designed for mobile AR
environments. By integrating environment-specific rules and dynamic tile weight
adjustments informed by reinforcement learning (RL), the proposed method
generates maps that are both contextually coherent and responsive to gameplay
needs. Comparative evaluations and user studies demonstrate that the framework
achieves superior map quality and delivers immersive experiences, making it
well-suited for narrative-driven AR games. Additionally, the method holds
promise for broader applications in education, simulation training, and
immersive extended reality (XR) experiences, where dynamic and adaptive
environments are critical.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Number of pages: 13, Number of figures: 4. Accepted for presentation
  at GRAPP 2025 - 20th International Conference on Computer Graphics Theory and
  Applications (for additional details on the conference visit
  https://grapp.scitevents.org). Disclaimer: This preprint may differ from the
  final version published in the conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theory of Optimistically Universal Online Learnability for General
  Concept Classes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steve Hanneke, Hongao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a full characterization of the concept classes that are
optimistically universally online learnable with $\{0, 1\}$ labels. The notion
of optimistically universal online learning was defined in [Hanneke, 2021] in
order to understand learnability under minimal assumptions. In this paper,
following the philosophy behind that work, we investigate two questions,
namely, for every concept class: (1) What are the minimal assumptions on the
data process admitting online learnability? (2) Is there a learning algorithm
which succeeds under every data process satisfying the minimal assumptions?
Such an algorithm is said to be optimistically universal for the given concept
class. We resolve both of these questions for all concept classes, and
moreover, as part of our solution, we design general learning algorithms for
each case. Finally, we extend these algorithms and results to the agnostic
case, showing an equivalence between the minimal assumptions on the data
process for learnability in the agnostic and realizable cases, for every
concept class, as well as the equivalence of optimistically universal
learnability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OMEGA: A Low-Latency GNN Serving System for Large <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geon-Woo Kim, Donghyun Kim, Jeongyoon Moon, Henry Liu, Tarannum Khan, Anand Iyer, Daehyeok Kim, Aditya Akella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have been widely adopted for their ability to
compute expressive node representations in graph datasets. However, serving
GNNs on large graphs is challenging due to the high communication, computation,
and memory overheads of constructing and executing computation graphs, which
represent information flow across large neighborhoods. Existing approximation
techniques in training can mitigate the overheads but, in serving, still lead
to high latency and/or accuracy loss. To this end, we propose OMEGA, a system
that enables low-latency GNN serving for large graphs with minimal accuracy
loss through two key ideas. First, OMEGA employs selective recomputation of
precomputed embeddings, which allows for reusing precomputed computation
subgraphs while selectively recomputing a small fraction to minimize accuracy
loss. Second, we develop computation graph parallelism, which reduces
communication overhead by parallelizing the creation and execution of
computation graphs across machines. Our evaluation with large graph datasets
and GNN models shows that OMEGA significantly outperforms state-of-the-art
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homophily-aware Heterogeneous <span class="highlight-title">Graph</span> Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haosen Wang, Chenglong Shi, Can Xu, Surong Yan, Pan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heterogeneous graph pre-training (HGP) has demonstrated remarkable
performance across various domains. However, the issue of heterophily in
real-world heterogeneous graphs (HGs) has been largely overlooked. To bridge
this research gap, we proposed a novel heterogeneous graph contrastive learning
framework, termed HGMS, which leverages connection strength and multi-view
self-expression to learn homophilous node representations. Specifically, we
design a heterogeneous edge dropping augmentation strategy that enhances the
homophily of augmented views. Moreover, we introduce a multi-view
self-expressive learning method to infer the homophily between nodes. In
practice, we develop two approaches to solve the self-expressive matrix. The
solved self-expressive matrix serves as an additional augmented view to provide
homophilous information and is used to identify false negatives in contrastive
loss. Extensive experimental results demonstrate the superiority of HGMS across
different downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complexity Control Facilitates Reasoning-Based Compositional
  Generalization in Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwang Zhang, Pengxiao Lin, Zhiwei Wang, Yaoyu Zhang, Zhi-Qin John Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have demonstrated impressive capabilities across various tasks,
yet their performance on compositional problems remains a subject of debate. In
this study, we investigate the internal mechanisms underlying Transformers'
behavior in compositional tasks. We find that complexity control strategies
significantly influence whether the model learns primitive-level rules that
generalize out-of-distribution (reasoning-based solutions) or relies solely on
memorized mappings (memory-based solutions). By applying masking strategies to
the model's information circuits and employing multiple complexity metrics, we
reveal distinct internal working mechanisms associated with different solution
types. Further analysis reveals that reasoning-based solutions exhibit a lower
complexity bias, which aligns with the well-studied neuron condensation
phenomenon. This lower complexity bias is hypothesized to be the key factor
enabling these solutions to learn reasoning rules. We validate these
conclusions across multiple real-world datasets, including image generation and
natural language processing tasks, confirming the broad applicability of our
findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Mistakenly submitted as a replacement to 2405.05409v4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Domain Shift in Federated Learning via Intra- and
  Inter-Domain Prototypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Q. Le, Ye Lin Tun, Yu Qiao, Minh N. H. Nguyen, Keon Oh Kim, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a decentralized machine learning
technique, allowing clients to train a global model collaboratively without
sharing private data. However, most FL studies ignore the crucial challenge of
heterogeneous domains where each client has a distinct feature distribution,
which is common in real-world scenarios. Prototype learning, which leverages
the mean feature vectors within the same classes, has become a prominent
solution for federated learning under domain skew. However, existing federated
prototype learning methods only consider inter-domain prototypes on the server
and overlook intra-domain characteristics. In this work, we introduce a novel
federated prototype learning method, namely I$^2$PFL, which incorporates
$\textbf{I}$ntra-domain and $\textbf{I}$nter-domain $\textbf{P}$rototypes, to
mitigate domain shifts and learn a generalized global model across multiple
domains in federated learning. To construct intra-domain prototypes, we propose
feature alignment with MixUp-based augmented prototypes to capture the
diversity of local domains and enhance the generalization of local features.
Additionally, we introduce a reweighting mechanism for inter-domain prototypes
to generate generalized prototypes to provide inter-domain knowledge and reduce
domain skew across multiple clients. Extensive experiments on the Digits,
Office-10, and PACS datasets illustrate the superior performance of our method
compared to other baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hyperplane Tree: A Piecewise Linear and Fully Interpre<span class="highlight-title">table</span>
  Decision-making Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Li, Jun Xu, William Ward Armstrong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel tree-based model, Learning Hyperplane Tree
(LHT), which outperforms state-of-the-art (SOTA) tree models for classification
tasks on several public datasets. The structure of LHT is simple and efficient:
it partitions the data using several hyperplanes to progressively distinguish
between target and non-target class samples. Although the separation is not
perfect at each stage, LHT effectively improves the distinction through
successive partitions. During testing, a sample is classified by evaluating the
hyperplanes defined in the branching blocks and traversing down the tree until
it reaches the corresponding leaf block. The class of the test sample is then
determined using the piecewise linear membership function defined in the leaf
blocks, which is derived through least-squares fitting and fuzzy logic. LHT is
highly transparent and interpretable--at each branching block, the contribution
of each feature to the classification can be clearly observed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Score-based 3D molecule generation with neural fields <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Kirchmeyer, Pedro O. Pinheiro, Saeed Saremi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new representation for 3D molecules based on their continuous
atomic density fields. Using this representation, we propose a new model based
on walk-jump sampling for unconditional 3D molecule generation in the
continuous space using neural fields. Our model, FuncMol, encodes molecular
fields into latent codes using a conditional neural field, samples noisy codes
from a Gaussian-smoothed distribution with Langevin MCMC (walk), denoises these
samples in a single step (jump), and finally decodes them into molecular
fields. FuncMol performs all-atom generation of 3D molecules without
assumptions on the molecular structure and scales well with the size of
molecules, unlike most approaches. Our method achieves competitive results on
drug-like molecules and easily scales to macro-cyclic peptides, with at least
one order of magnitude faster sampling. The code is available at
https://github.com/prescient-design/funcmol.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Efficacy of Meta-Learning: Unveiling Superior Data
  Diversity Utilization of MAML Over <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavita Selva, Satita Vittayaareekul, Brando Miranda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, data and model size dominate the narrative in the training of
super-large, powerful models. However, there has been a lack of exploration on
the effect of other attributes of the training dataset on model performance. We
hypothesize that dataset diversity can impact the performance of vision models.
Our study shows positive correlations between test set accuracy and data
diversity, providing an argument for furthering the research of dataset
attributes beyond size. We analyzed pre-training and model-agnostic
meta-learning methods on twelve popular visual datasets (e.g., Omniglot,
CIFAR-FS, Aircraft) and five model configurations, including MAML variants with
different numbers of inner gradient steps and supervised learning. We show
moderate to strong positive correlations (R-squared: 0.15-0.42) between
accuracy and data diversity and weaker but significant correlations (R-squared:
~0.2) between loss and diversity. These findings support our hypothesis and
demonstrate a promising way for a deeper exploration of how formal data
diversity influences model performance. This initial study highlights the
potential of (Task2Vec) data diversity as a valuable measure in the rapidly
evolving field of large-scale learning and emphasizes that understanding the
dataset is key to building more powerful and generalizable models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and
  Unstructured Parameter Prioritization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waqwoya Abebe, Sadegh Jafari, Sixing Yu, Akash Dutta, Jan Strube, Nathan R. Tallent, Luanzheng Guo, Pablo Munoz, Ali Jannesari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) is a powerful approach of automating the
design of efficient neural architectures. In contrast to traditional NAS
methods, recently proposed one-shot NAS methods prove to be more efficient in
performing NAS. One-shot NAS works by generating a singular weight-sharing
supernetwork that acts as a search space (container) of subnetworks. Despite
its achievements, designing the one-shot search space remains a major
challenge. In this work we propose a search space design strategy for Vision
Transformer (ViT)-based architectures. In particular, we convert the Segment
Anything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our
approach involves automating the search space design via layer-wise structured
pruning and parameter prioritization. While the structured pruning applies
probabilistic removal of certain transformer layers, parameter prioritization
performs weight reordering and slicing of MLP-blocks in the remaining layers.
We train supernetworks on several datasets using the sandwich rule. For
deployment, we enhance subnetwork discovery by utilizing a program autotuner to
identify efficient subnetworks within the search space. The resulting
subnetworks are 30-70% smaller in size compared to the original pre-trained SAM
ViT-B, yet outperform the pretrained model. Our work introduces a new and
effective method for ViT NAS search-space design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Bayesian Physics-Informed Kolmogorov-Arnold Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Gao, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification (UQ) plays a pivotal role in scientific machine
learning, especially when surrogate models are used to approximate complex
systems. Although multilayer perceptions (MLPs) are commonly employed as
surrogates, they often suffer from overfitting due to their large number of
parameters. Kolmogorov-Arnold networks (KANs) offer an alternative solution
with fewer parameters. However, gradient-based inference methods, such as
Hamiltonian Monte Carlo (HMC), may result in computational inefficiency when
applied to KANs, especially for large-scale datasets, due to the high cost of
back-propagation.To address these challenges, we propose a novel approach,
combining the dropout Tikhonov ensemble Kalman inversion (DTEKI) with Chebyshev
KANs. This gradient-free method effectively mitigates overfitting and enhances
numerical stability. Additionally, we incorporate the active subspace method to
reduce the parameter-space dimensionality, allowing us to improve the accuracy
of predictions and obtain more reliable uncertainty estimates.Extensive
experiments demonstrate the efficacy of our approach in various test cases,
including scenarios with large datasets and high noise levels. Our results show
that the new method achieves comparable or better accuracy, much higher
efficiency as well as stability compared to HMC, in addition to scalability.
Moreover, by leveraging the low-dimensional parameter subspace, our method
preserves prediction accuracy while substantially reducing further the
computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing Noise Assumptions of Learning Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surbhi Goel, Adam R. Klivans, Konstantinos Stavropoulos, Arsen Vasilyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We pose a fundamental question in computational learning theory: can we
efficiently test whether a training set satisfies the assumptions of a given
noise model? This question has remained unaddressed despite decades of research
on learning in the presence of noise. In this work, we show that this task is
tractable and present the first efficient algorithm to test various noise
assumptions on the training data.
  To model this question, we extend the recently proposed testable learning
framework of Rubinfeld and Vasilyan (2023) and require a learner to run an
associated test that satisfies the following two conditions: (1) whenever the
test accepts, the learner outputs a classifier along with a certificate of
optimality, and (2) the test must pass for any dataset drawn according to a
specified modeling assumption on both the marginal distribution and the noise
model. We then consider the problem of learning halfspaces over Gaussian
marginals with Massart noise (where each label can be flipped with probability
less than $1/2$ depending on the input features), and give a fully-polynomial
time testable learning algorithm.
  We also show a separation between the classical setting of learning in the
presence of structured noise and testable learning. In fact, for the simple
case of random classification noise (where each label is flipped with fixed
probability $\eta = 1/2$), we show that testable learning requires
super-polynomial time while classical learning is trivial.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual
  Defect Detection <span class="chip">ICTAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qisen Cheng, Shuhui Qu, Janghwan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised visual defect detection is critical in industrial applications,
requiring a representation space that captures normal data features while
detecting deviations. Achieving a balance between expressiveness and
compactness is challenging; an overly expressive space risks inefficiency and
mode collapse, impairing detection accuracy. We propose a novel approach using
an enhanced VQ-VAE framework optimized for unsupervised defect detection. Our
model introduces a patch-aware dynamic code assignment scheme, enabling
context-sensitive code allocation to optimize spatial representation. This
strategy enhances normal-defect distinction and improves detection accuracy
during inference. Experiments on MVTecAD, BTAD, and MTSD datasets show our
method achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, Accepted to 36th IEEE ICTAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">Graph</span> Representation Learning with Localized Topological
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuoyu Yan, Qi Zhao, Ze Ye, Tengfei Ma, Liangcai Gao, Zhi Tang, Yusu Wang, Chao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning on graphs is a fundamental problem that can be
crucial in various tasks. Graph neural networks, the dominant approach for
graph representation learning, are limited in their representation power.
Therefore, it can be beneficial to explicitly extract and incorporate
high-order topological and geometric information into these models. In this
paper, we propose a principled approach to extract the rich connectivity
information of graphs based on the theory of persistent homology. Our method
utilizes the topological features to enhance the representation learning of
graph neural networks and achieve state-of-the-art performance on various node
classification and link prediction benchmarks. We also explore the option of
end-to-end learning of the topological features, i.e., treating topological
computation as a differentiable operator during learning. Our theoretical
analysis and empirical study provide insights and potential guidelines for
employing topological features in graph learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in JMLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI Takes a Statistics Exam: A Comparison of Performance
  between ChatGPT3.5, ChatGPT4, and ChatGPT4o-mini 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monnie McGee, Bivin Sadler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many believe that use of generative AI as a private tutor has the potential
to shrink access and achievement gaps between students and schools with
abundant resources versus those with fewer resources. Shrinking the gap is
possible only if paid and free versions of the platforms perform with the same
accuracy. In this experiment, we investigate the performance of GPT versions
3.5, 4.0, and 4o-mini on the same 16-question statistics exam given to a class
of first-year graduate students. While we do not advocate using any generative
AI platform to complete an exam, the use of exam questions allows us to explore
aspects of ChatGPT's responses to typical questions that students might
encounter in a statistics course. Results on accuracy indicate that GPT 3.5
would fail the exam, GPT4 would perform well, and GPT4o-mini would perform
somewhere in between. While we acknowledge the existence of other Generative
AI/LLMs, our discussion concerns only ChatGPT because it is the most widely
used platform on college campuses at this time. We further investigate
differences among the AI platforms in the answers for each problem using
methods developed for text analytics, such as reading level evaluation and
topic modeling. Results indicate that GPT3.5 and 4o-mini have characteristics
that are more similar than either of them have with GPT4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 2 figures, 3 tables. Submitted for publication August,
  2024; revision submitted January 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention is All You Need Until You Need Retention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Murat Yaslioglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a novel Retention Layer mechanism for Transformer based
architectures, addressing their inherent lack of intrinsic retention
capabilities. Unlike human cognition, which can encode and dynamically recall
symbolic templates, Generative Pretrained Transformers rely solely on fixed
pretrained weights and ephemeral context windows, limiting their adaptability.
The proposed Retention Layer incorporates a persistent memory module capable of
real time data population, dynamic recall, and guided output generation. This
enhancement allows models to store, update, and reuse observed patterns across
sessions, enabling incremental learning and bridging the gap between static
pretraining and dynamic, context sensitive adaptation. The Retention Layer
design parallels social learning processes, encompassing attention, retention,
reproduction, and motivation stages. Technically, it integrates a memory
attention mechanism and episodic buffers to manage memory scalability, mitigate
overfitting, and ensure efficient recall. Applications span adaptive personal
assistants, real time fraud detection, autonomous robotics, content moderation,
and healthcare diagnostics. In each domain, the retention mechanism enables
systems to learn incrementally, personalize outputs, and respond to evolving
real world challenges effectively. By emulating key aspects of human learning,
this retention enhanced architecture fosters a more fluid and responsive AI
paradigm, paving the way for dynamic, session aware models that extend the
capabilities of traditional Transformers into domains requiring continual
adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding Extrapolation: a Causal Lens <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjing Kong, Guangyi Chen, Petar Stojanov, Haoxuan Li, Eric P. Xing, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Canonical work handling distribution shifts typically necessitates an entire
target distribution that lands inside the training distribution. However,
practical scenarios often involve only a handful of target samples, potentially
lying outside the training support, which requires the capability of
extrapolation. In this work, we aim to provide a theoretical understanding of
when extrapolation is possible and offer principled methods to achieve it
without requiring an on-support target distribution. To this end, we formulate
the extrapolation problem with a latent-variable model that embodies the
minimal change principle in causal mechanisms. Under this formulation, we cast
the extrapolation problem into a latent-variable identification problem. We
provide realistic conditions on shift properties and the estimation objectives
that lead to identification even when only one off-support target sample is
available, tackling the most challenging scenarios. Our theory reveals the
intricate interplay between the underlying manifold's smoothness and the shift
properties. We showcase how our theoretical results inform the design of
practical adaptation algorithms. Through experiments on both synthetic and
real-world data, we validate our theoretical findings and their practical
implications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoLoop: Fast Visual SLAM Fine-tuning through Agentic Curriculum
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Assaf Lahiany, Oren Gal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current visual SLAM systems face significant challenges in balancing
computational efficiency with robust loop closure handling. Traditional
approaches require careful manual tuning and incur substantial computational
overhead, while learning-based methods either lack explicit loop closure
capabilities or implement them through computationally expensive methods. We
present AutoLoop, a novel approach that combines automated curriculum learning
with efficient fine-tuning for visual SLAM systems. Our method employs a DDPG
(Deep Deterministic Policy Gradient) agent to dynamically adjust loop closure
weights during training, eliminating the need for manual hyperparameter search
while significantly reducing the required training steps. The approach
pre-computes potential loop closure pairs offline and leverages them through an
agent-guided curriculum, allowing the model to adapt efficiently to new
scenarios. Experiments conducted on TartanAir for training and validated across
multiple benchmarks including KITTI, EuRoC, ICL-NUIM and TUM RGB-D demonstrate
that AutoLoop achieves comparable or superior performance while reducing
training time by an order of magnitude compared to traditional approaches.
AutoLoop provides a practical solution for rapid adaptation of visual SLAM
systems, automating the weight tuning process that traditionally requires
multiple manual iterations. Our results show that this automated curriculum
strategy not only accelerates training but also maintains or improves the
model's performance across diverse environmental conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Federated Multi-Armed Bandit Learning for Content Dissemination
  using Swarm of UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an Unmanned Aerial Vehicle - enabled content management
architecture that is suitable for critical content access in communities of
users that are communication-isolated during diverse types of disaster
scenarios. The proposed architecture leverages a hybrid network of stationary
anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The
anchor UAVs are equipped with both vertical and lateral communication links,
and they serve local users, while the mobile micro-ferrying UAVs extend
coverage across communities with increased mobility. The focus is on developing
a content dissemination system that dynamically learns optimal caching policies
to maximize content availability. The core innovation is an adaptive content
dissemination framework based on distributed Federated Multi-Armed Bandit
learning. The goal is to optimize UAV content caching decisions based on
geo-temporal content popularity and user demand variations. A Selective Caching
Algorithm is also introduced to reduce redundant content replication by
incorporating inter-UAV information sharing. This method strategically
preserves the uniqueness in user preferences while amalgamating the
intelligence across a distributed learning system. This approach improves the
learning algorithm's ability to adapt to diverse user preferences. Functional
verification and performance evaluation confirm the proposed architecture's
utility across different network sizes, UAV swarms, and content popularity
patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 11 figures, 1 table, 4 algorithms, journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow
  in Shallow Linear Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierfrancesco Beneventano, Blake Woodworth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the gradient descent (GD) dynamics of a depth-2 linear neural
network with a single input and output. We show that GD converges at an
explicit linear rate to a global minimum of the training loss, even with a
large stepsize -- about $2/\textrm{sharpness}$. It still converges for even
larger stepsizes, but may do so very slowly. We also characterize the solution
to which GD converges, which has lower norm and sharpness than the gradient
flow solution. Our analysis reveals a trade off between the speed of
convergence and the magnitude of implicit regularization. This sheds light on
the benefits of training at the ``Edge of Stability'', which induces additional
regularization by delaying convergence and may have implications for training
more complex models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Delay Sensitive Hierarchical Federated Learning with Stochastic Local
  Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulmoneam Ali, Ahmed Arafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impact of local averaging on the performance of federated learning (FL)
systems is studied in the presence of communication delay between the clients
and the parameter server. To minimize the effect of delay, clients are assigned
into different groups, each having its own local parameter server (LPS) that
aggregates its clients' models. The groups' models are then aggregated at a
global parameter server (GPS) that only communicates with the LPSs. Such
setting is known as hierarchical FL (HFL). Unlike most works in the literature,
the number of local and global communication rounds in our work is randomly
determined by the (different) delays experienced by each group of clients.
Specifically, the number of local averaging rounds is tied to a wall-clock time
period coined the sync time $S$, after which the LPSs synchronize their models
by sharing them with the GPS. Such sync time $S$ is then reapplied until a
global wall-clock time is exhausted.
  First, an upper bound on the deviation between the updated model at each LPS
with respect to that available at the GPS is derived. This is then used as a
tool to derive the convergence analysis of our proposed delay-sensitive HFL
algorithm, first at each LPS individually, and then at the GPS. Our theoretical
convergence bound showcases the effects of the whole system's parameters,
including the number of groups, the number of clients per group, and the value
of $S$. Our results show that the value of $S$ should be carefully chosen,
especially since it implicitly governs how the delay statistics affect the
performance of HFL in situations where training time is restricted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the IEEE Transactions on Cognitive Communications and
  Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Machines for Deep RL in Noisy and Uncertain Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00120v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00120v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew C. Li, Zizhao Chen, Toryn Q. Klassen, Pashootan Vaezipoor, Rodrigo Toro Icarte, Sheila A. McIlraith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward Machines provide an automaton-inspired structure for specifying
instructions, safety constraints, and other temporally extended reward-worthy
behaviour. By exposing the underlying structure of a reward function, they
enable the decomposition of an RL task, leading to impressive gains in sample
efficiency. Although Reward Machines and similar formal specifications have a
rich history of application towards sequential decision-making problems, they
critically rely on a ground-truth interpretation of the domain-specific
vocabulary that forms the building blocks of the reward function--such
ground-truth interpretations are elusive in the real world due in part to
partial observability and noisy sensing. In this work, we explore the use of
Reward Machines for Deep RL in noisy and uncertain environments. We
characterize this problem as a POMDP and propose a suite of RL algorithms that
exploit task structure under uncertain interpretation of the domain-specific
vocabulary. Through theory and experiments, we expose pitfalls in naive
approaches to this problem while simultaneously demonstrating how task
structure can be successfully leveraged under noisy interpretations of the
vocabulary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Framework for Inference-time Scaling and Steering of Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models produce impressive results in modalities ranging from images
and video to protein design and text. However, generating samples with
user-specified properties remains a challenge. Recent research proposes
fine-tuning models to maximize rewards that capture desired properties, but
these methods require expensive training and are prone to mode collapse. In
this work, we propose Feynman Kac (FK) steering, an inference-time framework
for steering diffusion models with reward functions. FK steering works by
sampling a system of multiple interacting diffusion processes, called
particles, and resampling particles at intermediate steps based on scores
computed using functions called potentials. Potentials are defined using
rewards for intermediate states and are selected such that a high value
indicates that the particle will yield a high-reward sample. We explore various
choices of potentials, intermediate rewards, and samplers. We evaluate FK
steering on text-to-image and text diffusion models. For steering text-to-image
models with a human preference reward, we find that FK steering a 0.8B
parameter model outperforms a 2.6B parameter fine-tuned model on prompt
fidelity, with faster sampling and no training. For steering text diffusion
models with rewards for text quality and specific text attributes, we find that
FK steering generates lower perplexity, more linguistically acceptable outputs
and enables gradient-free control of attributes like toxicity. Our results
demonstrate that inference-time scaling and steering of diffusion models, even
with off-the-shelf rewards, can provide significant sample quality gains and
controllability benefits. Code is available at
https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Federated Learning for Functional Mean Estimation under
  Heterogeneous Privacy Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Cai, Abhinav Chakraborty, Lasse Vuursteen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a distributed machine learning technique designed
to preserve data privacy and security, and it has gained significant importance
due to its broad range of applications. This paper addresses the problem of
optimal functional mean estimation from discretely sampled data in a federated
setting.
  We consider a heterogeneous framework where the number of individuals,
measurements per individual, and privacy parameters vary across one or more
servers, under both common and independent design settings. In the common
design setting, the same design points are measured for each individual,
whereas in the independent design, each individual has their own random
collection of design points. Within this framework, we establish minimax upper
and lower bounds for the estimation error of the underlying mean function,
highlighting the nuanced differences between common and independent designs
under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and
accuracy and provide optimality results that quantify the fundamental limits of
private functional mean estimation across diverse distributed settings. These
results characterize the cost of privacy and offer practical insights into the
potential for privacy-preserving statistical analysis in federated
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages: 25 page article and 29 pages of appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Synthetic Data Generated by Deep Generative Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Decruyenaere, Heidelinde Dehaene, Paloma Rabaey, Christiaan Polet, Johan Decruyenaere, Thomas Demeester, Stijn Vansteelandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While synthetic data hold great promise for privacy protection, their
statistical analysis poses significant challenges that necessitate innovative
solutions. The use of deep generative models (DGMs) for synthetic data
generation is known to induce considerable bias and imprecision into synthetic
data analyses, compromising their inferential utility as opposed to original
data analyses. This bias and uncertainty can be substantial enough to impede
statistical convergence rates, even in seemingly straightforward analyses like
mean calculation. The standard errors of such estimators then exhibit slower
shrinkage with sample size than the typical 1 over root-$n$ rate. This
complicates fundamental calculations like p-values and confidence intervals,
with no straightforward remedy currently available. In response to these
challenges, we propose a new strategy that targets synthetic data created by
DGMs for specific data analyses. Drawing insights from debiased and targeted
machine learning, our approach accounts for biases, enhances convergence rates,
and facilitates the calculation of estimators with easily approximated large
sample variances. We exemplify our proposal through a simulation study on toy
data and two case studies on real-world data, highlighting the importance of
tailoring DGMs for targeted data analysis. This debiasing strategy contributes
to advancing the reliability and applicability of synthetic data in statistical
inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024), joint first authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Customizable LLM-Powered Chatbot for Behavioral Science Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenon Lamprou, Yashar Moshfeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Artificial Intelligence has resulted in the advent
of Large Language Models (LLMs) with the capacity to produce text that closely
resembles human communication. These models have been seamlessly integrated
into diverse applications, enabling interactive and responsive communication
across multiple platforms. The potential utility of chatbots transcends these
traditional applications, particularly in research contexts, wherein they can
offer valuable insights and facilitate the design of innovative experiments. In
this study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based
chatbot system designed to assist in behavioral science research. The system is
meticulously designed to function as an experimental instrument rather than a
conventional chatbot, necessitating users to input a username and experiment
code upon access. This setup facilitates precise data cross-referencing,
thereby augmenting the integrity and applicability of the data collected for
research purposes. It can be easily expanded to accommodate new basic events as
needed; and it allows researchers to integrate their own logging events without
the necessity of implementing a separate logging mechanism. It is worth noting
that our system was built to assist primarily behavioral science research but
is not limited to it, it can easily be adapted to assist information retrieval
research or interacting with chat bot agents in general.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Discrete-sequence <span class="highlight-title">Dataset</span> for Evaluating Online Unsupervised Anomaly
  Detection Approaches for Multivariate Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13951v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13951v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Correia, Jan-Christoph Goos, Thomas Bäck, Anna V. Kononova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarking anomaly detection approaches for multivariate time series is
challenging due to the lack of high-quality datasets. Current publicly
available datasets are too small, not diverse and feature trivial anomalies,
which hinders measurable progress in this research area. We propose a solution:
a diverse, extensive, and non-trivial dataset generated via state-of-the-art
simulation tools that reflects realistic behaviour of an automotive powertrain,
including its multivariate, dynamic and variable-state properties. To cater for
both unsupervised and semi-supervised anomaly detection settings, as well as
time series generation and forecasting, we make different versions of the
dataset available, where training and test subsets are offered in contaminated
and clean versions, depending on the task. We also provide baseline results
from a small selection of approaches based on deterministic and variational
autoencoders, as well as a non-parametric approach. As expected, the baseline
experimentation shows that the approaches trained on the semi-supervised
version of the dataset outperform their unsupervised counterparts, highlighting
a need for approaches more robust to contaminated training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE Transactions on Reliability journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Spurious Correlations using Counterfactual Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Paul Cohen, Louis Blankemeier, Akshay Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models driven by spurious correlations often yield poor generalization
performance. We propose the counterfactual (CF) alignment method to detect and
quantify spurious correlations of black box classifiers. Our methodology is
based on counterfactual images generated with respect to one classifier being
input into other classifiers to see if they also induce changes in the outputs
of these classifiers. The relationship between these responses can be
quantified and used to identify specific instances where a spurious correlation
exists. This is validated by observing intuitive trends in face-attribute and
waterbird classifiers, as well as by fabricating spurious correlations and
detecting their presence, both visually and quantitatively. Furthermore,
utilizing the CF alignment method, we demonstrate that we can evaluate robust
optimization methods (GroupDRO, JTT, and FLAC) by detecting a reduction in
spurious correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR), Code:
  https://github.com/ieee8023/latentshift</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PACE: Marrying generalization in PArameter-efficient fine-tuning with
  Consistency rEgularization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17137v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17137v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Ni, Shan Zhang, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained
transformers to downstream tasks. However, the optimization of tasks
performance often comes at the cost of generalizability in fine-tuned models.
To address this issue, we theoretically connect smaller weight gradient norms
during training and larger datasets to the improvements in model
generalization. Motivated by this connection, we propose reducing gradient
norms for enhanced generalization and aligning fine-tuned model with the
pre-trained counterpart to retain knowledge from large-scale pre-training data.
Yet, naive alignment does not guarantee gradient reduction and can potentially
cause gradient explosion, complicating efforts to manage gradients. To address
such an issue, we propose PACE, marrying generalization of PArameter-efficient
fine-tuning with Consistency rEgularization. We perturb features learned from
the adapter with the multiplicative noise and ensure the fine-tuned model
remains consistent for same sample under different perturbations. Theoretical
analysis shows that PACE not only implicitly regularizes gradients for enhanced
generalization, but also implicitly aligns the fine-tuned and pre-trained
models to retain knowledge. Experimental evidence supports our theories. PACE
surpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC,
few-shot learning, domain adaptation) showcasing its potential for
resource-efficient fine-tuning. It also improves LoRA in text classification
(GLUE) and mathematical reasoning (GSM-8K). The code is available at
https://github.com/MaxwellYaoNi/PACE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 as a spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Kernel Thinning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Gong, Kyuseong Choi, Raaz Dwivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The kernel thinning algorithm of Dwivedi & Mackey (2024) provides a
better-than-i.i.d. compression of a generic set of points. By generating
high-fidelity coresets of size significantly smaller than the input points, KT
is known to speed up unsupervised tasks like Monte Carlo integration,
uncertainty quantification, and non-parametric hypothesis testing, with minimal
loss in statistical accuracy. In this work, we generalize the KT algorithm to
speed up supervised learning problems involving kernel methods. Specifically,
we combine two classical algorithms--Nadaraya-Watson (NW) regression or kernel
smoothing, and kernel ridge regression (KRR)--with KT to provide a quadratic
speed-up in both training and inference times. We show how distribution
compression with KT in each setting reduces to constructing an appropriate
kernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators.
We prove that KT-based regression estimators enjoy significantly superior
computational efficiency over the full-data estimators and improved statistical
efficiency over i.i.d. subsampling of the training data. En route, we also
provide a novel multiplicative error guarantee for compressing with KT. We
validate our design choices with both simulations and real data experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Multi-Physics Simulations and Machine Learning to Define the
  Spatter Mechanism and Process Window in Laser Powder Bed Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olabode T. Ajenifujah, Francis Ogoke, Florian Wirth, Jack Beuth, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laser powder bed fusion (LPBF) has shown promise for wide range of
applications due to its ability to fabricate freeform geometries and generate a
controlled microstructure. However, components generated by LPBF still possess
sub-optimal mechanical properties due to the defects that are created during
laser-material interactions. In this work, we investigate mechanism of spatter
formation, using a high-fidelity modelling tool that was built to simulate the
multi-physics phenomena in LPBF. The modelling tool have the capability to
capture the 3D resolution of the meltpool and the spatter behavior. To
understand spatter behavior and formation, we reveal its properties at ejection
and evaluate its variation from the meltpool, the source where it is formed.
The dataset of the spatter and the meltpool collected consist of 50 % spatter
and 50 % melt pool samples, with features that include position components,
velocity components, velocity magnitude, temperature, density and pressure. The
relationship between the spatter and the meltpool were evaluated via
correlation analysis and machine learning (ML) algorithms for classification
tasks. Upon screening different ML algorithms on the dataset, a high accuracy
was observed for all the ML models, with ExtraTrees having the highest at 96 %
and KNN having the lowest at 94 %.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on <span class="highlight-title">Tabular</span>
  Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04491v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04491v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Holzmüller, Léo Grinsztajn, Ingo Steinwart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For classification and regression on tabular data, the dominance of
gradient-boosted decision trees (GBDTs) has recently been challenged by often
much slower deep learning methods with extensive hyperparameter tuning. We
address this discrepancy by introducing (a) RealMLP, an improved multilayer
perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and
RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark
with 118 datasets and compare them to hyperparameter-optimized versions on a
disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly
benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large
tabular datasets (1K--500K samples) show that RealMLP offers a favorable
time-accuracy tradeoff compared to other neural baselines and is competitive
with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and
GBDTs with improved default parameters can achieve excellent results without
hyperparameter tuning. Finally, we demonstrate that some of RealMLP's
improvements can also considerably improve the performance of TabR with default
parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Changes in v3: mention bug in XGBoost results, mention
  original name of he+5 method. Code is available at
  github.com/dholzmueller/pytabkit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble sampling for linear bandits: small ensembles suffice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08376v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08376v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Janz, Alexander E. Litvak, Csaba Szepesvári
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide the first useful and rigorous analysis of ensemble sampling for
the stochastic linear bandit setting. In particular, we show that, under
standard assumptions, for a $d$-dimensional stochastic linear bandit with an
interaction horizon $T$, ensemble sampling with an ensemble of size of order $d
\log T$ incurs regret at most of the order $(d \log T)^{5/2} \sqrt{T}$. Ours is
the first result in any structured setting not to require the size of the
ensemble to scale linearly with $T$ -- which defeats the purpose of ensemble
sampling -- while obtaining near $\smash{\sqrt{T}}$ order regret. Our result is
also the first to allow for infinite action sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferring stochastic low-rank recurrent neural networks from neural data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16749v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16749v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthijs Pals, A Erdem Sağtekin, Felix Pei, Manuel Gloeckler, Jakob H Macke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central aim in computational neuroscience is to relate the activity of
large populations of neurons to an underlying dynamical system. Models of these
neural dynamics should ideally be both interpretable and fit the observed data
well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability
by having tractable dynamics. However, it is unclear how to best fit low-rank
RNNs to data consisting of noisy observations of an underlying stochastic
system. Here, we propose to fit stochastic low-rank RNNs with variational
sequential Monte Carlo methods. We validate our method on several datasets
consisting of both continuous and spiking neural data, where we obtain lower
dimensional latent dynamics than current state of the art methods.
Additionally, for low-rank models with piecewise linear nonlinearities, we show
how to efficiently identify all fixed points in polynomial rather than
exponential cost in the number of units, making analysis of the inferred
dynamics tractable for large RNNs. Our method both elucidates the dynamical
systems underlying experimental recordings and provides a generative model
whose trajectories match observed variability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taming the Long Tail in Human Mobility Prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14970v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14970v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohang Xu, Renhe Jiang, Chuang Yang, Zipei Fan, Kaoru Sezaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the popularity of location-based services, human mobility prediction
plays a key role in enhancing personalized navigation, optimizing
recommendation systems, and facilitating urban mobility and planning. This
involves predicting a user's next POI (point-of-interest) visit using their
past visit history. However, the uneven distribution of visitations over time
and space, namely the long-tail problem in spatial distribution, makes it
difficult for AI models to predict those POIs that are less visited by humans.
In light of this issue, we propose the Long-Tail Adjusted Next POI Prediction
(LoTNext) framework for mobility prediction, combining a Long-Tailed Graph
Adjustment module to reduce the impact of the long-tailed nodes in the user-POI
interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss
by logit score and sample weight adjustment strategy. Also, we employ the
auxiliary prediction task to enhance generalization and accuracy. Our
experiments with two real-world trajectory datasets demonstrate that LoTNext
significantly surpasses existing state-of-the-art works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Surprising Ineffectiveness of <span class="highlight-title">Pre-Train</span>ed Visual Representations for
  Model-Based Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Schneider, Robert Krug, Narunas Vaskevicius, Luigi Palmieri, Joschka Boedecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Reinforcement Learning (RL) methods often require extensive amounts of
data. As opposed to model-free RL, model-based RL (MBRL) offers a potential
solution with efficient data utilization through planning. Additionally, RL
lacks generalization capabilities for real-world tasks. Prior work has shown
that incorporating pre-trained visual representations (PVRs) enhances sample
efficiency and generalization. While PVRs have been extensively studied in the
context of model-free RL, their potential in MBRL remains largely unexplored.
In this paper, we benchmark a set of PVRs on challenging control tasks in a
model-based RL setting. We investigate the data efficiency, generalization
capabilities, and the impact of different properties of PVRs on the performance
of model-based agents. Our results, perhaps surprisingly, reveal that for MBRL
current PVRs are not more sample efficient than learning representations from
scratch, and that they do not generalize better to out-of-distribution (OOD)
settings. To explain this, we analyze the quality of the trained dynamics
model. Furthermore, we show that data diversity and network architecture are
the most important contributors to OOD generalization performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024). Project page: https://schneimo.com/pvr4mbrl/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CGCOD: Class-Guided Camouflaged Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Zhang, Qing Zhang, Jiayun Wu, Youwei Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflaged Object Detection (COD) aims to identify objects that blend
seamlessly into their surroundings. The inherent visual complexity of
camouflaged objects, including their low contrast with the background, diverse
textures, and subtle appearance variations, often obscures semantic cues,
making accurate segmentation highly challenging. Existing methods primarily
rely on visual features, which are insufficient to handle the variability and
intricacy of camouflaged objects, leading to unstable object perception and
ambiguous segmentation results. To tackle these limitations, we introduce a
novel task, class-guided camouflaged object detection (CGCOD), which extends
traditional COD task by incorporating object-specific class knowledge to
enhance detection robustness and accuracy. To facilitate this task, we present
a new dataset, CamoClass, comprising real-world camouflaged objects with class
annotations. Furthermore, we propose a multi-stage framework, CGNet, which
incorporates a plug-and-play class prompt generator and a simple yet effective
class-guided detector. This establishes a new paradigm for COD, bridging the
gap between contextual understanding and class-guided detection. Extensive
experimental results demonstrate the effectiveness of our flexible framework in
improving the performance of proposed and existing detectors by leveraging
class-level textual information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoME: A Robust Mixed-Effects Bandit Algorithm for Optimizing Mobile
  Health Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06403v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06403v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Easton K. Huch, Jieru Shi, Madeline R. Abbott, Jessica R. Golbus, Alexander Moreno, Walter H. Dempsey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile health leverages personalized and contextually tailored interventions
optimized through bandit and reinforcement learning algorithms. In practice,
however, challenges such as participant heterogeneity, nonstationarity, and
nonlinear relationships hinder algorithm performance. We propose RoME, a Robust
Mixed-Effects contextual bandit algorithm that simultaneously addresses these
challenges via (1) modeling the differential reward with user- and
time-specific random effects, (2) network cohesion penalties, and (3) debiased
machine learning for flexible estimation of baseline rewards. We establish a
high-probability regret bound that depends solely on the dimension of the
differential-reward model, enabling us to achieve robust regret bounds even
when the baseline reward is highly complex. We demonstrate the superior
performance of the RoME algorithm in a simulation and two off-policy evaluation
studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Algorithms for Contextual Dynamic Pricing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matilde Tullii, Solenne Gaucher, Nadav Merlis, Vianney Perchet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contextual dynamic pricing, a seller sequentially prices goods based on
contextual information. Buyers will purchase products only if the prices are
below their valuations. The goal of the seller is to design a pricing strategy
that collects as much revenue as possible. We focus on two different valuation
models. The first assumes that valuations linearly depend on the context and
are further distorted by noise. Under minor regularity assumptions, our
algorithm achieves an optimal regret bound of $\tilde{\mathcal{O}}(T^{2/3})$,
improving the existing results. The second model removes the linearity
assumption, requiring only that the expected buyer valuation is
$\beta$-H\"older in the context. For this model, our algorithm obtains a regret
$\tilde{\mathcal{O}}(T^{d+2\beta/d+3\beta})$, where $d$ is the dimension of the
context space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRIMO: Private Regression in Multiple Outcomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Neel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new private regression setting we call Private Regression in
Multiple Outcomes (PRIMO), inspired by the common situation where a data
analyst wants to perform a set of $l$ regressions while preserving privacy,
where the features $X$ are shared across all $l$ regressions, and each
regression $i \in [l]$ has a different vector of outcomes $y_i$. Naively
applying existing private linear regression techniques $l$ times leads to a
$\sqrt{l}$ multiplicative increase in error over the standard linear regression
setting. We apply a variety of techniques including sufficient statistics
perturbation (SSP) and geometric projection-based methods to develop scalable
algorithms that outperform this baseline across a range of parameter regimes.
In particular, we obtain no dependence on l in the asymptotic error when $l$ is
sufficiently large. Empirically, on the task of genomic risk prediction with
multiple phenotypes we find that even for values of $l$ far smaller than the
theory would predict, our projection-based method improves the accuracy
relative to the variant that doesn't use the projection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model
  Deforestation: An Exemplification from the Amazon Rainforest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik R., Ramamoorthy A
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent automation supports us against cyclones, droughts, and seismic
events with recent technology advancements. Algorithmic learning has advanced
fields like neuroscience, genetics, and human-computer interaction. Time-series
data boosts progress. Challenges persist in adopting these approaches in
traditional fields. Neural networks face comprehension and bias issues. AI's
expansion across scientific areas is due to adaptable descriptors and
combinatorial argumentation. This article focuses on modeling Forest loss using
the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest
cover, demonstrated on Amazon Rainforest data against other forecasters like
Long Short-Term Memory, N-BEATS, RCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The experimental data used in this article has given wrong practical
  interpretation. The data has to be updated to improve this</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Optimal Tax Design in Nonatomic Congestion Games <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiwen Cui, Maryam Fazel, Simon S. Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multiplayer games, self-interested behavior among the players can harm the
social welfare. Tax mechanisms are a common method to alleviate this issue and
induce socially optimal behavior. In this work, we take the initial step of
learning the optimal tax that can maximize social welfare with limited feedback
in congestion games. We propose a new type of feedback named \emph{equilibrium
feedback}, where the tax designer can only observe the Nash equilibrium after
deploying a tax plan. Existing algorithms are not applicable due to the
exponentially large tax function space, nonexistence of the gradient, and
nonconvexity of the objective. To tackle these challenges, we design a
computationally efficient algorithm that leverages several novel components:
(1) a piece-wise linear tax to approximate the optimal tax; (2) extra linear
terms to guarantee a strongly convex potential function; (3) an efficient
subroutine to find the exploratory tax that can provide critical information
about the game. The algorithm can find an $\epsilon$-optimal tax with $O(\beta
F^2/\epsilon)$ sample complexity, where $\beta$ is the smoothness of the cost
function and $F$ is the number of facilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages. Accepted by Conference on Neural Information Processing
  Systems (NeurIPS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Artificial Intelligence Methods for Lead Time Prediction
  in Non-Cycled Areas of Automotive Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cornelius Hake, Jonas Weigele, Frederik Reichert, Christian Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The present study examines the effectiveness of applying Artificial
Intelligence methods in an automotive production environment to predict unknown
lead times in a non-cycle-controlled production area. Data structures are
analyzed to identify contextual features and then preprocessed using one-hot
encoding. Methods selection focuses on supervised machine learning techniques.
In supervised learning methods, regression and classification methods are
evaluated. Continuous regression based on target size distribution is not
feasible. Classification methods analysis shows that Ensemble Learning and
Support Vector Machines are the most suitable. Preliminary study results
indicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost
yield the best results. After further testing and extensive hyperparameter
optimization, the final method choice is the LightGBM algorithm. Depending on
feature availability and prediction interval granularity, relative prediction
accuracies of up to 90% can be achieved. Further tests highlight the importance
of periodic retraining of AI models to accurately represent complex production
processes using the database. The research demonstrates that AI methods can be
effectively applied to highly variable production data, adding business value
by providing an additional metric for various control tasks while outperforming
current non AI-based systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constrained Latent Action Policies for Model-Based Offline Reinforcement
  Learning <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Alles, Philip Becker-Ehmck, Patrick van der Smagt, Maximilian Karl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In offline reinforcement learning, a policy is learned using a static dataset
in the absence of costly feedback from the environment. In contrast to the
online setting, only using static datasets poses additional challenges, such as
policies generating out-of-distribution samples. Model-based offline
reinforcement learning methods try to overcome these by learning a model of the
underlying dynamics of the environment and using it to guide policy search. It
is beneficial but, with limited datasets, errors in the model and the issue of
value overestimation among out-of-distribution states can worsen performance.
Current model-based methods apply some notion of conservatism to the Bellman
update, often implemented using uncertainty estimation derived from model
ensembles. In this paper, we propose Constrained Latent Action Policies (C-LAP)
which learns a generative model of the joint distribution of observations and
actions. We cast policy learning as a constrained objective to always stay
within the support of the latent action distribution, and use the generative
capabilities of the model to impose an implicit constraint on the generated
actions. Thereby eliminating the need to use additional uncertainty penalties
on the Bellman update and significantly decreasing the number of gradient steps
required to learn a policy. We empirically evaluate C-LAP on the D4RL and
V-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art
methods, especially outperforming on datasets with visual observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Space Magnitude for Evaluating the Diversity of Latent
  Representations <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16054v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16054v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharina Limbeck, Rayna Andreeva, Rik Sarkar, Bastian Rieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The magnitude of a metric space is a novel invariant that provides a measure
of the 'effective size' of a space across multiple scales, while also capturing
numerous geometrical properties, such as curvature, density, or entropy. We
develop a family of magnitude-based measures of the intrinsic diversity of
latent representations, formalising a novel notion of dissimilarity between
magnitude functions of finite metric spaces. Our measures are provably stable
under perturbations of the data, can be efficiently calculated, and enable a
rigorous multi-scale characterisation and comparison of latent representations.
We show their utility and superior performance across different domains and
tasks, including (i) the automated estimation of diversity, (ii) the detection
of mode collapse, and (iii) the evaluation of generative models for text,
image, and graph data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 38th Conference on Neural Information Processing
  Systems (NeurIPS) 2024. The code for computing magnitude is available at
  https://github.com/aidos-lab/magnipy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximizing Uncertainty for Federated learning via Bayesian
  Optimisation-based Model Poisoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marios Aristodemou, Xiaolan Liu, Yuan Wang, Konstantinos G. Kyriakopoulos, Sangarapillai Lambotharan, Qingsong Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we transition from Narrow Artificial Intelligence towards Artificial Super
Intelligence, users are increasingly concerned about their privacy and the
trustworthiness of machine learning (ML) technology. A common denominator for
the metrics of trustworthiness is the quantification of uncertainty inherent in
DL algorithms, and specifically in the model parameters, input data, and model
predictions. One of the common approaches to address privacy-related issues in
DL is to adopt distributed learning such as federated learning (FL), where
private raw data is not shared among users. Despite the privacy-preserving
mechanisms in FL, it still faces challenges in trustworthiness. Specifically,
the malicious users, during training, can systematically create malicious model
parameters to compromise the models predictive and generative capabilities,
resulting in high uncertainty about their reliability. To demonstrate malicious
behaviour, we propose a novel model poisoning attack method named Delphi which
aims to maximise the uncertainty of the global model output. We achieve this by
taking advantage of the relationship between the uncertainty and the model
parameters of the first hidden layer of the local model. Delphi employs two
types of optimisation , Bayesian Optimisation and Least Squares Trust Region,
to search for the optimal poisoned model parameters, named as Delphi-BO and
Delphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise
the distance of the predictive probability distribution towards an uncertain
distribution of model output. Furthermore, we establish a mathematical proof
for the attack effectiveness demonstrated in FL. Numerical results demonstrate
that Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR
highlighting vulnerability of FL systems to model poisoning attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applying the maximum entropy principle to neural networks enhances
  multi-species distribution models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Ryckewaert, Diego Marcos, Christophe Botella, Maximilien Servajean, Pierre Bonnet, Alexis Joly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of citizen science initiatives has led to a significant
growth of biodiversity databases, and particularly presence-only (PO)
observations. PO data are invaluable for understanding species distributions
and their dynamics, but their use in a Species Distribution Model (SDM) is
curtailed by sampling biases and the lack of information on absences. Poisson
point processes are widely used for SDMs, with Maxent being one of the most
popular methods. Maxent maximises the entropy of a probability distribution
across sites as a function of predefined transformations of variables, called
features. In contrast, neural networks and deep learning have emerged as a
promising technique for automatic feature extraction from complex input
variables. Arbitrarily complex transformations of input variables can be
learned from the data efficiently through backpropagation and stochastic
gradient descent (SGD). In this paper, we propose DeepMaxent, which harnesses
neural networks to automatically learn shared features among species, using the
maximum entropy principle. To do so, it employs a normalised Poisson loss where
for each species, presence probabilities across sites are modelled by a neural
network. We evaluate DeepMaxent on a benchmark dataset known for its spatial
sampling biases, using PO data for calibration and presence-absence (PA) data
for validation across six regions with different biological groups and
covariates. Our results indicate that DeepMaxent performs better than Maxent
and other leading SDMs across all regions and taxonomic groups. The method
performs particularly well in regions of uneven sampling, demonstrating
substantial potential to increase SDM performances. In particular, our approach
yields more accurate predictions than traditional single-species models, which
opens up new possibilities for methodological enhancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Methods in Ecology and Evolution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Closer Look at Deep Learning Methods on <span class="highlight-title">Tabular</span> <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00956v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00956v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han-Jia Ye, Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data is prevalent across diverse domains in machine learning. While
classical methods like tree-based models have long been effective, Deep Neural
Network (DNN)-based methods have recently demonstrated promising performance.
However, the diverse characteristics of methods and the inherent heterogeneity
of tabular datasets make understanding and interpreting tabular methods both
challenging and prone to unstable observations. In this paper, we conduct
in-depth evaluations and comprehensive analyses of tabular methods, with a
particular focus on DNN-based models, using a benchmark of over 300 tabular
datasets spanning a wide range of task types, sizes, and domains. First, we
perform an extensive comparison of 32 state-of-the-art deep and tree-based
methods, evaluating their average performance across multiple criteria.
Although method ranks vary across datasets, we empirically find that
top-performing methods tend to concentrate within a small subset of tabular
models, regardless of the criteria used. Next, we investigate whether the
training dynamics of deep tabular models can be predicted based on dataset
properties. This approach not only offers insights into the behavior of deep
tabular methods but also identifies a core set of "meta-features" that reflect
dataset heterogeneity. The other subset includes datasets where method ranks
are consistent with the overall benchmark, acting as a reliable probe for
further tabular analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaLRP: Explaining Selective State Space Sequence Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07592v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07592v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnoush Rezaei Jafari, Grégoire Montavon, Klaus-Robert Müller, Oliver Eberle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent sequence modeling approaches using selective state space sequence
models, referred to as Mamba models, have seen a surge of interest. These
models allow efficient processing of long sequences in linear time and are
rapidly being adopted in a wide range of applications such as language
modeling, demonstrating promising performance. To foster their reliable use in
real-world scenarios, it is crucial to augment their transparency. Our work
bridges this critical gap by bringing explainability, particularly Layer-wise
Relevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of
relevance conservation, we identify specific components in the Mamba
architecture, which cause unfaithful explanations. To remedy this issue, we
propose MambaLRP, a novel algorithm within the LRP framework, which ensures a
more stable and reliable relevance propagation through these components. Our
proposed method is theoretically sound and excels in achieving state-of-the-art
explanation performance across a diverse range of models and datasets.
Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures,
uncovering various biases and evaluating their significance. It also enables
the analysis of previous speculations regarding the long-range capabilities of
Mamba models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Low-Ranked Self-Attention Transformer for Remaining Useful
  Lifetime Prediction of Optical Fiber Amplifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Schneider, Lutz Rapp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical fiber amplifiers are key elements in present optical networks.
Failures of these components result in high financial loss of income of the
network operator as the communication traffic over an affected link is
interrupted. Applying Remaining useful lifetime (RUL) prediction in the context
of Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming
system failures at an early stage, so that network outages can be minimized
through planning of targeted maintenance actions, ensures reliability and
safety. Optical fiber amplifier are complex systems, that work under various
operating conditions, which makes correct forecasting a difficult task.
Increased monitoring capabilities of systems results in datasets that
facilitate the application of data-driven RUL prediction methods. Deep learning
models in particular have shown good performance, but generalization based on
comparatively small datasets for RUL prediction is difficult. In this paper, we
propose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL
prediction method. SLAT is based on an encoder-decoder architecture, wherein
two parallel working encoders extract features for sensors and time steps. By
utilizing the self-attention mechanism, long-term dependencies can be learned
from long sequences. The implementation of sparsity in the attention matrix and
a low-rank parametrization reduce overfitting and increase generalization.
Experimental application to optical fiber amplifiers exemplified on EDFA, as
well as a reference dataset from turbofan engines, shows that SLAT outperforms
the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FADE: Towards Fairness-aware Augmentation for Domain Generalization via
  Classifier-Guided Score-based Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09495v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09495v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Lin, Dong Li, Chen Zhao, Minglai Shao, Guihong Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness-aware domain generalization (FairDG) has emerged as a critical
challenge for deploying trustworthy AI systems, particularly in scenarios
involving distribution shifts. Traditional methods for addressing fairness have
failed in domain generalization due to their lack of consideration for
distribution shifts. Although disentanglement has been used to tackle FairDG,
it is limited by its strong assumptions. To overcome these limitations, we
propose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as
a novel approach to effectively address the FairDG issue. Specifically, we
first pre-train a score-based diffusion model (SDM) and two classifiers to
equip the model with strong generalization capabilities across different
domains. Then, we guide the SDM using these pre-trained classifiers to
effectively eliminate sensitive information from the generated data. Finally,
the generated fair data is used to train downstream classifiers, ensuring
robust performance under new data distributions. Extensive experiments on three
real-world datasets demonstrate that FADE not only enhances fairness but also
improves accuracy in the presence of distribution shifts. Additionally, FADE
outperforms existing methods in achieving the best accuracy-fairness
trade-offs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallelizing Linear Transformers with the Delta Rule over Sequence
  Length 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06484v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06484v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers with linear attention (i.e., linear transformers) and
state-space models have recently been suggested as a viable linear-time
alternative to transformers with softmax attention. However, these models still
underperform transformers especially on tasks that require in-context
retrieval. While more expressive variants of linear transformers which replace
the additive update in linear transformers with the delta rule (DeltaNet) have
been found to be more effective at associative recall, existing algorithms for
training such models do not parallelize over sequence length and are thus
inefficient to train on modern hardware. This work describes a
hardware-efficient algorithm for training linear transformers with the delta
rule, which exploits a memory-efficient representation for computing products
of Householder matrices. This algorithm allows us to scale up DeltaNet to
standard language modeling settings. We train a 1.3B model for 100B tokens and
find that it outperforms recent linear-time baselines such as Mamba and GLA in
terms of perplexity and zero-shot performance on downstream tasks. We also
experiment with two hybrid models which combine DeltaNet layers with (1)
sliding-window attention layers every other layer or (2) two global attention
layers, and find that these hybrids outperform strong transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoHan: Robust Hand Detection in Operation Room 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roi Papo, Sapir Gershov, Tom Friedman, Itay Or, Gil Bolotin, Shlomi Laufer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-specific localization has garnered significant interest within the
computer vision community. Although there are numerous datasets with hand
annotations from various angles and settings, domain transfer techniques
frequently struggle in surgical environments. This is mainly due to the limited
availability of gloved hand instances and the unique challenges of operating
rooms (ORs). Thus, hand-detection models tailored to OR settings require
extensive training and expensive annotation processes. To overcome these
challenges, we present "RoHan" - a novel approach for robust hand detection in
the OR, leveraging advanced semi-supervised domain adaptation techniques to
tackle the challenges of varying recording conditions, diverse glove colors,
and occlusions common in surgical settings. Our methodology encompasses two
main stages: (1) data augmentation strategy that utilizes "Artificial Gloves,"
a method for augmenting publicly available hand datasets with synthetic images
of hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that
improves detection performance in real-world OR settings through iterative
prediction refinement and efficient frame filtering. We evaluate our method
using two datasets: simulated enterotomy repair and saphenous vein graft
harvesting. "RoHan" substantially reduces the need for extensive labeling and
model training, paving the way for the practical implementation of hand
detection technologies in medical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constructing Confidence Intervals for 'the' Generalization Error -- a
  Comprehensive Benchmark Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Schulz-Kümpel, Sebastian Fischer, Roman Hornung, Anne-Laure Boulesteix, Thomas Nagler, Bernd Bischl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When assessing the quality of prediction models in machine learning,
confidence intervals (CIs) for the generalization error, which measures
predictive performance, are a crucial tool. Luckily, there exist many methods
for computing such CIs and new promising approaches are continuously being
proposed. Typically, these methods combine various resampling procedures, most
popular among them cross-validation and bootstrapping, with different variance
estimation techniques. Unfortunately, however, there is currently no consensus
on when any of these combinations may be most reliably employed and how they
generally compare. In this work, we conduct a large-scale study comparing CIs
for the generalization error, the first one of such size, where we empirically
evaluate 13 different CI methods on a total of 19 tabular regression and
classification problems, using seven different inducers and a total of eight
loss functions. We give an overview of the methodological foundations and
inherent challenges of constructing CIs for the generalization error and
provide a concise review of all 13 methods in a unified framework. Finally, the
CI methods are evaluated in terms of their relative coverage frequency, width,
and runtime. Based on these findings, we can identify a subset of methods that
we would recommend. We also publish the datasets as a benchmarking suite on
OpenML and our code on GitHub to serve as a basis for further studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extended convexity and smoothness and their applications in deep
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binchuan Qi, Wei Gong, Li Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an optimization framework aimed at providing a
theoretical foundation for a class of composite optimization problems,
particularly those encountered in deep learning. In this framework, we
introduce $\mathcal{H}(\phi)$-convexity and $\mathcal{H}(\Phi)$-smoothness to
generalize the existing concepts of Lipschitz smoothness and strong convexity.
Furthermore, we analyze and establish the convergence of both gradient descent
and stochastic gradient descent methods for objective functions that are
$\mathcal{H}(\Phi)$-smooth. We prove that the optimal convergence rates of
these methods depend solely on the homogeneous degree of $\Phi$. Based on these
findings, we construct two types of non-convex and non-smooth optimization
problems: deterministic composite and stochastic composite optimization
problems, which encompass the majority of optimization problems in deep
learning. To address these problems, we develop the gradient structure control
algorithm and prove that it can locate approximate global optima. This marks a
significant departure from traditional non-convex analysis framework, which
typically settle for stationary points. Therefore, with the introduction of
$\mathcal{H}(\phi)$-convexity and $\mathcal{H}(\Phi)$-smoothness, along with
the GSC algorithm, the non-convex optimization mechanisms in deep learning can
be theoretically explained and supported. Finally, the effectiveness of the
proposed framework is substantiated through empirical experimentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-based Unsupervised Audio-visual Speech Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Eudes Ayilo, Mostafa Sadeghi, Romain Serizel, Xavier Alameda-Pineda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new unsupervised audio-visual speech enhancement (AVSE)
approach that combines a diffusion-based audio-visual speech generative model
with a non-negative matrix factorization (NMF) noise model. First, the
diffusion model is pre-trained on clean speech conditioned on corresponding
video data to simulate the speech generative distribution. This pre-trained
model is then paired with the NMF-based noise model to estimate clean speech
iteratively. Specifically, a diffusion-based posterior sampling approach is
implemented within the reverse diffusion process, where after each iteration, a
speech estimate is obtained and used to update the noise parameters.
Experimental results confirm that the proposed AVSE approach not only
outperforms its audio-only counterpart but also generalizes better than a
recent supervised-generative AVSE method. Additionally, the new inference
algorithm offers a better balance between inference speed and performance
compared to the previous diffusion-based method. Code and demo available at:
https://jeaneudesayilo.github.io/fast_UdiffSE
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting Equivariant Representations <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Abildtrup Hansen, Anna Calissano, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent representations are used extensively for downstream tasks, such as
visualization, interpolation or feature extraction of deep learning models.
Invariant and equivariant neural networks are powerful and well-established
models for enforcing inductive biases. In this paper, we demonstrate that the
inductive bias imposed on the by an equivariant model must also be taken into
account when using latent representations. We show how not accounting for the
inductive biases leads to decreased performance on downstream tasks, and vice
versa, how accounting for inductive biases can be done effectively by using an
invariant projection of the latent representations. We propose principles for
how to choose such a projection, and show the impact of using these principles
in two common examples: First, we study a permutation equivariant variational
auto-encoder trained for molecule graph generation; here we show that invariant
projections can be designed that incur no loss of information in the resulting
invariant representation. Next, we study a rotation-equivariant representation
used for image classification. Here, we illustrate how random invariant
projections can be used to obtain an invariant representation with a high
degree of retained information. In both cases, the analysis of invariant latent
representations proves superior to their equivariant counterparts. Finally, we
illustrate that the phenomena documented here for equivariant neural networks
have counterparts in standard neural networks where invariance is encouraged
via augmentation. Thus, while these ambiguities may be known by experienced
developers of equivariant models, we make both the knowledge as well as
effective tools to handle the ambiguities available to the broader community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was updated to reflect the version accepted to ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Confidence Sequence for Generalized Linear Models, with
  Applications to Bandits <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13977v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13977v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyun Lee, Se-Young Yun, Kwang-Sung Jun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a unified likelihood ratio-based confidence sequence (CS) for any
(self-concordant) generalized linear model (GLM) that is guaranteed to be
convex and numerically tight. We show that this is on par or improves upon
known CSs for various GLMs, including Gaussian, Bernoulli, and Poisson. In
particular, for the first time, our CS for Bernoulli has a
$\mathrm{poly}(S)$-free radius where $S$ is the norm of the unknown parameter.
Our first technical novelty is its derivation, which utilizes a time-uniform
PAC-Bayesian bound with a uniform prior/posterior, despite the latter being a
rather unpopular choice for deriving CSs. As a direct application of our new
CS, we propose a simple and natural optimistic algorithm called OFUGLB,
applicable to any generalized linear bandits (GLB; Filippi et al. (2010)). Our
analysis shows that the celebrated optimistic approach simultaneously attains
state-of-the-art regrets for various self-concordant (not necessarily bounded)
GLBs, and even $\mathrm{poly}(S)$-free for bounded GLBs, including logistic
bandits. The regret analysis, our second technical novelty, follows from
combining our new CS with a new proof technique that completely avoids the
previously widely used self-concordant control lemma (Faury et al., 2020, Lemma
9). Numerically, OFUGLB outperforms or is at par with prior algorithms for
logistic bandits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 2 figures, 2 tables; Accepted to the 38th Conference on
  Neural Information Processing Systems (NeurIPS 2024) (ver3: minor revisions,
  code refactoring; ver2: major revision, including new experiments,
  reorganization, fixing typos in the proofs of ver1, etc)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supply<span class="highlight-title">Graph</span>: A Benchmark <span class="highlight-title">Dataset</span> for Supply Chain Planning using <span class="highlight-title">Graph</span>
  Neural Networks <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15299v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15299v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, MD Shafikul Islam, Adipto Raihan Akib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained traction across different domains
such as transportation, bio-informatics, language processing, and computer
vision. However, there is a noticeable absence of research on applying GNNs to
supply chain networks. Supply chain networks are inherently graph-like in
structure, making them prime candidates for applying GNN methodologies. This
opens up a world of possibilities for optimizing, predicting, and solving even
the most complex supply chain problems. A major setback in this approach lies
in the absence of real-world benchmark datasets to facilitate the research and
resolution of supply chain problems using GNNs. To address the issue, we
present a real-world benchmark dataset for temporal tasks, obtained from one of
the leading FMCG companies in Bangladesh, focusing on supply chain planning for
production purposes. The dataset includes temporal data as node features to
enable sales predictions, production planning, and the identification of
factory issues. By utilizing this dataset, researchers can employ GNNs to
address numerous supply chain problems, thereby advancing the field of supply
chain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 4th workshop on Graphs and more Complex structures for
  Learning and Reasoning, colocated with AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning
  Framework <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Yanjiang Chen, Liheng Yu, Xu Wang, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal learning has become a pivotal technique to enable urban
intelligence. Traditional spatiotemporal models mostly focus on a specific task
by assuming a same distribution between training and testing sets. However,
given that urban systems are usually dynamic, multi-sourced with imbalanced
data distributions, current specific task-specific models fail to generalize to
new urban conditions and adapt to new domains without explicitly modeling
interdependencies across various dimensions and types of urban data. To this
end, we argue that there is an essential to propose a Continuous Multi-task
Spatio-Temporal learning framework (CMuST) to empower collective urban
intelligence, which reforms the urban spatiotemporal learning from
single-domain to cooperatively multi-dimensional and multi-task learning.
Specifically, CMuST proposes a new multi-dimensional spatiotemporal interaction
network (MSTI) to allow cross-interactions between context and main
observations as well as self-interactions within spatial and temporal aspects
to be exposed, which is also the core for capturing task-level commonality and
personalization. To ensure continuous task learning, a novel Rolling Adaptation
training scheme (RoAda) is devised, which not only preserves task uniqueness by
constructing data summarization-driven task prompts, but also harnesses
correlated patterns among tasks by iterative model behavior modeling. We
further establish a benchmark of three cities for multi-task spatiotemporal
learning, and empirically demonstrate the superiority of CMuST via extensive
evaluations on these datasets. The impressive improvements on both few-shot
streaming data and new domain tasks against existing SOAT methods are achieved.
Code is available at https://github.com/DILab-USTCSZ/CMuST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Distributed, Flexible Compositional Visual Representations via
  Soft Tensor Products 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bethia Sun, Maurice Pagnucco, Yang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the inception of the classicalist vs. connectionist debate, it has been
argued that the ability to systematically combine symbol-like entities into
compositional representations is crucial for human intelligence. In
connectionist systems, the field of disentanglement has gained prominence for
its ability to produce explicitly compositional representations; however, it
relies on a fundamentally symbolic, concatenative representation of
compositional structure that clashes with the continuous, distributed
foundations of deep learning. To resolve this tension, we extend Smolensky's
Tensor Product Representation (TPR) and introduce Soft TPR, a representational
form that encodes compositional structure in an inherently distributed,
flexible manner, along with Soft TPR Autoencoder, a theoretically-principled
architecture designed specifically to learn Soft TPRs. Comprehensive
evaluations in the visual representation learning domain demonstrate that the
Soft TPR framework consistently outperforms conventional disentanglement
alternatives -- achieving state-of-the-art disentanglement, boosting
representation learner convergence, and delivering superior sample efficiency
and low-sample regime performance in downstream tasks. These findings highlight
the promise of a distributed and flexible approach to representing
compositional structure by potentially enhancing alignment with the core
principles of deep learning over the conventional symbolic approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips 2024. 10 pages + supplementary</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware
  Self-Reflection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning (IT) is crucial to tailoring large language models (LLMs)
towards human-centric interactions. Recent advancements have shown that the
careful selection of a small, high-quality subset of IT data can significantly
enhance the performance of LLMs. Despite this, common approaches often rely on
additional models or data, which increases costs and limits widespread
adoption. In this work, we propose a novel approach, termed SelectIT, that
capitalizes on the foundational capabilities of the LLM itself. Specifically,
we exploit the intrinsic uncertainty present in LLMs to more effectively select
high-quality IT data, without the need for extra resources. Furthermore, we
introduce a curated IT dataset, the Selective Alpaca, created by applying
SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT
using Selective Alpaca leads to substantial model ability enhancement. The
robustness of SelectIT has also been corroborated in various foundation models
and domain-specific tasks. Our findings suggest that longer and more
computationally intensive IT data may serve as superior sources of IT, offering
valuable insights for future research in this area. Data, code, and scripts are
freely available at https://github.com/Blue-Raincoat/SelectIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Accelerated Algorithm for Stochastic Bilevel Optimization under
  Unbounded Smoothness <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19212v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19212v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochuan Gong, Jie Hao, Mingrui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a class of stochastic bilevel optimization problems
where the upper-level function is nonconvex with potentially unbounded
smoothness and the lower-level problem is strongly convex. These problems have
significant applications in sequential data learning, such as text
classification using recurrent neural networks. The unbounded smoothness is
characterized by the smoothness constant of the upper-level function scaling
linearly with the gradient norm, lacking a uniform upper bound. Existing
state-of-the-art algorithms require $\widetilde{O}(1/\epsilon^4)$ oracle calls
of stochastic gradient or Hessian/Jacobian-vector product to find an
$\epsilon$-stationary point. However, it remains unclear if we can further
improve the convergence rate when the assumptions for the function in the
population level also hold for each random realization almost surely. To
address this issue, we propose a new Accelerated Bilevel Optimization algorithm
named AccBO. The algorithm updates the upper-level variable by normalized
stochastic gradient descent with recursive momentum and the lower-level
variable by the stochastic Nesterov accelerated gradient descent algorithm with
averaging. We prove that our algorithm achieves an oracle complexity of
$\widetilde{O}(1/\epsilon^3)$ to find an $\epsilon$-stationary point, when the
lower-level stochastic gradient's variance is $O(\epsilon)$. Our proof relies
on a novel lemma characterizing the dynamics of stochastic Nesterov accelerated
gradient descent algorithm under distribution drift with high probability for
the lower-level variable, which is of independent interest and also plays a
crucial role in analyzing the hypergradient estimation error over time.
Experimental results on various tasks confirm that our proposed algorithm
achieves the predicted theoretical acceleration and significantly outperforms
baselines in bilevel optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. The code is available at
  https://github.com/MingruiLiu-ML-Lab/Accelerated-Bilevel-Optimization-Unbounded-Smoothness</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making AI Less "Thirsty": Uncovering and Addressing the Secret Water
  Footprint of AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03271v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03271v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Li, Jianyi Yang, Mohammad A. Islam, Shaolei Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing carbon footprint of artificial intelligence (AI) has been
undergoing public scrutiny. Nonetheless, the equally important water
(withdrawal and consumption) footprint of AI has largely remained under the
radar. For example, training the GPT-3 language model in Microsoft's
state-of-the-art U.S. data centers can directly evaporate 700,000 liters of
clean freshwater, but such information has been kept a secret. More critically,
the global AI demand is projected to account for 4.2-6.6 billion cubic meters
of water withdrawal in 2027, which is more than the total annual water
withdrawal of 4-6 Denmark or half of the United Kingdom. This is concerning, as
freshwater scarcity has become one of the most pressing challenges. To respond
to the global water challenges, AI can, and also must, take social
responsibility and lead by example by addressing its own water footprint. In
this paper, we provide a principled methodology to estimate the water footprint
of AI, and also discuss the unique spatial-temporal diversities of AI's runtime
water efficiency. Finally, we highlight the necessity of holistically
addressing water footprint along with carbon footprint to enable truly
sustainable AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Communications of the ACM. Source codes available at:
  https://github.com/Ren-Research/Making-AI-Less-Thirsty</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12117v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12117v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pinxue Zhao, Hailin Zhang, Fangcheng Fu, Xiaonan Nie, Qibin Liu, Fang Yang, Yuanbo Peng, Dian Jiao, Shuaipeng Li, Jinbao Xue, Yangyu Tao, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, Large Language Models (LLMs) have been trained using extended
context lengths to foster more creative applications. However, long context
training poses great challenges considering the constraint of GPU memory. It
not only leads to substantial activation memory consumption during training,
but also incurs considerable memory fragmentation. To facilitate long context
training, existing frameworks have adopted strategies such as recomputation and
various forms of parallelisms. Nevertheless, these techniques rely on redundant
computation or extensive communication, resulting in low Model FLOPS
Utilization (MFU). In this paper, we propose MEMO, a novel LLM training
framework designed for fine-grained activation memory management. Given the
quadratic scaling of computation and linear scaling of memory with sequence
lengths when using FlashAttention, we offload memory-consuming activations to
CPU memory after each layer's forward pass and fetch them during the backward
pass. To maximize the swapping of activations without hindering computation,
and to avoid exhausting limited CPU memory, we implement a token-wise
activation recomputation and swapping mechanism. Furthermore, we tackle the
memory fragmentation issue by employing a bi-level Mixed Integer Programming
(MIP) approach, optimizing memory reuse across transformer layers. Empirical
results demonstrate that MEMO achieves an average of 1.97x and 1.80x MFU
compared to Megatron-LM and DeepSpeed, respectively. This improvement is
attributed to MEMO's ability to minimize memory fragmentation, reduce
recomputation and intensive communication, and circumvent the delays associated
with the memory reorganization process due to fragmentation. By leveraging
fine-grained activation memory management, MEMO facilitates efficient training
of 7B LLM with 1 million sequence length on just 8 A800 GPUs, achieving an MFU
of 52.30%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Localisation of Spatial-Temporal <span class="highlight-title">Graph</span> Neural Network <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04239v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04239v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenying Duan, Shujun Guo, Wei huang, Hong Rao, Xiaoxi He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial-temporal data, fundamental to many intelligent applications, reveals
dependencies indicating causal links between present measurements at specific
locations and historical data at the same or other locations. Within this
context, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged
as valuable tools for modelling these dependencies, especially through a
data-driven approach rather than pre-defined spatial graphs. While this
approach offers higher accuracy, it presents increased computational demands.
Addressing this challenge, this paper delves into the concept of localisation
within ASTGNNs, introducing an innovative perspective that spatial dependencies
should be dynamically evolving over time. We introduce \textit{DynAGS}, a
localised ASTGNN framework aimed at maximising efficiency and accuracy in
distributed deployment. This framework integrates dynamic localisation,
time-evolving spatial graphs, and personalised localisation, all orchestrated
around the Dynamic Graph Generator, a light-weighted central module leveraging
cross attention. The central module can integrate historical information in a
node-independent manner to enhance the feature representation of nodes at the
current moment. This improved feature representation is then used to generate a
dynamic sparse graph without the need for costly data exchanges, and it
supports personalised localisation. Performance assessments across two core
ASTGNN architectures and nine real-world datasets from various applications
reveal that \textit{DynAGS} outshines current benchmarks, underscoring that the
dynamic modelling of spatial dependencies can drastically improve model
expressibility, flexibility, and system efficiency, especially in distributed
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by KDD'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OminiControl: Minimal and Universal Control for Diffusion Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15098v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15098v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce OminiControl, a highly versatile and
parameter-efficient framework that integrates image conditions into pre-trained
Diffusion Transformer (DiT) models. At its core, OminiControl leverages a
parameter reuse mechanism, enabling the DiT to encode image conditions using
itself as a powerful backbone and process them with its flexible multi-modal
attention processors. Unlike existing methods, which rely heavily on additional
encoder modules with complex architectures, OminiControl (1) effectively and
efficiently incorporates injected image conditions with only ~0.1% additional
parameters, and (2) addresses a wide range of image conditioning tasks in a
unified manner, including subject-driven generation and spatially-aligned
conditions such as edges, depth, and more. Remarkably, these capabilities are
achieved by training on images generated by the DiT itself, which is
particularly beneficial for subject-driven generation. Extensive evaluations
demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted
models in both subject-driven and spatially-aligned conditional generation.
Additionally, we release our training dataset, Subjects200K, a diverse
collection of over 200,000 identity-consistent images, along with an efficient
data synthesis pipeline to advance research in subject-consistent generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models as Network Optimizers: Explorations and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00453v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00453v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihuai Liang, Bo Yang, Pengyu Chen, Xianjin Li, Yifan Xue, Zhiwen Yu, Xuelin Cao, Yan Zhang, Mérouane Debbah, H. Vincent Poor, Chau Yuen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network optimization is a fundamental challenge in the Internet of Things
(IoT) network, often characterized by complex features that make it difficult
to solve these problems. Recently, generative diffusion models (GDMs) have
emerged as a promising new approach to network optimization, with the potential
to directly address these optimization problems. However, the application of
GDMs in this field is still in its early stages, and there is a noticeable lack
of theoretical research and empirical findings. In this study, we first explore
the intrinsic characteristics of generative models. Next, we provide a concise
theoretical proof and intuitive demonstration of the advantages of generative
models over discriminative models in network optimization. Based on this
exploration, we implement GDMs as optimizers aimed at learning high-quality
solution distributions for given inputs, sampling from these distributions
during inference to approximate or achieve optimal solutions. Specifically, we
utilize denoising diffusion probabilistic models (DDPMs) and employ a
classifier-free guidance mechanism to manage conditional guidance based on
input parameters. We conduct extensive experiments across three challenging
network optimization problems. By investigating various model configurations
and the principles of GDMs as optimizers, we demonstrate the ability to
overcome prediction errors and validate the convergence of generated solutions
to optimal solutions. We provide code and data at
https://github.com/qiyu3816/DiffSG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhao, Tingwei Chen, Zhijie Cai, Xiaoyang Li, Hang Li, Qimei Chen, Guangxu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Wi-Fi sensing has garnered significant attention due to its
numerous benefits, such as privacy protection, low cost, and penetration
ability. Extensive research has been conducted in this field, focusing on areas
such as gesture recognition, people identification, and fall detection.
However, many data-driven methods encounter challenges related to domain shift,
where the model fails to perform well in environments different from the
training data. One major factor contributing to this issue is the limited
availability of Wi-Fi sensing datasets, which makes models learn excessive
irrelevant information and over-fit to the training set. Unfortunately,
collecting large-scale Wi-Fi sensing datasets across diverse scenarios is a
challenging task. To address this problem, we propose CrossFi, a siamese
network-based approach that excels in both in-domain scenario and cross-domain
scenario, including few-shot, zero-shot scenarios, and even works in few-shot
new-class scenario where testing set contains new categories. The core
component of CrossFi is a sample-similarity calculation network called CSi-Net,
which improves the structure of the siamese network by using an attention
mechanism to capture similarity information, instead of simply calculating the
distance or cosine similarity. Based on it, we develop an extra Weight-Net that
can generate a template for each class, so that our CrossFi can work in
different scenarios. Experimental results demonstrate that our CrossFi achieves
state-of-the-art performance across various scenarios. In gesture recognition
task, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72%
in one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario,
and 84.75% in one-shot new-class scenario. The code for our model is publicly
available at https://github.com/RS2002/CrossFi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Silent Majority: Demystifying Memorization Effect in the Presence of
  Spurious Correlations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu You, Haocheng Dai, Yifei Min, Jasjeet S. Sekhon, Sarang Joshi, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models often rely on simple spurious features -- patterns in
training data that correlate with targets but are not causally related to them,
like image backgrounds in foreground classification. This reliance typically
leads to imbalanced test performance across minority and majority groups. In
this work, we take a closer look at the fundamental cause of such imbalanced
performance through the lens of memorization, which refers to the ability to
predict accurately on \textit{atypical} examples (minority groups) in the
training set but failing in achieving the same accuracy in the testing set.
This paper systematically shows the ubiquitous existence of spurious features
in a small set of neurons within the network, providing the first-ever evidence
that memorization may contribute to imbalanced group performance. Through three
experimental sources of converging empirical evidence, we find the property of
a small subset of neurons or channels in memorizing minority group information.
Inspired by these findings, we articulate the hypothesis: the imbalanced group
performance is a byproduct of ``noisy'' spurious memorization confined to a
small set of neurons. To further substantiate this hypothesis, we show that
eliminating these unnecessary spurious memorization patterns via a novel
framework during training can significantly affect the model performance on
minority groups. Our experimental results across various architectures and
benchmarks offer new insights on how neural networks encode core and spurious
knowledge, laying the groundwork for future research in demystifying robustness
to spurious correlation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot Video Restoration and Enhancement Using <span class="highlight-title">Pre-Train</span>ed Image
  Diffusion Model <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Cao, Huanjing Yue, Xin Liu, Jingyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based zero-shot image restoration and enhancement models have
achieved great success in various tasks of image restoration and enhancement.
However, directly applying them to video restoration and enhancement results in
severe temporal flickering artifacts. In this paper, we propose the first
framework for zero-shot video restoration and enhancement based on the
pre-trained image diffusion model. By replacing the spatial self-attention
layer with the proposed short-long-range (SLR) temporal attention layer, the
pre-trained image diffusion model can take advantage of the temporal
correlation between frames. We further propose temporal consistency guidance,
spatial-temporal noise sharing, and an early stopping sampling strategy to
improve temporally consistent sampling. Our method is a plug-and-play module
that can be inserted into any diffusion-based image restoration or enhancement
methods to further improve their performance. Experimental results demonstrate
the superiority of our proposed method. Our code is available at
https://github.com/cao-cong/ZVRD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine unlearning through fine-grained model parameters perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04385v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04385v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Zuo, Zhuo Tang, Kenli Li, Anwitaman Datta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning techniques, which involve retracting data records and
reducing influence of said data on trained models, help with the user privacy
protection objective but incur significant computational costs. Weight
perturbation-based unlearning is a general approach, but it typically involves
globally modifying the parameters. We propose fine-grained Top-K and Random-k
parameters perturbed inexact machine unlearning strategies that address the
privacy needs while keeping the computational costs tractable.
  In order to demonstrate the efficacy of our strategies we also tackle the
challenge of evaluating the effectiveness of machine unlearning by considering
the model's generalization performance across both unlearning and remaining
data. To better assess the unlearning effect and model generalization, we
propose novel metrics, namely, the forgetting rate and memory retention rate.
However, for inexact machine unlearning, current metrics are inadequate in
quantifying the degree of forgetting that occurs after unlearning strategies
are applied. To address this, we introduce SPD-GAN, which subtly perturbs the
distribution of data targeted for unlearning. Then, we evaluate the degree of
unlearning by measuring the performance difference of the models on the
perturbed unlearning data before and after the unlearning process. By
implementing these innovative techniques and metrics, we achieve
computationally efficacious privacy protection in machine learning applications
without significant sacrifice of model performance. Furthermore, this approach
provides a novel method for evaluating the degree of unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clarify Confused Nodes via Separated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02285v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02285v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Zhou, Shengbo Gong, Xuanze Chen, Chenxuan Xie, Shanqing Yu, Qi Xuan, Xiaoniu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have achieved remarkable advances in
graph-oriented tasks. However, real-world graphs invariably contain a certain
proportion of heterophilous nodes, challenging the homophily assumption of
traditional GNNs and hindering their performance. Most existing studies
continue to design generic models with shared weights between heterophilous and
homophilous nodes. Despite the incorporation of high-order messages or
multi-channel architectures, these efforts often fall short. A minority of
studies attempt to train different node groups separately but suffer from
inappropriate separation metrics and low efficiency. In this paper, we first
propose a new metric, termed Neighborhood Confusion (NC), to facilitate a more
reliable separation of nodes. We observe that node groups with different levels
of NC values exhibit certain differences in intra-group accuracy and visualized
embeddings. These pave the way for Neighborhood Confusion-guided Graph
Convolutional Network (NCGCN), in which nodes are grouped by their NC values
and accept intra-group weight sharing and message passing. Extensive
experiments on both homophilous and heterophilous benchmarks demonstrate that
our framework can effectively separate nodes and yield significant performance
improvement compared to the latest methods. The source code will be available
in https://github.com/GISec-Team/NCGNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized
  Variational Autoencoders for Financial Trading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilei Zhao, Wentao Zhang, Tingran Yang, Yong Jiang, Fei Huang, Wei Yang Bryan Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In financial trading, factor models are widely used to price assets and
capture excess returns from mispricing. Recently, we have witnessed the rise of
variational autoencoder-based latent factor models, which learn latent factors
self-adaptively. While these models focus on modeling overall market
conditions, they often fail to effectively capture the temporal patterns of
individual stocks. Additionally, representing multiple factors as single values
simplifies the model but limits its ability to capture complex relationships
and dependencies. As a result, the learned factors are of low quality and lack
diversity, reducing their effectiveness and robustness across different trading
periods. To address these issues, we propose a Spatio-Temporal factOR Model
based on dual vector quantized variational autoencoders, named STORM, which
extracts features of stocks from temporal and spatial perspectives, then fuses
and aligns these features at the fine-grained and semantic level, and
represents the factors as multi-dimensional embeddings. The discrete codebooks
cluster similar factor embeddings, ensuring orthogonality and diversity, which
helps distinguish between different factors and enables factor selection in
financial trading. To show the performance of the proposed factor model, we
apply it to two downstream experiments: portfolio management on two stock
datasets and individual trading tasks on six specific stocks. The extensive
experiments demonstrate STORM's flexibility in adapting to downstream tasks and
superior performance over baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Cone Gradient Descent for Training Physics-Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngsik Hwang, Dong-Young Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) have emerged as a prominent approach
for solving partial differential equations (PDEs) by minimizing a combined loss
function that incorporates both boundary loss and PDE residual loss. Despite
their remarkable empirical performance in various scientific computing tasks,
PINNs often fail to generate reasonable solutions, and such pathological
behaviors remain difficult to explain and resolve. In this paper, we identify
that PINNs can be adversely trained when gradients of each loss function
exhibit a significant imbalance in their magnitudes and present a negative
inner product value. To address these issues, we propose a novel optimization
framework, Dual Cone Gradient Descent (DCGD), which adjusts the direction of
the updated gradient to ensure it falls within a dual cone region. This region
is defined as a set of vectors where the inner products with both the gradients
of the PDE residual loss and the boundary loss are non-negative. Theoretically,
we analyze the convergence properties of DCGD algorithms in a non-convex
setting. On a variety of benchmark equations, we demonstrate that DCGD
outperforms other optimization algorithms in terms of various evaluation
metrics. In particular, DCGD achieves superior predictive accuracy and enhances
the stability of training for failure modes of PINNs and complex PDEs, compared
to existing optimally tuned models. Moreover, DCGD can be further improved by
combining it with popular strategies for PINNs, including learning rate
annealing and the Neural Tangent Kernel (NTK).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Thirty-eighth Annual Conference on Neural Information Processing
  Systems, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal-in-the-Loop for Learning with Imbalanced Noisy Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Brandon Graham-Knight, Jamil Fayyad, Nourhan Bayasi, Patricia Lasserre, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance and label noise are pervasive in large-scale datasets, yet
much of machine learning research assumes well-labeled, balanced data, which
rarely reflects real world conditions. Existing approaches typically address
either label noise or class imbalance in isolation, leading to suboptimal
results when both issues coexist. In this work, we propose
Conformal-in-the-Loop (CitL), a novel training framework that addresses both
challenges with a conformal prediction-based approach. CitL evaluates sample
uncertainty to adjust weights and prune unreliable examples, enhancing model
resilience and accuracy with minimal computational cost. Our extensive
experiments include a detailed analysis showing how CitL effectively emphasizes
impactful data in noisy, imbalanced datasets. Our results show that CitL
consistently boosts model performance, achieving up to a 6.1% increase in
classification accuracy and a 5.0 mIoU improvement in segmentation. Our code is
publicly available: CitL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EdgeSight: Enabling Modeless and Cost-Efficient Inference at the Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ChonLam Lao, Jiaqi Gao, Ganesh Ananthanarayanan, Aditya Akella, Minlan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional ML inference is evolving toward modeless inference, which
abstracts the complexity of model selection from users, allowing the system to
automatically choose the most appropriate model for each request based on
accuracy and resource requirements. While prior studies have focused on
modeless inference within data centers, this paper tackles the pressing need
for cost-efficient modeless inference at the edge -- particularly within its
unique constraints of limited device memory, volatile network conditions, and
restricted power consumption.
  To overcome these challenges, we propose EdgeSight, a system that provides
cost-efficient EdgeSight serving for diverse DNNs at the edge. EdgeSight
employs an edge-data center (edge-DC) architecture, utilizing confidence
scaling to reduce the number of model options while meeting diverse accuracy
requirements. Additionally, it supports lossy inference in volatile network
environments. Our experimental results show that EdgeSight outperforms existing
systems by up to 1.6x in P99 latency for modeless services. Furthermore, our
FPGA prototype demonstrates similar performance at certain accuracy levels,
with a power consumption reduction of up to 3.34x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Outlines for Code: Literate Programming in the LLM Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kensen Shi, Deniz Altınbüken, Saswat Anand, Mihai Christodorescu, Katja Grünwedel, Alexa Koenings, Sai Naidu, Anurag Pathak, Marc Rasi, Fredde Ribeiro, Brandon Ruffin, Siddhant Sanyam, Maxim Tabachnyk, Sara Toth, Roy Tu, Tobias Welp, Pengcheng Yin, Manzil Zaheer, Satish Chandra, Charles Sutton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose using natural language outlines as a novel modality and
interaction surface for providing AI assistance to developers throughout the
software development process. An NL outline for a code function comprises
multiple statements written in concise prose, which partition the code and
summarize its main ideas in the style of literate programming. Crucially, we
find that modern LLMs can generate accurate and high-quality NL outlines in
practice. Moreover, NL outlines enable a bidirectional sync between code and
NL, allowing changes in one to be automatically reflected in the other. We
discuss many use cases for NL outlines: they can accelerate understanding and
navigation of code and diffs, simplify code maintenance, augment code search,
steer code generation, and more. We then propose and compare multiple LLM
prompting techniques for generating outlines and ask professional developers to
judge outline quality. Finally, we present two case studies applying NL
outlines toward code review and malware detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Diffuser (CoD): Mastering Continual Offline Reinforcement
  Learning with Experience Rehearsal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jifeng Hu, Li Shen, Sili Huang, Zhejian Yang, Hechang Chen, Lichao Sun, Yi Chang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural networks, especially recent diffusion-based models, have
shown remarkable superiority in gaming, control, and QA systems, where the
training tasks' datasets are usually static. However, in real-world
applications, such as robotic control of reinforcement learning (RL), the tasks
are changing, and new tasks arise in a sequential order. This situation poses
the new challenge of plasticity-stability trade-off for training an agent who
can adapt to task changes and retain acquired knowledge. In view of this, we
propose a rehearsal-based continual diffusion model, called Continual Diffuser
(CoD), to endow the diffuser with the capabilities of quick adaptation
(plasticity) and lasting retention (stability). Specifically, we first
construct an offline benchmark that contains 90 tasks from multiple domains.
Then, we train the CoD on each task with sequential modeling and conditional
generation for making decisions. Next, we preserve a small portion of previous
datasets as the rehearsal buffer and replay it to retain the acquired
knowledge. Extensive experiments on a series of tasks show CoD can achieve a
promising plasticity-stability trade-off and outperform existing
diffusion-based methods and other representative baselines on most tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction
  Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Fang, Xinle Deng, Kangwei Liu, Ningyu Zhang, Jingyang Qian, Penghui Yang, Xiaohui Fan, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models excel at interpreting complex natural language
instructions, enabling them to perform a wide range of tasks. In the life
sciences, single-cell RNA sequencing (scRNA-seq) data serves as the "language
of cellular biology", capturing intricate gene expression patterns at the
single-cell level. However, interacting with this "language" through
conventional tools is often inefficient and unintuitive, posing challenges for
researchers. To address these limitations, we present InstructCell, a
multi-modal AI copilot that leverages natural language as a medium for more
direct and flexible single-cell analysis. We construct a comprehensive
multi-modal instruction dataset that pairs text-based instructions with
scRNA-seq profiles from diverse tissues and species. Building on this, we
develop a multi-modal cell language architecture capable of simultaneously
interpreting and processing both modalities. InstructCell empowers researchers
to accomplish critical tasks-such as cell type annotation, conditional
pseudo-cell generation, and drug sensitivity prediction-using straightforward
natural language commands. Extensive evaluations demonstrate that InstructCell
consistently meets or exceeds the performance of existing single-cell
foundation models, while adapting to diverse experimental conditions. More
importantly, InstructCell provides an accessible and intuitive tool for
exploring complex single-cell data, lowering technical barriers and enabling
deeper biological insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell,
  Models: https://huggingface.co/zjunlp/Instructcell-chat,
  https://huggingface.co/zjunlp/InstructCell-instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding E<span class="highlight-title">merge</span>nt Abilities of Language Models from the Loss
  Perspective <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15796v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15796v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiao Du, Aohan Zeng, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have put into question the belief that emergent abilities in
language models are exclusive to large models. This skepticism arises from two
observations: 1) smaller models can also exhibit high performance on emergent
abilities and 2) there is doubt on the discontinuous metrics used to measure
these abilities. In this paper, we propose to study emergent abilities in the
lens of pre-training loss, instead of model size or training compute. We
demonstrate that the Transformer models with the same pre-training loss, but
different model and data sizes, generate the same performance on various
downstream tasks, with a fixed data corpus, tokenization, and model
architecture. We also discover that a model exhibits emergent abilities on
certain tasks -- regardless of the continuity of metrics -- when its
pre-training loss falls below a specific threshold. Before reaching this
threshold, its performance remains at the level of random guessing. This
inspires us to redefine emergent abilities as those that manifest in models
with lower pre-training losses, highlighting that these abilities cannot be
predicted by merely extrapolating the performance trends of models with higher
pre-training losses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures. Accepted in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven inventory management for new products: A warm-start and
  adjusted Dyna-$Q$ approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinye Qu, Longxiao Liu, Wenjie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel reinforcement learning algorithm for
inventory management of newly launched products with no or limited historical
demand information. The algorithm follows the classic Dyna-$Q$ structure,
balancing the model-based and model-free approaches, while accelerating the
training process of Dyna-$Q$ and mitigating the model discrepancy generated by
the model-based feedback. Warm-start information from the demand data of
existing similar products can be incorporated into the algorithm to further
stabilize the early-stage training and reduce the variance of the estimated
optimal policy. Our approach is validated through a case study of bakery
inventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7%
reduction in average daily cost compared with $Q$-learning, and up to a 77.5%
reduction in training time within the same horizon compared with classic
Dyna-$Q$. By incorporating the warm-start information, it can be found that the
adjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and
relatively low shortage percentages among all the algorithms under a 30-day
testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unconditional stability of a recurrent neural circuit implementing
  divisive normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18946v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18946v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivang Rawat, David J. Heeger, Stefano Martiniani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stability in recurrent neural models poses a significant challenge,
particularly in developing biologically plausible neurodynamical models that
can be seamlessly trained. Traditional cortical circuit models are notoriously
difficult to train due to expansive nonlinearities in the dynamical system,
leading to an optimization problem with nonlinear stability constraints that
are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in
tasks involving sequential data but lack biological plausibility and
interpretability. In this work, we address these challenges by linking dynamic
divisive normalization (DN) to the stability of ORGaNICs, a biologically
plausible recurrent cortical circuit model that dynamically achieves DN and
that has been shown to simulate a wide range of neurophysiological phenomena.
By using the indirect method of Lyapunov, we prove the remarkable property of
unconditional local stability for an arbitrary-dimensional ORGaNICs circuit
when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a
system of coupled damped harmonic oscillators, which enables us to derive the
circuit's energy function, providing a normative principle of what the circuit,
and individual neurons, aim to accomplish. Further, for a generic recurrent
weight matrix, we prove the stability of the 2D model and demonstrate
empirically that stability holds in higher dimensions. Finally, we show that
ORGaNICs can be trained by backpropagation through time without gradient
clipping/scaling, thanks to its intrinsic stability property and adaptive time
constants, which address the problems of exploding, vanishing, and oscillating
gradients. By evaluating the model's performance on RNN benchmarks, we find
that ORGaNICs outperform alternative neurodynamical models on static image
classification tasks and perform comparably to LSTMs on sequential tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the Effect of Network Pruning on Performance and
  Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan von Rad, Florian Seuffert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) are often over-parameterized for their tasks and
can be compressed quite drastically by removing weights, a process called
pruning. We investigate the impact of different pruning techniques on the
classification performance and interpretability of GoogLeNet. We systematically
apply unstructured and structured pruning, as well as connection sparsity
(pruning of input weights) methods to the network and analyze the outcomes
regarding the network's performance on the validation set of ImageNet. We also
compare different retraining strategies, such as iterative pruning and one-shot
pruning. We find that with sufficient retraining epochs, the performance of the
networks can approximate the performance of the default GoogLeNet - and even
surpass it in some cases. To assess interpretability, we employ the Mechanistic
Interpretability Score (MIS) developed by Zimmermann et al. . Our experiments
reveal that there is no significant relationship between interpretability and
pruning rate when using MIS as a measure. Additionally, we observe that
networks with extremely low accuracy can still achieve high MIS scores,
suggesting that the MIS may not always align with intuitive notions of
interpretability, such as understanding the basis of correct decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using
  Passive Langevin Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Snow, Vikram Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a finite-sample analysis of a passive stochastic gradient
Langevin dynamics (PSGLD) algorithm. This algorithm is designed to achieve
adaptive inverse reinforcement learning (IRL). Adaptive IRL aims to estimate
the cost function of a forward learner performing a stochastic gradient
algorithm (e.g., policy gradient reinforcement learning) by observing their
estimates in real-time. The PSGLD algorithm is considered passive because it
incorporates noisy gradients provided by an external stochastic gradient
algorithm (forward learner), of which it has no control. The PSGLD algorithm
acts as a randomized sampler to achieve adaptive IRL by reconstructing the
forward learner's cost function nonparametrically from the stationary measure
of a Langevin diffusion. This paper analyzes the non-asymptotic (finite-sample)
performance; we provide explicit bounds on the 2-Wasserstein distance between
PSGLD algorithm sample measure and the stationary measure encoding the cost
function, and provide guarantees for a kernel density estimation scheme which
reconstructs the cost function from empirical samples. Our analysis uses tools
from the study of Markov diffusion operators. The derived bounds have both
practical and theoretical significance. They provide finite-time guarantees for
an adaptive IRL mechanism, and substantially generalize the analytical
framework of a line of research in passive stochastic gradient algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Automata Embeddings for Goal-Conditioned Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beyazit Yalcinkaya, Niklas Lauffer, Marcell Vazquez-Chanlatte, Sanjit A. Seshia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-conditioned reinforcement learning is a powerful way to control an AI
agent's behavior at runtime. That said, popular goal representations, e.g.,
target states or natural language, are either limited to Markovian tasks or
rely on ambiguous task semantics. We propose representing temporal goals using
compositions of deterministic finite automata (cDFAs) and use cDFAs to guide RL
agents. cDFAs balance the need for formal temporal semantics with ease of
interpretation: if one can understand a flow chart, one can understand a cDFA.
On the other hand, cDFAs form a countably infinite concept class with Boolean
semantics, and subtle changes to the automaton can result in very different
tasks, making them difficult to condition agent behavior on. To address this,
we observe that all paths through a DFA correspond to a series of reach-avoid
tasks and propose pre-training graph neural network embeddings on "reach-avoid
derived" DFAs. Through empirical evaluation, we demonstrate that the proposed
pre-training method enables zero-shot generalization to various cDFA task
classes and accelerated policy specialization without the myopic suboptimality
of hierarchical methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 2 OLMo 2 Furious 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present OLMo 2, the next generation of our fully open language models.
OLMo 2 includes dense autoregressive models with improved architecture and
training recipe, pretraining data mixtures, and instruction tuning recipes. Our
modified model architecture and training recipe achieve both better training
stability and improved per-token efficiency. Our updated pretraining data
mixture introduces a new, specialized data mix called Dolmino Mix 1124, which
significantly improves model capabilities across many downstream task
benchmarks when introduced via late-stage curriculum training (i.e. specialized
data during the annealing phase of pretraining). Finally, we incorporate best
practices from T\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data
and extending our final-stage reinforcement learning with verifiable rewards
(RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to
compute, often matching or outperforming open-weight only models like Llama 3.1
and Qwen 2.5 while using fewer FLOPs and with fully transparent training data,
code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or
surpassing open-weight only models of comparable size, including Qwen 2.5,
Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B
and 13B scales, both pretrained and post-trained, including their full training
data, training code and recipes, training logs and thousands of intermediate
checkpoints. The final instruction model is available on the Ai2 Playground as
a free research demo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Model demo available at playground.allenai.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Cross-Domain Representations for Transferable Drug
  Perturbations on Single-Cell Transcriptional Responses <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Liu, Shikai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phenotypic drug discovery has attracted widespread attention because of its
potential to identify bioactive molecules. Transcriptomic profiling provides a
comprehensive reflection of phenotypic changes in cellular responses to
external perturbations. In this paper, we propose XTransferCDR, a novel
generative framework designed for feature decoupling and transferable
representation learning across domains. Given a pair of perturbed expression
profiles, our approach decouples the perturbation representations from basal
states through domain separation encoders and then cross-transfers them in the
latent space. The transferred representations are then used to reconstruct the
corresponding perturbed expression profiles via a shared decoder. This
cross-transfer constraint effectively promotes the learning of transferable
drug perturbation representations. We conducted extensive evaluations of our
model on multiple datasets, including single-cell transcriptional responses to
drugs and single- and combinatorial genetic perturbations. The experimental
results show that XTransferCDR achieved better performance than current
state-of-the-art methods, showcasing its potential to advance phenotypic drug
discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 39th Annual AAAI Conference on Artificial Intelligenc
  (AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Long Video Tokenization via Coordinate-based Patch
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiwon Jang, Sihyun Yu, Jinwoo Shin, Pieter Abbeel, Younggyo Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient tokenization of videos remains a challenge in training vision
models that can process long videos. One promising direction is to develop a
tokenizer that can encode long video clips, as it would enable the tokenizer to
leverage the temporal coherence of videos better for tokenization. However,
training existing tokenizers on long videos often incurs a huge training cost
as they are trained to reconstruct all the frames at once. In this paper, we
introduce CoordTok, a video tokenizer that learns a mapping from
coordinate-based representations to the corresponding patches of input videos,
inspired by recent advances in 3D generative models. In particular, CoordTok
encodes a video into factorized triplane representations and reconstructs
patches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows
for training large tokenizer models directly on long videos without requiring
excessive training resources. Our experiments show that CoordTok can
drastically reduce the number of tokens for encoding long video clips. For
instance, CoordTok can encode a 128-frame video with 128$\times$128 resolution
into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar
reconstruction quality. We further show that this efficient video tokenization
enables memory-efficient training of a diffusion transformer that can generate
128 frames at once.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available on the project webpage:
  https://huiwon-jang.github.io/coordtok/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unifying Information-theoretic Perspective on Evaluating Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Fox, Samarth Swarup, Abhijin Adiga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering the difficulty of interpreting generative model output, there is
significant current research focused on determining meaningful evaluation
metrics. Several recent approaches utilize "precision" and "recall," borrowed
from the classification domain, to individually quantify the output fidelity
(realism) and output diversity (representation of the real data variation),
respectively. With the increase in metric proposals, there is a need for a
unifying perspective, allowing for easier comparison and clearer explanation of
their benefits and drawbacks. To this end, we unify a class of
kth-nearest-neighbors (kNN)-based metrics under an information-theoretic lens
using approaches from kNN density estimation. Additionally, we propose a
tri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall
Cross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity
and two distinct aspects of diversity, inter- and intra-class. Our
domain-agnostic metric, derived from the information-theoretic concepts of
entropy and cross-entropy, can be dissected for both sample- and mode-level
analysis. Our detailed experimental results demonstrate the sensitivity of our
metric components to their respective qualities and reveal undesirable
behaviors of other metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Long-Term Student Outcomes from Short-Term EdTech Log Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Gao, Amelia Leon, Andrea Jetten, Jasmine Turner, Husni Almoubayyed, Stephen Fancsali, Emma Brunskill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Educational stakeholders are often particularly interested in sparse, delayed
student outcomes, like end-of-year statewide exams. The rare occurrence of such
assessments makes it harder to identify students likely to fail such
assessments, as well as making it slow for researchers and educators to be able
to assess the effectiveness of particular educational tools. Prior work has
primarily focused on using logs from students full usage (e.g. year-long) of an
educational product to predict outcomes, or considered predictive accuracy
using a few minutes to predict outcomes after a short (e.g. 1 hour) session. In
contrast, we investigate machine learning predictors using students' logs
during their first few hours of usage can provide useful predictive insight
into those students' end-of-school year external assessment. We do this on
three diverse datasets: from students in Uganda using a literacy game product,
and from students in the US using two mathematics intelligent tutoring systems.
We consider various measures of the accuracy of the resulting predictors,
including its ability to identify students at different parts along the
assessment performance distribution. Our findings suggest that short-term log
usage data, from 2-5 hours, can be used to provide valuable signal about
students' long-term external performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 15th International Learning Analytics and Knowledge
  Conference (LAK2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Misclassification Network-Based Method for Comparative Genomic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wan He, Tina Eliassi-Rad, Samuel V. Scarpino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifying genome sequences based on metadata has been an active area of
research in comparative genomics for decades with many important applications
across the life sciences. Established methods for classifying genomes can be
broadly grouped into sequence alignment-based and alignment-free models.
Conventional alignment-based models rely on genome similarity measures
calculated based on local sequence alignments or consistent ordering among
sequences. However, such methods are computationally expensive when dealing
with large ensembles of even moderately sized genomes. In contrast,
alignment-free (AF) approaches measure genome similarity based on summary
statistics in an unsupervised setting and are efficient enough to analyze large
datasets. However, both alignment-based and AF methods typically assume fixed
scoring rubrics that lack the flexibility to assign varying importance to
different parts of the sequences based on prior knowledge. In this study, we
integrate AI and network science approaches to develop a comparative genomic
analysis framework that addresses these limitations. Our approach, termed the
Genome Misclassification Network Analysis (GMNA), simultaneously leverages
misclassified instances, a learned scoring rubric, and label information to
classify genomes based on associated metadata and better understand potential
drivers of misclassification. We evaluate the utility of the GMNA using Naive
Bayes and convolutional neural network models, supplemented by additional
experiments with transformer-based models, to construct SARS-CoV-2 sampling
location classifiers using over 500,000 viral genome sequences and study the
resulting network of misclassifications. We demonstrate the global health
potential of the GMNA by leveraging the SARS-CoV-2 genome misclassification
networks to investigate the role human mobility played in structuring
geographic clustering of SARS-CoV-2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowDock: Geometric Flow Matching for Generative Protein-Ligand Docking
  and Affinity Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Morehead, Jianlin Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful generative AI models of protein-ligand structure have recently been
proposed, but few of these methods support both flexible protein-ligand docking
and affinity estimation. Of those that do, none can directly model multiple
binding ligands concurrently or have been rigorously benchmarked on
pharmacologically relevant drug targets, hindering their widespread adoption in
drug discovery efforts. In this work, we propose FlowDock, the first deep
geometric generative model based on conditional flow matching that learns to
directly map unbound (apo) structures to their bound (holo) counterparts for an
arbitrary number of binding ligands. Furthermore, FlowDock provides predicted
structural confidence scores and binding affinity values with each of its
generated protein-ligand complex structures, enabling fast virtual screening of
new (multi-ligand) drug targets. For the well-known PoseBusters Benchmark
dataset, FlowDock outperforms single-sequence AlphaFold 3 with a 51% blind
docking success rate using unbound (apo) protein input structures and without
any information derived from multiple sequence alignments, and for the
challenging new DockGen-E dataset, FlowDock outperforms single-sequence
AlphaFold 3 and matches single-sequence Chai-1 for binding pocket
generalization. Additionally, in the ligand category of the 16th community-wide
Critical Assessment of Techniques for Structure Prediction (CASP16), FlowDock
ranked among the top-5 methods for pharmacological binding affinity estimation
across 140 protein-ligand complexes, demonstrating the efficacy of its learned
representations in virtual screening. Source code, data, and pre-trained models
are available at https://github.com/BioinfoMachineLearning/FlowDock.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 tables, 2 algorithms, 7 figures. Code, data, pre-trained
  models, and baseline method predictions are available at
  https://github.com/BioinfoMachineLearning/FlowDock</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of
  Micro-UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content
management system for disaster scenarios where communication infrastructure is
generally compromised. Utilizing a hybrid network of stationary and mobile
Micro-UAVs, this system aims to provide crucial content access to isolated
communities. In the developed architecture, stationary anchor UAVs, equipped
with vertical and lateral links, serve users in individual disaster-affected
communities. and mobile micro-ferrying UAVs, with enhanced mobility, extend
coverage across multiple such communities. The primary goal is to devise a
content dissemination system that dynamically learns caching policies to
maximize content accessibility to users left without communication
infrastructure. The core contribution is an adaptive content dissemination
framework that employs a decentralized Top-k Multi-Armed Bandit learning
approach for efficient UAV caching decisions. This approach accounts for
geo-temporal variations in content popularity and diverse user demands.
Additionally, a Selective Caching Algorithm is proposed to minimize redundant
content copies by leveraging inter-UAV information sharing. Through functional
verification and performance evaluation, the proposed framework demonstrates
improved system performance and adaptability across varying network sizes,
micro-UAV swarms, and content popularity distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures, 2 algorithms, 2 tables, journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Key-Exchange Convolutional Auto-Encoder for Data Augmentation in Early
  Knee Osteoarthritis Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Aladine Chetouani, Mohamed Jarraya, Yung Hsin Chen, Yuhua Ru, Fang Chen, Fabian Bauer, Liping Zhang, Didier Hans, Rachid Jennane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knee Osteoarthritis (KOA) is a common musculoskeletal condition that
significantly affects mobility and quality of life, particularly in elderly
populations. However, training deep learning models for early KOA
classification is often hampered by the limited availability of annotated
medical datasets, owing to the high costs and labour-intensive nature of data
labelling. Traditional data augmentation techniques, while useful, rely on
simple transformations and fail to introduce sufficient diversity into the
dataset. To address these challenges, we propose the Key-Exchange Convolutional
Auto-Encoder (KECAE) as an innovative Artificial Intelligence (AI)-based data
augmentation strategy for early KOA classification. Our model employs a
convolutional autoencoder with a novel key-exchange mechanism that generates
synthetic images by selectively exchanging key pathological features between
X-ray images, which not only diversifies the dataset but also ensures the
clinical validity of the augmented data. A hybrid loss function is introduced
to supervise feature learning and reconstruction, integrating multiple
components, including reconstruction, supervision, and feature separation
losses. Experimental results demonstrate that the KECAE-generated data
significantly improve the performance of KOA classification models, with
accuracy gains of up to 1.98% across various standard and state-of-the-art
architectures. Furthermore, a clinical validation study involving expert
radiologists confirms the anatomical plausibility and diagnostic realism of the
synthetic outputs. These findings highlight the potential of KECAE as a robust
tool for augmenting medical datasets in early KOA detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonsmooth Nonconvex-Nonconcave Minimax Optimization: Primal-Dual
  Balancing and Iteration Complexity Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10825v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10825v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajin Li, Linglingzhi Zhu, Anthony Man-Cho So
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonconvex-nonconcave minimax optimization has gained widespread interest over
the last decade. However, most existing works focus on variants of gradient
descent-ascent (GDA) algorithms, which are only applicable to smooth
nonconvex-concave settings. To address this limitation, we propose a novel
algorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which
can effectively handle a broad range of structured nonsmooth
nonconvex-nonconcave minimax problems. Specifically, we consider the setting
where the primal function has a nonsmooth composite structure and the dual
function possesses the Kurdyka-Lojasiewicz (KL) property with exponent $\theta
\in [0,1)$. We introduce a novel convergence analysis framework for smoothed
PLDA, the key components of which are our newly developed nonsmooth primal
error bound and dual error bound. Using this framework, we show that smoothed
PLDA can find both $\epsilon$-game-stationary points and
$\epsilon$-optimization-stationary points of the problems of interest in
$\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ iterations. Furthermore, when
$\theta \in [0,\frac{1}{2}]$, smoothed PLDA achieves the optimal iteration
complexity of $\mathcal{O}(\epsilon^{-2})$. To further demonstrate the
effectiveness and wide applicability of our analysis framework, we show that
certain max-structured problem possesses the KL property with exponent
$\theta=0$ under mild assumptions. As a by-product, we establish
algorithm-independent quantitative relationships among various stationarity
concepts, which may be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Mathematical Programming</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal LLMs Can Reason about Aesthetics in Zero-Shot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Jiang, Changwen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability
shall be elicited to evaluate the aesthetics of artworks. To facilitate this
investigation, we construct MM-StyleBench, a novel high-quality dataset for
benchmarking artistic stylization. We then develop a principled method for
human preference modeling and perform a systematic correlation analysis between
MLLMs' responses and human preference. Our experiments reveal an inherent
hallucination issue of MLLMs in art evaluation, associated with response
subjectivity. ArtCoT is proposed, demonstrating that art-specific task
decomposition and the use of concrete language boost MLLMs' reasoning ability
for aesthetics. Our findings offer valuable insights into MLLMs for art and can
benefit a wide range of downstream applications, such as style transfer and
artistic image generation. Code available at
https://github.com/songrise/MLLM4Art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WIP, Homepage https://github.com/songrise/MLLM4Art</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fake News Video Explanation Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhi Chen, Zhong Qian, Peifeng Li, Qiaoming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal explanation involves the assessment of the veracity of a variety
of different content, and relies on multiple information modalities to
comprehensively consider the relevance and consistency between modalities. Most
existing fake news video detection methods focus on improving accuracy while
ignoring the importance of providing explanations. In this paper, we propose a
novel problem - Fake News Video Explanation (FNVE) - Given a multimodal news
containing both video and caption text, we aim to generate natural language
explanations to reveal the truth of predictions. To this end, we develop
FakeNVE, a new dataset of explanations for truthfully multimodal posts, where
each explanation is a natural language (English) sentence describing the
attribution of a news thread. We benchmark FakeNVE by using a multimodal
transformer-based architecture. Subsequently, a BART-based autoregressive
decoder is used as the generator. Empirical results show compelling results for
various baselines (applicable to FNVE) across multiple evaluation metrics. We
also perform human evaluation on explanation generation, achieving high scores
for both adequacy and fluency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal and Multi-scale Spatial Environment Understanding for
  Immersive Visual Text-to-Speech <span class="chip">AAAI'2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11409v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11409v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Liu, Shuwei He, Yifan Hu, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Text-to-Speech (VTTS) aims to take the environmental image as the
prompt to synthesize the reverberant speech for the spoken content. The
challenge of this task lies in understanding the spatial environment from the
image. Many attempts have been made to extract global spatial visual
information from the RGB space of an spatial image. However, local and depth
image information are crucial for understanding the spatial environment, which
previous works have ignored. To address the issues, we propose a novel
multi-modal and multi-scale spatial environment understanding scheme to achieve
immersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and
Depth spaces of the spatial image to learn more comprehensive spatial
information, and the multi-scale seeks to model the local and global spatial
knowledge simultaneously. Specifically, we first split the RGB and Depth images
into patches and adopt the Gemini-generated environment captions to guide the
local spatial understanding. After that, the multi-modal and multi-scale
features are integrated by the local-aware global spatial understanding. In
this way, M2SE-VTTS effectively models the interactions between local and
global spatial contexts in the multi-modal spatial environment. Objective and
subjective evaluations suggest that our model outperforms the advanced
baselines in environmental speech generation. The code and audio samples are
available at: https://github.com/AI-S2-Lab/M2SE-VTTS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,2 figures, Accepted by AAAI'2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Karatsuba Matrix Multiplication and its Efficient Custom Hardware
  Implementations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor E. Pogue, Nicola Nicolici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the Karatsuba algorithm reduces the complexity of large integer
multiplication, the extra additions required minimize its benefits for smaller
integers of more commonly-used bitwidths. In this work, we propose the
extension of the scalar Karatsuba multiplication algorithm to matrix
multiplication, showing how this maintains the reduction in multiplication
complexity of the original Karatsuba algorithm while reducing the complexity of
the extra additions. Furthermore, we propose new matrix multiplication hardware
architectures for efficiently exploiting this extension of the Karatsuba
algorithm in custom hardware. We show that the proposed algorithm and hardware
architectures can provide real area or execution time improvements for integer
matrix multiplication compared to scalar Karatsuba or conventional matrix
multiplication algorithms, while also supporting implementation through proven
systolic array and conventional multiplier architectures at the core. We
provide a complexity analysis of the algorithm and architectures and evaluate
the proposed designs both in isolation and in an end-to-end deep learning
accelerator system compared to baseline designs and prior state-of-the-art
works implemented on the same type of compute platform, demonstrating their
ability to increase the performance-per-area of matrix multiplication hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Computers;
  Associated source code available on github at
  https://github.com/trevorpogue/algebraic-nnhw</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rule-Based <span class="highlight-title">Graph</span> Programs Matching the Time Complexity of Imperative
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziad Ismaili Alaoui, Detlef Plump
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report on a recent breakthrough in rule-based graph programming, which
allows us to match the time complexity of some fundamental imperative graph
algorithms. In general, achieving the complexity of graph algorithms in
conventional languages using graph transformation rules is challenging due to
the cost of graph matching. Previous work demonstrated that with rooted rules,
certain algorithms can be implemented in the graph programming language GP 2
such that their runtime matches the time complexity of imperative
implementations. However, this required input graphs to have a bounded node
degree and (for some algorithms) to be connected. In this paper, we overcome
these limitations by enhancing the graph data structure generated by the GP 2
compiler and exploiting the new structure in programs. We present three case
studies: the first program checks whether input graphs are connected, the
second program checks whether input graphs are acyclic, and the third program
solves the single-source shortest-paths problem for graphs with integer
edge-weights. The first two programs run in linear time on (possibly
disconnected) input graphs with arbitrary node degrees. The third program runs
in time O(nm) on arbitrary input graphs, matching the time complexity of
imperative implementations of the Bellman-Ford algorithm. For each program, we
formally prove its correctness and time complexity, and provide runtime
experiments on various graph classes.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Database <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Query Processing with Heterogeneous Machines <span class="chip">ICDT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Frisk, Paraschos Koutris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of computing a full Conjunctive Query in parallel using
$p$ heterogeneous machines. Our computational model is similar to the MPC
model, but each machine has its own cost function mapping from the number of
bits it receives to a cost. An optimal algorithm should minimize the maximum
cost across all machines. We consider algorithms over a single communication
round and give a lower bound and matching upper bound for databases where each
relation has the same cardinality. We do this for both linear cost functions
like in previous work, but also for more general cost functions. For databases
with relations of different cardinalities, we also find a lower bound, and give
matching upper bounds for specific queries like the cartesian product, the
join, the star query, and the triangle query. Our approach is inspired by the
HyperCube algorithm, but there are additional challenges involved when machines
have heterogeneous cost functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the full version of a paper accepted to ICDT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge <span class="highlight-title">Graph</span>-based <span class="highlight-title">Retrie</span>val-Augmented Generation for Schema Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuangtao Ma, Sriom Chakrabarti, Arijit Khan, Bálint Molnár
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional similarity-based schema matching methods are incapable of
resolving semantic ambiguities and conflicts in domain-specific complex mapping
scenarios due to missing commonsense and domain-specific knowledge. The
hallucination problem of large language models (LLMs) also makes it challenging
for LLM-based schema matching to address the above issues. Therefore, we
propose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema
Matching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces
novel vector-based, graph traversal-based, and query-based graph retrievals, as
well as a hybrid approach and ranking schemes that identify the most relevant
subgraphs from external large knowledge graphs (KGs). We showcase that KG-based
retrieval-augmented LLMs are capable of generating more accurate results for
complex matching cases without any re-training. Our experimental results show
that KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,
Jellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the
MIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the
pre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and
21.97% in terms of precision and F1 score on the Synthea dataset, respectively.
The results also demonstrate that our approach is more efficient in end-to-end
schema matching, and scales to retrieve from large KGs. Our case studies on the
dataset from the real-world schema matching scenario exhibit that the
hallucination problem of LLMs for schema matching is well mitigated by our
solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenMLDB: A Real-Time <span class="highlight-title">Relational</span> Data Feature Computation System for
  Online ML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhe Zhou, Wei Zhou, Liguo Qi, Hao Zhang, Dihao Chen, Bingsheng He, Mian Lu, Guoliang Li, Fan Wu, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and consistent feature computation is crucial for a wide range of
online ML applications. Typically, feature computation is divided into two
distinct phases, i.e., offline stage for model training and online stage for
model serving. These phases often rely on execution engines with different
interface languages and function implementations, causing significant
inconsistencies. Moreover, many online ML features involve complex time-series
computations (e.g., functions over varied-length table windows) that differ
from standard streaming and analytical queries. Existing data processing
systems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for
these computations, making them unsuitable for real-time online ML applications
that demand timely feature updates.
  This paper presents OpenMLDB, a feature computation system deployed in
4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB
first employs a unified query plan generator for consistent computation results
across the offline and online stages, significantly reducing feature deployment
overhead. Second, OpenMLDB provides an online execution engine that resolves
performance bottlenecks caused by long window computations (via
pre-aggregation) and multi-table window unions (via data self-adjusting). It
also provides a high-performance offline execution engine with window parallel
optimization and time-aware data skew resolving. Third, OpenMLDB features a
compact data format and stream-focused indexing to maximize memory usage and
accelerate data access. Evaluations in testing and real workloads reveal
significant performance improvements and resource savings compared to the
baseline systems. The open community of OpenMLDB now has over 150 contributors
and gained 1.6k stars on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge <span class="highlight-title">prompt</span> chaining for semantic modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Pei Ding, Jingge Du, Zaiwen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of building semantics for structured data such as CSV, JSON, and XML
files is highly relevant in the knowledge representation field. Even though we
have a vast of structured data on the internet, mapping them to domain
ontologies to build semantics for them is still very challenging as it requires
the construction model to understand and learn graph-structured knowledge.
Otherwise, the task will require human beings' effort and cost. In this paper,
we proposed a novel automatic semantic modeling framework: Knowledge Prompt
Chaining. It can serialize the graph-structured knowledge and inject it into
the LLMs properly in a Prompt Chaining architecture. Through this knowledge
injection and prompting chaining, the model in our framework can learn the
structure information and latent space of the graph and generate the semantic
labels and semantic graphs following the chains' insturction naturally. Based
on experimental results, our method achieves better performance than existing
leading techniques, despite using reduced structured input data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functorial aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10968v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10968v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David I. Spivak, Richard Garner, Aaron David Fairbanks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study polynomial comonads and polynomial bicomodules. Polynomial comonads
amount to categories. Polynomial bicomodules between categories amount to
parametric right adjoint functors between corresponding copresheaf categories.
These may themselves be understood as generalized polynomial functors. They are
also called data migration functors because of applications in categorical
database theory. We investigate several universal constructions in the framed
bicategory of categories, retrofunctors, and parametric right adjoints. We then
use the theory we develop to model database aggregation alongside querying, all
within this rich ecosystem.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-14T00:00:00Z">2025-01-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">91</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PokerBench: Training Large Language Models to become Professional Poker
  Players <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Zhuang, Akshat Gupta, Richard Yang, Aniket Rahane, Zhengyu Li, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PokerBench - a benchmark for evaluating the poker-playing
abilities of large language models (LLMs). As LLMs excel in traditional NLP
tasks, their application to complex, strategic games like poker poses a new
challenge. Poker, an incomplete information game, demands a multitude of skills
such as mathematics, reasoning, planning, strategy, and a deep understanding of
game theory and human psychology. This makes Poker the ideal next frontier for
large language models. PokerBench consists of a comprehensive compilation of
11,000 most important scenarios, split between pre-flop and post-flop play,
developed in collaboration with trained poker players. We evaluate prominent
models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,
finding that all state-of-the-art LLMs underperform in playing optimal poker.
However, after fine-tuning, these models show marked improvements. We validate
PokerBench by having models with different scores compete with each other,
demonstrating that higher scores on PokerBench lead to higher win rates in
actual poker games. Through gameplay between our fine-tuned model and GPT-4, we
also identify limitations of simple supervised fine-tuning for learning optimal
playing strategy, suggesting the need for more advanced methodologies for
effectively training language models to excel in games. PokerBench thus
presents a unique benchmark for a quick and reliable evaluation of the
poker-playing ability of LLMs as well as a comprehensive benchmark to study the
progress of LLMs in complex game-playing scenarios. The dataset and code will
be made available at: \url{https://github.com/pokerllm/pokerbench}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Robustness of Multilingual LLMs on Real-World Noisy Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Aliakbarzadeh, Lucie Flek, Akbar Karimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are trained on Web data that might contain
spelling errors made by humans. But do they become robust to similar real-world
noise? In this paper, we investigate the effect of real-world spelling mistakes
on the performance of 9 language models, with parameters ranging from 0.2B to
13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name
Entity Recognition (NER), and Intent Classification (IC). We perform our
experiments on 6 different languages and build a dictionary of real-world noise
for them using the Wikipedia edit history. We show that the performance gap of
the studied models on the clean and noisy test data averaged across all the
datasets and languages ranges from 2.3 to 4.3 absolute percentage points. In
addition, mT5 models, in general, show more robustness compared to BLOOM,
Falcon, and BERT-like models. In particular, mT5 (13B), was the most robust on
average overall, across the 3 tasks, and in 4 of the 6 languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Automated Interpretability with Output-Centric Feature
  Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated interpretability pipelines generate natural language descriptions
for the concepts represented by features in large language models (LLMs), such
as plants or the first word in a sentence. These descriptions are derived using
inputs that activate the feature, which may be a dimension or a direction in
the model's representation space. However, identifying activating inputs is
costly, and the mechanistic role of a feature in model behavior is determined
both by how inputs cause a feature to activate and by how feature activation
affects outputs. Using steering evaluations, we reveal that current pipelines
provide descriptions that fail to capture the causal effect of the feature on
outputs. To fix this, we propose efficient, output-centric methods for
automatically generating feature descriptions. These methods use the tokens
weighted higher after feature stimulation or the highest weight tokens after
applying the vocabulary "unembedding" head directly to the feature. Our
output-centric descriptions better capture the causal effect of a feature on
model outputs than input-centric descriptions, but combining the two leads to
the best performance on both input and output evaluations. Lastly, we show that
output-centric descriptions can be used to find inputs that activate features
previously thought to be "dead".
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiniMax-01: Scaling <span class="highlight-title">Foundation</span> Models with Lightning Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,
which are comparable to top-tier models while offering superior capabilities in
processing longer contexts. The core lies in lightning attention and its
efficient scaling. To maximize computational capacity, we integrate it with
Mixture of Experts (MoE), creating a model with 32 experts and 456 billion
total parameters, of which 45.9 billion are activated for each token. We
develop an optimized parallel strategy and highly efficient
computation-communication overlap techniques for MoE and lightning attention.
This approach enables us to conduct efficient training and inference on models
with hundreds of billions of parameters across contexts spanning millions of
tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens
during training and extrapolate to 4 million tokens during inference at an
affordable cost. Our vision-language model, MiniMax-VL-01 is built through
continued training with 512 billion vision-language tokens. Experiments on both
standard and in-house benchmarks show that our models match the performance of
state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32
times longer context window. We publicly release MiniMax-01 at
https://github.com/MiniMax-AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A technical report from MiniMax. The authors are listed in
  alphabetical order. We open-sourced our MiniMax-01 at
  https://github.com/MiniMax-AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Everybody Likes to Sleep: A Computer-Assisted Comparison of Object
  Naming Data from 30 Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alžběta Kučerová, Johann-Mattis List
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object naming - the act of identifying an object with a word or a phrase - is
a fundamental skill in interpersonal communication, relevant to many
disciplines, such as psycholinguistics, cognitive linguistics, or language and
vision research. Object naming datasets, which consist of concept lists with
picture pairings, are used to gain insights into how humans access and select
names for objects in their surroundings and to study the cognitive processes
involved in converting visual stimuli into semantic concepts. Unfortunately,
object naming datasets often lack transparency and have a highly idiosyncratic
structure. Our study tries to make current object naming data transparent and
comparable by using a multilingual, computer-assisted approach that links
individual items of object naming lists to unified concepts. Our current sample
links 17 object naming datasets that cover 30 languages from 10 different
language families. We illustrate how the comparative dataset can be explored by
searching for concepts that recur across the majority of datasets and comparing
the conceptual spaces of covered object naming datasets with classical basic
vocabulary lists from historical linguistics and linguistic typology. Our
findings can serve as a basis for enhancing cross-linguistic object naming
research and as a guideline for future studies dealing with object naming
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the Global WordNet Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Pedophile Attribution Techniques for Online Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiba Fallatah, Ching Suen, Olga Ormandjieva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliance on anonymity in social media has increased its popularity on these
platforms among all ages. The availability of public Wi-Fi networks has
facilitated a vast variety of online content, including social media
applications. Although anonymity and ease of access can be a convenient means
of communication for their users, it is difficult to manage and protect its
vulnerable users against sexual predators. Using an automated identification
system that can attribute predators to their text would make the solution more
attainable. In this survey, we provide a review of the methods of pedophile
attribution used in social media platforms. We examine the effect of the size
of the suspect set and the length of the text on the task of attribution.
Moreover, we review the most-used datasets, features, classification techniques
and performance measures for attributing sexual predators. We found that few
studies have proposed tools to mitigate the risk of online sexual predators,
but none of them can provide suspect attribution. Finally, we list several open
research problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HALoGEN: Fantastic LLM Hallucinations and Where to Find Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhilasha Ravichander, Shrusti Ghela, David Wadden, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their impressive ability to generate high-quality and fluent text,
generative large language models (LLMs) also produce hallucinations: statements
that are misaligned with established world knowledge or provided input context.
However, measuring hallucination can be challenging, as having humans verify
model generations on-the-fly is both expensive and time-consuming. In this
work, we release HALoGEN, a comprehensive hallucination benchmark consisting
of: (1) 10,923 prompts for generative models spanning nine domains including
programming, scientific attribution, and summarization, and (2) automatic
high-precision verifiers for each use case that decompose LLM generations into
atomic units, and verify each unit against a high-quality knowledge source. We
use this framework to evaluate ~150,000 generations from 14 language models,
finding that even the best-performing models are riddled with hallucinations
(sometimes up to 86% of generated atomic facts depending on the domain). We
further define a novel error classification for LLM hallucinations based on
whether they likely stem from incorrect recollection of training data (Type A
errors), or incorrect knowledge in training data (Type B errors), or are
fabrication (Type C errors). We hope our framework provides a foundation to
enable the principled study of why generative models hallucinate, and advances
the development of trustworthy large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AfriHate: A Multilingual Collection of Hate Speech and Abusive Language
  <span class="highlight-title">Dataset</span>s for African Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Saminu Mohammad Aliyu, Nelson Odhiambo Onyango, Lilian D. A. Wanzare, Samuel Rutunda, Lukman Jibril Aliyu, Esubalew Alemneh, Oumaima Hourrane, Hagos Tesfahun Gebremichael, Elyas Abdi Ismail, Meriem Beloucif, Ebrahim Chekol Jibril, Andiswa Bukula, Rooweither Mabuya, Salomey Osei, Abigail Oppong, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw, Chiamaka Ijeoma Chukwuneke, Paul Röttger, Seid Muhie Yimam, Nedjma Ousidhoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech and abusive language are global phenomena that need
socio-cultural background knowledge to be understood, identified, and
moderated. However, in many regions of the Global South, there have been
several documented occurrences of (1) absence of moderation and (2) censorship
due to the reliance on keyword spotting out of context. Further, high-profile
individuals have frequently been at the center of the moderation process, while
large and targeted hate speech campaigns against minorities have been
overlooked. These limitations are mainly due to the lack of high-quality data
in the local languages and the failure to include local communities in the
collection, annotation, and moderation processes. To address this issue, we
present AfriHate: a multilingual collection of hate speech and abusive language
datasets in 15 African languages. Each instance in AfriHate is annotated by
native speakers familiar with the local culture. We report the challenges
related to the construction of the datasets and present various classification
baseline results with and without using LLMs. The datasets, individual
annotations, and hate speech and offensive language lexicons are available on
https://github.com/AfriHate/AfriHate
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Robustness of LLMs to Sociodemo<span class="highlight-title">graph</span>ically-Conditioned
  Paraphrasing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pulkit Arora, Akbar Karimi, Lucie Flek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive performance in various NLP
tasks. However, there are concerns about their reliability in different domains
of linguistic variations. Many works have proposed robustness evaluation
measures for local adversarial attacks, but we need globally robust models
unbiased to different language styles. We take a broader approach to explore a
wider range of variations across sociodemographic dimensions to perform
structured reliability tests on the reasoning capacity of language models. We
extend the SocialIQA dataset to create diverse paraphrased sets conditioned on
sociodemographic styles. The assessment aims to provide a deeper understanding
of LLMs in (a) their capability of generating demographic paraphrases with
engineered prompts and (b) their reasoning capabilities in real-world, complex
language scenarios. We also explore measures such as perplexity,
explainability, and ATOMIC performance of paraphrases for fine-grained
reliability analysis of LLMs on these sets. We find that demographic-specific
paraphrasing significantly impacts the performance of language models,
indicating that the subtleties of language variations remain a significant
challenge. The code and dataset will be made available for reproducibility and
future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of Efficient Adapter-Based Fine-Tuning of
  State-of-the-Art Transformer Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Mashkoor Siddiqui, Mohammad Ali Sheikh, Muhammad Aleem, Kajol R Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the efficacy of various adapter architectures on
supervised binary classification tasks from the SuperGLUE benchmark as well as
a supervised multi-class news category classification task from Kaggle.
Specifically, we compare classification performance and time complexity of
three transformer models, namely DistilBERT, ELECTRA, and BART, using
conventional fine-tuning as well as nine state-of-the-art (SoTA) adapter
architectures. Our analysis reveals performance differences across adapter
architectures, highlighting their ability to achieve comparable or better
performance relative to fine-tuning at a fraction of the training time. Similar
results are observed on the new classification task, further supporting our
findings and demonstrating adapters as efficient and flexible alternatives to
fine-tuning. This study provides valuable insights and guidelines for selecting
and implementing adapters in diverse natural language processing (NLP)
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eliciting In-context <span class="highlight-title">Retrie</span>val and Reasoning for Long-context Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context language models (LCLMs) promise to
transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With
their expanded context windows, LCLMs can process entire knowledge bases and
perform retrieval and reasoning directly -- a capability we define as
In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like
LOFT often overestimate LCLM performance by providing overly simplified
contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs
in more realistic scenarios by including confounding passages retrieved with
strong retrievers. We then propose three methods to enhance LCLM performance:
(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which
uses attention heads to filter and de-noise long contexts during decoding, and
(3) joint retrieval head training alongside the generation head. Our evaluation
of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with
our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on
LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised
fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks
despite being a much smaller model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASTRID -- An Automated and Scalable TRIaD for the Evaluation of
  RAG-based Clinical Question Answering Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohita Chowdhury, Yajie Vera He, Aisling Higham, Ernest Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive potential in clinical
question answering (QA), with Retrieval Augmented Generation (RAG) emerging as
a leading approach for ensuring the factual accuracy of model responses.
However, current automated RAG metrics perform poorly in clinical and
conversational use cases. Using clinical human evaluations of responses is
expensive, unscalable, and not conducive to the continuous iterative
development of RAG systems. To address these challenges, we introduce ASTRID -
an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging
RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy
(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is
designed to better capture the faithfulness of a model's response to the
knowledge base without penalising conversational elements. To validate our
triad, we curate a dataset of over 200 real-world patient questions posed to an
LLM-based QA agent during surgical follow-up for cataract surgery - the highest
volume operation in the world - augmented with clinician-selected questions for
emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate
that CF can predict human ratings of faithfulness better than existing
definitions for conversational use cases. Furthermore, we show that evaluation
using our triad consisting of CF, RA, and CR exhibits alignment with clinician
assessment for inappropriate, harmful, or unhelpful responses. Finally, using
nine different LLMs, we demonstrate that the three metrics can closely agree
with human evaluations, highlighting the potential of these metrics for use in
LLM-driven automated evaluation pipelines. We also publish the prompts and
datasets for these experiments, providing valuable resources for further
research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math
  Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zain Ul Abedin, Shahzeb Qamar, Lucie Flek, Akbar Karimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have shown impressive capabilities in math
problem-solving tasks, their robustness to noisy inputs is not well-studied. In
this work, we propose ArithmAttack to examine how robust the LLMs are when they
encounter noisy prompts that contain extra noise in the form of punctuation
marks. While being easy to implement, ArithmAttack does not cause any
information loss since words are not added or deleted from the context. We
evaluate the robustness of seven LLMs, including LLama3, Mistral, and
Mathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that
all the studied models show vulnerability to such noise, with more noise
leading to poorer performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CWEval: Outcome-driven Evaluation on Functionality and Security of LLM
  Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have significantly aided developers by
generating or assisting in code writing, enhancing productivity across various
tasks. While identifying incorrect code is often straightforward, detecting
vulnerabilities in functionally correct code is more challenging, especially
for developers with limited security knowledge, which poses considerable
security risks of using LLM-generated code and underscores the need for robust
evaluation benchmarks that assess both functional correctness and security.
Current benchmarks like CyberSecEval and SecurityEval attempt to solve it but
are hindered by unclear and impractical specifications, failing to assess both
functionality and security accurately. To tackle these deficiencies, we
introduce CWEval, a novel outcome-driven evaluation framework designed to
enhance the evaluation of secure code generation by LLMs. This framework not
only assesses code functionality but also its security simultaneously with
high-quality task specifications and outcome-driven test oracles which provides
high accuracy. Coupled with CWEval-bench, a multilingual, security-critical
coding benchmark, CWEval provides a rigorous empirical security evaluation on
LLM-generated code, overcoming previous benchmarks' shortcomings. Through our
evaluations, CWEval reveals a notable portion of functional but insecure code
produced by LLMs, and shows a serious inaccuracy of previous evaluations,
ultimately contributing significantly to the field of secure code generation.
We open-source our artifact at: https://github.com/Co1lin/CWEval .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in LLM4Code 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenCSG Chinese Corpus: A Series of High-quality Chinese <span class="highlight-title">Dataset</span>s for
  LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiong Yu, Ziyun Dai, Zekun Wang, Wei Wang, Ran Chen, Ji Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities, but
their success heavily relies on the quality of pretraining corpora. For Chinese
LLMs, the scarcity of high-quality Chinese datasets presents a significant
challenge, often limiting their performance. To address this issue, we propose
the OpenCSG Chinese Corpus, a series of high-quality datasets specifically
designed for LLM pretraining, post-training, and fine-tuning. This corpus
includes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and
Smoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets
focus on filtered, high-quality content derived from diverse Chinese web
sources; Cosmopedia-chinese provides synthetic, textbook-style data for
knowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and
diverse chat-format data. The OpenCSG Chinese Corpus is characterized by its
high-quality text, diverse coverage across domains, and scalable, reproducible
data curation processes. Additionally, we conducted extensive experimental
analyses, including evaluations on smaller parameter models, which demonstrated
significant performance improvements in tasks such as C-Eval, showcasing the
effectiveness of the corpus for training Chinese LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The datasets are available on
  https://huggingface.co/collections/opencsg/chinese-fineweb-66cfed105f502ece8f29643e
  ; The code is on https://github.com/yuyijiong/fineweb-edu-chinese</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction
  Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Fang, Xinle Deng, Kangwei Liu, Ningyu Zhang, Jingyang Qian, Penghui Yang, Xiaohui Fan, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models excel at interpreting complex natural language
instructions, enabling them to perform a wide range of tasks. In the life
sciences, single-cell RNA sequencing (scRNA-seq) data serves as the "language
of cellular biology", capturing intricate gene expression patterns at the
single-cell level. However, interacting with this "language" through
conventional tools is often inefficient and unintuitive, posing challenges for
researchers. To address these limitations, we present InstructCell, a
multi-modal AI copilot that leverages natural language as a medium for more
direct and flexible single-cell analysis. We construct a comprehensive
multi-modal instruction dataset that pairs text-based instructions with
scRNA-seq profiles from diverse tissues and species. Building on this, we
develop a multi-modal cell language architecture capable of simultaneously
interpreting and processing both modalities. InstructCell empowers researchers
to accomplish critical tasks-such as cell type annotation, conditional
pseudo-cell generation, and drug sensitivity prediction-using straightforward
natural language commands. Extensive evaluations demonstrate that InstructCell
consistently meets or exceeds the performance of existing single-cell
foundation models, while adapting to diverse experimental conditions. More
importantly, InstructCell provides an accessible and intuitive tool for
exploring complex single-cell data, lowering technical barriers and enabling
deeper biological insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell;
  Models: https://huggingface.co/zjunlp/Instructcell-chat,
  https://huggingface.co/zjunlp/InstructCell-instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Potential and Perils of Large Language Models as Judges of Unstructured
  Textual Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements in large language models have unlocked remarkable
capabilities when it comes to processing and summarizing unstructured text
data. This has implications for the analysis of rich, open-ended datasets, such
as survey responses, where LLMs hold the promise of efficiently distilling key
themes and sentiments. However, as organizations increasingly turn to these
powerful AI systems to make sense of textual feedback, a critical question
arises, can we trust LLMs to accurately represent the perspectives contained
within these text based datasets? While LLMs excel at generating human-like
summaries, there is a risk that their outputs may inadvertently diverge from
the true substance of the original responses. Discrepancies between the
LLM-generated outputs and the actual themes present in the data could lead to
flawed decision-making, with far-reaching consequences for organizations. This
research investigates the effectiveness of LLMs as judge models to evaluate the
thematic alignment of summaries generated by other LLMs. We utilized an
Anthropic Claude model to generate thematic summaries from open-ended survey
responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as
LLM judges. The LLM-as-judge approach was compared to human evaluations using
Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable
alternative to traditional human centric evaluation methods. Our findings
reveal that while LLMs as judges offer a scalable solution comparable to human
raters, humans may still excel at detecting subtle, context-specific nuances.
This research contributes to the growing body of knowledge on AI assisted text
analysis. We discuss limitations and provide recommendations for future
research, emphasizing the need for careful consideration when generalizing LLM
judge models across various contexts and use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refusal Behavior in Large Language Models: A Nonlinear Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Hildebrandt, Andreas Maier, Patrick Krauss, Achim Schilling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Refusal behavior in large language models (LLMs) enables them to decline
responding to harmful, unethical, or inappropriate prompts, ensuring alignment
with ethical standards. This paper investigates refusal behavior across six
LLMs from three architectural families. We challenge the assumption of refusal
as a linear phenomenon by employing dimensionality reduction techniques,
including PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms
exhibit nonlinear, multidimensional characteristics that vary by model
architecture and layer. These findings highlight the need for nonlinear
interpretability to improve alignment research and inform safer AI deployment
strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-situ <span class="highlight-title">graph</span> reasoning and knowledge expansion using <span class="highlight-title">Graph</span>-PReFLexOR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of automated scientific discovery has fueled progress from
symbolic logic to modern AI, forging new frontiers in reasoning and pattern
recognition. Transformers function as potential systems, where every possible
relationship remains latent potentiality until tasks impose constraints, akin
to measurement. Yet, refining their sampling requires more than probabilistic
selection: solutions must conform to specific structures or rules, ensuring
consistency and the invocation of general principles. We present
Graph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for
Exploratory Optimization of Reasoning), a framework that combines graph
reasoning with symbolic abstraction to dynamically expand domain knowledge.
Inspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a
structured mapping, where tasks yield knowledge graphs, abstract patterns, and
ultimately, final answers. Inspired by category theory, it encodes concepts as
nodes and their relationships as edges, supporting hierarchical inference and
adaptive learning through isomorphic representations. Demonstrations include
hypothesis generation, materials design, and creative reasoning, such as
discovering relationships between mythological concepts like 'thin places' with
materials science. We propose a 'knowledge garden growth' strategy that
integrates insights across domains, promoting interdisciplinary connections.
Results with a 3-billion-parameter Graph-PReFLexOR model show superior
reasoning depth and adaptability, underscoring the potential for transparent,
multidisciplinary AI-driven discovery. It lays the groundwork for general
autonomous reasoning solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying the Importance of Data Alignment in Downstream Model
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krrish Chawla, Aryan Sahai, Mario DePavia, Sudharsan Sundar, Brando Miranda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrary to the conventional emphasis on dataset size, we explore the role of
data alignment -- an often overlooked aspect of data quality -- in training
capable Large Language Models (LLMs). To do so, we use the Task2Vec-based
alignment coefficient, a quantitative measure of the similarity between two
datasets, to quantify the impact of alignment between training data and
evaluation data on downstream performance. In particular, we conduct controlled
\textit{interventional} experiments for two settings: 1. the impact of
increased alignment coefficients between various pre-training (pt) against
evaluation datasets, and 2. the impact of increased alignment coefficients
between domain specific fine-tuning (ft) against domain specific evaluation.
The domain specific task we explore is Autoformalization -- the machine
translation task between natural language and code for formal verification. In
both settings, we find a strong, predictable negative correlation between the
alignment coefficient of a model's training and evaluation data and the model's
loss/perplexity on the respective downstream task. These findings suggest a
re-evaluation of LLM training approaches, demonstrating the relevance of data
alignment compared to data quantity, especially in specialized downstream tasks
such as Autoformalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Theater Stage as Laboratory: <span class="highlight-title">Review</span> of Real-Time Comedy LLM Systems
  for Live Performance <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Wojciech Mirowski, Boyd Branch, Kory Wallace Mathewson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this position paper, we review the eclectic recent history of academic and
artistic works involving computational systems for humor generation, and focus
specifically on live performance. We make the case that AI comedy should be
evaluated in live conditions, in front of audiences sharing either physical or
online spaces, and under real-time constraints. We further suggest that
improvised comedy is therefore the perfect substrate for deploying and
assessing computational humor systems. Using examples of successful AI-infused
shows, we demonstrate that live performance raises three sets of challenges for
computational humor generation: 1) questions around robotic embodiment,
anthropomorphism and competition between humans and machines, 2) questions
around comedic timing and the nature of audience interaction, and 3) questions
about the human interpretation of seemingly absurd AI-generated humor. We argue
that these questions impact the choice of methodologies for evaluating
computational humor, as any such method needs to work around the constraints of
live audiences and performance spaces. These interrogations also highlight
different types of collaborative relationship of human comedians towards AI
tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1st Workshop on Computational Humor (CHum), COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Attention <span class="highlight-title">Merging</span> for low resource tasks: A case study of
  Child ASR <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natarajan Balaji Shankar, Zilai Wang, Eray Eren, Abeer Alwan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Speech Foundation Models (SFMs) excel in various speech tasks, their
performance for low-resource tasks such as child Automatic Speech Recognition
(ASR) is hampered by limited pretraining data. To address this, we explore
different model merging techniques to leverage knowledge from models trained on
larger, more diverse speech corpora. This paper also introduces Selective
Attention (SA) Merge, a novel method that selectively merges task vectors from
attention matrices to enhance SFM performance on low-resource tasks.
Experiments on the MyST database show significant reductions in relative word
error rate of up to 14%, outperforming existing model merging and data
augmentation techniques. By combining data augmentation techniques with SA
Merge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for
the Whisper-small model, highlighting the potential of SA Merge for improving
low-resource ASR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Zero-Shot & Explainable Video Description by Reasoning over
  <span class="highlight-title">Graph</span>s of Events in Space and Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihai Masala, Marius Leordeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current era of Machine Learning, Transformers have become the de facto
approach across a variety of domains, such as computer vision and natural
language processing. Transformer-based solutions are the backbone of current
state-of-the-art methods for language generation, image and video
classification, segmentation, action and object recognition, among many others.
Interestingly enough, while these state-of-the-art methods produce impressive
results in their respective domains, the problem of understanding the
relationship between vision and language is still beyond our reach. In this
work, we propose a common ground between vision and language based on events in
space and time in an explainable and programmatic way, to connect
learning-based vision and language state of the art models and provide a
solution to the long standing problem of describing videos in natural language.
We validate that our algorithmic approach is able to generate coherent, rich
and relevant textual descriptions on videos collected from a variety of
datasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern
LLM-as-a-Jury approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models For Text Classification: Case Study And
  Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arina Kostina, Marios D. Dikaiakos, Dimosthenis Stefanidis, George Pallis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlocking the potential of Large Language Models (LLMs) in data
classification represents a promising frontier in natural language processing.
In this work, we evaluate the performance of different LLMs in comparison with
state-of-the-art deep-learning and machine-learning models, in two different
classification scenarios: i) the classification of employees' working locations
based on job reviews posted online (multiclass classification), and 2) the
classification of news articles as fake or not (binary classification). Our
analysis encompasses a diverse range of language models differentiating in
size, quantization, and architecture. We explore the impact of alternative
prompting techniques and evaluate the models based on the weighted F1-score.
Also, we examine the trade-off between performance (F1-score) and time
(inference response time) for each language model to provide a more nuanced
understanding of each model's practical applicability. Our work reveals
significant variations in model responses based on the prompting strategies. We
find that LLMs, particularly Llama3 and GPT-4, can outperform traditional
methods in complex classification tasks, such as multiclass classification,
though at the cost of longer inference times. In contrast, simpler ML models
offer better performance-to-time trade-offs in simpler binary classification
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tag&Tab: <span class="highlight-title">Pretrain</span>ing Data Detection in Large Language Models Using
  Keyword-Based Membership Inference Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagiv Antebi, Edan Habler, Asaf Shabtai, Yuval Elovici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become essential digital task assistance
tools. Their training relies heavily on the collection of vast amounts of data,
which may include copyright-protected or sensitive information. Recent studies
on the detection of pretraining data in LLMs have primarily focused on
sentence-level or paragraph-level membership inference attacks (MIAs), usually
involving probability analysis of the target model prediction tokens. However,
the proposed methods often demonstrate poor performance, specifically in terms
of accuracy, failing to account for the semantic importance of textual content
and word significance. To address these shortcomings, we propose Tag&Tab, a
novel approach for detecting data that has been used as part of the LLM
pretraining. Our method leverages advanced natural language processing (NLP)
techniques to tag keywords in the input text - a process we term Tagging. Then,
the LLM is used to obtain the probabilities of these keywords and calculate
their average log-likelihood to determine input text membership, a process we
refer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,
MIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate
an average increase in the AUC scores ranging from 4.1% to 12.1% over
state-of-the-art methods. Tag&Tab not only sets a new standard for data leakage
detection in LLMs, but its outstanding performance is a testament to the
importance of words in MIAs on LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jochre 3 and the Yiddish OCR corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Assaf Urieli, Amber Clooney, Michelle Sigiel, Grisha Leyfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe the construction of a publicly available Yiddish OCR Corpus, and
describe and evaluate the open source OCR tool suite Jochre 3, including an
Alto editor for corpus annotation, OCR software for Alto OCR layer generation,
and a customizable OCR search engine. The current version of the Yiddish OCR
corpus contains 658 pages, 186K tokens and 840K glyphs. The Jochre 3 OCR tool
uses various fine-tuned YOLOv8 models for top-down page layout analysis, and a
custom CNN network for glyph recognition. It attains a CER of 1.5% on our test
corpus, far out-performing all other existing public models for Yiddish. We
analyzed the full 660M word Yiddish Book Center with Jochre 3 OCR, and the new
OCR is searchable through the Yiddish Book Center OCR search engine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Religious Bias Landscape in Language and Text-to-Image Models: Analysis,
  Detection, and Debiasing Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajwad Abrar, Nafisa Tabassum Oeshy, Mohsinul Kabir, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Note: This paper includes examples of potentially offensive content related
to religious bias, presented solely for academic purposes. The widespread
adoption of language models highlights the need for critical examinations of
their inherent biases, particularly concerning religion. This study
systematically investigates religious bias in both language models and
text-to-image generation models, analyzing both open-source and closed-source
systems. We construct approximately 400 unique, naturally occurring prompts to
probe language models for religious bias across diverse tasks, including mask
filling, prompt completion, and image generation. Our experiments reveal
concerning instances of underlying stereotypes and biases associated
disproportionately with certain religions. Additionally, we explore
cross-domain biases, examining how religious bias intersects with demographic
factors such as gender, age, and nationality. This study further evaluates the
effectiveness of targeted debiasing techniques by employing corrective prompts
designed to mitigate the identified biases. Our findings demonstrate that
language models continue to exhibit significant biases in both text and image
generation tasks, emphasizing the urgent need to develop fairer language models
to achieve global acceptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEAL: Speaker Error Correction using Acoustic-conditioned Large Language
  Models <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Kumar, Rohit Paturi, Amber Afshan, Sundararajan Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speaker Diarization (SD) is a crucial component of modern end-to-end ASR
pipelines. Traditional SD systems, which are typically audio-based and operate
independently of ASR, often introduce speaker errors, particularly during
speaker transitions and overlapping speech. Recently, language models including
fine-tuned large language models (LLMs) have shown to be effective as a
second-pass speaker error corrector by leveraging lexical context in the
transcribed output. In this work, we introduce a novel acoustic conditioning
approach to provide more fine-grained information from the acoustic diarizer to
the LLM. We also show that a simpler constrained decoding strategy reduces LLM
hallucinations, while avoiding complicated post-processing. Our approach
significantly reduces the speaker error rates by 24-43% across Fisher,
Callhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble of Large Language Models for Curated Labeling and Rating of
  Free-text Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxing Qiu, Dongliang Guo, Papini Natalie, Peace Noelle, Levinson Cheri, Teague R. Henry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Free-text responses are commonly collected in psychological studies,
providing rich qualitative insights that quantitative measures may not capture.
Labeling curated topics of research interest in free-text data by multiple
trained human coders is typically labor-intensive and time-consuming. Though
large language models (LLMs) excel in language processing, LLM-assisted
labeling techniques relying on closed-source LLMs cannot be directly applied to
free-text data, without explicit consent for external use.
  In this study, we propose a framework of assembling locally-deployable LLMs
to enhance the labeling of predetermined topics in free-text data under privacy
constraints. Analogous to annotation by multiple human raters, this framework
leverages the heterogeneity of diverse open-source LLMs. The ensemble approach
seeks a balance between the agreement and disagreement across LLMs, guided by a
relevancy scoring methodology that utilizes embedding distances between topic
descriptions and LLMs' reasoning. We evaluated the ensemble approach using both
publicly accessible Reddit data from eating disorder related forums, and
free-text responses from eating disorder patients, both complemented by human
annotations.
  We found that: (1) there is heterogeneity in the performance of labeling
among same-sized LLMs, with some showing low sensitivity but high precision,
while others exhibit high sensitivity but low precision. (2) Compared to
individual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal
precision-sensitivity trade-off in predicting human annotations. (3) The
relevancy scores across LLMs showed greater agreement than dichotomous labels,
indicating that the relevancy scoring method effectively mitigates the
heterogeneity in LLMs' labeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OptiChat: Bridging Optimization Models and Practitioners with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Gonzalo Esteban Constante-Flores, Krishna Sri Ipsit Mantri, Sai Madhukiran Kompalli, Akshdeep Singh Ahluwalia, Can Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization models have been applied to solve a wide variety of
decision-making problems. These models are usually developed by optimization
experts but are used by practitioners without optimization expertise in various
application domains. As a result, practitioners often struggle to interact with
and draw useful conclusions from optimization models independently. To fill
this gap, we introduce OptiChat, a natural language dialogue system designed to
help practitioners interpret model formulation, diagnose infeasibility, analyze
sensitivity, retrieve information, evaluate modifications, and provide
counterfactual explanations. By augmenting large language models (LLMs) with
functional calls and code generation tailored for optimization models, we
enable seamless interaction and minimize the risk of hallucinations in
OptiChat. We develop a new dataset to evaluate OptiChat's performance in
explaining optimization models. Experiments demonstrate that OptiChat
effectively bridges the gap between optimization models and practitioners,
delivering autonomous, accurate, and instant responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Best Practices for Open <span class="highlight-title">Dataset</span>s for LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bommarito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl, Sebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang, Leandro von Werra, Mitchell Baker, Julie Belião, Kasia Chmielinski, Marzieh Fadaee, Lisa Gutermuth, Hynek Kydlíček, Greg Leppert, EM Lewis-Jong, Solana Larsen, Shayne Longpre, Angela Oduor Lungati, Cullen Miller, Victor Miller, Max Ryabinin, Kathleen Siminyu, Andrew Strait, Mark Surman, Anna Tumadóttir, Maurice Weber, Rebecca Weiss, Lee White, Thomas Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many AI companies are training their large language models (LLMs) on data
without the permission of the copyright owners. The permissibility of doing so
varies by jurisdiction: in countries like the EU and Japan, this is allowed
under certain restrictions, while in the United States, the legal landscape is
more ambiguous. Regardless of the legal status, concerns from creative
producers have led to several high-profile copyright lawsuits, and the threat
of litigation is commonly cited as a reason for the recent trend towards
minimizing the information shared about training datasets by both corporate and
public interest actors. This trend in limiting data information causes harm by
hindering transparency, accountability, and innovation in the broader ecosystem
by denying researchers, auditors, and impacted individuals access to the
information needed to understand AI models.
  While this could be mitigated by training language models on open access and
public domain data, at the time of writing, there are no such models (trained
at a meaningful scale) due to the substantial technical and sociological
challenges in assembling the necessary corpus. These challenges include
incomplete and unreliable metadata, the cost and complexity of digitizing
physical records, and the diverse set of legal and technical skills required to
ensure relevance and responsibility in a quickly changing landscape. Building
towards a future where AI systems can be trained on openly licensed data that
is responsibly curated and governed requires collaboration across legal,
technical, and policy domains, along with investments in metadata standards,
digitization, and fostering a culture of openness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention
  for Enabled Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Lee, Singh Suniljit, Yong Siang Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the development of a multimodal sentiment analysis model
that integrates text, audio, and visual data to enhance sentiment
classification. The goal is to improve emotion detection by capturing the
complex interactions between these modalities, thereby enabling more accurate
and nuanced sentiment interpretation. The study evaluates three feature fusion
strategies -- late stage fusion, early stage fusion, and multi-headed attention
-- within a transformer-based architecture. Experiments were conducted using
the CMU-MOSEI dataset, which includes synchronized text, audio, and visual
inputs labeled with sentiment scores. Results show that early stage fusion
significantly outperforms late stage fusion, achieving an accuracy of 71.87\%,
while the multi-headed attention approach offers marginal improvement, reaching
72.39\%. The findings suggest that integrating modalities early in the process
enhances sentiment classification, while attention mechanisms may have limited
impact within the current framework. Future work will focus on refining feature
fusion techniques, incorporating temporal data, and exploring dynamic feature
weighting to further improve model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Speech Multi-View Feature Fusion through Conditional
  Computation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqiao Shan, Yuhao Zhang, Yuchen Han, Bei Li, Xiaofeng Zhao, Yuang Li, Min Zhang, Hao Yang, Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have highlighted the efficacy of self-supervised learning
(SSL) features in various speech-related tasks, providing lightweight and
versatile multi-view speech representations. However, our study reveals that
while SSL features expedite model convergence, they conflict with traditional
spectral features like FBanks in terms of update directions. In response, we
propose a novel generalized feature fusion framework grounded in conditional
computation, featuring a gradient-sensitive gating network and a multi-stage
dropout strategy. This framework mitigates feature conflicts and bolsters model
robustness to multi-view input features. By integrating SSL and spectral
features, our approach accelerates convergence and maintains performance on par
with spectral models across multiple speech translation tasks on the MUSTC
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Narrative Clustering in Large Language Models: A Layerwise
  Analysis of BERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Awritrojit Banerjee, Achim Schilling, Patrick Krauss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the internal mechanisms of BERT, a transformer-based
large language model, with a focus on its ability to cluster narrative content
and authorial style across its layers. Using a dataset of narratives developed
via GPT-4, featuring diverse semantic content and stylistic variations, we
analyze BERT's layerwise activations to uncover patterns of localized neural
processing. Through dimensionality reduction techniques such as Principal
Component Analysis (PCA) and Multidimensional Scaling (MDS), we reveal that
BERT exhibits strong clustering based on narrative content in its later layers,
with progressively compact and distinct clusters. While strong stylistic
clustering might occur when narratives are rephrased into different text types
(e.g., fables, sci-fi, kids' stories), minimal clustering is observed for
authorial style specific to individual writers. These findings highlight BERT's
prioritization of semantic content over stylistic features, offering insights
into its representational capabilities and processing hierarchy. This study
contributes to understanding how transformer models like BERT encode linguistic
information, paving the way for future interdisciplinary research in artificial
intelligence and cognitive neuroscience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2408.03062,
  arXiv:2408.04270, arXiv:2307.01577</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ READ: Reinforcement-based Adversarial Learning for Text Classification
  with Limited Labeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Sharma, Shanu Kumar, Avinash Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained transformer models such as BERT have shown massive gains across
many text classification tasks. However, these models usually need enormous
labeled data to achieve impressive performances. Obtaining labeled data is
often expensive and time-consuming, whereas collecting unlabeled data using
some heuristics is relatively much cheaper for any task. Therefore, this paper
proposes a method that encapsulates reinforcement learning-based text
generation and semi-supervised adversarial learning approaches in a novel way
to improve the model's performance. Our method READ, Reinforcement-based
Adversarial learning, utilizes an unlabeled dataset to generate diverse
synthetic text through reinforcement learning, improving the model's
generalization capability using adversarial learning. Our experimental results
show that READ outperforms the existing state-of-art methods on multiple
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for
  Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Liang, Yuwei Wang, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fine-tuning of Large Language Models (LLMs) is pivotal for achieving
optimal performance across diverse downstream tasks. However, while full
fine-tuning delivers superior results, it entails significant computational and
resource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,
address these challenges by reducing the number of trainable parameters, but
they often struggle with rank adjustment efficiency and task-specific
adaptability. We propose Triangular Adaptive Low-Rank Adaptation
(TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles,
which dynamically optimizes the allocation of trainable parameters.
TriAdaptLoRA introduces three key innovations: 1) a triangular split of
transformation matrices into lower and upper triangular components to maximize
parameter utilization, 2) a parameter importance metric based on normalized
Frobenius norms for efficient adaptation, and 3) an adaptive rank-growth
strategy governed by dynamic thresholds, allowing flexible parameter allocation
across training steps. Experiments conducted on a variety of natural language
understanding and generation tasks demonstrate that TriAdaptLoRA consistently
outperforms existing PEFT methods. It achieves superior performance, enhanced
stability, and reduced computational overhead, particularly under linear
threshold-driven rank growth. These results highlight its efficacy as a
scalable and resource-efficient solution for fine-tuning LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formalising lexical and syntactic diversity for data sampling in French 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Estève, Manon Scholivet, Agata Savary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diversity is an important property of datasets and sampling data for
diversity is useful in dataset creation. Finding the optimally diverse sample
is expensive, we therefore present a heuristic significantly increasing
diversity relative to random sampling. We also explore whether different kinds
of diversity -- lexical and syntactic -- correlate, with the purpose of
sampling for expensive syntactic diversity through inexpensive lexical
diversity. We find that correlations fluctuate with different datasets and
versions of diversity measures. This shows that an arbitrarily chosen measure
may fall short of capturing diversity-related properties of datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Wait, did you mean the doctor?": Collecting a Dialogue Corpus for
  Topical Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amandine Decker, Vincent Tourneur, Maxime Amblard, Ellen Breitholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue is at the core of human behaviour and being able to identify the
topic at hand is crucial to take part in conversation. Yet, there are few
accounts of the topical organisation in casual dialogue and of how people
recognise the current topic in the literature. Moreover, analysing topics in
dialogue requires conversations long enough to contain several topics and types
of topic shifts. Such data is complicated to collect and annotate. In this
paper we present a dialogue collection experiment which aims to build a corpus
suitable for topical analysis. We will carry out the collection with a
messaging tool we developed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gandalf the Red: Adaptive Security for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Pfister, Václav Volhejn, Manuel Knott, Santiago Arias, Julia Bazińska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Damián Pascual-Ortiz, Jakub Podolak, Adrià Romero-López, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, Mateo Rojas-Carulla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluations of defenses against prompt attacks in large language
model (LLM) applications often overlook two critical factors: the dynamic
nature of adversarial behavior and the usability penalties imposed on
legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security
Utility Threat Model), which explicitly separates attackers from legitimate
users, models multi-step interactions, and rigorously expresses the
security-utility in an optimizable form. We further address the shortcomings in
existing evaluations by introducing Gandalf, a crowd-sourced, gamified
red-teaming platform designed to generate realistic, adaptive attack datasets.
Using Gandalf, we collect and release a dataset of 279k prompt attacks.
Complemented by benign user data, our analysis reveals the interplay between
security and utility, showing that defenses integrated in the LLM (e.g., system
prompts) can degrade usability even without blocking requests. We demonstrate
that restricted application domains, defense-in-depth, and adaptive defenses
are effective strategies for building secure and useful LLM applications. Code
is available at
\href{https://github.com/lakeraai/dsec-gandalf}{\texttt{https://github.com/lakeraai/dsec-gandalf}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Niklas Pfister, V\'aclav Volhejn and Manuel Knott contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Aviation Incident Narratives Using Topic Modeling and
  Clustering Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziida Nanyonga, Hassan Wasswa, Ugur Turhan, Keith Joiner, Graham Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aviation safety is a global concern, requiring detailed investigations into
incidents to understand contributing factors comprehensively. This study uses
the National Transportation Safety Board (NTSB) dataset. It applies advanced
natural language processing (NLP) techniques, including Latent Dirichlet
Allocation (LDA), Non-Negative Matrix Factorization (NMF), Latent Semantic
Analysis (LSA), Probabilistic Latent Semantic Analysis (pLSA), and K-means
clustering. The main objectives are identifying latent themes, exploring
semantic relationships, assessing probabilistic connections, and cluster
incidents based on shared characteristics. This research contributes to
aviation safety by providing insights into incident narratives and
demonstrating the versatility of NLP and topic modelling techniques in
extracting valuable information from complex datasets. The results, including
topics identified from various techniques, provide an understanding of
recurring themes. Comparative analysis reveals that LDA performed best with a
coherence value of 0.597, pLSA of 0.583, LSA of 0.542, and NMF of 0.437.
K-means clustering further reveals commonalities and unique insights into
incident narratives. In conclusion, this study uncovers latent patterns and
thematic structures within incident narratives, offering a comparative analysis
of multiple-topic modelling techniques. Future research avenues include
exploring temporal patterns, incorporating additional datasets, and developing
predictive models for early identification of safety issues. This research lays
the groundwork for enhancing the understanding and improvement of aviation
safety by utilising the wealth of information embedded in incident narratives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aviation Safety Enhancement via NLP & Deep Learning: Classifying Flight
  Phases in ATSB Safety Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziida Nanyonga, Hassan Wasswa, Graham Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aviation safety is paramount, demanding precise analysis of safety
occurrences during different flight phases. This study employs Natural Language
Processing (NLP) and Deep Learning models, including LSTM, CNN, Bidirectional
LSTM (BLSTM), and simple Recurrent Neural Networks (sRNN), to classify flight
phases in safety reports from the Australian Transport Safety Bureau (ATSB).
The models exhibited high accuracy, precision, recall, and F1 scores, with LSTM
achieving the highest performance of 87%, 88%, 87%, and 88%, respectively. This
performance highlights their effectiveness in automating safety occurrence
analysis. The integration of NLP and Deep Learning technologies promises
transformative enhancements in aviation safety analysis, enabling targeted
safety measures and streamlined report handling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NLP, Aviation Safety, ATSB, Deep learning, Flight phase. arXiv admin
  note: substantial text overlap with arXiv:2501.01694</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GRAPH</span>MOE: Amplifying Cognitive Depth of Mixture-of-Experts Network via
  Introducing Self-Rethinking Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Bo Lv, Zifan Zheng, Bohao Yang, Kun Zhao, Ning Liao, Xiaoxing Wang, Feiyu Xiong, Zhiyu Li, Nayu Liu, Jingchi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Mixture-of-Experts (MoE) networks benefit from utilizing multiple
smaller expert models as opposed to a single large network. However, these
experts typically operate independently, leaving a question open about whether
interconnecting these models could enhance the performance of MoE networks. In
response, we introduce GRAPHMOE, a novel method aimed at augmenting the
cognitive depth of language models via a self-rethinking mechanism constructed
on Pseudo GraphMoE networks. GRAPHMOE employs a recurrent routing strategy to
simulate iterative thinking steps, thereby facilitating the flow of information
among expert nodes. We implement the GRAPHMOE architecture using Low-Rank
Adaptation techniques (LoRA) and conduct extensive experiments on various
benchmark datasets. The experimental results reveal that GRAPHMOE outperforms
other LoRA based models, achieving state-of-the-art (SOTA) performance.
Additionally, this study explores a novel recurrent routing strategy that may
inspire further advancements in enhancing the reasoning capabilities of
language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Label Refinement Matters More than Preference Optimization
  under Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowen Ye, Cassidy Laidlaw, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) post-training relies on two stages of human supervision:
task demonstrations for supervised finetuning (SFT), followed by preference
comparisons for reinforcement learning from human feedback (RLHF). As LMs
become more capable, the tasks they are given become harder to supervise. Will
post-training remain effective under unreliable supervision? To test this, we
simulate unreliable demonstrations and comparison feedback using small LMs and
time-constrained humans. We find that in the presence of unreliable
supervision, SFT still retains some effectiveness, but DPO (a common RLHF
algorithm) fails to improve the model beyond SFT. To address this, we propose
iterative label refinement (ILR) as an alternative to RLHF. ILR improves the
SFT data by using comparison feedback to decide whether human demonstrations
should be replaced by model-generated alternatives, then retrains the model via
SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with
unreliable supervision (math, coding, and safe instruction-following). Our
findings suggest that as LMs are used for complex tasks where human supervision
is unreliable, RLHF may no longer be the best use of human comparison feedback;
instead, it is better to direct feedback towards improving the training data
rather than continually training the model. Our code and data are available at
https://github.com/helloelwin/iterative-label-refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning with Embedding Layer Surgery and Task-wise Beam
  Search using Whisper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chin Yuen Kwok, Jia Qi Yip, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Multilingual ASR models only support a fraction of the world's
languages. Continual Learning (CL) aims to tackle this problem by adding new
languages to pre-trained models while avoiding the loss of performance on
existing languages, also known as Catastrophic Forgetting (CF). However,
existing CL methods overlook the adaptation of the token embedding lookup table
at the decoder, despite its significant contribution to CF. We propose
Embedding Layer Surgery where separate copies of the token embeddings are
created for each new languages, and one of the copies is selected to replace
the old languages embeddings when transcribing the corresponding new language.
Unfortunately, this approach means LID errors also cause incorrect ASR
embedding selection. Our Task-wise Beam Search allows self-correction for such
mistakes. By adapting Whisper to 10 hours of data for each of 10 unseen
languages from Common Voice, results show that our method reduces the Average
WER (AWER) of pre-trained languages from 14.2% to 11.9% compared with
Experience Replay, without compromising the AWER of the unseen languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 2024 IEEE Spoken Language Technology Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReARTeR: <span class="highlight-title">Retrie</span>val-Augmented Reasoning with Trustworthy Process
  Rewarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, Han Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)
hold promise in knowledge-intensive tasks but face limitations in complex
multi-step reasoning. While recent methods have integrated RAG with
chain-of-thought reasoning or test-time search using Process Reward Models
(PRMs), these approaches encounter challenges such as a lack of explanations,
bias in PRM training data, early-step bias in PRM scores, and insufficient
post-training optimization of reasoning potential. To address these issues, we
propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding
(ReARTeR), a framework that enhances RAG systems' reasoning capabilities
through post-training and test-time scaling. At test time, ReARTeR introduces
Trustworthy Process Rewarding via a Process Reward Model for accurate scalar
scoring and a Process Explanation Model (PEM) for generating natural language
explanations, enabling step refinement. During post-training, it utilizes Monte
Carlo Tree Search guided by Trustworthy Process Rewarding to collect
high-quality step-level preference data, optimized through Iterative Preference
Optimization. ReARTeR addresses three core challenges: (1) misalignment between
PRM and PEM, tackled through off-policy preference learning; (2) bias in PRM
training data, mitigated by balanced annotation methods and stronger
annotations for challenging examples; and (3) early-step bias in PRM, resolved
through a temporal-difference-based look-ahead search strategy. Experimental
results on multi-step reasoning benchmarks demonstrate significant
improvements, underscoring ReARTeR's potential to advance the reasoning
capabilities of RAG systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Language Models for Grammatical Acceptability: A Comparative
  Study of Fine-Tuning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shobhit Ratan, Farley Knight, Ghada Jerfel, Sze Chung Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the fine-tuning (FT) of the Open Pre-trained Transformer
(OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By
comparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and
Parameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation
(LoRA), we demonstrate significant improvements in computational efficiency
while maintaining high accuracy. Our experiments reveal that while VFT achieves
the highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and
iteration time by more than 50%, and increases accuracy in PBFT case. Context
Distillation (CD), though computationally efficient, underperformed with
accuracy around 31%. Our findings contribute to democratizing access to large
language models (LLM) by reducing computational barriers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning with <span class="highlight-title">Graph</span>s: Structuring Implicit Knowledge to Enhance LLMs
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Han, Yaochen Xie, Hui Liu, Xianfeng Tang, Sreyashi Nag, William Headden, Hui Liu, Yang Li, Chen Luo, Shuiwang Ji, Qi He, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable success across a
wide range of tasks; however, they still encounter challenges in reasoning
tasks that require understanding and inferring relationships between distinct
pieces of information within text sequences. This challenge is particularly
pronounced in tasks involving multi-step processes, such as logical reasoning
and multi-hop question answering, where understanding implicit relationships
between entities and leveraging multi-hop connections in the given context are
crucial. Graphs, as fundamental data structures, explicitly represent pairwise
relationships between entities, thereby offering the potential to enhance LLMs'
reasoning capabilities. External graphs have proven effective in supporting
LLMs across multiple tasks. However, in many reasoning tasks, no pre-existing
graph structure is provided. Can we structure implicit knowledge derived from
context into graphs to assist LLMs in reasoning? In this paper, we propose
Reasoning with Graphs (RwG) by first constructing explicit graphs from the
context and then leveraging these graphs to enhance LLM reasoning performance
on reasoning tasks. Extensive experiments demonstrate the effectiveness of the
proposed method in improving both logical reasoning and multi-hop question
answering tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Verification and Refinement of Language Model Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonho Ko, Jinheon Baek, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance across a wide
range of natural language tasks. However, a critical challenge remains in that
they sometimes generate factually incorrect answers. To address this, while
many previous work has focused on identifying errors in their generation and
further refining them, they are slow in deployment since they are designed to
verify the response from LLMs only after their entire generation (from the
first to last tokens) is done. Further, we observe that once LLMs generate
incorrect tokens early on, there is a higher likelihood that subsequent tokens
will also be factually incorrect. To this end, in this work, we propose
Streaming-VR (Streaming Verification and Refinement), a novel approach designed
to enhance the efficiency of verification and refinement of LLM outputs.
Specifically, the proposed Streaming-VR enables on-the-fly verification and
correction of tokens as they are being generated, similar to a streaming
process, ensuring that each subset of tokens is checked and refined in
real-time by another LLM as the LLM constructs its response. Through
comprehensive evaluations on multiple datasets, we demonstrate that our
approach not only enhances the factual accuracy of LLMs, but also offers a more
efficient solution compared to prior refinement methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Among parameter-efficient fine-tuning methods, freezing has emerged as a
popular strategy for speeding up training, reducing catastrophic forgetting,
and improving downstream performance. We investigate the impact of freezing the
decoder in a multi-task setup comprising diverse natural language tasks, aiming
to reduce deployment overhead and enhance portability to novel tasks. Our
experiments, conducted by fine-tuning both individual and multi-task setups on
the AlexaTM model, reveal that freezing decoders is highly effective for tasks
with natural language outputs and mitigates catastrophic forgetting in
multilingual tasks. However, we find that pairing frozen decoders with a larger
model can effectively maintain or even enhance performance in structured and QA
tasks, making it a viable strategy for a broader range of task types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-Centric Projection of <span class="highlight-title">Prompt</span>ing Techniques and Implications for
  Synthetic Training Data for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Dhamani, Mary Lou Maher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in prompting techniques and multi-agent systems for Large
Language Models (LLMs) have produced increasingly complex approaches. However,
we lack a framework for characterizing and comparing prompting techniques or
understanding their relationship to multi-agent LLM systems. This position
paper introduces and explains the concepts of linear contexts (a single,
continuous sequence of interactions) and non-linear contexts (branching or
multi-path) in LLM systems. These concepts enable the development of an
agent-centric projection of prompting techniques, a framework that can reveal
deep connections between prompting strategies and multi-agent systems. We
propose three conjectures based on this framework: (1) results from non-linear
prompting techniques can predict outcomes in equivalent multi-agent systems,
(2) multi-agent system architectures can be replicated through single-LLM
prompting techniques that simulate equivalent interaction patterns, and (3)
these equivalences suggest novel approaches for generating synthetic training
data. We argue that this perspective enables systematic cross-pollination of
research findings between prompting and multi-agent domains, while providing
new directions for improving both the design and training of future LLM
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures. Accepted at ICAART 2025. Derived from an early
  draft at 2312.17601. arXiv admin note: substantial text overlap with
  arXiv:2312.17601</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Talk to Right Specialists: Routing and Planning in Multi-agent System
  for Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feijie Wu, Zitao Li, Fei Wei, Yaliang Li, Bolin Ding, Jing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging large language models (LLMs), an agent can utilize
retrieval-augmented generation (RAG) techniques to integrate external knowledge
and increase the reliability of its responses. Current RAG-based agents
integrate single, domain-specific knowledge sources, limiting their ability and
leading to hallucinated or inaccurate responses when addressing cross-domain
queries. Integrating multiple knowledge bases into a unified RAG-based agent
raises significant challenges, including increased retrieval overhead and data
sovereignty when sensitive data is involved. In this work, we propose RopMura,
a novel multi-agent system that addresses these limitations by incorporating
highly efficient routing and planning mechanisms. RopMura features two key
components: a router that intelligently selects the most relevant agents based
on knowledge boundaries and a planner that decomposes complex multi-hop queries
into manageable steps, allowing for coordinating cross-domain responses.
Experimental results demonstrate that RopMura effectively handles both
single-hop and multi-hop queries, with the routing mechanism enabling precise
answers for single-hop queries and the combined routing and planning mechanisms
achieving accurate, multi-step resolutions for complex queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work In Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Inverted Image Pyramid Networks for Visual Perception and
  Multimodal Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image pyramids are widely adopted in top-performing methods to obtain
multi-scale features for precise visual perception and understanding. However,
current image pyramids use the same large-scale model to process multiple
resolutions of images, leading to significant computational cost. To address
this challenge, we propose a novel network architecture, called
Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses
pretrained models (ViTs or CNNs) as branches to process multi-scale images,
where images of higher resolutions are processed by smaller network branches to
balance computational cost and performance. To integrate information from
different spatial scales, we further propose a novel cross-branch feature
interaction mechanism. To validate PIIP, we apply it to various perception
models and a representative multimodal large language model called LLaVA, and
conduct extensive experiments on various tasks such as object detection,
segmentation, image classification and multimodal understanding. PIIP achieves
superior performance compared to single-branch and existing multi-resolution
approaches with lower computational cost. When applied to InternViT-6B, a
large-scale vision foundation model, PIIP can improve its performance by 1%-2%
on detection and segmentation with only 40%-60% of the original computation,
finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For
multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and
74.5% on MMBench with only 2.8M training data. Our code is released at
https://github.com/OpenGVLab/PIIP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Knowledge <span class="highlight-title">Graph</span> Embedding Techniques, Methods,
  and Challenges: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Liu, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have attracted a lot of attention in various
fields due to their superior performance, aiming to train hundreds of millions
or more parameters on large amounts of text data to understand and generate
natural language. As the superior performance of LLMs becomes apparent, they
are increasingly being applied to knowledge graph embedding (KGE) related tasks
to improve the processing results. As a deep learning model in the field of
Natural Language Processing (NLP), it learns a large amount of textual data to
predict the next word or generate content related to a given text. However,
LLMs have recently been invoked to varying degrees in different types of KGE
related scenarios such as multi-modal KGE and open KGE according to their task
characteristics. In this paper, we investigate a wide range of approaches for
performing LLMs-related tasks in different types of KGE scenarios. To better
compare the various approaches, we summarize each KGE scenario in a
classification. In addition to the categorization methods, we provide a tabular
overview of the methods and their source code links for a more direct
comparison. In the article we also discuss the applications in which the
methods are mainly used and suggest several forward-looking directions for the
development of this new research area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multigenre AI-powered Story Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edirlei Soares de Lima, Margot M. E. Neggers, Antonio L. Furtado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper shows how to construct genre patterns, whose purpose is to guide
interactive story composition in a way that enforces thematic consistency. To
start the discussion we argue, based on previous seminal works, for the
existence of five fundamental genres, namely comedy, romance - in the sense of
epic plots, flourishing since the twelfth century -, tragedy, satire, and
mystery. To construct the patterns, a simple two-phase process is employed:
first retrieving examples that match our genre characterizations, and then
applying a form of most specific generalization to the groups of examples in
order to find their commonalities. In both phases, AI agents are instrumental,
with our PatternTeller prototype being called to operate the story composition
process, offering the opportunity to generate stories from a given premise of
the user, to be developed under the guidance of the chosen pattern and trying
to accommodate the user's suggestions along the composition stages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added publication details to references that were published after the
  submission of the previous version (references [18] and [19])</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic <span class="highlight-title">Prompt</span>
  Optimization for Text Generation <span class="chip">AAAI-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02748v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02748v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing automatic prompt engineering methods are typically designed for
discriminative tasks, where new task prompts are iteratively refined with
limited feedback from a single metric reflecting a single aspect. However,
these approaches are suboptimal for generative tasks, which require more
nuanced guidance beyond a single numeric metric to improve the prompt and
optimize multiple aspects of the generated text. To address these challenges,
we propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt
Optimization (CriSPO) approach. CriSPO introduces a critique-suggestion module
as its core component. This module spontaneously discovers aspects, and
compares generated and reference texts across these aspects, providing specific
suggestions for prompt modification. These clear critiques and actionable
suggestions guide a receptive optimizer module to make more substantial
changes, exploring a broader and more effective search space. To further
improve CriSPO with multi-metric optimization, we introduce an Automatic Suffix
Tuning (AST) extension to enhance the performance of task prompts across
multiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4
summarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score
improvement on summarization and substantial improvement of various metrics on
QA. Code available at https://github.com/amazon-science/crispo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bilingual Evaluation of Language Models on General Knowledge in
  University Entrance Exams with Minimal Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Sánchez Salido, Roser Morante, Julio Gonzalo, Guillermo Marco, Jorge Carrillo-de-Albornoz, Laura Plaza, Enrique Amigó, Andrés Fernández, Alejandro Benito-Santos, Adrián Ghajari Espinosa, Victor Fresno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article we present UNED-ACCESS 2024, a bilingual dataset that
consists of 1003 multiple-choice questions of university entrance level exams
in Spanish and English. Questions are originally formulated in Spanish and
translated manually into English, and have not ever been publicly released. A
selection of current open-source and proprietary models are evaluated in a
uniform zero-shot experimental setting both on the UNED-ACCESS 2024 dataset and
on an equivalent subset of MMLU questions. Results show that (i) reasoning
questions are challenging for models, (ii) smaller models perform worse than
larger models and degrade faster in Spanish than in English and (iii) the
performance gap between languages is negligible for the best models and grows
up to 37% for smaller models. Model ranking on UNED-ACCESS 2024 is almost
identical in English and Spanish, and has also a high correlation (0.98
Pearson) with ranking on MMLU, suggesting that a small dataset is sufficiently
diverse and representative to measure performance by discipline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to thrive in hostile and ever-changing natural environments,
mammalian brains evolved to store large amounts of knowledge about the world
and continually integrate new information while avoiding catastrophic
forgetting. Despite the impressive accomplishments, large language models
(LLMs), even with retrieval-augmented generation (RAG), still struggle to
efficiently and effectively integrate a large amount of new experiences after
pre-training. In this work, we introduce HippoRAG, a novel retrieval framework
inspired by the hippocampal indexing theory of human long-term memory to enable
deeper and more efficient knowledge integration over new experiences. HippoRAG
synergistically orchestrates LLMs, knowledge graphs, and the Personalized
PageRank algorithm to mimic the different roles of neocortex and hippocampus in
human memory. We compare HippoRAG with existing RAG methods on multi-hop
question answering and show that our method outperforms the state-of-the-art
methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves
comparable or better performance than iterative retrieval like IRCoT while
being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into
IRCoT brings further substantial gains. Finally, we show that our method can
tackle new types of scenarios that are out of reach of existing methods. Code
and data are available at https://github.com/OSU-NLP-Group/HippoRAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code and data:
  https://github.com/OSU-NLP-Group/HippoRAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-guided Image Restoration and Semantic Enhancement for Text-to-Image
  Person <span class="highlight-title">Retrie</span>val 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09059v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09059v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Yuan Dong, Nikolaos V. Boulgouris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific
person images according to the given textual descriptions. A primary challenge
in this task is bridging the substantial representational gap between visual
and textual modalities. The prevailing methods map texts and images into
unified embedding space for matching, while the intricate semantic
correspondences between texts and images are still not effectively constructed.
To address this issue, we propose a novel TIPR framework to build fine-grained
interactions and alignment between person images and the corresponding texts.
Specifically, via fine-tuning the Contrastive Language-Image Pre-training
(CLIP) model, a visual-textual dual encoder is firstly constructed, to
preliminarily align the image and text features. Secondly, a Text-guided Image
Restoration (TIR) auxiliary task is proposed to map abstract textual entities
to specific image regions, improving the alignment between local textual and
visual embeddings. Additionally, a cross-modal triplet loss is presented to
handle hard samples, and further enhance the model's discriminability for minor
differences. Moreover, a pruning-based text data augmentation approach is
proposed to enhance focus on essential elements in descriptions, thereby
avoiding excessive model attention to less significant information. The
experimental results show our proposed method outperforms state-of-the-art
methods on three popular benchmark datasets, and the code will be made publicly
available at https://github.com/Delong-liu-bupt/SEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was withdrawn due to a dispute among the authors regarding
  the content of the article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logic Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aldo Gangemi, Andrea Giovanni Nuzzolese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic Knowledge Graphs (SKG) face challenges with scalability,
flexibility, contextual understanding, and handling unstructured or ambiguous
information. However, they offer formal and structured knowledge enabling
highly interpretable and reliable results by means of reasoning and querying.
Large Language Models (LLMs) overcome those limitations making them suitable in
open-ended tasks and unstructured environments. Nevertheless, LLMs are neither
interpretable nor reliable. To solve the dichotomy between LLMs and SKGs we
envision Logic Augmented Generation (LAG) that combines the benefits of the two
worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate
potentially infinite relations and tacit knowledge on-demand. SKGs are key for
injecting a discrete heuristic dimension with clear logical and factual
boundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,
medical diagnostics and climate projections. Understanding the properties and
limitations of LAG, which are still mostly unknown, is of utmost importance for
enabling a variety of tasks involving tacit knowledge in order to provide
interpretable and effective results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized LLM Response Generation with Parameterized Memory Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Zhang, Yejin Kim, Xiaozhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited remarkable proficiency in
comprehending and generating natural language. On the other hand, personalized
LLM response generation holds the potential to offer substantial benefits for
individuals in critical areas such as medical. Existing research has explored
memory-augmented methods to prompt the LLM with pre-stored user-specific
knowledge for personalized response generation in terms of new queries. We
contend that such paradigm is unable to perceive fine-granularity information.
In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approach
using parameter-efficient fine-tuning (PEFT) and along with a Bayesian
Optimisation searching strategy to achieve \textbf{L}LM
\textbf{P}ersonalization(\textbf{MiLP}).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01028v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01028v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As retrieval-augmented generation prevails in large language models,
embedding models are becoming increasingly crucial. Despite the growing number
of general embedding models, prior work often overlooks the critical role of
training data quality. In this work, we introduce KaLM-Embedding, a general
multilingual embedding model that leverages a large quantity of cleaner, more
diverse, and domain-specific training data. Our model has been trained with key
techniques proven to enhance performance: (1) persona-based synthetic data to
create diversified examples distilled from LLMs, (2) ranking consistency
filtering to remove less informative samples, and (3) semi-homogeneous task
batch sampling to improve training efficacy. Departing from traditional
BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,
facilitating the adaptation of auto-regressive language models for general
embedding tasks. Extensive evaluations of the MTEB benchmark across multiple
languages show that our model outperforms others of comparable size, setting a
new standard for multilingual embedding models with <1B parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. 23 pages, 6 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WebWalker: Benchmarking LLMs in Web Traversal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) demonstrates remarkable performance
across tasks in open-domain question-answering. However, traditional search
engines may retrieve shallow content, limiting the ability of LLMs to handle
complex, multi-layered information. To address it, we introduce WebWalkerQA, a
benchmark designed to assess the ability of LLMs to perform web traversal. It
evaluates the capacity of LLMs to traverse a website's subpages to extract
high-quality data systematically. We propose WebWalker, which is a multi-agent
framework that mimics human-like web navigation through an explore-critic
paradigm. Extensive experimental results show that WebWalkerQA is challenging
and demonstrates the effectiveness of RAG combined with WebWalker, through the
horizontal and vertical integration in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inductive Learning of Logical Theories with LLMs: An Expressivity-Graded
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Pedro Gandarela, Danilo S. Carvalho, André Freitas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a novel systematic methodology to analyse the capabilities
and limitations of Large Language Models (LLMs) with feedback from a formal
inference engine, on logic theory induction. The analysis is complexity-graded
w.r.t. rule dependency structure, allowing quantification of specific inference
challenges on LLM performance. Integrating LLMs with formal methods is a
promising frontier in the Natural Language Processing field, as an important
avenue for improving model inference control and explainability. In particular,
inductive learning over complex sets of facts and rules, poses unique
challenges for current autoregressive models, as they lack explicit symbolic
grounding. While they can be complemented by formal systems, the properties
delivered by LLMs regarding inductive learning, are not well understood and
quantified. Empirical results indicate that the largest LLMs can achieve
competitive results against a SOTA Inductive Logic Programming (ILP) system
baseline, but also that tracking long predicate relationship chains is a more
difficult obstacle than theory complexity for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are LLMs Good Literature <span class="highlight-title">Review</span> Writers? Evaluating the Literature
  <span class="highlight-title">Review</span> Writing Ability of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemei Tang, Xufeng Duan, Zhenguang G. Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The literature review is a crucial form of academic writing that involves
complex processes of literature collection, organization, and summarization.
The emergence of large language models (LLMs) has introduced promising tools to
automate these processes. However, their actual capabilities in writing
comprehensive literature reviews remain underexplored, such as whether they can
generate accurate and reliable references. To address this gap, we propose a
framework to assess the literature review writing ability of LLMs
automatically. We evaluate the performance of LLMs across three tasks:
generating references, writing abstracts, and writing literature reviews. We
employ external tools for a multidimensional evaluation, which includes
assessing hallucination rates in references, semantic coverage, and factual
consistency with human-written context. By analyzing the experimental results,
we find that, despite advancements, even the most sophisticated models still
cannot avoid generating hallucinated references. Additionally, different models
exhibit varying performance in literature review writing across different
disciplines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Persian LLMs for Instruction Following: A Novel <span class="highlight-title">Dataset</span> and
  Training Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11186v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11186v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojjat Mokhtarabadi, Ziba Zamani, Abbas Maazallahi, Mohammad Hossein Manshaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned large language models have demonstrated remarkable
capabilities in following human instructions across various domains. However,
their proficiency remains notably deficient in many low-resource languages. To
address this challenge, we begin by introducing FarsInstruct a comprehensive
instruction dataset designed to enhance the instruction following ability of
large language models specifically for the Persian language a significant yet
underrepresented language globally. FarsInstruct encompasses a wide range of
task types and datasets, each containing a mix of straightforward to complex
manual written instructions, as well as translations from the Public Pool of
Prompts, ensuring a rich linguistic and cultural representation. Furthermore,
we introduce Co-CoLA, a framework designed to enhance the multi-task
adaptability of LoRA-tuned models. Through extensive experimental analyses, our
study showcases the effectiveness of the FarsInstruct dataset coupled with
training by the Co-CoLA framework, in improving the performance of large
language models within the Persian context. As of the current writing,
FarsInstruct comprises 197 templates across 21 distinct datasets, and we intend
to update it consistently, thus augmenting its applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills
  in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Chernyshev, Vitaliy Polshkov, Ekaterina Artemova, Alex Myasnikov, Vlad Stepanov, Alexei Miasnikov, Sergei Tilga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current evaluation of mathematical skills in LLMs is limited, as existing
benchmarks are either relatively small, primarily focus on elementary and
high-school problems, or lack diversity in topics. Additionally, the inclusion
of visual elements in tasks remains largely under-explored.
  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100
unpublished open-ended university-level problems sourced from teaching
materials. It is balanced across six core subjects, with 20% of multimodal
problems. Given the open-ended nature of U-MATH problems, we employ an LLM to
judge the correctness of generated solutions. To this end, we release
$\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.
  The evaluation of general domain, math-specific, and multimodal LLMs
highlights the challenges presented by U-MATH. Our findings reveal that LLMs
achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%
on visual problems. The solution assessment proves challenging for LLMs, with
the best LLM judge having an F1-score of 80% on $\mu$-MATH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-way Parallel Named Entity Annotated Corpus for English, Tamil
  and Sinhala 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surangika Ranathunga, Asanka Ranasinghea, Janaka Shamala, Ayodya Dandeniyaa, Rashmi Galappaththia, Malithi Samaraweeraa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a multi-way parallel English-Tamil-Sinhala corpus
annotated with Named Entities (NEs), where Sinhala and Tamil are low-resource
languages. Using pre-trained multilingual Language Models (mLMs), we establish
new benchmark Named Entity Recognition (NER) results on this dataset for
Sinhala and Tamil. We also carry out a detailed investigation on the NER
capabilities of different types of mLMs. Finally, we demonstrate the utility of
our NER system on a low-resource Neural Machine Translation (NMT) task. Our
dataset is publicly released: https://github.com/suralk/multiNER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Addressing Hallucinations in Language Models with Knowledge <span class="highlight-title">Graph</span>
  Embeddings as an Additional Modality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktoriia Chekalina, Anton Razzhigaev, Elizaveta Goncharova, Andrey Kuznetsov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present an approach to reduce hallucinations in Large
Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional
modality. Our method involves transforming input text into a set of KG
embeddings and using an adapter to integrate these embeddings into the language
model space, without relying on external retrieval processes.
  To facilitate this, we created WikiEntities, a dataset containing over 3
million Wikipedia texts annotated with entities from Wikidata and their
corresponding embeddings from PyTorch-BigGraph. This dataset serves as a
valuable resource for training Entity Linking models and adapting the described
method to various LLMs using specialized adapters.
  Our method does not require fine-tuning of the language models themselves;
instead, we only train the adapter. This ensures that the model's performance
on other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA
2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and
demonstrated that our approach improves performance on the HaluEval, True-False
benchmarks and FEVER dataset. The results indicate that incorporating KGs as a
new modality can effectively reduce hallucinations and improve the factual
accuracy of language models, all without the need for external retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JsonTuning: Towards Generalizable, Robust, and Controllable Instruction
  Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02953v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02953v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Gao, Wenxuan Zhang, Guizhen Chen, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning is vital for enhancing the performance of large language
models (LLMs), but existing text-to-text methods, referred to as TextTuning,
struggle with issues such as generalization, robustness, and controllability
due to their lack of explicit task structures. We introduce JsonTuning, a
structure-to-structure approach that uses JSON structures to represent tasks.
This method improves generalization by clarifying task elements and their
relations, boosts robustness by minimizing ambiguity, and enhances
controllability by allowing precise control over outputs. We conduct an
extensive comparative analysis between JsonTuning and TextTuning using various
language models and benchmarks. Our findings reveal that JsonTuning
consistently surpasses TextTuning in terms of performance, robustness, and
controllability across different scenarios. By overcoming the limitations of
TextTuning, JsonTuning demonstrates significant potential for developing more
effective and reliable LLMs capable of handling diverse scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TreeKV: Smooth Key-Value Cache Compression with Tree Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient key-value (KV) cache compression is critical for scaling
transformer-based Large Language Models (LLMs) in long sequences and
resource-limited settings. Existing methods evict tokens based on their
positions or importance scores, but position-based strategies can miss crucial
information outside predefined regions, while those relying on global
importance scores resulting in strong regional biases, limiting the KV cache's
overall context retention and potentially impairing the performance of LLMs on
complex tasks. Our wavelet analysis reveals that as tokens approach the end of
sequence, their contributions to generation gradually increase and tends to
diverge more from neighboring tokens, indicating a smooth transition with
increasing complexity and variability from distant to nearby context. Motivated
by this observation, we propose TreeKV, an intuitive, training-free method that
employs a tree structure for smooth cache compression. TreeKV maintains a fixed
cache size, allowing LLMs to deliver high-quality output even in long text
scenarios. Unlike most compression methods, TreeKV is applicable to both the
generation and prefilling stages. TreeKV consistently surpasses all baseline
models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs
trained with short context window to generalize to longer window with a 16x
cache reduction. On the Longbench benchmark, TreeKV achieves the best
performance with only 6\% of the budget at optimal efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaptVC: High Quality Voice Conversion with Adaptive Learning <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01347v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01347v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehun Kim, Ji-Hoon Kim, Yeunju Choi, Tan Dat Nguyen, Seongkyu Mun, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of voice conversion is to transform the speech of a source speaker
to sound like that of a reference speaker while preserving the original
content. A key challenge is to extract disentangled linguistic content from the
source and voice style from the reference. While existing approaches leverage
various methods to isolate the two, a generalization still requires further
attention, especially for robustness in zero-shot scenarios. In this paper, we
achieve successful disentanglement of content and speaker features by tuning
self-supervised speech features with adapters. The adapters are trained to
dynamically encode nuanced features from rich self-supervised features, and the
decoder fuses them to produce speech that accurately resembles the reference
with minimal loss of content. Moreover, we leverage a conditional flow matching
decoder with cross-attention speaker conditioning to further boost the
synthesis quality and efficiency. Subjective and objective evaluations in a
zero-shot scenario demonstrate that the proposed method outperforms existing
models in speech quality and similarity to the reference speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2025; demo available https://mm.kaist.ac.kr/projects/AdaptVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformers and Large Language Models for Efficient Intrusion Detection
  Systems: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Kheddar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With significant advancements in Transformers LLMs, NLP has extended its
reach into many research fields due to its enhanced capabilities in text
generation and user interaction. One field benefiting greatly from these
advancements is cybersecurity. In cybersecurity, many parameters that need to
be protected and exchanged between senders and receivers are in the form of
text and tabular data, making NLP a valuable tool in enhancing the security
measures of communication protocols. This survey paper provides a comprehensive
analysis of the utilization of Transformers and LLMs in cyber-threat detection
systems. The methodology of paper selection and bibliometric analysis is
outlined to establish a rigorous framework for evaluating existing research.
The fundamentals of Transformers are discussed, including background
information on various cyber-attacks and datasets commonly used in this field.
The survey explores the application of Transformers in IDSs, focusing on
different architectures such as Attention-based models, LLMs like BERT and GPT,
CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.
Furthermore, it explores the diverse environments and applications where
Transformers and LLMs-based IDS have been implemented, including computer
networks, IoT devices, critical infrastructure protection, cloud computing,
SDN, as well as in autonomous vehicles. The paper also addresses research
challenges and future directions in this area, identifying key issues such as
interpretability, scalability, and adaptability to evolving threats, and more.
Finally, the conclusion summarizes the findings and highlights the significance
of Transformers and LLMs in enhancing cyber-threat detection capabilities,
while also outlining potential avenues for further research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.04760 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of
  Large Language Models in Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangru Lin, Shaoguang Mao, Emanuele La Malfa, Valentin Hofmann, Adrian de Wynter, Xun Wang, Si-Qing Chen, Michael Wooldridge, Janet B. Pierrehumbert, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is not monolithic. While benchmarks, including those designed for
multiple languages, are often used as proxies to evaluate the performance of
Large Language Models (LLMs), they tend to overlook the nuances of
within-language variation, and thus fail to model the experience of speakers of
non-standard dialects. Focusing on African American Vernacular English (AAVE),
we present the first study aimed at objectively assessing the fairness and
robustness of LLMs in handling dialects in canonical reasoning tasks, including
algorithm, math, logic, and integrated reasoning. We introduce \textbf{ReDial}
(\textbf{Re}asoning with \textbf{Dial}ect Queries), a benchmark containing
1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE
speakers, including experts with computer science backgrounds, to rewrite seven
popular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate
widely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model
families. Our findings reveal that \textbf{almost all of these widely used
models show significant brittleness and unfairness to queries in AAVE}. Our
work establishes a systematic and objective framework for analyzing LLM bias in
dialectal queries. Moreover, it highlights how mainstream LLMs provide unfair
service to dialect speakers in reasoning tasks, laying a critical foundation
for relevant future research. Code and data can be accessed at
https://github.com/fangru-lin/redial_dialect_robustness_fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoPE: Mixture of <span class="highlight-title">Prompt</span> Experts for Parameter-Efficient and Scalable
  Multimodal Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10568v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10568v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Jiang, Lingbo Liu, Changwen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the demonstrated parameter efficiency of prompt-based multimodal
fusion methods, their limited adaptivity and expressiveness often result in
suboptimal performance compared to other tuning approaches. In this paper, we
introduce the Mixture of Prompt Experts (MoPE), the first technique designed to
overcome these limitations by decomposing standard prompts to capture
instance-level features adaptively. Building on this decomposition, MoPE
enhances prompt fusion's expressiveness by leveraging multimodal pairing priors
to route the most effective prompt for each instance dynamically. Compared to
vanilla prompting, our MoPE-based fusion method exhibits greater
expressiveness, scaling more effectively with the training data and the overall
number of trainable parameters. We also investigate regularization terms for
expert routing, which lead to emergent expert specialization with enhanced
adaptiveness and interpretablity. Extensive experiments across six multimodal
datasets spanning four modalities demonstrate state-of-the-art performance for
prompt fusion, matching or even surpassing the performance of fine-tuning while
requiring only 0.8% of the trainable parameters. Project homepage:
https://github.com/songrise/MoPE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review, Extended version of arxiv:2312.03734</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Qingping Yang, Yingwei Ma, Runtao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of mathematical reasoning capabilities is essential for
advancing Artificial General Intelligence (AGI). While Large Language Models
(LLMs) have shown impressive performance in solving mathematical problems,
existing benchmarks such as GSM8K and MATH present limitations, including
narrow problem definitions with specific numbers and reliance on predetermined
rules that hinder accurate assessments of reasoning and generality. This paper
introduces the UTMath Benchmark, a robust evaluation framework designed to
assess LLMs through extensive unit tests, with a focus on both the accuracy and
generality of model responses. It comprises 1,053 cutting-edge problems
spanning nine mathematical domains, with an average of 68 test cases per
problem. UTMath is highly challenging, with the best-performing model, o1-mini,
solving only 32.57\% of the problems, followed by o1-preview at 27.16\%, and
GPT-4o at 26.93\%. Furthermore, we present the Reasoning-to-Coding of Thoughts
(RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to
code generation, thereby facilitating the production of more sophisticated
solutions and enhancing overall performance and efficiency. Additionally, we
also release the UTMath-Train training dataset (more than 70k samples), to
support the community in further exploring mathematical reasoning. Our
benchmark can be accessed via the following link:
https://github.com/UTMathGroup/UTMath
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIOMEDICA: An Open Biomedical Image-Caption Archive, <span class="highlight-title">Dataset</span>, and
  Vision-Language Models Derived from Scientific Literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey J Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Austin Wolfgang Katzer, Collin Chiu, Anita Rau, Xiaohan Wang, Yuhui Zhang, Alfred Seunghoon Song, Robert Tibshirani, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of vision-language models (VLMs) is driven by large-scale and
diverse multimodal datasets. However, progress toward generalist biomedical
VLMs is limited by the lack of annotated, publicly accessible datasets across
biology and medicine. Existing efforts are restricted to narrow domains,
missing the full diversity of biomedical knowledge encoded in scientific
literature. To address this gap, we introduce BIOMEDICA, a scalable,
open-source framework to extract, annotate, and serialize the entirety of the
PubMed Central Open Access subset into an easy-to-use, publicly accessible
dataset. Our framework produces a comprehensive archive with over 24 million
unique image-text pairs from over 6 million articles. Metadata and
expert-guided annotations are also provided. We demonstrate the utility and
accessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style
models continuously pre-trained on the BIOMEDICA dataset via streaming,
eliminating the need to download 27 TB of data locally. On average, our models
achieve state-of-the-art performance across 40 tasks - spanning pathology,
radiology, ophthalmology, dermatology, surgery, molecular biology,
parasitology, and cell biology - excelling in zero-shot classification with a
6.56% average improvement (as high as 29.8% and 17.5% in dermatology and
ophthalmology, respectively), and stronger image-text retrieval, all while
using 10x less compute. To foster reproducibility and collaboration, we release
our codebase and dataset for the broader research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLM-101B: An Open LLM and How to Train It with $100K Budget 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03852v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03852v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, Yequan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are considered important approaches towards
foundational machine intelligence, achieving remarkable success in Natural
Language Processing and multimodal tasks, among others. However, the carbon
footprints and financial costs originating from heavy pre-training computation
is a non-negligible issue. Progressive training methods, inspired by the
neurogenesis process that grows neural structures, have shown potential to
accelerate LLM pre-training. However, the algorithms, implementation, and
practices for progressively training LLMs beyond 100B parameters remain
underexplored. In this paper, we show that our model, namely FLM-101B, trained
with our growth strategy under a budget of \$100K, reaches 80\% of the
baselines' performances with only 10\% of their floating-point operations. We
believe that further studies on progressive training will benefit the community
by cutting down the costs and promoting green AI. The checkpoint of FLM-101B is
released at https://huggingface.co/CofeAI/FLM-101B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Makes Cryptic Crosswords Challenging for LLMs? <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Sadallah, Daria Kotova, Ekaterina Kochmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptic crosswords are puzzles that rely on general knowledge and the
solver's ability to manipulate language on different levels, dealing with
various types of wordplay. Previous research suggests that solving such puzzles
is challenging even for modern NLP models, including Large Language Models
(LLMs). However, there is little to no research on the reasons for their poor
performance on this task. In this paper, we establish the benchmark results for
three popular LLMs: Gemma2, LLaMA3 and ChatGPT, showing that their performance
on this task is still significantly below that of humans. We also investigate
why these models struggle to achieve superior performance. We release our code
and introduced datasets at
https://github.com/bodasadallah/decrypting-crosswords.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025. arXiv admin note: text overlap with arXiv:2403.12094</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-matrix Factorization Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose novel attention architectures, Multi-matrix Factorization
Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard
Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain
as strong performance under stringent Key-Value cache (KV cache) constraints.
MFA enhances model capacity by efficiently scaling up both the number and
dimension of attention heads through low-rank matrix factorization in the
Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory
requirements by repurposing the key cache as value through value projection
re-parameterization. MFA's design enables strong model capacity when working
under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache
limits with minor performance trade-off. Notably, in our extensive and
large-scale experiments, the proposed architecture outperforms MLA and performs
comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Mathematical Reasoning Beyond Accuracy <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The leaderboard of Large Language Models (LLMs) in mathematical tasks has
been continuously updated. However, the majority of evaluations focus solely on
the final results, neglecting the quality of the intermediate steps. This
oversight can mask underlying problems, such as logical errors or unnecessary
steps in the reasoning process. To measure reasoning beyond final-answer
accuracy, we introduce ReasonEval, a new methodology for evaluating the quality
of reasoning steps. ReasonEval employs validity and redundancy to characterize
the reasoning quality, as well as accompanying LLMs to assess them
automatically. We explore different design options for the LLM-based evaluators
and empirically demonstrate that ReasonEval, when instantiated with base models
possessing strong mathematical knowledge and trained with high-quality labeled
data, consistently outperforms baseline methods in the meta-evaluation
datasets. We also highlight the strong generalization capabilities of
ReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we
find that an increase in final-answer accuracy does not necessarily guarantee
an improvement in the overall quality of the reasoning steps for challenging
mathematical problems. Additionally, we observe that ReasonEval can play a
significant role in data selection. We open-source the best-performing model,
meta-evaluation script, and all evaluation results to facilitate future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2 is the AAAI 2025 camera ready version. Project site with code:
  https://github.com/GAIR-NLP/ReasonEval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering
  Benchmark <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu MD, Naome A. Etori, Aimérou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best, Irfan Essa, Stephen Edward Moore, Chris Fourie, Mercy Nyamewaa Asiedu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language model(LLM) performance on medical
multiple choice question (MCQ) benchmarks have stimulated interest from
healthcare providers and patients globally. Particularly in low-and
middle-income countries (LMICs) facing acute physician shortages and lack of
specialists, LLMs offer a potentially scalable pathway to enhance healthcare
access and reduce costs. However, their effectiveness in the Global South,
especially across the African continent, remains to be established. In this
work, we introduce AfriMed-QA, the first large scale Pan-African English
multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open
and closed-ended) sourced from over 60 medical schools across 16 countries,
covering 32 medical specialties. We further evaluate 30 LLMs across multiple
axes including correctness and demographic bias. Our findings show significant
performance variation across specialties and geographies, MCQ performance
clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general
models and smaller edge-friendly LLMs struggle to achieve a passing score.
Interestingly, human evaluations show a consistent consumer preference for LLM
answers and explanations when compared with clinician answers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Beam Search Integrating CTC, Attention, and Transducer Decoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yui Sudo, Muhammad Shakeel, Yosuke Fukumoto, Brian Yan, Jiatong Shi, Yifan Peng, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end automatic speech recognition (E2E-ASR) can be classified by its
decoder architectures, such as connectionist temporal classification (CTC),
recurrent neural network transducer (RNN-T), attention-based encoder-decoder,
and Mask-CTC models. Each decoder architecture has advantages and
disadvantages, leading practitioners to switch between these different models
depending on application requirements. Instead of building separate models, we
propose a joint modeling scheme where four decoders (CTC, RNN-T, attention, and
Mask-CTC) share the same encoder -- we refer to this as 4D modeling. The 4D
model is trained jointly, which will bring model regularization and maximize
the model robustness thanks to their complementary properties. To efficiently
train the 4D model, we introduce a two-stage training strategy that stabilizes
the joint training. In addition, we propose three novel joint beam search
algorithms by combining three decoders (CTC, RNN-T, and attention) to further
improve performance. These three beam search algorithms differ in which decoder
is used as the primary decoder. We carefully evaluate the performance and
computational tradeoffs associated with each algorithm. Experimental results
demonstrate that the jointly trained 4D model outperforms the E2E-ASR models
trained with only one individual decoder. Furthermore, we demonstrate that the
proposed joint beam search algorithm outperforms the previously proposed
CTC/attention decoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to IEEE/ACM Transactions on Audio Speech and Language
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11869v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11869v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaang Li, Quan Wang, Zhongnan Wang, Yongdong Zhang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) require model editing to efficiently update
specific knowledge within them and avoid factual errors. Most model editing
methods are solely designed for single-time use and result in a significant
forgetting effect in lifelong editing scenarios, where sequential edits are
conducted over time. Previous approaches manage sequential edits by freezing
original parameters and discretely allocating new parameters for each knowledge
update. However, these methods lack robustness to minor input variations due to
the discrete mapping between data and parameters. To overcome this challenge,
we propose ELDER, a novel approach to create a continuous association between
data and adapters. ELDER integrates multiple LoRAs through a router network and
is trained to establish a smooth data-adapter association, thereby enhancing
the edit robustness and generalization of semantically equivalent inputs. To
ensure inputs containing the same knowledge will be processed by the same
LoRAs, we design a novel loss to guide the model link LoRA allocations with
edit knowledge. Furthermore, we propose a deferral mechanism to retain the
original LLM capabilities post-edit. Extensive experiments on GPT-2 XL and
LLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong
setting, outperforming eight baselines while exhibiting strong scalability and
preserving LLMs' general abilities on downstream tasks. Our code is available
at https://github.com/JiaangL/ELDER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Retrie</span>val-Reasoning Large Language Model-based Synthetic Clinical Trial
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Xu, Fang Wu, Yuanyuan Zhang, Yue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) exhibits promise in the clinical domain. However, it is
constrained by data scarcity and ethical considerations, as the generation of
clinical trials presents significant challenges due to stringent privacy
regulations, high costs, and the extended duration required for conducting
studies with human participants. Despite the advancements of large language
models (LLMs) in general generation tasks, their potential in facilitating the
generation of synthetic clinical trials is under-explored. To address this gap,
we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs
to generate artificial yet realistic and diverse clinical trials with binary
success/failure labels. Experiments conducted on real clinical trials from the
\url{ClinicalTrials.gov} database demonstrate that our synthetic data can
effectively augment real datasets. Furthermore, by fine-tuning a pre-trained
model as a binary classifier on synthetic clinical trial datasets, we
demonstrate that this augmentation enhances model training for downstream tasks
such as trial outcome prediction. Our findings suggest that LLMs for synthetic
clinical trial generation hold promise for accelerating clinical research and
upholding ethical standards for patient privacy. The code is publicly available
at
https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Efficient Split Learning for Fine-Tuning Large Language Models in
  Edge Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuguang Li, Shaohua Wu, Liang Li, Songge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we propose an energy-efficient split learning (SL) framework
for fine-tuning large language models (LLMs) using geo-distributed personal
data at the network edge, where LLMs are split and alternately across massive
mobile devices and an edge server. Considering the device heterogeneity and
channel dynamics in edge networks, a \underline{C}ut l\underline{A}yer and
computing \underline{R}esource \underline{D}ecision (CARD) algorithm is
developed to minimize training delay and energy consumption. Simulation results
demonstrate that the proposed approach reduces the average training delay and
server's energy consumption by 70.8% and 53.1%, compared to the benchmarks,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Command, Cultivate: An Exploratory Study of System-2 Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17075v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17075v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Wang, Yuxiang Zhang, Yanxu Zhu, Xinyan Wen, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The o1 system card identifies the o1 models as the most robust within OpenAI,
with their defining characteristic being the progression from rapid, intuitive
thinking to slower, more deliberate reasoning. This observation motivated us to
investigate the influence of System-2 thinking patterns on model safety. In our
preliminary research, we conducted safety evaluations of the o1 model,
including complex jailbreak attack scenarios using adversarial natural language
prompts and mathematical encoding prompts. Our findings indicate that the o1
model demonstrates relatively improved safety performance; however, it still
exhibits vulnerabilities, particularly against jailbreak attacks employing
mathematical encoding. Through detailed case analysis, we identified specific
patterns in the o1 model's responses. We also explored the alignment of
System-2 safety in open-source models using prompt engineering and supervised
fine-tuning techniques. Experimental results show that some simple methods to
encourage the model to carefully scrutinize user requests are beneficial for
model safety. Additionally, we proposed a implementation plan for process
supervision to enhance safety alignment. The implementation details and
experimental results will be provided in future versions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In this version, the DPO and reinforcement learning methods have been
  added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\text{Transformer}^2$: Self-adaptive LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Sun, Edoardo Cetin, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-adaptive large language models (LLMs) aim to solve the challenges posed
by traditional fine-tuning methods, which are often computationally intensive
and static in their ability to handle diverse tasks. We introduce
$\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for
unseen tasks in real-time by selectively adjusting only the singular components
of their weight matrices. During inference, $\text{Transformer}^2$ employs a
two-pass mechanism: first, a dispatch system identifies the task properties,
and then task-specific "expert" vectors, trained using reinforcement learning,
are dynamically mixed to obtain targeted behavior for the incoming prompt. Our
method outperforms ubiquitous approaches such as LoRA, with fewer parameters
and greater efficiency. $\text{Transformer}^2$ demonstrates versatility across
different LLM architectures and modalities, including vision-language tasks.
$\text{Transformer}^2$ represents a significant leap forward, offering a
scalable, efficient solution for enhancing the adaptability and task-specific
performance of LLMs, paving the way for truly dynamic, self-organizing AI
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 panges, 11 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient descent with generalized Newton's method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Bu, Shiyun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the generalized Newton's method (GeN) -- a Hessian-informed
approach that applies to any optimizer such as SGD and Adam, and covers the
Newton-Raphson method as a sub-case. Our method automatically and dynamically
selects the learning rate that accelerates the convergence, without the
intensive tuning of the learning rate scheduler. In practice, our method is
easily implementable, since it only requires additional forward passes with
almost zero computational overhead (in terms of training time and memory cost),
if the overhead is amortized over many iterations. We present extensive
experiments on language and vision tasks (e.g. GPT and ResNet) to showcase that
GeN optimizers match the state-of-the-art performance, which was achieved with
carefully tuned learning rate schedulers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Let the Rule Speak: Enhancing In-context Learning Debiasing with
  Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixi Lin, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning, which allows large language models to perform diverse
tasks with a few demonstrations, is found to have imbalanced per-class
prediction accuracy on multi-class text classification. Although notable output
correction methods have been developed to tackle the issue and simultaneously
improve downstream prediction accuracy, they may fail to answer the core
interpretability challenges: why and which certain classes need corrections,
and more importantly, a tailored correction for per-sample, per-class's
probability. To address such interpretability gaps, we first find that the
imbalance arises from certain classes consistently receiving high ICL output
probabilities, whereas others receiving lower or mixed ranges, so the former is
more frequently chosen, resulting in higher accuracy; more crucially, we find
that these ranges have significantly varying degrees of influence on the
accuracy bias, highlighting the need for precise, interpretable probability
corrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule
Optimization based Debiasing method, that (1) detects which classes need
corrections, and (2) for each correction-needed class, detects its probability
ranges and applies asymmetric amplifications or reductions to correct them
interpretably. Notably, across seven benchmark datasets, FuRud reduces the
pairwise class accuracy bias (COBias) by more than half (56%), while achieving
a relative increase of 21% in accuracy, outperforming state-of-the-art
debiasing methods. Moreover, FuRud can optimize downstream tasks with as few as
10 optimization examples. Furthermore, FuRud can work for prompt formats that
lead to highly skewed predictions. For example, FuRud greatly improves ICL
outputs which use letter options, with 44% relative accuracy increase and 54%
relative COBias reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double Equivariance for Inductive Link Prediction for Both New Nodes and
  New Relation Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01313v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01313v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhou, Yucheng Zhang, Jianfei Gao, Yangze Zhou, Bruno Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of fully inductive link prediction in knowledge graphs has gained
significant attention, with various graph neural networks being proposed to
address it. This task presents greater challenges than traditional inductive
link prediction tasks with only new nodes, as models must be capable of
zero-shot generalization to both unseen nodes and unseen relation types in the
inference graph. Despite the development of novel models, a unifying
theoretical understanding of their success remains elusive, and the limitations
of these methods are not well-studied. In this work, we introduce the concept
of double permutation-equivariant representations and demonstrate its necessity
for effective performance in this task. We show that many existing models,
despite their diverse architectural designs, conform to this framework.
However, we also identify inherent limitations in double
permutation-equivariant representations, which restrict these models's ability
to learn effectively on datasets with varying characteristics. Our findings
suggest that while double equivariance is necessary for meta-learning across
knowledge graphs from different domains, it is not sufficient. There remains a
fundamental gap between double permutation-equivariant models and the concept
of foundation models designed to learn patterns across all domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large
  Language Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohao Yang, He Zhao, Dinh Phung, Wray Buntine, Lan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling has been a widely used tool for unsupervised text analysis.
However, comprehensive evaluations of a topic model remain challenging.
Existing evaluation methods are either less comparable across different models
(e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic
quality or document representation quality) at a time, which is insufficient to
reflect the overall model performance. In this paper, we propose WALM (Word
Agreement with Language Model), a new evaluation method for topic modeling that
considers the semantic quality of document representations and topics in a
joint manner, leveraging the power of Large Language Models (LLMs). With
extensive experiments involving different types of topic models, WALM is shown
to align with human judgment and can serve as a complementary evaluation method
to the existing ones, bringing a new perspective to topic modeling. Our
software package is available at
https://github.com/Xiaohao-Yang/Topic_Model_Evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming in Transactions of the Association for Computational
  Linguistics (TACL) published by MIT Press</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">151</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAViD: Modeling Dynamic Affordance of 3D Objects using <span class="highlight-title">Pre-train</span>ed Video
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonwoo Kim, Sangwon Beak, Hanbyul Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the ability of humans to use objects is crucial for AI to
improve daily life. Existing studies for learning such ability focus on
human-object patterns (e.g., contact, spatial relation, orientation) in static
situations, and learning Human-Object Interaction (HOI) patterns over time
(i.e., movement of human and object) is relatively less explored. In this
paper, we introduce a novel type of affordance named Dynamic Affordance. For a
given input 3D object mesh, we learn dynamic affordance which models the
distribution of both (1) human motion and (2) human-guided object pose during
interactions. As a core idea, we present a method to learn the 3D dynamic
affordance from synthetically generated 2D videos, leveraging a pre-trained
video diffusion model. Specifically, we propose a pipeline that first generates
2D HOI videos from the 3D object and then lifts them into 3D to generate 4D HOI
samples. Once we generate diverse 4D HOI samples on various target objects, we
train our DAViD, where we present a method based on the Low-Rank Adaptation
(LoRA) module for pre-trained human motion diffusion model (MDM) and an object
pose diffusion model with human pose guidance. Our motion diffusion model is
extended for multi-object interactions, demonstrating the advantage of our
pipeline with LoRA for combining the concepts of object usage. Through
extensive experiments, we demonstrate our DAViD outperforms the baselines in
generating human motion with HOIs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://snuvclab.github.io/david/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MangaNinja: Line Art Colorization with Precise Reference Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiheng Liu, Ka Leong Cheng, Xi Chen, Jie Xiao, Hao Ouyang, Kai Zhu, Yu Liu, Yujun Shen, Qifeng Chen, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Derived from diffusion models, MangaNinjia specializes in the task of
reference-guided line art colorization. We incorporate two thoughtful designs
to ensure precise character detail transcription, including a patch shuffling
module to facilitate correspondence learning between the reference color image
and the target line art, and a point-driven control scheme to enable
fine-grained color matching. Experiments on a self-collected benchmark
demonstrate the superiority of our model over current solutions in terms of
precise colorization. We further showcase the potential of the proposed
interactive point control in handling challenging cases, cross-character
colorization, multi-reference harmonization, beyond the reach of existing
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: https://johanan528.github.io/MangaNinjia/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using
  Real-Time Warped Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling aims to transform random noise into structured outputs.
In this work, we enhance video diffusion models by allowing motion control via
structured latent noise sampling. This is achieved by just a change in data: we
pre-process training videos to yield structured noise. Consequently, our method
is agnostic to diffusion model design, requiring no changes to model
architectures or training pipelines. Specifically, we propose a novel noise
warping algorithm, fast enough to run in real time, that replaces random
temporal Gaussianity with correlated warped noise derived from optical flow
fields, while preserving the spatial Gaussianity. The efficiency of our
algorithm enables us to fine-tune modern video diffusion base models using
warped noise with minimal overhead, and provide a one-stop solution for a wide
range of user-friendly motion control: local object motion control, global
camera movement control, and motion transfer. The harmonization between
temporal coherence and spatial Gaussianity in our warped noise leads to
effective motion control while maintaining per-frame pixel quality. Extensive
experiments and user studies demonstrate the advantages of our method, making
it a robust and scalable approach for controlling motion in video diffusion
models. Video results are available on our webpage:
https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow/; source
code and model checkpoints are available on GitHub:
https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting 4D Hand Trajectory from Monocular Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Ye, Yao Feng, Omid Taheri, Haiwen Feng, Shubham Tulsiani, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HaPTIC, an approach that infers coherent 4D hand trajectories from
monocular videos. Current video-based hand pose reconstruction methods
primarily focus on improving frame-wise 3D pose using adjacent frames rather
than studying consistent 4D hand trajectories in space. Despite the additional
temporal cues, they generally underperform compared to image-based methods due
to the scarcity of annotated video data. To address these issues, we repurpose
a state-of-the-art image-based transformer to take in multiple frames and
directly predict a coherent trajectory. We introduce two types of lightweight
attention layers: cross-view self-attention to fuse temporal information, and
global cross-attention to bring in larger spatial context. Our method infers 4D
hand trajectories similar to the ground truth while maintaining strong 2D
reprojection alignment. We apply the method to both egocentric and allocentric
videos. It significantly outperforms existing methods in global trajectory
accuracy while being comparable to the state-of-the-art in single-image pose
estimation. Project website: https://judyye.github.io/haptic-www
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omni-RGPT: Unifying Image and Video Region-level Understanding via Token
  Marks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-Chiang Frank Wang, Ryo Hachiuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Omni-RGPT, a multimodal large language model designed to
facilitate region-level comprehension for both images and videos. To achieve
consistent region representation across spatio-temporal dimensions, we
introduce Token Mark, a set of tokens highlighting the target regions within
the visual feature space. These tokens are directly embedded into spatial
regions using region prompts (e.g., boxes or masks) and simultaneously
incorporated into the text prompt to specify the target, establishing a direct
connection between visual and text tokens. To further support robust video
understanding without requiring tracklets, we introduce an auxiliary task that
guides Token Mark by leveraging the consistency of the tokens, enabling stable
region interpretation across the video. Additionally, we introduce a
large-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT
achieves state-of-the-art results on image and video-based commonsense
reasoning benchmarks while showing strong performance in captioning and
referring expression comprehension tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://miranheo.github.io/omni-rgpt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GameFactory: Creating New Games with Generative Interactive Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative game engines have the potential to revolutionize game development
by autonomously creating new content and reducing manual workload. However,
existing video-based game generation methods fail to address the critical
challenge of scene generalization, limiting their applicability to existing
games with fixed styles and scenes. In this paper, we present GameFactory, a
framework focused on exploring scene generalization in game video generation.
To enable the creation of entirely new and diverse games, we leverage
pre-trained video diffusion models trained on open-domain video data. To bridge
the domain gap between open-domain priors and small-scale game dataset, we
propose a multi-phase training strategy that decouples game style learning from
action control, preserving open-domain generalization while achieving action
controllability. Using Minecraft as our data source, we release GF-Minecraft, a
high-quality and diversity action-annotated video dataset for research.
Furthermore, we extend our framework to enable autoregressive
action-controllable game video generation, allowing the production of
unlimited-length interactive game videos. Experimental results demonstrate that
GameFactory effectively generates open-domain, diverse, and action-controllable
game videos, representing a significant step forward in AI-driven game
generation. Our dataset and project page are publicly available at
\url{https://vvictoryuki.github.io/gamefactory/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Adversarial Post-Training for One-Step Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion models are widely used for image and video generation, but
their iterative generation process is slow and expansive. While existing
distillation approaches have demonstrated the potential for one-step generation
in the image domain, they still suffer from significant quality degradation. In
this work, we propose Adversarial Post-Training (APT) against real data
following diffusion pre-training for one-step video generation. To improve the
training stability and quality, we introduce several improvements to the model
architecture and training procedures, along with an approximated R1
regularization objective. Empirically, our experiments show that our
adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,
24fps videos in real time using a single forward evaluation step. Additionally,
our model is capable of generating 1024px images in a single step, achieving
quality comparable to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiniMax-01: Scaling <span class="highlight-title">Foundation</span> Models with Lightning Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,
which are comparable to top-tier models while offering superior capabilities in
processing longer contexts. The core lies in lightning attention and its
efficient scaling. To maximize computational capacity, we integrate it with
Mixture of Experts (MoE), creating a model with 32 experts and 456 billion
total parameters, of which 45.9 billion are activated for each token. We
develop an optimized parallel strategy and highly efficient
computation-communication overlap techniques for MoE and lightning attention.
This approach enables us to conduct efficient training and inference on models
with hundreds of billions of parameters across contexts spanning millions of
tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens
during training and extrapolate to 4 million tokens during inference at an
affordable cost. Our vision-language model, MiniMax-VL-01 is built through
continued training with 512 billion vision-language tokens. Experiments on both
standard and in-house benchmarks show that our models match the performance of
state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32
times longer context window. We publicly release MiniMax-01 at
https://github.com/MiniMax-AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A technical report from MiniMax. The authors are listed in
  alphabetical order. We open-sourced our MiniMax-01 at
  https://github.com/MiniMax-AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Semantic Future Prediction through Multimodal Visual Sequence
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic future prediction is important for autonomous systems navigating
dynamic environments. This paper introduces FUTURIST, a method for multimodal
future semantic prediction that uses a unified and efficient visual sequence
transformer architecture. Our approach incorporates a multimodal masked visual
modeling objective and a novel masking mechanism designed for multimodal
training. This allows the model to effectively integrate visible information
from various modalities, improving prediction accuracy. Additionally, we
propose a VAE-free hierarchical tokenization process, which reduces
computational complexity, streamlines the training pipeline, and enables
end-to-end training with high-resolution, multimodal inputs. We validate
FUTURIST on the Cityscapes dataset, demonstrating state-of-the-art performance
in future semantic segmentation for both short- and mid-term forecasting. We
provide the implementation code at https://github.com/Sta8is/FUTURIST .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LayerAnimate: Layer-specific Control for Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxue Yang, Lue Fan, Zuzen Lin, Feng Wang, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animated video separates foreground and background elements into layers, with
distinct processes for sketching, refining, coloring, and in-betweening.
Existing video generation methods typically treat animation as a monolithic
data domain, lacking fine-grained control over individual layers. In this
paper, we introduce LayerAnimate, a novel architectural approach that enhances
fine-grained control over individual animation layers within a video diffusion
model, allowing users to independently manipulate foreground and background
elements in distinct layers. To address the challenge of limited layer-specific
data, we propose a data curation pipeline that features automated element
segmentation, motion-state hierarchical merging, and motion coherence
refinement. Through quantitative and qualitative comparisons, and user study,
we demonstrate that LayerAnimate outperforms current methods in terms of
animation quality, control precision, and usability, making it an ideal tool
for both professional animators and amateur enthusiasts. This framework opens
up new possibilities for layer-specific animation applications and creative
flexibility. Our code is available at https://layeranimate.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://layeranimate.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wu, Zicheng Zhang, Muer Tie, Ziqing Ai, Zhongxue Gan, Wenchao Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework
designed for large scenes. The framework comprises four main components: VIO
Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO
Front End, RGB frames are processed through dense bundle adjustment and
uncertainty estimation to extract scene geometry and poses. Based on this
output, the mapping module incrementally constructs and maintains a 2D Gaussian
map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,
Score Manager, and Pose Refinement, which collectively improve mapping speed
and localization accuracy. This enables the SLAM system to handle large-scale
urban environments with up to 50 million Gaussian ellipsoids. To ensure global
consistency in large-scale scenes, we design a Loop Closure module, which
innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian
Splatting for loop closure detection and correction of the Gaussian map.
Additionally, we propose a Dynamic Eraser to address the inevitable presence of
dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor
and outdoor environments demonstrate that our approach achieves localization
performance on par with Visual-Inertial Odometry while surpassing recent
GS/NeRF SLAM methods. It also significantly outperforms all existing methods in
terms of mapping and rendering quality. Furthermore, we developed a mobile app
and verified that our framework can generate high-quality Gaussian maps in real
time using only a smartphone camera and a low-frequency IMU sensor. To the best
of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method
capable of operating in outdoor environments and supporting kilometer-scale
large scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Bayesian Neural Networks Explicitly Model Input Uncertainty? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Valdenegro-Toro, Marco Zullich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inputs to machine learning models can have associated noise or uncertainties,
but they are often ignored and not modelled. It is unknown if Bayesian Neural
Networks and their approximations are able to consider uncertainty in their
inputs. In this paper we build a two input Bayesian Neural Network (mean and
standard deviation) and evaluate its capabilities for input uncertainty
estimation across different methods like Ensembles, MC-Dropout, and Flipout.
Our results indicate that only some uncertainty estimation methods for
approximate Bayesian NNs can model input uncertainty, in particular Ensembles
and Flipout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures, VISAPP 2025 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-ST: A Multimodal Large Language Model for Fine-Grained
  Spatial-Temporal Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal large language models (MLLMs) have shown
promising results, yet existing approaches struggle to effectively handle both
temporal and spatial localization simultaneously. This challenge stems from two
key issues: first, incorporating spatial-temporal localization introduces a
vast number of coordinate combinations, complicating the alignment of
linguistic and visual coordinate representations; second, encoding fine-grained
temporal and spatial information during video feature compression is inherently
difficult. To address these issues, we propose LLaVA-ST, a MLLM for
fine-grained spatial-temporal multimodal understanding. In LLaVA-ST, we propose
Language-Aligned Positional Embedding, which embeds the textual coordinate
special token into the visual space, simplifying the alignment of fine-grained
spatial-temporal correspondences. Additionally, we design the Spatial-Temporal
Packer, which decouples the feature compression of temporal and spatial
resolutions into two distinct point-to-region attention processing streams.
Furthermore, we propose ST-Align dataset with 4.3M training samples for
fine-grained spatial-temporal multimodal understanding. With ST-align, we
present a progressive training pipeline that aligns the visual and textual
feature through sequential coarse-to-fine stages.Additionally, we introduce an
ST-Align benchmark to evaluate spatial-temporal interleaved fine-grained
understanding tasks, which include Spatial-Temporal Video Grounding (STVG) ,
Event Localization and Captioning (ELC) and Spatial Video Grounding (SVG).
LLaVA-ST achieves outstanding performance on 11 benchmarks requiring
fine-grained temporal, spatial, or spatial-temporal interleaving multimodal
understanding. Our code, data and benchmark will be released at Our code, data
and benchmark will be released at https://github.com/appletea233/LLaVA-ST .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SmartEraser: Remove Anything from Images using Masked-Region Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object removal has so far been dominated by the mask-and-inpaint paradigm,
where the masked region is excluded from the input, leaving models relying on
unmasked areas to inpaint the missing region. However, this approach lacks
contextual information for the masked area, often resulting in unstable
performance. In this work, we introduce SmartEraser, built with a new removing
paradigm called Masked-Region Guidance. This paradigm retains the masked region
in the input, using it as guidance for the removal process. It offers several
distinct advantages: (a) it guides the model to accurately identify the object
to be removed, preventing its regeneration in the output; (b) since the user
mask often extends beyond the object itself, it aids in preserving the
surrounding context in the final result. Leveraging this new paradigm, we
present Syn4Removal, a large-scale object removal dataset, where instance
segmentation data is used to copy and paste objects onto images as removal
targets, with the original images serving as ground truths. Experimental
results demonstrate that SmartEraser significantly outperforms existing
methods, achieving superior performance in object removal, especially in
complex scenes with intricate compositions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project at: https://longtaojiang.github.io/smarteraser.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Driven Water Segmentation with deep learning models for Enhanced
  Flood Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjida Afrin Mou, Tasfia Noor Chowdhury, Adib Ibn Mannan, Sadia Nourin Mim, Lubana Tarannum, Tasrin Noman, Jamal Uddin Ahamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flooding is a major natural hazard causing significant fatalities and
economic losses annually, with increasing frequency due to climate change.
Rapid and accurate flood detection and monitoring are crucial for mitigating
these impacts. This study compares the performance of three deep learning
models UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid in
flood detection, utilizing images from drones, in field observations, and
social media. This study involves creating a new dataset that augments
wellknown benchmark datasets with flood-specific images, enhancing the
robustness of the models. The UNet, ResNet, and DeepLab v3 architectures are
tested to determine their effectiveness in various environmental conditions and
geographical locations, and the strengths and limitations of each model are
also discussed here, providing insights into their applicability in different
scenarios by predicting image segmentation masks. This fully automated approach
allows these models to isolate flooded areas in images, significantly reducing
processing time compared to traditional semi-automated methods. The outcome of
this study is to predict segmented masks for each image effected by a flood
disaster and the validation accuracy of these models. This methodology
facilitates timely and continuous flood monitoring, providing vital data for
emergency response teams to reduce loss of life and economic damages. It offers
a significant reduction in the time required to generate flood maps, cutting
down the manual processing time. Additionally, we present avenues for future
research, including the integration of multimodal data sources and the
development of robust deep learning architectures tailored specifically for
flood detection tasks. Overall, our work contributes to the advancement of
flood management strategies through innovative use of deep learning
technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards an End-to-End (E2E) Adversarial Learning and Application in the
  Physical World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dudi Biton, Jacob Shams, Koda Satoru, Asaf Shabtai, Yuval Elovici, Ben Nassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traditional learning process of patch-based adversarial attacks,
conducted in the digital domain and then applied in the physical domain (e.g.,
via printed stickers), may suffer from reduced performance due to adversarial
patches' limited transferability from the digital domain to the physical
domain. Given that previous studies have considered using projectors to apply
adversarial attacks, we raise the following question: can adversarial learning
(i.e., patch generation) be performed entirely in the physical domain with a
projector? In this work, we propose the Physical-domain Adversarial Patch
Learning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework
that converts adversarial learning from the digital domain to the physical
domain using a projector. We evaluate PAPLA across multiple scenarios,
including controlled laboratory settings and realistic outdoor environments,
demonstrating its ability to ensure attack success compared to conventional
digital learning-physical application (DL-PA) methods. We also analyze the
impact of environmental factors, such as projection surface color, projector
strength, ambient light, distance, and angle of the target object relative to
the camera, on the effectiveness of projected patches. Finally, we demonstrate
the feasibility of the attack against a parked car and a stop sign in a
real-world outdoor environment. Our results show that under specific
conditions, E2E adversarial learning in the physical domain eliminates the
transferability issue and ensures evasion by object detectors. Finally, we
provide insights into the challenges and opportunities of applying adversarial
learning in the physical domain and explain where such an approach is more
effective than using a sticker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Deep Active Learning for Medical Imaging: Replay-Base
  Architecture for Context Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Daniel, M. Rita Verdelho, Catarina Barata, Carlos Santiago
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning for medical imaging faces challenges in adapting and
generalizing to new contexts. Additionally, it often lacks sufficient labeled
data for specific tasks requiring significant annotation effort. Continual
Learning (CL) tackles adaptability and generalizability by enabling lifelong
learning from a data stream while mitigating forgetting of previously learned
knowledge. Active Learning (AL) reduces the number of required annotations for
effective training. This work explores both approaches (CAL) to develop a novel
framework for robust medical image analysis. Based on the automatic recognition
of shifts in image characteristics, Replay-Base Architecture for Context
Adaptation (RBACA) employs a CL rehearsal method to continually learn from
diverse contexts, and an AL component to select the most informative instances
for annotation. A novel approach to evaluate CAL methods is established using a
defined metric denominated IL-Score, which allows for the simultaneous
assessment of transfer learning, forgetting, and final model performance. We
show that RBACA works in domain and class-incremental learning scenarios, by
assessing its IL-Score on the segmentation and diagnosis of cardiac images. The
results show that RBACA outperforms a baseline framework without CAL, and a
state-of-the-art CAL method across various memory sizes and annotation budgets.
Our code is available in https://github.com/RuiDaniel/RBACA .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images
  using Choquet Integral and Differential Evolution Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Reza Takhsha, Maryam Rastgarpour, Mozhgan Naderi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has profoundly impacted billions globally. It
challenges public health and healthcare systems due to its rapid spread and
severe respiratory effects. An effective strategy to mitigate the COVID-19
pandemic involves integrating testing to identify infected individuals. While
RT-PCR is considered the gold standard for diagnosing COVID-19, it has some
limitations such as the risk of false negatives. To address this problem, this
paper introduces a novel Deep Learning Diagnosis System that integrates
pre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble
learning framework to achieve precise identification of COVID-19 cases from
Chest X-ray (CXR) images. We combine feature vectors from the final hidden
layers of pre-trained DCNNs using the Choquet integral to capture interactions
between different DCNNs that a linear approach cannot. We employed
Sugeno-$\lambda$ measure theory to derive fuzzy measures for subsets of
networks to enable aggregation. We utilized Differential Evolution to estimate
fuzzy densities. We developed a TensorFlow-based layer for Choquet operation to
facilitate efficient aggregation, due to the intricacies involved in
aggregating feature vectors. Experimental results on the COVIDx dataset show
that our ensemble model achieved 98\% accuracy in three-class classification
and 99.50\% in binary classification, outperforming its components-DenseNet-201
(97\% for three-class, 98.75\% for binary), Inception-v3 (96.25\% for
three-class, 98.50\% for binary), and Xception (94.50\% for three-class, 98\%
for binary)-and surpassing many previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeineb Haouari, Jonas Weidner, Ivan Ezhov, Aswathi Varma, Daniel Rueckert, Bjoern Menze, Benedikt Wiestler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glioblastoma, a highly aggressive brain tumor, poses major challenges due to
its poor prognosis and high morbidity rates. Partial differential
equation-based models offer promising potential to enhance therapeutic outcomes
by simulating patient-specific tumor behavior for improved radiotherapy
planning. However, model calibration remains a bottleneck due to the high
computational demands of optimization methods like Monte Carlo sampling and
evolutionary algorithms. To address this, we recently introduced an approach
leveraging a neural forward solver with gradient-based optimization to
significantly reduce calibration time. This approach requires a highly accurate
and fully differentiable forward model. We investigate multiple architectures,
including (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a
3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best
overall results, excelling in both tumor outline matching and voxel-level
prediction of tumor cell concentration. It halved the MSE relative to the
baseline model and achieved the highest Dice score across all tumor cell
concentration thresholds. Our study demonstrates significant enhancement in
forward solver performance and outlines important future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FramePainter: Endowing Interactive Image Editing with Video Diffusion
  Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabo Zhang, Xinpeng Zhou, Yihan Zeng, Hang Xu, Hui Li, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive image editing allows users to modify images through visual
interaction operations such as drawing, clicking, and dragging. Existing
methods construct such supervision signals from videos, as they capture how
objects change with various physical interactions. However, these models are
usually built upon text-to-image diffusion models, so necessitate (i) massive
training samples and (ii) an additional reference encoder to learn real-world
dynamics and visual consistency. In this paper, we reformulate this task as an
image-to-video generation problem, so that inherit powerful video diffusion
priors to reduce training costs and ensure temporal consistency. Specifically,
we introduce FramePainter as an efficient instantiation of this formulation.
Initialized with Stable Video Diffusion, it only uses a lightweight sparse
control encoder to inject editing signals. Considering the limitations of
temporal attention in handling large motion between two frames, we further
propose matching attention to enlarge the receptive field while encouraging
dense correspondence between edited and source image tokens. We highlight the
effectiveness and efficiency of FramePainter across various of editing signals:
it domainantly outperforms previous state-of-the-art methods with far less
training data, achieving highly seamless and coherent editing of images, \eg,
automatically adjust the reflection of the cup. Moreover, FramePainter also
exhibits exceptional generalization in scenarios not present in real-world
videos, \eg, transform the clownfish into shark-like shape. Our code will be
available at https://github.com/YBYBZhang/FramePainter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/YBYBZhang/FramePainter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine El Boudouri, Amine Bohi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expressions play a crucial role in human communication serving as a
powerful and impactful means to express a wide range of emotions. With
advancements in artificial intelligence and computer vision, deep neural
networks have emerged as effective tools for facial emotion recognition. In
this paper, we propose EmoNeXt, a novel deep learning framework for facial
expression recognition based on an adapted ConvNeXt architecture network. We
integrate a Spatial Transformer Network (STN) to focus on feature-rich regions
of the face and Squeeze-and-Excitation blocks to capture channel-wise
dependencies. Moreover, we introduce a self-attention regularization term,
encouraging the model to generate compact feature vectors. We demonstrate the
superiority of our model over existing state-of-the-art deep learning models on
the FER2013 dataset regarding emotion classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures and 2 tables. 2023 IEEE 25th International
  Workshop on Multimedia Signal Processing (MMSP), Poitiers, France</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Deep Hyperspectral Inpainting with the Plug and Play and
  Deep Image Prior Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Li, Mehrdad Yaghoobi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images are typically composed of hundreds of narrow and
contiguous spectral bands, each containing information regarding the material
composition of the imaged scene. However, these images can be affected by
various sources of noise, distortions, or data loss, which can significantly
degrade their quality and usefulness. This paper introduces a convergent
guaranteed algorithm, LRS-PnP-DIP(1-Lip), which successfully addresses the
instability issue of DHP that has been reported before. The proposed algorithm
extends the successful joint low-rank and sparse model to further exploit the
underlying data structures beyond the conventional and sometimes restrictive
unions of subspace models. A stability analysis guarantees the convergence of
the proposed algorithm under mild assumptions , which is crucial for its
application in real-world scenarios. Extensive experiments demonstrate that the
proposed solution consistently delivers visually and quantitatively superior
inpainting results, establishing state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 9 Figures, 7 Tables. arXiv admin note: text overlap with
  arXiv:2306.08128</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Critical Synthesis of Uncertainty Quantification and <span class="highlight-title">Foundation</span> Models
  in Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Landgraf, Rongjun Qin, Markus Ulrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent foundation models have enabled significant breakthroughs in
monocular depth estimation, a clear path towards safe and reliable deployment
in the real-world remains elusive. Metric depth estimation, which involves
predicting absolute distances, poses particular challenges, as even the most
advanced foundation models remain prone to critical errors. Since quantifying
the uncertainty has emerged as a promising endeavor to address these
limitations and enable trustworthy deployment, we fuse five different
uncertainty quantification methods with the current state-of-the-art
DepthAnythingV2 foundation model. To cover a wide range of metric depth
domains, we evaluate their performance on four diverse datasets. Our findings
identify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a
particularly promising approach, offering reliable uncertainty estimates while
maintaining predictive performance and computational efficiency on par with the
baseline, encompassing both training and inference time. By fusing uncertainty
quantification and foundation models within the context of monocular depth
estimation, this paper lays a critical foundation for future research aimed at
improving not only model performance but also its explainability. Extending
this critical synthesis of uncertainty quantification and foundation models
into other crucial tasks, such as semantic segmentation and pose estimation,
presents exciting opportunities for safer and more reliable machine vision
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CG-MER: A Card Game-based Multimodal <span class="highlight-title">dataset</span> for Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nessrine Farhat, Amine Bohi, Leila Ben Letaifa, Rim Slama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of affective computing has seen significant advancements in
exploring the relationship between emotions and emerging technologies. This
paper presents a novel and valuable contribution to this field with the
introduction of a comprehensive French multimodal dataset designed specifically
for emotion recognition. The dataset encompasses three primary modalities:
facial expressions, speech, and gestures, providing a holistic perspective on
emotions. Moreover, the dataset has the potential to incorporate additional
modalities, such as Natural Language Processing (NLP) to expand the scope of
emotion recognition research. The dataset was curated through engaging
participants in card game sessions, where they were prompted to express a range
of emotions while responding to diverse questions. The study included 10
sessions with 20 participants (9 females and 11 males). The dataset serves as a
valuable resource for furthering research in emotion recognition and provides
an avenue for exploring the intricate connections between human emotions and
digital technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures and 4 tables. Sixteenth International Conference
  on Machine Vision (ICMV 2023), Yerevan, Armenia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Zeng, Jie Song, Han Zheng, Hao Jiang, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved cutting-edge performance in image generation.
However, their lengthy denoising process and computationally intensive score
estimation network impede their scalability in low-latency and
resource-constrained scenarios. Post-training quantization (PTQ) compresses and
accelerates diffusion models without retraining, but it inevitably introduces
additional quantization noise, resulting in mean and variance deviations. In
this work, we propose D2-DPM, a dual denoising mechanism aimed at precisely
mitigating the adverse effects of quantization noise on the noise estimation
network. Specifically, we first unravel the impact of quantization noise on the
sampling equation into two components: the mean deviation and the variance
deviation. The mean deviation alters the drift coefficient of the sampling
equation, influencing the trajectory trend, while the variance deviation
magnifies the diffusion coefficient, impacting the convergence of the sampling
trajectory. The proposed D2-DPM is thus devised to denoise the quantization
noise at each time step, and then denoise the noisy sample through the inverse
diffusion iterations. Experimental results demonstrate that D2-DPM achieves
superior generation quality, yielding a 1.42 lower FID than the full-precision
model while achieving 3.99x compression and 11.67x bit-operation acceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, acceptted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric 2D Gaussian Splatting: Background Removal and
  Occlusion-Aware Pruning for Compact Object Models <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Rogge, Didier Stricker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Gaussian Splatting approaches are effective for reconstructing entire
scenes but lack the option to target specific objects, making them
computationally expensive and unsuitable for object-specific applications. We
propose a novel approach that leverages object masks to enable targeted
reconstruction, resulting in object-centric models. Additionally, we introduce
an occlusion-aware pruning strategy to minimize the number of Gaussians without
compromising quality. Our method reconstructs compact object models, yielding
object-centric Gaussian and mesh representations that are up to 96\% smaller
and up to 71\% faster to train compared to the baseline while retaining
competitive quality. These representations are immediately usable for
downstream applications such as appearance editing and physics simulation
without additional processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICPRAM 2025 (https://icpram.scitevents.org/Home.aspx)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Multimodal Models for Fine-Grained Image Analysis: A
  Comparative Study Across Diverse Visual Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenii Evstafev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces a benchmark designed to evaluate the capabilities of
multimodal models in analyzing and interpreting images. The benchmark focuses
on seven key visual aspects: main object, additional objects, background,
detail, dominant colors, style, and viewpoint. A dataset of 14,580 images,
generated from diverse text prompts, was used to assess the performance of
seven leading multimodal models. These models were evaluated on their ability
to accurately identify and describe each visual aspect, providing insights into
their strengths and weaknesses for comprehensive image understanding. The
findings of this benchmark have significant implications for the development
and selection of multimodal models for various image analysis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 tables, 2 charts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revolutionizing Communication with Deep Learning and XAI for Enhanced
  Arabic Sign Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazen Balat, Rewaa Awaad, Ahmed B. Zaky, Salah A. Aly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces an integrated approach to recognizing Arabic Sign
Language (ArSL) using state-of-the-art deep learning models such as
MobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced
by explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and
RGB Arabic Alphabets Sign Language (AASL) datasets are employed, with
EfficientNet-B2 achieving peak accuracies of 99.48\% and 98.99\%, respectively.
Key innovations include sophisticated data augmentation methods to mitigate
class imbalance, implementation of stratified 5-fold cross-validation for
better generalization, and the use of Grad-CAM for clear model decision
transparency. The proposed system not only sets new benchmarks in recognition
accuracy but also emphasizes interpretability, making it suitable for
applications in healthcare, education, and inclusive communication
technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 25 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucong Meng, Zhiwei Yang, Zhijian Song, Yonghong Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accelerated MRI reconstruction poses a challenging ill-posed inverse
problem due to the significant undersampling in k-space. Deep neural networks,
such as CNNs and ViT, have shown substantial performance improvements for this
task while encountering the dilemma between global receptive fields and
efficient computation. To this end, this paper pioneers exploring Mamba, a new
paradigm for long-range dependency modeling with linear complexity, for
efficient and effective MRI reconstruction. However, directly applying Mamba to
MRI reconstruction faces three significant issues: (1) Mamba's row-wise and
column-wise scanning disrupts k-space's unique spectrum, leaving its potential
in k-space learning unexplored. (2) Existing Mamba methods unfold feature maps
with multiple lengthy scanning paths, leading to long-range forgetting and high
computational burden. (3) Mamba struggles with spatially-varying contents,
resulting in limited diversity of local representations. To address these, we
propose a dual-domain multi-scale Mamba for MRI reconstruction from the
following perspectives: (1) We pioneer vision Mamba in k-space learning. A
circular scanning is customized for spectrum unfolding, benefiting the global
modeling of k-space. (2) We propose a multi-scale Mamba with an efficient
scanning strategy in both image and k-space domains. It mitigates long-range
forgetting and achieves a better trade-off between efficiency and performance.
(3) We develop a local diversity enhancement module to improve the
spatially-varying representation of Mamba. Extensive experiments are conducted
on three public datasets for MRI reconstruction under various undersampling
patterns. Comprehensive results demonstrate that our method significantly
outperforms state-of-the-art methods with lower computational cost.
Implementation code will be available at
https://github.com/XiaoMengLiLiLi/DM-Mamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Backdoor Attack to Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, Olivier Déforges, Kassem Kallas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of deep learning (DL) has increased computing complexity and energy
use, prompting the adoption of application specific integrated circuits (ASICs)
for energy-efficient edge and mobile deployment. However, recent studies have
demonstrated the vulnerability of these accelerators to energy attacks. Despite
the development of various inference time energy attacks in prior research,
backdoor energy attacks remain unexplored. In this paper, we design an
innovative energy backdoor attack against deep neural networks (DNNs) operating
on sparsity-based accelerators. Our attack is carried out in two distinct
phases: backdoor injection and backdoor stealthiness. Experimental results
using ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet
datasets show the effectiveness of our proposed attack in increasing energy
consumption on trigger samples while preserving the model's performance for
clean/regular inputs. This demonstrates the vulnerability of DNNs to energy
backdoor attacks. The source code of our attack is available at:
https://github.com/hbrachemi/energy_backdoor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bootstrapping Corner Cases: High-Resolution Inpainting for Safety
  Critical Detect and Avoid for Automated Flying 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Lyhs, Lars Hinneburg, Michael Fischer, Florian Ölsner, Stefan Milz, Jeremy Tschirner, Patrick Mäder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning techniques have shown tremendous potential,
especially for object detection on camera images. For this reason, they are
also used to enable safety-critical automated processes such as autonomous
drone flights. We present a study on object detection for Detect and Avoid, a
safety critical function for drones that detects air traffic during automated
flights for safety reasons. An ill-posed problem is the generation of good and
especially large data sets, since detection itself is the corner case. Most
models suffer from limited ground truth in raw data, \eg recorded air traffic
or frontal flight with a small aircraft. It often leads to poor and critical
detection rates. We overcome this problem by using inpainting methods to
bootstrap the dataset such that it explicitly contains the corner cases of the
raw data. We provide an overview of inpainting methods and generative models
and present an example pipeline given a small annotated dataset. We validate
our method by generating a high-resolution dataset, which we make publicly
available and present it to an independent object detector that was fully
trained on real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-visual Deepfake Detection With Local Temporal Inconsistencies <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcella Astrid, Enjie Ghorbel, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an audio-visual deepfake detection approach that aims to
capture fine-grained temporal inconsistencies between audio and visual
modalities. To achieve this, both architectural and data synthesis strategies
are introduced. From an architectural perspective, a temporal distance map,
coupled with an attention mechanism, is designed to capture these
inconsistencies while minimizing the impact of irrelevant temporal
subsequences. Moreover, we explore novel pseudo-fake generation techniques to
synthesize local inconsistencies. Our approach is evaluated against
state-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating
its effectiveness in detecting audio-visual deepfakes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAR Strikes Back: A New Hope for RSVQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucrezia Tosato, Flora Weissgerber, Laurent Wendling, Sylvain Lobry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing visual question answering (RSVQA) is a task that automatically
extracts information from satellite images and processes a question to predict
the answer from the images in textual form, helping with the interpretation of
the image. While different methods have been proposed to extract information
from optical images with different spectral bands and resolutions, no method
has been proposed to answer questions from Synthetic Aperture Radar (SAR)
images. SAR images capture electromagnetic information from the scene, and are
less affected by atmospheric conditions, such as clouds. In this work, our
objective is to introduce SAR in the RSVQA task, finding the best way to use
this modality. In our research, we carry out a study on different pipelines for
the task of RSVQA taking into account information from both SAR and optical
data. To this purpose, we also present a dataset that allows for the
introduction of SAR images in the RSVQA framework. We propose two different
models to include the SAR modality. The first one is an end-to-end method in
which we add an additional encoder for the SAR modality. In the second
approach, we build on a two-stage framework. First, relevant information is
extracted from SAR and, optionally, optical data. This information is then
translated into natural language to be used in the second step which only
relies on a language model to provide the answer. We find that the second
pipeline allows us to obtain good results with SAR images alone. We then try
various types of fusion methods to use SAR and optical images together, finding
that a fusion at the decision level achieves the best results on the proposed
dataset. We show that SAR data offers additional information when fused with
the optical modality, particularly for questions related to specific land cover
classes, such as water areas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Birds Eye View Perception Models with Frozen <span class="highlight-title">Foundation</span>
  Models: DINOv2 and Metric3Dv2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seamie Hayes, Ganesh Sistu, Ciarán Eising
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Birds Eye View perception models require extensive data to perform and
generalize effectively. While traditional datasets often provide abundant
driving scenes from diverse locations, this is not always the case. It is
crucial to maximize the utility of the available training data. With the advent
of large foundation models such as DINOv2 and Metric3Dv2, a pertinent question
arises: can these models be integrated into existing model architectures to not
only reduce the required training data but surpass the performance of current
models? We choose two model architectures in the vehicle segmentation domain to
alter: Lift-Splat-Shoot, and Simple-BEV. For Lift-Splat-Shoot, we explore the
implementation of frozen DINOv2 for feature extraction and Metric3Dv2 for depth
estimation, where we greatly exceed the baseline results by 7.4 IoU while
utilizing only half the training data and iterations. Furthermore, we introduce
an innovative application of Metric3Dv2's depth information as a PseudoLiDAR
point cloud incorporated into the Simple-BEV architecture, replacing
traditional LiDAR. This integration results in a +3 IoU improvement compared to
the Camera-only model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Electronic Imaging - Autonomous
  Vehicles and Machines Connference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoHan: Robust Hand Detection in Operation Room 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roi Papo, Sapir Gershov, Tom Friedman, Itay Or, Gil Bolotin, Shlomi Laufer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-specific localization has garnered significant interest within the
computer vision community. Although there are numerous datasets with hand
annotations from various angles and settings, domain transfer techniques
frequently struggle in surgical environments. This is mainly due to the limited
availability of gloved hand instances and the unique challenges of operating
rooms (ORs). Thus, hand-detection models tailored to OR settings require
extensive training and expensive annotation processes. To overcome these
challenges, we present "RoHan" - a novel approach for robust hand detection in
the OR, leveraging advanced semi-supervised domain adaptation techniques to
tackle the challenges of varying recording conditions, diverse glove colors,
and occlusions common in surgical settings. Our methodology encompasses two
main stages: (1) data augmentation strategy that utilizes "Artificial Gloves,"
a method for augmenting publicly available hand datasets with synthetic images
of hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that
improves detection performance in real-world OR settings through iterative
prediction refinement and efficient frame filtering. We evaluate our method
using two datasets: simulated enterotomy repair and saphenous vein graft
harvesting. "RoHan" substantially reduces the need for extensive labeling and
model training, paving the way for the practical implementation of hand
detection technologies in medical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Change Captioning in Remote Sensing: Evolution to SAT-Cap -- A
  Single-Stage Transformer Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuduo Wang, Weikang Yu, Pedram Ghamisi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change captioning has become essential for accurately describing changes in
multi-temporal remote sensing data, providing an intuitive way to monitor
Earth's dynamics through natural language. However, existing change captioning
methods face two key challenges: high computational demands due to multistage
fusion strategy, and insufficient detail in object descriptions due to limited
semantic extraction from individual images. To solve these challenges, we
propose SAT-Cap based on the transformers model with a single-stage feature
fusion for remote sensing change captioning. In particular, SAT-Cap integrates
a Spatial-Channel Attention Encoder, a Difference-Guided Fusion module, and a
Caption Decoder. Compared to typical models that require multi-stage fusion in
transformer encoder and fusion module, SAT-Cap uses only a simple cosine
similarity-based fusion module for information integration, reducing the
complexity of the model architecture. By jointly modeling spatial and channel
information in Spatial-Channel Attention Encoder, our approach significantly
enhances the model's ability to extract semantic information from objects in
multi-temporal remote sensing images. Extensive experiments validate the
effectiveness of SAT-Cap, achieving CIDEr scores of 140.23% on the LEVIR-CC
dataset and 97.74% on the DUBAI-CC dataset, surpassing current state-of-the-art
methods. The code and pre-trained models will be available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> EarthView: A Large Scale Remote Sensing <span class="highlight-title">Dataset</span> for Self-Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Velazquez, Pau Rodriguez López, Sergio Alonso, Josep M. Gonfaus, Jordi Gonzalez, Gerardo Richarte, Javier Marin, <span class="highlight-author">Yoshua Bengio</span>, Alexandre Lacoste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents EarthView, a comprehensive dataset specifically designed
for self-supervision on remote sensing data, intended to enhance deep learning
applications on Earth monitoring tasks. The dataset spans 15 tera pixels of
global remote-sensing data, combining imagery from a diverse range of sources,
including NEON, Sentinel, and a novel release of 1m spatial resolution data
from Satellogic. Our dataset provides a wide spectrum of image data with
varying resolutions, harnessed from different sensors and organized coherently
into an accessible HuggingFace dataset in parquet format. This data spans five
years, from 2017 to 2022. Accompanying the dataset, we introduce EarthMAE, a
tailored Masked Autoencoder, developed to tackle the distinct challenges of
remote sensing data. Trained in a self-supervised fashion, EarthMAE effectively
processes different data modalities such as hyperspectral, multispectral,
topographical data, segmentation maps, and temporal structure. This model helps
us show that pre-training on Satellogic data improves performance on downstream
tasks. While there is still a gap to fill in MAE for heterogeneous data, we
regard this innovative combination of an expansive, diverse dataset and a
versatile model adapted for self-supervised learning as a stride forward in
deep learning for Earth monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2nd Workshop on Computer Vision for Earth Observation (CV4EO)
  Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding the classification of hepatocellular carcinoma on 3D CT-scans
  using deep and handcrafted radiological features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Sarfati, A. Bône, M-M. Rohé, C. Aubé, M. Ronot, P. Gori, I. Bloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hepatocellular carcinoma is the most spread primary liver cancer across the
world ($\sim$80\% of the liver tumors). The gold standard for HCC diagnosis is
liver biopsy. However, in the clinical routine, expert radiologists provide a
visual diagnosis by interpreting hepatic CT-scans according to a standardized
protocol, the LI-RADS, which uses five radiological criteria with an associated
decision tree. In this paper, we propose an automatic approach to predict
histology-proven HCC from CT images in order to reduce radiologists'
inter-variability. We first show that standard deep learning methods fail to
accurately predict HCC from CT-scans on a challenging database, and propose a
two-step approach inspired by the LI-RADS system to improve the performance. We
achieve improvements from 6 to 18 points of AUC with respect to deep learning
baselines trained with different architectures. We also provide clinical
validation of our method, achieving results that outperform non-expert
radiologists and are on par with expert ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE ISBI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CellOMaps: A Compact Representation for Robust Classification of Lung
  Adenocarcinoma Growth Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arwa Al-Rubaian, Gozde N. Gunesli, Wajd A. Althakfi, Ayesha Azam, David Snead, Nasir M. Rajpoot, Shan E Ahmed Raza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lung adenocarcinoma (LUAD) is a morphologically heterogeneous disease,
characterized by five primary histological growth patterns. The classification
of such patterns is crucial due to their direct relation to prognosis but the
high subjectivity and observer variability pose a major challenge. Although
several studies have developed machine learning methods for growth pattern
classification, they either only report the predominant pattern per slide or
lack proper evaluation. We propose a generalizable machine learning pipeline
capable of classifying lung tissue into one of the five patterns or as
non-tumor. The proposed pipeline's strength lies in a novel compact Cell
Organization Maps (cellOMaps) representation that captures the cellular spatial
patterns from Hematoxylin and Eosin whole slide images (WSIs). The proposed
pipeline provides state-of-the-art performance on LUAD growth pattern
classification when evaluated on both internal unseen slides and external
datasets, significantly outperforming the current approaches. In addition, our
preliminary results show that the model's outputs can be used to predict
patients Tumor Mutational Burden (TMB) levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentPose: Progressive Distribution Alignment via Feature Agent for
  Human Pose Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Zhang, Jinwei Liu, Xiatian Zhu, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose distillation is widely adopted to reduce model size in human pose
estimation. However, existing methods primarily emphasize the transfer of
teacher knowledge while often neglecting the performance degradation resulted
from the curse of capacity gap between teacher and student. To address this
issue, we propose AgentPose, a novel pose distillation method that integrates a
feature agent to model the distribution of teacher features and progressively
aligns the distribution of student features with that of the teacher feature,
effectively overcoming the capacity gap and enhancing the ability of knowledge
transfer. Our comprehensive experiments conducted on the COCO dataset
substantiate the effectiveness of our method in knowledge transfer,
particularly in scenarios with a high capacity gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Vision <span class="highlight-title">Foundation</span> Models for Input Monitoring in Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nert Keser, Halil Ibrahim Orhan, Niki Amini-Naieni, Gesina Schwalbe, Alois Knoll, Matthias Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) remain challenged by distribution shifts in
complex open-world domains like automated driving (AD): Absolute robustness
against yet unknown novel objects (semantic shift) or styles like lighting
conditions (covariate shift) cannot be guaranteed. Hence, reliable
operation-time monitors for identification of out-of-training-data-distribution
(OOD) scenarios are imperative. Current approaches for OOD classification are
untested for complex domains like AD, are limited in the kinds of shifts they
detect, or even require supervision with OOD samples. To prepare for
unanticipated shifts, we instead establish a framework around a principled,
unsupervised, and model-agnostic method that unifies detection of all kinds of
shifts: Find a full model of the training data's feature distribution, to then
use its density at new points as in-distribution (ID) score. To implement this,
we propose to combine the newly available Vision Foundation Models (VFM) as
feature extractors with one of four alternative density modeling techniques. In
an extensive benchmark of 4 VFMs against 20 baselines, we show the superior
performance of VFM feature encodings compared to shift-specific OOD monitors.
Additionally, we find that sophisticated architectures outperform larger latent
space dimensionality; and our method identifies samples with higher risk of
errors on downstream tasks, despite being model-agnostic. This suggests that
VFMs are promising to realize model-agnostic, unsupervised, reliable safety
monitors in complex vision tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skeleton and Font Generation Network for Zero-shot Chinese Character
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mobai Xue, Jun Du, Zhenrong Zhang, Jiefeng Ma, Qikai Chang, Pengfei Hu, Jianshu Zhang, Yu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic font generation remains a challenging research issue, primarily due
to the vast number of Chinese characters, each with unique and intricate
structures. Our investigation of previous studies reveals inherent bias capable
of causing structural changes in characters. Specifically, when generating a
Chinese character similar to, but different from, those in the training
samples, the bias is prone to either correcting or ignoring these subtle
variations. To address this concern, we propose a novel Skeleton and Font
Generation Network (SFGN) to achieve a more robust Chinese character font
generation. Our approach includes a skeleton builder and font generator. The
skeleton builder synthesizes content features using low-resource text input,
enabling our technique to realize font generation independently of content
image inputs. Unlike previous font generation methods that treat font style as
a global embedding, we introduce a font generator to align content and style
features on the radical level, which is a brand-new perspective for font
generation. Except for common characters, we also conduct experiments on
misspelled characters, a substantial portion of which slightly differs from the
common ones. Our approach visually demonstrates the efficacy of generated
images and outperforms current state-of-the-art font generation methods.
Moreover, we believe that misspelled character generation have significant
pedagogical implications and verify such supposition through experiments. We
used generated misspelled characters as data augmentation in Chinese character
error correction tasks, simulating the scenario where students learn
handwritten Chinese characters with the help of misspelled characters. The
significantly improved performance of error correction tasks demonstrates the
effectiveness of our proposed approach and the value of misspelled character
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Attentive Spatio-Temporal Calibration for Precise Intermediate
  Layer Matching in ANN-to-SNN Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Hong, Yueming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) are promising for low-power computation due to
their event-driven mechanism but often suffer from lower accuracy compared to
Artificial Neural Networks (ANNs). ANN-to-SNN knowledge distillation can
improve SNN performance, but previous methods either focus solely on label
information, missing valuable intermediate layer features, or use a layer-wise
approach that neglects spatial and temporal semantic inconsistencies, leading
to performance degradation.To address these limitations, we propose a novel
method called self-attentive spatio-temporal calibration (SASTC). SASTC uses
self-attention to identify semantically aligned layer pairs between ANN and
SNN, both spatially and temporally. This enables the autonomous transfer of
relevant semantic information. Extensive experiments show that SASTC
outperforms existing methods, effectively solving the mismatching problem.
Superior accuracy results include 95.12% on CIFAR-10, 79.40% on CIFAR-100 with
2 time steps, and 68.69% on ImageNet with 4 time steps for static datasets, and
97.92% on DVS-Gesture and 83.60% on DVS-CIFAR10 for neuromorphic datasets. This
marks the first time SNNs have outperformed ANNs on both CIFAR-10 and
CIFAR-100, shedding the new light on the potential applications of SNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring visual language models as a powerful tool in the diagnosis of
  Ewing Sarcoma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvaro Pastor-Naranjo, Pablo Meseguer, Rocío del Amor, Jose Antonio Lopez-Guerrero, Samuel Navarro, Katia Scotlandi, Antonio Llombart-Bosch, Isidro Machado, Valery Naranjo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ewing's sarcoma (ES), characterized by a high density of small round blue
cells without structural organization, presents a significant health concern,
particularly among adolescents aged 10 to 19. Artificial intelligence-based
systems for automated analysis of histopathological images are promising to
contribute to an accurate diagnosis of ES. In this context, this study explores
the feature extraction ability of different pre-training strategies for
distinguishing ES from other soft tissue or bone sarcomas with similar
morphology in digitized tissue microarrays for the first time, as far as we
know. Vision-language supervision (VLS) is compared to fully-supervised
ImageNet pre-training within a multiple instance learning paradigm. Our
findings indicate a substantial improvement in diagnostic accuracy with the
adaption of VLS using an in-domain dataset. Notably, these models not only
enhance the accuracy of predicted classes but also drastically reduce the
number of trainable parameters and computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, 2 tables. Oral presentation at KES-InMed 2024
  held in Madeira, Portugal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Low-Light Human Pose Estimation through Illumination-Texture
  Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Zhang, Ze Li, Xiatian Zhu, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As critical visual details become obscured, the low visibility and high ISO
noise in extremely low-light images pose a significant challenge to human pose
estimation. Current methods fail to provide high-quality representations due to
reliance on pixel-level enhancements that compromise semantics and the
inability to effectively handle extreme low-light conditions for robust feature
learning. In this work, we propose a frequency-based framework for low-light
human pose estimation, rooted in the "divide-and-conquer" principle. Instead of
uniformly enhancing the entire image, our method focuses on task-relevant
information. By applying dynamic illumination correction to the low-frequency
components and low-rank denoising to the high-frequency components, we
effectively enhance both the semantic and texture information essential for
accurate pose estimation. As a result, this targeted enhancement method results
in robust, high-quality representations, significantly improving pose
estimation performance. Extensive experiments demonstrating its superiority
over state-of-the-art methods in various challenging low-light scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But
  Only If You Can Trust Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Caetano, Christiaan Viviers, Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection holds significant importance across many
applications. While semantic and domain-shift OOD problems are well-studied,
this work focuses on covariate shifts - subtle variations in the data
distribution that can degrade machine learning performance. We hypothesize that
detecting these subtle shifts can improve our understanding of in-distribution
boundaries, ultimately improving OOD detection. In adversarial discriminators
trained with Batch Normalization (BN), real and adversarial samples form
distinct domains with unique batch statistics - a property we exploit for OOD
detection. We introduce DisCoPatch, an unsupervised Adversarial Variational
Autoencoder (VAE) framework that harnesses this mechanism. During inference,
batches consist of patches from the same image, ensuring a consistent data
distribution that allows the model to rely on batch statistics. DisCoPatch uses
the VAE's suboptimal outputs (generated and reconstructed) as negative samples
to train the discriminator, thereby improving its ability to delineate the
boundary between in-distribution samples and covariate shifts. By tightening
this boundary, DisCoPatch achieves state-of-the-art results in public OOD
detection benchmarks. The proposed model not only excels in detecting covariate
shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior
methods on public Near-OOD (95.0%) benchmarks. With a compact model size of
25MB, it achieves high OOD detection performance at notably lower latency than
existing methods, making it an efficient and practical solution for real-world
OOD detection applications. The code will be made publicly available
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximizing Uncertainty for Federated learning via Bayesian
  Optimisation-based Model Poisoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marios Aristodemou, Xiaolan Liu, Yuan Wang, Konstantinos G. Kyriakopoulos, Sangarapillai Lambotharan, Qingsong Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we transition from Narrow Artificial Intelligence towards Artificial Super
Intelligence, users are increasingly concerned about their privacy and the
trustworthiness of machine learning (ML) technology. A common denominator for
the metrics of trustworthiness is the quantification of uncertainty inherent in
DL algorithms, and specifically in the model parameters, input data, and model
predictions. One of the common approaches to address privacy-related issues in
DL is to adopt distributed learning such as federated learning (FL), where
private raw data is not shared among users. Despite the privacy-preserving
mechanisms in FL, it still faces challenges in trustworthiness. Specifically,
the malicious users, during training, can systematically create malicious model
parameters to compromise the models predictive and generative capabilities,
resulting in high uncertainty about their reliability. To demonstrate malicious
behaviour, we propose a novel model poisoning attack method named Delphi which
aims to maximise the uncertainty of the global model output. We achieve this by
taking advantage of the relationship between the uncertainty and the model
parameters of the first hidden layer of the local model. Delphi employs two
types of optimisation , Bayesian Optimisation and Least Squares Trust Region,
to search for the optimal poisoned model parameters, named as Delphi-BO and
Delphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise
the distance of the predictive probability distribution towards an uncertain
distribution of model output. Furthermore, we establish a mathematical proof
for the attack effectiveness demonstrated in FL. Numerical results demonstrate
that Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR
highlighting vulnerability of FL systems to model poisoning attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining imaging and shape features for prediction tasks of Alzheimer's
  disease classification and brain age regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nairouz Shehata, Carolina Piçarra, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate combining imaging and shape features extracted from MRI for
the clinically relevant tasks of brain age prediction and Alzheimer's disease
classification. Our proposed model fuses ResNet-extracted image embeddings with
shape embeddings from a bespoke graph neural network. The shape embeddings are
derived from surface meshes of 15 brain structures, capturing detailed
geometric information. Combined with the appearance features from T1-weighted
images, we observe improvements in the prediction performance on both tasks,
with substantial gains for classification. We evaluate the model using public
datasets, including CamCAN, IXI, and OASIS3, demonstrating the effectiveness of
fusing imaging and shape features for brain analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAC-Net_Geometric and attention-based Network for Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuang Zhu, Xingli Gan, Min Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth completion is a key task in autonomous driving, aiming to complete
sparse LiDAR depth measurements into high-quality dense depth maps through
image guidance. However, existing methods usually treat depth maps as an
additional channel of color images, or directly perform convolution on sparse
data, failing to fully exploit the 3D geometric information in depth maps,
especially with limited performance in complex boundaries and sparse areas. To
address these issues, this paper proposes a depth completion network combining
channel attention mechanism and 3D global feature perception (CGA-Net). The
main innovations include: 1) Utilizing PointNet++ to extract global 3D
geometric features from sparse depth maps, enhancing the scene perception
ability of low-line LiDAR data; 2) Designing a channel-attention-based
multimodal feature fusion module to efficiently integrate sparse depth, RGB
images, and 3D geometric features; 3) Combining residual learning with CSPN++
to optimize the depth refinement stage, further improving the completion
quality in edge areas and complex scenes. Experiments on the KITTI depth
completion dataset show that CGA-Net can significantly improve the prediction
accuracy of dense depth maps, achieving a new state-of-the-art (SOTA), and
demonstrating strong robustness to sparse and complex scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13pages,4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Threshold Attention Network for Semantic Segmentation of Remote Sensing
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Long, Yongjun Zhang, Zhongwei Cui, Yujie Xu, Xuexue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of remote sensing images is essential for various
applications, including vegetation monitoring, disaster management, and urban
planning. Previous studies have demonstrated that the self-attention mechanism
(SA) is an effective approach for designing segmentation networks that can
capture long-range pixel dependencies. SA enables the network to model the
global dependencies between the input features, resulting in improved
segmentation outcomes. However, the high density of attentional feature maps
used in this mechanism causes exponential increases in computational
complexity. Additionally, it introduces redundant information that negatively
impacts the feature representation. Inspired by traditional threshold
segmentation algorithms, we propose a novel threshold attention mechanism
(TAM). This mechanism significantly reduces computational effort while also
better modeling the correlation between different regions of the feature map.
Based on TAM, we present a threshold attention network (TANet) for semantic
segmentation. TANet consists of an attentional feature enhancement module
(AFEM) for global feature enhancement of shallow features and a threshold
attention pyramid pooling module (TAPP) for acquiring feature information at
different scales for deep features. We have conducted extensive experiments on
the ISPRS Vaihingen and Potsdam datasets. The results demonstrate the validity
and superiority of our proposed TANet compared to the most state-of-the-art
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V-Trans4Style: Visual Transition <span class="highlight-title">Recommend</span>ation for Video Production
  Style Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooja Guhan, Tsung-Wei Huang, Guan-Ming Su, Subhadra Gopalakrishnan, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce V-Trans4Style, an innovative algorithm tailored for dynamic
video content editing needs. It is designed to adapt videos to different
production styles like documentaries, dramas, feature films, or a specific
YouTube channel's video-making technique. Our algorithm recommends optimal
visual transitions to help achieve this flexibility using a more bottom-up
approach. We first employ a transformer-based encoder-decoder network to learn
recommending temporally consistent and visually seamless sequences of visual
transitions using only the input videos. We then introduce a style conditioning
module that leverages this model to iteratively adjust the visual transitions
obtained from the decoder through activation maximization. We demonstrate the
efficacy of our method through experiments conducted on our newly introduced
AutoTransition++ dataset. It is a 6k video version of AutoTransition Dataset
that additionally categorizes its videos into different production style
categories. Our encoder-decoder model outperforms the state-of-the-art
transition recommendation method, achieving improvements of 10% to 80% in
Recall@K and mean rank values over baseline. Our style conditioning module
results in visual transitions that improve the capture of the desired video
production style characteristics by an average of around 12% in comparison to
other methods when measured with similarity metrics. We hope that our work
serves as a foundation for exploring and understanding video production styles
further.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Dynamics in Video: Instruction Tuning for Improved Facial
  Expression Perception and Contextual Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxing Zhao, Boyuan Sun, Xiang Chen, Xihan Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression captioning has found widespread application across various
domains. Recently, the emergence of video Multimodal Large Language Models
(MLLMs) has shown promise in general video understanding tasks. However,
describing facial expressions within videos poses two major challenges for
these models: (1) the lack of adequate datasets and benchmarks, and (2) the
limited visual token capacity of video MLLMs. To address these issues, this
paper introduces a new instruction-following dataset tailored for dynamic
facial expression caption. The dataset comprises 5,033 high-quality video clips
annotated manually, containing over 700,000 tokens. Its purpose is to improve
the capability of video MLLMs to discern subtle facial nuances. Furthermore, we
propose FaceTrack-MM, which leverages a limited number of tokens to encode the
main character's face. This model demonstrates superior performance in tracking
faces and focusing on the facial expressions of the main characters, even in
intricate multi-person scenarios. Additionally, we introduce a novel evaluation
metric combining event extraction, relation classification, and the longest
common subsequence (LCS) algorithm to assess the content consistency and
temporal sequence consistency of generated text. Moreover, we present
FEC-Bench, a benchmark designed to assess the performance of existing video
MLLMs in this specific task. All data and source code will be made publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Video Moment <span class="highlight-title">Retrie</span>val via Off-the-shelf Multimodal Large
  Language Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifang Xu, Yunzhuo Sun, Benxiang Zhai, Ming Li, Wenxin Liang, Yang Li, Sidan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The target of video moment retrieval (VMR) is predicting temporal spans
within a video that semantically match a given linguistic query. Existing VMR
methods based on multimodal large language models (MLLMs) overly rely on
expensive high-quality datasets and time-consuming fine-tuning. Although some
recent studies introduce a zero-shot setting to avoid fine-tuning, they
overlook inherent language bias in the query, leading to erroneous
localization. To tackle the aforementioned challenges, this paper proposes
Moment-GPT, a tuning-free pipeline for zero-shot VMR utilizing frozen MLLMs.
Specifically, we first employ LLaMA-3 to correct and rephrase the query to
mitigate language bias. Subsequently, we design a span generator combined with
MiniGPT-v2 to produce candidate spans adaptively. Finally, to leverage the
video comprehension capabilities of MLLMs, we apply VideoChatGPT and span
scorer to select the most appropriate spans. Our proposed method substantially
outperforms the state-ofthe-art MLLM-based and zero-shot models on several
public datasets, including QVHighlights, ActivityNet-Captions, and
Charades-STA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SkipClick: Combining Quick Responses and Low-Level Features for
  Interactive Segmentation in Winter Sports Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Schön, Julian Lorenz, Daniel Kienzle, Rainer Lienhart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel architecture for interactive segmentation
in winter sports contexts. The field of interactive segmentation deals with the
prediction of high-quality segmentation masks by informing the network about
the objects position with the help of user guidance. In our case the guidance
consists of click prompts. For this task, we first present a baseline
architecture which is specifically geared towards quickly responding after each
click. Afterwards, we motivate and describe a number of architectural
modifications which improve the performance when tasked with segmenting winter
sports equipment on the WSESeg dataset. With regards to the average NoC@85
metric on the WSESeg classes, we outperform SAM and HQ-SAM by 2.336 and 7.946
clicks, respectively. When applied to the HQSeg-44k dataset, our system
delivers state-of-the-art results with a NoC@90 of 6.00 and NoC@95 of 9.89. In
addition to that, we test our model on a novel dataset containing masks for
humans during skiing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures, 6 tables, 12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Guide Dog: Egocentric Path Prediction on Smartphone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Jadhav, Jeffery Cao, Abhishree Shetty, Urvashi Priyam Kumar, Aditi Sharma, Ben Sukboontip, Jayant Sravan Tamarapalli, Jingyi Zhang, Anirudh Koul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces AI Guide Dog (AIGD), a lightweight egocentric
navigation assistance system for visually impaired individuals, designed for
real-time deployment on smartphones. AIGD addresses key challenges in blind
navigation by employing a vision-only, multi-label classification approach to
predict directional commands, ensuring safe traversal across diverse
environments. We propose a novel technique to enable goal-based outdoor
navigation by integrating GPS signals and high-level directions, while also
addressing uncertain multi-path predictions for destination-free indoor
navigation. Our generalized model is the first navigation assistance system to
handle both goal-oriented and exploratory navigation scenarios across indoor
and outdoor settings, establishing a new state-of-the-art in blind navigation.
We present methods, datasets, evaluations, and deployment insights to encourage
further innovations in assistive navigation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Hyperspectral Image Panshapring via Sparse Spatial-Spectral
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Ming Lee, Yu-Fan Lin, Li-Wei Kang, Chih-Chung Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution hyperspectral imaging plays a crucial role in various remote
sensing applications, yet its acquisition often faces fundamental limitations
due to hardware constraints. This paper introduces S$^{3}$RNet, a novel
framework for hyperspectral image pansharpening that effectively combines
low-resolution hyperspectral images (LRHSI) with high-resolution multispectral
images (HRMSI) through sparse spatial-spectral representation. The core of
S$^{3}$RNet is the Multi-Branch Fusion Network (MBFN), which employs parallel
branches to capture complementary features at different spatial and spectral
scales. Unlike traditional approaches that treat all features equally, our
Spatial-Spectral Attention Weight Block (SSAWB) dynamically adjusts feature
weights to maintain sparse representation while suppressing noise and
redundancy. To enhance feature propagation, we incorporate the Dense Feature
Aggregation Block (DFAB), which efficiently aggregates inputted features
through dense connectivity patterns. This integrated design enables S$^{3}$RNet
to selectively emphasize the most informative features from differnt scale
while maintaining computational efficiency. Comprehensive experiments
demonstrate that S$^{3}$RNet achieves state-of-the-art performance across
multiple evaluation metrics, showing particular strength in maintaining high
reconstruction quality even under challenging noise conditions. The code will
be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IGARSS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early prediction of the transferability of bovine embryos from
  videomicroscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasmine Hachani, Patrick Bouthemy, Elisa Fromont, Sylvie Ruffini, Ludivine Laffont, Alline de Paula Reis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Videomicroscopy is a promising tool combined with machine learning for
studying the early development of in vitro fertilized bovine embryos and
assessing its transferability as soon as possible. We aim to predict the embryo
transferability within four days at most, taking 2D time-lapse microscopy
videos as input. We formulate this problem as a supervised binary
classification problem for the classes transferable and not transferable. The
challenges are three-fold: 1) poorly discriminating appearance and motion, 2)
class ambiguity, 3) small amount of annotated data. We propose a 3D
convolutional neural network involving three pathways, which makes it
multi-scale in time and able to handle appearance and motion in different ways.
For training, we retain the focal loss. Our model, named SFR, compares
favorably to other methods. Experiments demonstrate its effectiveness and
accuracy for our challenging biological task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2024 IEEE International Conference on Image
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VENOM: Text-driven Unrestricted Adversarial Example Generation with
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Kuurila-Zhang, Haoyu Chen, Guoying Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks have proven effective in deceiving machine learning
models by subtly altering input images, motivating extensive research in recent
years. Traditional methods constrain perturbations within $l_p$-norm bounds,
but advancements in Unrestricted Adversarial Examples (UAEs) allow for more
complex, generative-model-based manipulations. Diffusion models now lead UAE
generation due to superior stability and image quality over GANs. However,
existing diffusion-based UAE methods are limited to using reference images and
face challenges in generating Natural Adversarial Examples (NAEs) directly from
random noise, often producing uncontrolled or distorted outputs. In this work,
we introduce VENOM, the first text-driven framework for high-quality
unrestricted adversarial examples generation through diffusion models. VENOM
unifies image content generation and adversarial synthesis into a single
reverse diffusion process, enabling high-fidelity adversarial examples without
sacrificing attack success rate (ASR). To stabilize this process, we
incorporate an adaptive adversarial guidance strategy with momentum, ensuring
that the generated adversarial examples $x^*$ align with the distribution
$p(x)$ of natural images. Extensive experiments demonstrate that VENOM achieves
superior ASR and image quality compared to prior methods, marking a significant
advancement in adversarial example generation and providing insights into model
vulnerabilities for improved defense development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cloud Removal With PolSAR-Optical Data Fusion Using A Two-Flow Residual
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Wang, Wenjuan Zhang, Bing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical remote sensing images play a crucial role in the observation of the
Earth's surface. However, obtaining complete optical remote sensing images is
challenging due to cloud cover. Reconstructing cloud-free optical images has
become a major task in recent years. This paper presents a two-flow
Polarimetric Synthetic Aperture Radar (PolSAR)-Optical data fusion cloud
removal algorithm (PODF-CR), which achieves the reconstruction of missing
optical images. PODF-CR consists of an encoding module and a decoding module.
The encoding module includes two parallel branches that extract PolSAR image
features and optical image features. To address speckle noise in PolSAR images,
we introduce dynamic filters in the PolSAR branch for image denoising. To
better facilitate the fusion between multimodal optical images and PolSAR
images, we propose fusion blocks based on cross-skip connections to enable
interaction of multimodal data information. The obtained fusion features are
refined through an attention mechanism to provide better conditions for the
subsequent decoding of the fused images. In the decoding module, multi-scale
convolution is introduced to obtain multi-scale information. Additionally, to
better utilize comprehensive scattering information and polarization
characteristics to assist in the restoration of optical images, we use a
dataset for cloud restoration called OPT-BCFSAR-PFSAR, which includes
backscatter coefficient feature images and polarization feature images obtained
from PoLSAR data and optical images. Experimental results demonstrate that this
method outperforms existing methods in both qualitative and quantitative
evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demo<span class="highlight-title">graph</span>ic Variability in Face Image Quality Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wassim Kabbani, Kiran Raja, Raghavendra Ramachandra, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face image quality assessment (FIQA) algorithms are being integrated into
online identity management applications. These applications allow users to
upload a face image as part of their document issuance process, where the image
is then run through a quality assessment process to make sure it meets the
quality and compliance requirements. Concerns about demographic bias have been
raised about biometric systems, given the societal implications this may cause.
It is therefore important that demographic variability in FIQA algorithms is
assessed such that mitigation measures can be created. In this work, we study
the demographic variability of all face image quality measures included in the
ISO/IEC 29794-5 international standard across three demographic variables: age,
gender, and skin tone. The results are rather promising and show no clear bias
toward any specific demographic group for most measures. Only two quality
measures are found to have considerable variations in their outcomes for
different groups on the skin tone variable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tarsier2: Advancing Large Vision-Language Models from Detailed Video
  Description to Comprehensive Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, Yuan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM)
designed for generating detailed and accurate video descriptions, while also
exhibiting superior general video understanding capabilities. Tarsier2 achieves
significant advancements through three key upgrades: (1) Scaling pre-training
data from 11M to 40M video-text pairs, enriching both volume and diversity; (2)
Performing fine-grained temporal alignment during supervised fine-tuning; (3)
Using model-based sampling to automatically construct preference data and
applying DPO training for optimization. Extensive experiments show that
Tarsier2-7B consistently outperforms leading proprietary models, including
GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K
benchmark, Tarsier2-7B improves F1 by 2.8\% over GPT-4o and 5.8\% over
Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\%
performance advantage over GPT-4o and +24.9\% over Gemini-1.5-Pro. Tarsier2-7B
also sets new state-of-the-art results across 15 public benchmarks, spanning
tasks such as video question-answering, video grounding, hallucination test,
and embodied question-answering, demonstrating its versatility as a robust
generalist vision-language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Algorithmic Bias in Multiclass CNN Classifications Using
  Causal Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Sik Byun, Wendy Wan Yee Hui, Wai Kwong Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study describes a procedure for applying causal modeling to detect and
mitigate algorithmic bias in a multiclass classification problem. The dataset
was derived from the FairFace dataset, supplemented with emotional labels
generated by the DeepFace pre-trained model. A custom Convolutional Neural
Network (CNN) was developed, consisting of four convolutional blocks, followed
by fully connected layers and dropout layers to mitigate overfitting. Gender
bias was identified in the CNN model's classifications: Females were more
likely to be classified as "happy" or "sad," while males were more likely to be
classified as "neutral." To address this, the one-vs-all (OvA) technique was
applied. A causal model was constructed for each emotion class to adjust the
CNN model's predicted class probabilities. The adjusted probabilities for the
various classes were then aggregated by selecting the class with the highest
probability. The resulting debiased classifications demonstrated enhanced
gender fairness across all classes, with negligible impact--or even a slight
improvement--on overall accuracy. This study highlights that algorithmic
fairness and accuracy are not necessarily trade-offs. All data and code for
this study are publicly available for download.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make-A-Character 2: Anima<span class="highlight-title">table</span> 3D Character Generation From a Single
  Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Liu, Yutong Wang, Jiahao Chen, Jianfang Li, Tangli Xue, Longlong Li, Jianqiang Ren, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report introduces Make-A-Character 2, an advanced system for generating
high-quality 3D characters from single portrait photographs, ideal for game
development and digital human applications. Make-A-Character 2 builds upon its
predecessor by incorporating several significant improvements for image-based
head generation. We utilize the IC-Light method to correct non-ideal
illumination in input photos and apply neural network-based color correction to
harmonize skin tones between the photos and game engine renders. We also employ
the Hierarchical Representation Network to capture high-frequency facial
structures and conduct adaptive skeleton calibration for accurate and
expressive facial animations. The entire image-to-3D-character generation
process takes less than 2 minutes. Furthermore, we leverage transformer
architecture to generate co-speech facial and gesture actions, enabling
real-time conversation with the generated character. These technologies have
been integrated into our conversational AI avatar products.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ deepTerra -- AI Land Classification Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Keith Wilkinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  deepTerra is a comprehensive platform designed to facilitate the
classification of land surface features using machine learning and satellite
imagery. The platform includes modules for data collection, image augmentation,
training, testing, and prediction, streamlining the entire workflow for image
classification tasks. This paper presents a detailed overview of the
capabilities of deepTerra, shows how it has been applied to various research
areas, and discusses the future directions it might take.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State-of-the-Art Transformer Models for Image Super-Resolution:
  Techniques, Challenges, and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasish Dutta, Deepjyoti Chetia, Neeharika Sonowal, Sanjib Kr Kalita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Super-Resolution (SR) aims to recover a high-resolution image from its
low-resolution counterpart, which has been affected by a specific degradation
process. This is achieved by enhancing detail and visual quality. Recent
advancements in transformer-based methods have remolded image super-resolution
by enabling high-quality reconstructions surpassing previous deep-learning
approaches like CNN and GAN-based. This effectively addresses the limitations
of previous methods, such as limited receptive fields, poor global context
capture, and challenges in high-frequency detail recovery. Additionally, the
paper reviews recent trends and advancements in transformer-based SR models,
exploring various innovative techniques and architectures that combine
transformers with traditional networks to balance global and local contexts.
These neoteric methods are critically analyzed, revealing promising yet
unexplored gaps and potential directions for future research. Several
visualizations of models and techniques are included to foster a holistic
understanding of recent trends. This work seeks to offer a structured roadmap
for researchers at the forefront of deep learning, specifically exploring the
impact of transformers on super-resolution techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Intra- and Cross-frame Topological Consistency Scheme for
  Semi-supervised Atherosclerotic Coronary Plaque Segmentation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Zhang, Zihan Li, Dandan Shan, Yuehui Qiu, Qingqi Hong, Qingqiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the precision of segmenting coronary atherosclerotic plaques from
CT Angiography (CTA) images is pivotal for advanced Coronary Atherosclerosis
Analysis (CAA), which distinctively relies on the analysis of vessel
cross-section images reconstructed via Curved Planar Reformation. This task
presents significant challenges due to the indistinct boundaries and structures
of plaques and blood vessels, leading to the inadequate performance of current
deep learning models, compounded by the inherent difficulty in annotating such
complex data. To address these issues, we propose a novel dual-consistency
semi-supervised framework that integrates Intra-frame Topological Consistency
(ITC) and Cross-frame Topological Consistency (CTC) to leverage labeled and
unlabeled data. ITC employs a dual-task network for simultaneous segmentation
mask and Skeleton-aware Distance Transform (SDT) prediction, achieving similar
prediction of topology structure through consistency constraint without
additional annotations. Meanwhile, CTC utilizes an unsupervised estimator for
analyzing pixel flow between skeletons and boundaries of adjacent frames,
ensuring spatial continuity. Experiments on two CTA datasets show that our
method surpasses existing semi-supervised methods and approaches the
performance of supervised methods on CAA. In addition, our method also performs
better than other methods on the ACDC dataset, demonstrating its
generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haomiao Xiong, Yunzhi Zhuge, Jiawen Zhu, Lu Zhang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) exhibit impressive capabilities in
2D tasks, yet encounter challenges in discerning the spatial positions,
interrelations, and causal logic in scenes when transitioning from 2D to 3D
representations. We find that the limitations mainly lie in: i) the high
annotation cost restricting the scale-up of volumes of 3D scene data, and ii)
the lack of a straightforward and effective way to perceive 3D information
which results in prolonged training durations and complicates the streamlined
framework. To this end, we develop pipeline based on open-source 2D MLLMs and
LLMs to generate high-quality 3D-text pairs and construct 3DS-160K , to enhance
the pre-training process. Leveraging this high-quality pre-training data, we
introduce the 3UR-LLM model, an end-to-end 3D MLLM designed for precise
interpretation of 3D scenes, showcasing exceptional capability in navigating
the complexities of the physical world. 3UR-LLM directly receives 3D point
cloud as input and project 3D features fused with text instructions into a
manageable set of tokens. Considering the computation burden derived from these
hybrid tokens, we design a 3D compressor module to cohesively compress the 3D
spatial cues and textual narrative. 3UR-LLM achieves promising performance with
respect to the previous SOTAs, for instance, 3UR-LLM exceeds its counterparts
by 7.1\% CIDEr on ScanQA, while utilizing fewer training resources. The code
and model weights for 3UR-LLM and the 3DS-160K benchmark are available at
3UR-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Multimedia (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitong Gong, Yunzhi Zhuge, Lu Zhang, Yifan Wang, Pingping Zhang, Lijun Wang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The essence of audio-visual segmentation (AVS) lies in locating and
delineating sound-emitting objects within a video stream. While
Transformer-based methods have shown promise, their handling of long-range
dependencies struggles due to quadratic computational costs, presenting a
bottleneck in complex scenarios. To overcome this limitation and facilitate
complex multi-modal comprehension with linear complexity, we introduce
AVS-Mamba, a selective state space model to address the AVS task. Our framework
incorporates two key components for video understanding and cross-modal
learning: Temporal Mamba Block for sequential video processing and
Vision-to-Audio Fusion Block for advanced audio-vision integration. Building on
this, we develop the Multi-scale Temporal Encoder, aimed at enhancing the
learning of visual features across scales, facilitating the perception of
intra- and inter-frame information. To perform multi-modal fusion, we propose
the Modality Aggregation Decoder, leveraging the Vision-to-Audio Fusion Block
to integrate visual features into audio features across both frame and temporal
levels. Further, we adopt the Contextual Integration Pyramid to perform
audio-to-vision spatial-temporal context collaboration. Through these
innovative contributions, our approach achieves new state-of-the-art results on
the AVSBench-object and AVSBench-semantic datasets. Our source code and model
weights are available at AVS-Mamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Multimedia (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Low-cost and Ultra-lightweight Binary Neural Network for Traffic
  Signal Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingke Xiao, Yue Su, Liang Yu, Guanglong Qu, Yutong Jia, Yukuan Chang, Xu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of neural networks in vehicle platforms and wearable
Artificial Intelligence-of-Things (AIOT) scenarios has become a research area
that has attracted much attention. With the continuous evolution of deep
learning technology, many image classification models are committed to
improving recognition accuracy, but this is often accompanied by problems such
as large model resource usage, complex structure, and high power consumption,
which makes it challenging to deploy on resource-constrained platforms. Herein,
we propose an ultra-lightweight binary neural network (BNN) model designed for
hardware deployment, and conduct image classification research based on the
German Traffic Sign Recognition Benchmark (GTSRB) dataset. In addition, we also
verify it on the Chinese Traffic Sign (CTS) and Belgian Traffic Sign (BTS)
datasets. The proposed model shows excellent recognition performance with an
accuracy of up to 97.64%, making it one of the best performing BNN models in
the GTSRB dataset. Compared with the full-precision model, the accuracy loss is
controlled within 1%, and the parameter storage overhead of the model is only
10% of that of the full-precision model. More importantly, our network model
only relies on logical operations and low-bit width fixed-point addition and
subtraction operations during the inference phase, which greatly simplifies the
design complexity of the processing element (PE). Our research shows the great
potential of BNN in the hardware deployment of computer vision models,
especially in the field of computer vision tasks related to autonomous driving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Motion and Temporal Cues for Unsupervised Video Object
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Zhuge, Hongyu Gu, Lu Zhang, Jinqing Qi, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenges in unsupervised video object
segmentation (UVOS) by proposing an efficient algorithm, termed MTNet, which
concurrently exploits motion and temporal cues. Unlike previous methods that
focus solely on integrating appearance with motion or on modeling temporal
relations, our method combines both aspects by integrating them within a
unified framework. MTNet is devised by effectively merging appearance and
motion features during the feature extraction process within encoders,
promoting a more complementary representation. To capture the intricate
long-range contextual dynamics and information embedded within videos, a
temporal transformer module is introduced, facilitating efficacious inter-frame
interactions throughout a video clip. Furthermore, we employ a cascade of
decoders all feature levels across all feature levels to optimally exploit the
derived features, aiming to generate increasingly precise segmentation masks.
As a result, MTNet provides a strong and compact framework that explores both
temporal and cross-modality knowledge to robustly localize and track the
primary object accurately in various challenging scenarios efficiently.
Extensive experiments across diverse benchmarks conclusively show that our
method not only attains state-of-the-art performance in unsupervised video
object segmentation but also delivers competitive results in video salient
object detection. These findings highlight the method's robust versatility and
its adeptness in adapting to a range of segmentation tasks. Source code is
available on https://github.com/hy0523/MTNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balance Divergence for Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yafei Qi, Chen Wang, Zhaoning Zhang, Yaping Liu, Yongmin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation has been widely adopted in computer vision task
processing, since it can effectively enhance the performance of lightweight
student networks by leveraging the knowledge transferred from cumbersome
teacher networks. Most existing knowledge distillation methods utilize
Kullback-Leibler divergence to mimic the logit output probabilities between the
teacher network and the student network. Nonetheless, these methods may neglect
the negative parts of the teacher's ''dark knowledge'' because the divergence
calculations may ignore the effect of the minute probabilities from the
teacher's logit output. This deficiency may lead to suboptimal performance in
logit mimicry during the distillation process and result in an imbalance of
information acquired by the student network. In this paper, we investigate the
impact of this imbalance and propose a novel method, named Balance Divergence
Distillation. By introducing a compensatory operation using reverse
Kullback-Leibler divergence, our method can improve the modeling of the
extremely small values in the negative from the teacher and preserve the
learning capacity for the positive. Furthermore, we test the impact of
different temperature coefficients adjustments, which may conducted to further
balance for knowledge transferring. We evaluate the proposed method on several
computer vision tasks, including image classification and semantic
segmentation. The evaluation results show that our method achieves an accuracy
improvement of 1%~3% for lightweight students on both CIFAR-100 and ImageNet
dataset, and a 4.55% improvement in mIoU for PSP-ResNet18 on the Cityscapes
dataset. The experiments show that our method is a simple yet highly effective
solution that can be smoothly applied to different knowledge distillation
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnoosh Koleini, Muhammad Usama Saleem, Pu Wang, Hongfei Xue, Ahmed Helmy, Abbey Fenwick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D human pose estimation from single-camera images and
videos have relied on parametric models, like SMPL. However, these models
oversimplify anatomical structures, limiting their accuracy in capturing true
joint locations and movements, which reduces their applicability in
biomechanics, healthcare, and robotics. Biomechanically accurate pose
estimation, on the other hand, typically requires costly marker-based motion
capture systems and optimization techniques in specialized labs. To bridge this
gap, we propose BioPose, a novel learning-based framework for predicting
biomechanically accurate 3D human pose directly from monocular videos. BioPose
includes three key components: a Multi-Query Human Mesh Recovery model
(MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose
refinement technique. MQ-HMR leverages a multi-query deformable transformer to
extract multi-scale fine-grained image features, enabling precise human mesh
recovery. NeurIK treats the mesh vertices as virtual markers, applying a
spatial-temporal network to regress biomechanically accurate 3D poses under
anatomical constraints. To further improve 3D pose estimations, a 2D-informed
refinement step optimizes the query tokens during inference by aligning the 3D
structure with 2D pose observations. Experiments on benchmark datasets
demonstrate that BioPose significantly outperforms state-of-the-art methods.
Project website:
\url{https://m-usamasaleem.github.io/publication/BioPose/BioPose.html}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Inverted Image Pyramid Networks for Visual Perception and
  Multimodal Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image pyramids are widely adopted in top-performing methods to obtain
multi-scale features for precise visual perception and understanding. However,
current image pyramids use the same large-scale model to process multiple
resolutions of images, leading to significant computational cost. To address
this challenge, we propose a novel network architecture, called
Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses
pretrained models (ViTs or CNNs) as branches to process multi-scale images,
where images of higher resolutions are processed by smaller network branches to
balance computational cost and performance. To integrate information from
different spatial scales, we further propose a novel cross-branch feature
interaction mechanism. To validate PIIP, we apply it to various perception
models and a representative multimodal large language model called LLaVA, and
conduct extensive experiments on various tasks such as object detection,
segmentation, image classification and multimodal understanding. PIIP achieves
superior performance compared to single-branch and existing multi-resolution
approaches with lower computational cost. When applied to InternViT-6B, a
large-scale vision foundation model, PIIP can improve its performance by 1%-2%
on detection and segmentation with only 40%-60% of the original computation,
finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For
multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and
74.5% on MMBench with only 2.8M training data. Our code is released at
https://github.com/OpenGVLab/PIIP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BMIP: Bi-directional Modality Interaction <span class="highlight-title">Prompt</span> Learning for VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song-Lin Lv, Yu-Yang Chen, Zhi Zhou, Ming Yang, Lan-Zhe Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have exhibited remarkable generalization
capabilities, and prompt learning for VLMs has attracted great attention for
the ability to adapt pre-trained VLMs to specific downstream tasks. However,
existing studies mainly focus on single-modal prompts or uni-directional
modality interaction, overlooking the powerful alignment effects resulting from
the interaction between the vision and language modalities. To this end, we
propose a novel prompt learning method called
$\underline{\textbf{B}}i-directional \underline{\textbf{M}}odality
\underline{\textbf{I}}nteraction \underline{\textbf{P}}rompt (BMIP)$, which
dynamically weights bi-modal information through learning the information of
the attention layer, enhancing trainability and inter-modal consistency
compared to simple information aggregation methods. To evaluate the
effectiveness of prompt learning methods, we propose a more realistic
evaluation paradigm called open-world generalization complementing the widely
adopted cross-dataset transfer and domain generalization tasks. Comprehensive
experiments on various datasets reveal that BMIP not only outperforms current
state-of-the-art methods across all three evaluation paradigms but is also
flexible enough to be combined with other prompt-based methods for consistent
performance enhancement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSReg: Prior-guided Sparse Mixture of Experts for Point Cloud
  Registration <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshui Huang, Zhou Huang, Yifan Zuo, Yongshun Gong, Chengdong Zhang, Deyang Liu, Yuming Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discriminative feature is crucial for point cloud registration. Recent
methods improve the feature discriminative by distinguishing between
non-overlapping and overlapping region points. However, they still face
challenges in distinguishing the ambiguous structures in the overlapping
regions. Therefore, the ambiguous features they extracted resulted in a
significant number of outlier matches from overlapping regions. To solve this
problem, we propose a prior-guided SMoE-based registration method to improve
the feature distinctiveness by dispatching the potential correspondences to the
same experts. Specifically, we propose a prior-guided SMoE module by fusing
prior overlap and potential correspondence embeddings for routing, assigning
tokens to the most suitable experts for processing. In addition, we propose a
registration framework by a specific combination of Transformer layer and
prior-guided SMoE module. The proposed method not only pays attention to the
importance of locating the overlapping areas of point clouds, but also commits
to finding more accurate correspondences in overlapping areas. Our extensive
experiments demonstrate the effectiveness of our method, achieving
state-of-the-art registration recall (95.7\%/79.3\%) on the 3DMatch/3DLoMatch
benchmark. Moreover, we also test the performance on ModelNet40 and demonstrate
excellent performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automotive Elevation Mapping with Interferometric Synthetic Aperture
  Radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyla A. Kabuli, Griffin Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radar is a low-cost and ubiquitous automotive sensor, but is limited by array
resolution and sensitivity when performing direction of arrival analysis.
Synthetic Aperture Radar (SAR) is a class of techniques to improve azimuth
resolution and sensitivity for radar. Interferometric SAR (InSAR) can be used
to extract elevation from the variations in phase measurements in SAR images.
Utilizing InSAR we show that a typical, low-resolution radar array mounted on a
vehicle can be used to accurately localize detections in 3D space for both
urban and agricultural environments. We generate point clouds in each
environment by combining InSAR with a signal processing scheme tailored to
automotive driving. This low-compute approach allows radar to be used as a
primary sensor to map fine details in complex driving environments, and be used
to make autonomous perception decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLAVARS: A Multimodal <span class="highlight-title">Foundation</span>al Language and Vision Alignment Model
  for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Corley, Simone Fobi Nsutezo, Anthony Ortiz, Caleb Robinson, Rahul Dodhia, Juan M. Lavista Ferres, Peyman Najafirad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing imagery is dense with objects and contextual visual
information. There is a recent trend to combine paired satellite images and
text captions for pretraining performant encoders for downstream tasks.
However, while contrastive image-text methods like CLIP enable vision-language
alignment and zero-shot classification ability, vision-only downstream
performance tends to degrade compared to image-only pretraining, such as MAE.
In this paper, we propose FLAVARS, a pretraining method that combines the best
of both contrastive learning and masked modeling, along with geospatial
alignment via contrastive location encoding. We find that FLAVARS significantly
outperforms a baseline of SkyCLIP for vision-only tasks such as KNN
classification and semantic segmentation, +6\% mIOU on SpaceNet1, while
retaining the ability to perform zero-shot classification, unlike MAE
pretrained methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Classical, Deep, and Generative Models for Human Activity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Meem Hossain, The Anh Han, Safina Showkat Ara, Zia Ush Shamszaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) has gained significant importance with the
growing use of sensor-equipped devices and large datasets. This paper evaluates
the performance of three categories of models : classical machine learning,
deep learning architectures, and Restricted Boltzmann Machines (RBMs) using
five key benchmark datasets of HAR (UCI-HAR, OPPORTUNITY, PAMAP2, WISDM, and
Berkeley MHAD). We assess various models, including Decision Trees, Random
Forests, Convolutional Neural Networks (CNN), and Deep Belief Networks (DBNs),
using metrics such as accuracy, precision, recall, and F1-score for a
comprehensive comparison. The results show that CNN models offer superior
performance across all datasets, especially on the Berkeley MHAD. Classical
models like Random Forest do well on smaller datasets but face challenges with
larger, more complex data. RBM-based models also show notable potential,
particularly for feature learning. This paper offers a detailed comparison to
help researchers choose the most suitable model for HAR tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 21 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Contextual Anomalies by Discovering Consistent Spatial Regions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengye Yang, Richard J. Radke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a method for modeling spatial context to enable video anomaly
detection. The main idea is to discover regions that share similar object-level
activities by clustering joint object attributes using Gaussian mixture models.
We demonstrate that this straightforward approach, using orders of magnitude
fewer parameters than competing models, achieves state-of-the-art performance
in the challenging spatial-context-dependent Street Scene dataset. As a side
benefit, the high-resolution discovered regions learned by the model also
provide explainable normalcy maps for human operators without the need for any
pre-trained segmentation model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Performance of Object Detection Models in Electron Microscopy
  Using Random Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ni Li, Ryan Jacobs, Matthew Lynch, Vidit Agrawal, Kevin Field, Dane Morgan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying prediction uncertainty when applying object detection models to
new, unlabeled datasets is critical in applied machine learning. This study
introduces an approach to estimate the performance of deep learning-based
object detection models for quantifying defects in transmission electron
microscopy (TEM) images, focusing on detecting irradiation-induced cavities in
TEM images of metal alloys. We developed a random forest regression model that
predicts the object detection F1 score, a statistical metric used to evaluate
the ability to accurately locate and classify objects of interest. The random
forest model uses features extracted from the predictions of the object
detection model whose uncertainty is being quantified, enabling fast prediction
on new, unlabeled images. The mean absolute error (MAE) for predicting F1 of
the trained model on test data is 0.09, and the $R^2$ score is 0.77, indicating
there is a significant correlation between the random forest regression model
predicted and true defect detection F1 scores. The approach is shown to be
robust across three distinct TEM image datasets with varying imaging and
material domains. Our approach enables users to estimate the reliability of a
defect detection and segmentation model predictions and assess the
applicability of the model to their specific datasets, providing valuable
information about possible domain shifts and whether the model needs to be
fine-tuned or trained on additional data to be maximally effective for the
desired use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Zero-Shot & Explainable Video Description by Reasoning over
  <span class="highlight-title">Graph</span>s of Events in Space and Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihai Masala, Marius Leordeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current era of Machine Learning, Transformers have become the de facto
approach across a variety of domains, such as computer vision and natural
language processing. Transformer-based solutions are the backbone of current
state-of-the-art methods for language generation, image and video
classification, segmentation, action and object recognition, among many others.
Interestingly enough, while these state-of-the-art methods produce impressive
results in their respective domains, the problem of understanding the
relationship between vision and language is still beyond our reach. In this
work, we propose a common ground between vision and language based on events in
space and time in an explainable and programmatic way, to connect
learning-based vision and language state of the art models and provide a
solution to the long standing problem of describing videos in natural language.
We validate that our algorithmic approach is able to generate coherent, rich
and relevant textual descriptions on videos collected from a variety of
datasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern
LLM-as-a-Jury approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RWKV-UNet: Improving UNet with Long-Range Cooperation for Effective
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntao Jiang, Jiangning Zhang, Weixuan Liu, Muxuan Gao, Xiaobin Hu, Xiaoxiao Yan, Feiyue Huang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there have been significant advancements in deep learning
for medical image analysis, especially with convolutional neural networks
(CNNs) and transformer models. However, CNNs face limitations in capturing
long-range dependencies while transformers suffer high computational
complexities. To address this, we propose RWKV-UNet, a novel model that
integrates the RWKV (Receptance Weighted Key Value) structure into the U-Net
architecture. This integration enhances the model's ability to capture
long-range dependencies and improve contextual understanding, which is crucial
for accurate medical image segmentation. We build a strong encoder with
developed inverted residual RWKV (IR-RWKV) blocks combining CNNs and RWKVs. We
also propose a Cross-Channel Mix (CCM) module to improve skip connections with
multi-scale feature fusion, achieving global channel information integration.
Experiments on benchmark datasets, including Synapse, ACDC, BUSI, CVC-ClinicDB,
CVC-ColonDB, Kvasir-SEG, ISIC 2017 and GLAS show that RWKV-UNet achieves
state-of-the-art performance on various types of medical image segmentation.
Additionally, smaller variants, RWKV-UNet-S and RWKV-UNet-T, balance accuracy
and computational efficiency, making them suitable for broader clinical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, Yi Wang, Yuming Jiang, Yaohui Wang, Peng Gao, Xinyuan Chen, Hengjie Li, Dahua Lin, Yu Qiao, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Vchitect-2.0, a parallel transformer architecture designed to
scale up video diffusion models for large-scale text-to-video generation. The
overall Vchitect-2.0 system has several key designs. (1) By introducing a novel
Multimodal Diffusion Block, our approach achieves consistent alignment between
text descriptions and generated video frames, while maintaining temporal
coherence across sequences. (2) To overcome memory and computational
bottlenecks, we propose a Memory-efficient Training framework that incorporates
hybrid parallelism and other memory reduction techniques, enabling efficient
training of long video sequences on distributed systems. (3) Additionally, our
enhanced data processing pipeline ensures the creation of Vchitect T2V
DataVerse, a high-quality million-scale training dataset through rigorous
annotation and aesthetic evaluation. Extensive benchmarking demonstrates that
Vchitect-2.0 outperforms existing methods in video quality, training
efficiency, and scalability, serving as a suitable base for high-fidelity video
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Poseidon: A ViT-based Architecture for Multi-Frame Pose Estimation with
  Adaptive Frame Weighting and Multi-Scale Feature Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cesare Davide Pace, Alessandro Marco De Nunzio, Claudio De Stefano, Francesco Fontanella, Mario Molinara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human pose estimation, a vital task in computer vision, involves detecting
and localising human joints in images and videos. While single-frame pose
estimation has seen significant progress, it often fails to capture the
temporal dynamics for understanding complex, continuous movements. We propose
Poseidon, a novel multi-frame pose estimation architecture that extends the
ViTPose model by integrating temporal information for enhanced accuracy and
robustness to address these limitations. Poseidon introduces key innovations:
(1) an Adaptive Frame Weighting (AFW) mechanism that dynamically prioritises
frames based on their relevance, ensuring that the model focuses on the most
informative data; (2) a Multi-Scale Feature Fusion (MSFF) module that
aggregates features from different backbone layers to capture both fine-grained
details and high-level semantics; and (3) a Cross-Attention module for
effective information exchange between central and contextual frames, enhancing
the model's temporal coherence. The proposed architecture improves performance
in complex video scenarios and offers scalability and computational efficiency
suitable for real-world applications. Our approach achieves state-of-the-art
performance on the PoseTrack21 and PoseTrack18 datasets, achieving mAP scores
of 88.3 and 87.8, respectively, outperforming existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FARE: A Deep Learning-Based Framework for Radar-based Face Recognition
  and Out-of-distribution Detection <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabri Mustafa Kahya, Boran Hamdi Sivrikaya, Muhammet Sami Yavuz, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel pipeline for face recognition and
out-of-distribution (OOD) detection using short-range FMCW radar. The proposed
system utilizes Range-Doppler and micro Range-Doppler Images. The architecture
features a primary path (PP) responsible for the classification of
in-distribution (ID) faces, complemented by intermediate paths (IPs) dedicated
to OOD detection. The network is trained in two stages: first, the PP is
trained using triplet loss to optimize ID face classification. In the second
stage, the PP is frozen, and the IPs-comprising simple linear autoencoder
networks-are trained specifically for OOD detection. Using our dataset
generated with a 60 GHz FMCW radar, our method achieves an ID classification
accuracy of 99.30% and an OOD detection AUROC of 96.91%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Gotin, Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have revealed that modern image and video quality assessment
(IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can
manipulate a video through preprocessing to artificially increase its quality
score according to a certain metric, despite no actual improvement in visual
quality. Most of the attacks studied in the literature are white-box attacks,
while black-box attacks in the context of VQA have received less attention.
Moreover, some research indicates a lack of transferability of adversarial
examples generated for one model to another when applied to VQA. In this paper,
we propose a cross-modal attack method, IC2VQA, aimed at exploring the
vulnerabilities of modern VQA models. This approach is motivated by the
observation that the low-level feature spaces of images and videos are similar.
We investigate the transferability of adversarial perturbations across
different modalities; specifically, we analyze how adversarial perturbations
generated on a white-box IQA model with an additional CLIP module can
effectively target a VQA model. The addition of the CLIP module serves as a
valuable aid in increasing transferability, as the CLIP model is known for its
effective capture of low-level semantics. Extensive experiments demonstrate
that IC2VQA achieves a high success rate in attacking three black-box VQA
models. We compare our method with existing black-box attack strategies,
highlighting its superiority in terms of attack success within the same number
of iterations and levels of attack strength. We believe that the proposed
method will contribute to the deeper analysis of robust VQA metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for VISAPP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BiDepth Multimodal Neural Network: Bidirectional Depth Deep Learning
  Arcitecture for Spatial-Temporal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Ehsani, Fenglian Pan, Qingpei Hu, Jian Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of spatial-temporal (ST) information in dynamic systems,
such as urban mobility and weather patterns, is a crucial yet challenging
problem. The complexity stems from the intricate interplay between spatial
proximity and temporal relevance, where both long-term trends and short-term
fluctuations are present in convoluted patterns. Existing approaches, including
traditional statistical methods and conventional neural networks, may provide
inaccurate results due to the lack of an effective mechanism that
simultaneously incorporates information at variable temporal depths while
maintaining spatial context, resulting in a trade-off between comprehensive
long-term historical analysis and responsiveness to short-term new information.
To bridge this gap, this paper proposes the BiDepth Multimodal Neural Network
(BDMNN) with bidirectional depth modulation that enables a comprehensive
understanding of both long-term seasonality and short-term fluctuations,
adapting to the complex ST context. Case studies with real-world public data
demonstrate significant improvements in prediction accuracy, with a 12%
reduction in Mean Squared Error for urban traffic prediction and a 15%
improvement in rain precipitation forecasting compared to state-of-the-art
benchmarks, without demanding extra computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to Applied Intelligence for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging 2D Masked Reconstruction for Domain Adaptation of 3D Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansoo Park, Chanwoo Kim, Jihyeon Kim, Hoseong Cho, Nhat Nguyen Bao Truong, Taehwan Kim, Seungryul Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RGB-based 3D pose estimation methods have been successful with the
development of deep learning and the emergence of high-quality 3D pose
datasets. However, most existing methods do not operate well for testing images
whose distribution is far from that of training data. However, most existing
methods do not operate well for testing images whose distribution is far from
that of training data. This problem might be alleviated by involving diverse
data during training, however it is non-trivial to collect such diverse data
with corresponding labels (i.e. 3D pose). In this paper, we introduced an
unsupervised domain adaptation framework for 3D pose estimation that utilizes
the unlabeled data in addition to labeled data via masked image modeling (MIM)
framework. Foreground-centric reconstruction and attention regularization are
further proposed to increase the effectiveness of unlabeled data usage.
Experiments are conducted on the various datasets in human and hand pose
estimation tasks, especially using the cross-domain scenario. We demonstrated
the effectiveness of ours by achieving the state-of-the-art accuracy on all
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Gaussian Splatting with Normal Information for Mesh Extraction and
  Improved Rendering <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meenakshi Krishnan, Liam Fowl, Ramani Duraiswami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable 3D Gaussian splatting has emerged as an efficient and flexible
rendering technique for representing complex scenes from a collection of 2D
views and enabling high-quality real-time novel-view synthesis. However, its
reliance on photometric losses can lead to imprecisely reconstructed geometry
and extracted meshes, especially in regions with high curvature or fine detail.
We propose a novel regularization method using the gradients of a signed
distance function estimated from the Gaussians, to improve the quality of
rendering while also extracting a surface mesh. The regularizing normal
supervision facilitates better rendering and mesh reconstruction, which is
crucial for downstream applications in video generation, animation, AR-VR and
gaming. We demonstrate the effectiveness of our approach on datasets such as
Mip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on
photorealism metrics compared to other mesh extracting rendering methods
without compromising mesh quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2025: Workshop on Generative Data Augmentation for Real-World
  Signal Processing Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Rate-In: Information-Driven Adaptive Dropout Rates for Improved
  Inference-Time Uncertainty Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tal Zeevi, Ravid Shwartz-Ziv, <span class="highlight-author">Yann LeCun</span>, Lawrence H. Staib, John A. Onofrey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty estimation is crucial for deploying neural networks in
risk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a
widely used technique for approximating predictive uncertainty by performing
stochastic forward passes with dropout during inference. However, using static
dropout rates across all layers and inputs can lead to suboptimal uncertainty
estimates, as it fails to adapt to the varying characteristics of individual
inputs and network layers. Existing approaches optimize dropout rates during
training using labeled data, resulting in fixed inference-time parameters that
cannot adjust to new data distributions, compromising uncertainty estimates in
Monte Carlo simulations.
  In this paper, we propose Rate-In, an algorithm that dynamically adjusts
dropout rates during inference by quantifying the information loss induced by
dropout in each layer's feature maps. By treating dropout as controlled noise
injection and leveraging information-theoretic principles, Rate-In adapts
dropout rates per layer and per input instance without requiring ground truth
labels. By quantifying the functional information loss in feature maps, we
adaptively tune dropout rates to maintain perceptual quality across diverse
medical imaging tasks and architectural configurations. Our extensive empirical
study on synthetic data and real-world medical imaging tasks demonstrates that
Rate-In improves calibration and sharpens uncertainty estimates compared to
fixed or heuristic dropout rates without compromising predictive performance.
Rate-In offers a practical, unsupervised, inference-time approach to optimizing
dropout for more reliable predictive uncertainty estimation in critical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated author affiliation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Eigen Models for Human Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current personalized neural head avatars face a trade-off: lightweight models
lack detail and realism, while high-quality, animatable avatars require
significant computational resources, making them unsuitable for commodity
devices. To address this gap, we introduce Gaussian Eigen Models (GEM), which
provide high-quality, lightweight, and easily controllable head avatars. GEM
utilizes 3D Gaussian primitives for representing the appearance combined with
Gaussian splatting for rendering. Building on the success of mesh-based 3D
morphable face models (3DMM), we define GEM as an ensemble of linear eigenbases
for representing the head appearance of a specific subject. In particular, we
construct linear bases to represent the position, scale, rotation, and opacity
of the 3D Gaussians. This allows us to efficiently generate Gaussian primitives
of a specific head shape by a linear combination of the basis vectors, only
requiring a low-dimensional parameter vector that contains the respective
coefficients. We propose to construct these linear bases (GEM) by distilling
high-quality compute-intense CNN-based Gaussian avatar models that can generate
expression-dependent appearance changes like wrinkles. These high-quality
models are trained on multi-view videos of a subject and are distilled using a
series of principal component analyses. Once we have obtained the bases that
represent the animatable appearance space of a specific human, we learn a
regressor that takes a single RGB image as input and predicts the
low-dimensional parameter vector that corresponds to the shown facial
expression. In a series of experiments, we compare GEM's self-reenactment and
cross-person reenactment results to state-of-the-art 3D avatar methods,
demonstrating GEM's higher visual quality and better generalization to new
expressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://zielon.github.io/gem/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems
  using Disparity Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Larey, Eyal Rond, Omer Achrack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition technologies are increasingly used in various applications,
yet they are vulnerable to face spoofing attacks. These spoofing attacks often
involve unique 3D structures, such as printed papers or mobile device screens.
Although stereo-depth cameras can detect such attacks effectively, their
high-cost limits their widespread adoption. Conversely, two-sensor systems
without extrinsic calibration offer a cost-effective alternative but are unable
to calculate depth using stereo techniques. In this work, we propose a method
to overcome this challenge by leveraging facial attributes to derive disparity
information and estimate relative depth for anti-spoofing purposes, using
non-calibrated systems. We introduce a multi-modal anti-spoofing model, coined
Disparity Model, that incorporates created disparity maps as a third modality
alongside the two original sensor modalities. We demonstrate the effectiveness
of the Disparity Model in countering various spoof attacks using a
comprehensive dataset collected from the Intel RealSense ID Solution F455. Our
method outperformed existing methods in the literature, achieving an Equal
Error Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False
Positive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the
errors of the best comparison method, respectively. Additionally, we introduce
a model ensemble that addresses 3D spoof attacks as well, achieving an EER of
2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a
state-of-the-art solution for the challenging task of anti-spoofing in
non-calibrated systems that lack depth information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RMem: Restricted Memory Banks Improve Video Object Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbao Zhou, Ziqi Pang, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent video object segmentation (VOS) benchmarks evolving to
challenging scenarios, we revisit a simple but overlooked strategy: restricting
the size of memory banks. This diverges from the prevalent practice of
expanding memory banks to accommodate extensive historical information. Our
specially designed "memory deciphering" study offers a pivotal insight
underpinning such a strategy: expanding memory banks, while seemingly
beneficial, actually increases the difficulty for VOS modules to decode
relevant features due to the confusion from redundant information. By
restricting memory banks to a limited number of essential frames, we achieve a
notable improvement in VOS accuracy. This process balances the importance and
freshness of frames to maintain an informative memory bank within a bounded
capacity. Additionally, restricted memory banks reduce the training-inference
discrepancy in memory lengths compared with continuous expansion. This fosters
new opportunities in temporal reasoning and enables us to introduce the
previously overlooked "temporal positional embedding." Finally, our insights
are embodied in "RMem" ("R" for restricted), a simple yet effective VOS
modification that excels at challenging VOS scenarios and establishes new state
of the art for object state changes (on the VOST dataset) and long videos (on
the Long Videos dataset). Our code and demo are available at
https://restricted-memory.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024, Project Page: https://restricted-memory.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FaVoR: Features via Voxel Rendering for Camera Relocalization <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07571v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07571v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincenzo Polizzi, Marco Cannici, Davide Scaramuzza, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera relocalization methods range from dense image alignment to direct
camera pose regression from a query image. Among these, sparse feature matching
stands out as an efficient, versatile, and generally lightweight approach with
numerous applications. However, feature-based methods often struggle with
significant viewpoint and appearance changes, leading to matching failures and
inaccurate pose estimates. To overcome this limitation, we propose a novel
approach that leverages a globally sparse yet locally dense 3D representation
of 2D features. By tracking and triangulating landmarks over a sequence of
frames, we construct a sparse voxel map optimized to render image patch
descriptors observed during tracking. Given an initial pose estimate, we first
synthesize descriptors from the voxels using volumetric rendering and then
perform feature matching to estimate the camera pose. This methodology enables
the generation of descriptors for unseen views, enhancing robustness to view
changes. We extensively evaluate our method on the 7-Scenes and Cambridge
Landmarks datasets. Our results show that our method significantly outperforms
existing state-of-the-art feature representation techniques in indoor
environments, achieving up to a 39% improvement in median translation error.
Additionally, our approach yields comparable results to other methods for
outdoor scenarios while maintaining lower memory and computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), Tucson, Arizona, US, Feb 28-Mar 4, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vid2Sim: Realistic and Interactive Simulation from Video for Urban
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xie, Zhizheng Liu, Zhenghao Peng, Wayne Wu, Bolei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sim-to-real gap has long posed a significant challenge for robot learning in
simulation, preventing the deployment of learned models in the real world.
Previous work has primarily focused on domain randomization and system
identification to mitigate this gap. However, these methods are often limited
by the inherent constraints of the simulation and graphics engines. In this
work, we propose Vid2Sim, a novel framework that effectively bridges the
sim2real gap through a scalable and cost-efficient real2sim pipeline for neural
3D scene reconstruction and simulation. Given a monocular video as input,
Vid2Sim can generate photorealistic and physically interactable 3D simulation
environments to enable the reinforcement learning of visual navigation agents
in complex urban environments. Extensive experiments demonstrate that Vid2Sim
significantly improves the performance of urban navigation in the digital twins
and real world by 31.2% and 68.3% in success rate compared with agents trained
with prior simulation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://metadriverse.github.io/vid2sim/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Compression Autoencoder for Efficient High-Resolution Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10733v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10733v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder
models for accelerating high-resolution diffusion models. Existing autoencoder
models have demonstrated impressive results at a moderate spatial compression
ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for
high spatial compression ratios (e.g., 64x). We address this challenge by
introducing two key techniques: (1) Residual Autoencoding, where we design our
models to learn residuals based on the space-to-channel transformed features to
alleviate the optimization difficulty of high spatial-compression autoencoders;
(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases
training strategy for mitigating the generalization penalty of high
spatial-compression autoencoders. With these designs, we improve the
autoencoder's spatial compression ratio up to 128 while maintaining the
reconstruction quality. Applying our DC-AE to latent diffusion models, we
achieve significant speedup without accuracy drop. For example, on ImageNet
512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup
on H100 GPU for UViT-H while achieving a better FID, compared with the widely
used SD-VAE-f8 autoencoder. Our code is available at
https://github.com/mit-han-lab/efficientvit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. First two authors contributed equally to this work. Update:
  add USiT (UViT+SiT sampler) results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling White-Box Transformers for Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20299v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20299v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinrui Yang, Xianhang Li, Druv Pai, Yuyin Zhou, Yi Ma, Yaodong Yu, Cihang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CRATE, a white-box transformer architecture designed to learn compressed and
sparse representations, offers an intriguing alternative to standard vision
transformers (ViTs) due to its inherent mathematical interpretability. Despite
extensive investigations into the scaling behaviors of language and vision
transformers, the scalability of CRATE remains an open question which this
paper aims to address. Specifically, we propose CRATE-$\alpha$, featuring
strategic yet minimal modifications to the sparse coding block in the CRATE
architecture design, and a light training recipe designed to improve the
scalability of CRATE. Through extensive experiments, we demonstrate that
CRATE-$\alpha$ can effectively scale with larger model sizes and datasets. For
example, our CRATE-$\alpha$-B substantially outperforms the prior best CRATE-B
model accuracy on ImageNet classification by 3.7%, achieving an accuracy of
83.2%. Meanwhile, when scaling further, our CRATE-$\alpha$-L obtains an
ImageNet classification accuracy of 85.1%. More notably, these model
performance improvements are achieved while preserving, and potentially even
enhancing the interpretability of learned CRATE models, as we demonstrate
through showing that the learned token representations of increasingly larger
trained CRATE-$\alpha$ models yield increasingly higher-quality unsupervised
object segmentation of images. The project page is
https://rayjryang.github.io/CRATE-alpha/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://rayjryang.github.io/CRATE-alpha/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of <span class="highlight-title">Foundation</span> Models in Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) are large-scale deep learning models that are
developed using large datasets and self-supervised learning methods. These
models serve as a base for different downstream tasks, including healthcare.
FMs have been adopted with great success across various domains within
healthcare. Existing healthcare-based surveys have not yet included all of
these domains. Therefore, we provide a detailed survey of FMs in healthcare. We
focus on the history, learning strategies, flagship models, applications, and
challenges of FMs. We explore how FMs such as the BERT and GPT families are
reshaping various healthcare domains, including clinical large language models,
medical image analysis, and omics. Furthermore, we provide a detailed taxonomy
of healthcare applications facilitated by FMs, such as clinical NLP, medical
computer vision, graph learning, and other biology-related tasks. Despite the
promising opportunities FMs provide, they also have several associated
challenges, which are explained in detail. We also outline open research issues
and potential lessons learned to provide researchers and practitioners with
insights into the capabilities of FMs in healthcare to advance their deployment
and mitigate associated risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-guided Image Restoration and Semantic Enhancement for Text-to-Image
  Person <span class="highlight-title">Retrie</span>val 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09059v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09059v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Yuan Dong, Nikolaos V. Boulgouris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific
person images according to the given textual descriptions. A primary challenge
in this task is bridging the substantial representational gap between visual
and textual modalities. The prevailing methods map texts and images into
unified embedding space for matching, while the intricate semantic
correspondences between texts and images are still not effectively constructed.
To address this issue, we propose a novel TIPR framework to build fine-grained
interactions and alignment between person images and the corresponding texts.
Specifically, via fine-tuning the Contrastive Language-Image Pre-training
(CLIP) model, a visual-textual dual encoder is firstly constructed, to
preliminarily align the image and text features. Secondly, a Text-guided Image
Restoration (TIR) auxiliary task is proposed to map abstract textual entities
to specific image regions, improving the alignment between local textual and
visual embeddings. Additionally, a cross-modal triplet loss is presented to
handle hard samples, and further enhance the model's discriminability for minor
differences. Moreover, a pruning-based text data augmentation approach is
proposed to enhance focus on essential elements in descriptions, thereby
avoiding excessive model attention to less significant information. The
experimental results show our proposed method outperforms state-of-the-art
methods on three popular benchmark datasets, and the code will be made publicly
available at https://github.com/Delong-liu-bupt/SEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was withdrawn due to a dispute among the authors regarding
  the content of the article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relaxed Rotational Equivariance via $G$-Biases in Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Wu, Yingjie Liu, Licheng Sun, Jian Yang, Hanlin Dong, Shing-Ho J. Lin, Xuan Tang, Jinpeng Mi, Bo Jin, Xian Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group Equivariant Convolution (GConv) can capture rotational equivariance
from original data. It assumes uniform and strict rotational equivariance
across all features as the transformations under the specific group. However,
the presentation or distribution of real-world data rarely conforms to strict
rotational equivariance, commonly referred to as Rotational Symmetry-Breaking
(RSB) in the system or dataset, making GConv unable to adapt effectively to
this phenomenon. Motivated by this, we propose a simple but highly effective
method to address this problem, which utilizes a set of learnable biases called
$G$-Biases under the group order to break strict group constraints and then
achieve a Relaxed Rotational Equivariant Convolution (RREConv). To validate the
efficiency of RREConv, we conduct extensive ablation experiments on the
discrete rotational group $\mathcal{C}_n$. Experiments demonstrate that the
proposed RREConv-based methods achieve excellent performance compared to
existing GConv-based methods in both classification and 2D object detection
tasks on the natural image datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feedback-driven object detection and iterative model improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated object detection has become increasingly valuable across diverse
applications, yet efficient, high-quality annotation remains a persistent
challenge. In this paper, we present the development and evaluation of a
platform designed to interactively improve object detection models. The
platform allows uploading and annotating images as well as fine-tuning object
detection models. Users can then manually review and refine annotations,
further creating improved snapshots that are used for automatic object
detection on subsequent image uploads - a process we refer to as semi-automatic
annotation resulting in a significant gain in annotation efficiency.
  Whereas iterative refinement of model results to speed up annotation has
become common practice, we are the first to quantitatively evaluate its
benefits with respect to time, effort, and interaction savings. Our
experimental results show clear evidence for a significant time reduction of up
to 53% for semi-automatic compared to manual annotation. Importantly, these
efficiency gains did not compromise annotation quality, while matching or
occasionally even exceeding the accuracy of manual annotations. These findings
demonstrate the potential of our lightweight annotation platform for creating
high-quality object detection datasets and provide best practices to guide
future development of annotation platforms.
  The platform is open-source, with the frontend and backend repositories
available on GitHub (https://github.com/ml-lab-htw/iterative-annotate). To
support the understanding of our labeling process, we have created an
explanatory video demonstrating the methodology using microscopy images of E.
coli bacteria as an example. The video is available on YouTube
(https://www.youtube.com/watch?v=CM9uhE8NN5E).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AI4EA24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark
  Detection <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jui-Che Chiang, Hou-Ning Hu, Bo-Syuan Hou, Chia-Yu Tseng, Yu-Lun Liu, Min-Hung Chen, Yen-Yu Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although facial landmark detection (FLD) has gained significant progress,
existing FLD methods still suffer from performance drops on partially
non-visible faces, such as faces with occlusions or under extreme lighting
conditions or poses. To address this issue, we introduce ORFormer, a novel
transformer-based method that can detect non-visible regions and recover their
missing features from visible parts. Specifically, ORFormer associates each
image patch token with one additional learnable token called the messenger
token. The messenger token aggregates features from all but its patch. This
way, the consensus between a patch and other patches can be assessed by
referring to the similarity between its regular and messenger embeddings,
enabling non-visible region identification. Our method then recovers occluded
patches with features aggregated by the messenger tokens. Leveraging the
recovered features, ORFormer compiles high-quality heatmaps for the downstream
FLD task. Extensive experiments show that our method generates heatmaps
resilient to partial occlusions. By integrating the resultant heatmaps into
existing FLD methods, our method performs favorably against the state of the
arts on challenging datasets such as WFLW and COFW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025 Project Link: https://ben0919.github.io/ORFormer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversified Augmentation with Domain Adaptation for Debiased Video
  Temporal Grounding <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlong Ren, Gangjian Zhang, Haifeng Sun, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding in videos (TSGV) faces challenges due to public
TSGV datasets containing significant temporal biases, which are attributed to
the uneven temporal distributions of target moments. Existing methods generate
augmented videos, where target moments are forced to have varying temporal
locations. However, since the video lengths of the given datasets have small
variations, only changing the temporal locations results in poor generalization
ability in videos with varying lengths. In this paper, we propose a novel
training framework complemented by diversified data augmentation and a domain
discriminator. The data augmentation generates videos with various lengths and
target moment locations to diversify temporal distributions. However, augmented
videos inevitably exhibit distinct feature distributions which may introduce
noise. To address this, we design a domain adaptation auxiliary task to
diminish feature discrepancies between original and augmented videos. We also
encourage the model to produce distinct predictions for videos with the same
text queries but different moment locations to promote debiased training.
Experiments on Charades-CD and ActivityNet-CD datasets demonstrate the
effectiveness and generalization abilities of our method in multiple grounding
structures, achieving state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSCViT: A Small-size ViT architecture with Multi-Scale Self-Attention
  Mechanism for Tiny <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowei Zhang, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformer (ViT) has demonstrated significant potential in various
vision tasks due to its strong ability in modelling long-range dependencies.
However, such success is largely fueled by training on massive samples. In real
applications, the large-scale datasets are not always available, and ViT
performs worse than Convolutional Neural Networks (CNNs) if it is only trained
on small scale dataset (called tiny dataset), since it requires large amount of
training data to ensure its representational capacity. In this paper, a
small-size ViT architecture with multi-scale self-attention mechanism and
convolution blocks is presented (dubbed MSCViT) to model different scales of
attention at each layer. Firstly, we introduced wavelet convolution, which
selectively combines the high-frequency components obtained by frequency
division with our convolution channel to extract local features. Then, a
lightweight multi-head attention module is developed to reduce the number of
tokens and computational costs. Finally, the positional encoding (PE) in the
backbone is replaced by a local feature extraction module. Compared with the
original ViT, it is parameter-efficient and is particularly suitable for tiny
datasets. Extensive experiments have been conducted on tiny datasets, in which
our model achieves an accuracy of 84.68% on CIFAR-100 with 14.0M parameters and
2.5 GFLOPs, without pre-training on large datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WINE: Wavelet-Guided GAN Inversion and Editing for High-Fidelity
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaewon Kim, Seung-Jun Moon, Gyeong-Moon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advanced GAN inversion models aim to convey high-fidelity information
from original images to generators through methods using generator tuning or
high-dimensional feature learning. Despite these efforts, accurately
reconstructing image-specific details remains as a challenge due to the
inherent limitations both in terms of training and structural aspects, leading
to a bias towards low-frequency information. In this paper, we look into the
widely used pixel loss in GAN inversion, revealing its predominant focus on the
reconstruction of low-frequency features. We then propose WINE, a
Wavelet-guided GAN Inversion aNd Editing model, which transfers the
high-frequency information through wavelet coefficients via newly proposed
wavelet loss and wavelet fusion scheme. Notably, WINE is the first attempt to
interpret GAN inversion in the frequency domain. Our experimental results
showcase the precision of WINE in preserving high-frequency details and
enhancing image quality. Even in editing scenarios, WINE outperforms existing
state-of-the-art GAN inversion models with a fine balance between editability
and reconstruction quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Du Chen, Liyi Chen, Zhengqiang Zhang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equipped with the continuous representation capability of Multi-Layer
Perceptron (MLP), Implicit Neural Representation (INR) has been successfully
employed for Arbitrary-scale Super-Resolution (ASR). However, the limited
receptive field of the linear layers in MLP restricts the representation
capability of INR, while it is computationally expensive to query the MLP
numerous times to render each pixel. Recently, Gaussian Splatting (GS) has
shown its advantages over INR in both visual quality and rendering speed in 3D
tasks, which motivates us to explore whether GS can be employed for the ASR
task. However, directly applying GS to ASR is exceptionally challenging because
the original GS is an optimization-based method through overfitting each single
scene, while in ASR we aim to learn a single model that can generalize to
different images and scaling factors. We overcome these challenges by
developing two novel techniques. Firstly, to generalize GS for ASR, we
elaborately design an architecture to predict the corresponding
image-conditioned Gaussians of the input low-resolution image in a feed-forward
manner. Secondly, we implement an efficient differentiable 2D GPU/CUDA-based
scale-aware rasterization to render super-resolved images by sampling discrete
RGB values from the predicted contiguous Gaussians. Via end-to-end training,
our optimized network, namely GSASR, can perform ASR for any image and unseen
scaling factors. Extensive experiments validate the effectiveness of our
proposed method. The project page can be found at
\url{https://mt-cly.github.io/GSASR.github.io/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Sub-<span class="highlight-title">graph</span> Distillation for Robust Semi-supervised Continual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Fan, Yu Wang, Pengfei Zhu, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) has shown promising results and comparable
performance to learning at once in a fully supervised manner. However, CL
strategies typically require a large number of labeled samples, making their
real-life deployment challenging. In this work, we focus on semi-supervised
continual learning (SSCL), where the model progressively learns from partially
labeled data with unknown categories. We provide a comprehensive analysis of
SSCL and demonstrate that unreliable distributions of unlabeled data lead to
unstable training and refinement of the progressing stages. This problem
severely impacts the performance of SSCL. To address the limitations, we
propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for
semi-supervised continual learning, which leverages both semantic and
structural information to achieve more stable knowledge distillation on
unlabeled data and exhibit robustness against distribution bias. Firstly, we
formalize a general model of structural distillation and design a dynamic graph
construction for the continual learning progress. Next, we define a structure
distillation vector and design a dynamic sub-graph distillation algorithm,
which enables end-to-end training and adaptability to scale up tasks. The
entire proposed method is adaptable to various CL methods and supervision
settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,
and ImageNet-100, with varying supervision ratios, demonstrate the
effectiveness of our proposed approach in mitigating the catastrophic
forgetting problem in semi-supervised continual learning scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Less is More: The Influence of Pruning on the Explainability of CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08878v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08878v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Merkle, David Weber, Pascal Schöttle, Stephan Schlögl, Martin Nocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last century, deep learning models have become the state-of-the-art
for solving complex computer vision problems. These modern computer vision
models have millions of parameters, which presents two major challenges: (1)
the increased computational requirements hamper the deployment in
resource-constrained environments, such as mobile or IoT devices, and (2)
explaining the complex decisions of such networks to humans is challenging.
Network pruning is a technical approach to reduce the complexity of models,
where less important parameters are removed. The work presented in this paper
investigates whether this reduction in technical complexity also helps with
perceived explainability. To do so, we conducted a pre-study and two
human-grounded experiments, assessing the effects of different pruning ratios
on explainability. Overall, we evaluate four different compression rates (i.e.,
2, 4, 8, and 32) with 37 500 tasks on Mechanical Turk. Results indicate that
lower compression rates have a positive influence on explainability, while
higher compression rates show negative effects. Furthermore, we were able to
identify sweet spots that increase both the perceived explainability and the
model's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spurious Feature Eraser: Stabilizing Test-Time Adaptation for
  Vision-Language <span class="highlight-title">Foundation</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language foundation models have exhibited remarkable success across a
multitude of downstream tasks due to their scalability on extensive image-text
paired data. However, these models also display significant limitations when
applied to downstream tasks, such as fine-grained image classification, as a
result of ``decision shortcuts'' that hinder their generalization capabilities.
In this work, we find that the CLIP model possesses a rich set of features,
encompassing both \textit{desired invariant causal features} and
\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP
on downstream tasks originates from its inability to effectively utilize
pre-trained features in accordance with specific task requirements. To address
this challenge, we propose a simple yet effective method, Spurious Feature
Eraser (SEraser), to alleviate the decision shortcuts by erasing the spurious
features. Specifically, we introduce a test-time prompt tuning paradigm that
optimizes a learnable prompt, thereby compelling the model to exploit invariant
features while disregarding decision shortcuts during the inference phase. The
proposed method effectively alleviates excessive dependence on potentially
misleading spurious information. We conduct comparative analysis of the
proposed method against various approaches which validates the significant
superiority.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImagiNet: A Multi-Content Benchmark for Synthetic Image Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delyan Boychev, Radostin Cholakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent generative models produce images with a level of authenticity that
makes them nearly indistinguishable from real photos and artwork. Potential
harmful use cases of these models, necessitate the creation of robust synthetic
image detectors. However, current datasets in the field contain generated
images with questionable quality or have examples from one predominant content
type which leads to poor generalizability of the underlying detectors. We find
that the curation of a balanced amount of high-resolution generated images
across various content types is crucial for the generalizability of detectors,
and introduce ImagiNet, a dataset of 200K examples, spanning four categories:
photos, paintings, faces, and miscellaneous. Synthetic images in ImagiNet are
produced with both open-source and proprietary generators, whereas real
counterparts for each content type are collected from public datasets. The
structure of ImagiNet allows for a two-track evaluation system: i)
classification as real or synthetic and ii) identification of the generative
model. To establish a strong baseline, we train a ResNet-50 model using a
self-supervised contrastive objective (SelfCon) for each track which achieves
evaluation AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%,
even under conditions that involve compression and resizing. The provided model
is generalizable enough to achieve zero-shot state-of-the-art performance on
previous synthetic detection benchmarks. We provide ablations to demonstrate
the importance of content types and publish code and data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Datasets and Evaluators of AI Safety, AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition
  via <span class="highlight-title">Foundation</span> Models <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02188v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02188v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjith George, Sebastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accuracy of face recognition systems has improved significantly in the
past few years, thanks to the large amount of data collected and advancements
in neural network architectures. However, these large-scale datasets are often
collected without explicit consent, raising ethical and privacy concerns. To
address this, there have been proposals to use synthetic datasets for training
face recognition models. Yet, such models still rely on real data to train the
generative models and generally exhibit inferior performance compared to those
trained on real datasets. One of these datasets, DigiFace, uses a graphics
pipeline to generate different identities and intra-class variations without
using real data in model training. However, the performance of this approach is
poor on face recognition benchmarks, possibly due to the lack of realism in the
images generated by the graphics pipeline. In this work, we introduce a novel
framework for realism transfer aimed at enhancing the realism of synthetically
generated face images. Our method leverages the large-scale face foundation
model, and we adapt the pipeline for realism enhancement. By integrating the
controllable aspects of the graphics pipeline with our realism enhancement
technique, we generate a large amount of realistic variations, combining the
advantages of both approaches. Our empirical evaluations demonstrate that
models trained using our enhanced dataset significantly improve the performance
of face recognition systems over the baseline. The source code and dataset will
be publicly accessible at the following link:
https://www.idiap.ch/paper/digi2real
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The dataset would be available here:
  https://www.idiap.ch/paper/digi2real Accepted for Publication in WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaTalk: Efficient Holistic Gesture Synthesis with Selective State
  Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09471v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09471v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gesture synthesis is a vital realm of human-computer interaction, with
wide-ranging applications across various fields like film, robotics, and
virtual reality. Recent advancements have utilized the diffusion model and
attention mechanisms to improve gesture synthesis. However, due to the high
computational complexity of these techniques, generating long and diverse
sequences with low latency remains a challenge. We explore the potential of
state space models (SSMs) to address the challenge, implementing a two-stage
modeling strategy with discrete motion priors to enhance the quality of
gestures. Leveraging the foundational Mamba block, we introduce MambaTalk,
enhancing gesture diversity and rhythm through multimodal integration.
Extensive experiments demonstrate that our method matches or exceeds the
performance of state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurlPS 2024, Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Agent: Leveraging LLMs For Audio Generation, Editing and
  Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Wang, Chi-Keung Tang, Yu-Wing Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Audio-Agent, a multimodal framework for audio generation,
editing and composition based on text or video inputs. Conventional approaches
for text-to-audio (TTA) tasks often make single-pass inferences from text
descriptions. While straightforward, this design struggles to produce
high-quality audio when given complex text conditions. In our method, we
utilize a pre-trained TTA diffusion network as the audio generation agent to
work in tandem with GPT-4, which decomposes the text condition into atomic,
specific instructions and calls the agent for audio generation. In doing so,
Audio-Agent can generate high-quality audio that is closely aligned with the
provided text or video exhibiting complex and multiple events, while supporting
variable-length and variable-volume generation. For video-to-audio (VTA) tasks,
most existing methods require training a timestamp detector to synchronize
video events with the generated audio, a process that can be tedious and
time-consuming. Instead, we propose a simpler approach by fine-tuning a
pre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both
semantic and temporal conditions that bridge the video and audio modality.
Consequently, our framework contributes a comprehensive solution for both TTA
and VTA tasks without substantial computational overhead in training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EventHallusion: Diagnosing Event Hallucinations in Video LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16597v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16597v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Na Zhao, Jingjing Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Multimodal Large Language Models (MLLMs) have made significant
progress in the video comprehension field. Despite remarkable content reasoning
and instruction following capabilities they demonstrated, the hallucination
problem of these VideoLLMs is less explored compared with its counterpart in
the image domain. To mitigate this gap, we propose EventHallusion, a novel
benchmark that focuses on assessing the VideoLLMs' hallucination toward event,
the crux of video analysis. From a hallucination attribution perspective, our
EventHallusion benchmark is curated to assess a VideoLLM's susceptibility
toward language priors and vision-language biases. On the other hand, we also
propose a simple yet effective method, called Temporal Contrastive Decoding
(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD
method rectifies the model's bias toward its priors during the decoding stage
by comparing the original video with a modified version, in which temporal cues
are disrupted. Through comprehensive evaluation of eight open-source and two
closed-source VideoLLMs on the proposed EventHallusion benchmark, we observe
that the open-source models suffer significantly from hallucination problems,
whereas the closed-source ones perform markedly better. By further equipping
open-source VideoLLMs with the proposed TCD approach, evident performance
improvements are achieved across most metrics in the EventHallusion benchmark.
Our codes and benchmark data are available at
https://github.com/Stevetich/EventHallusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System
  Model Fields with Generative Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Hess, Michael Aich, Baoxiang Pan, Niklas Boers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and high-resolution Earth system model (ESM) simulations are
essential to assess the ecological and socio-economic impacts of anthropogenic
climate change, but are computationally too expensive to be run at sufficiently
high spatial resolution. Recent machine learning approaches have shown
promising results in downscaling ESM simulations, outperforming
state-of-the-art statistical approaches. However, existing methods require
computationally costly retraining for each ESM and extrapolate poorly to
climates unseen during training. We address these shortcomings by learning a
consistency model (CM) that efficiently and accurately downscales arbitrary ESM
simulations without retraining in a zero-shot manner. Our approach yields
probabilistic downscaled fields at a resolution only limited by the
observational reference data. We show that the CM outperforms state-of-the-art
diffusion models at a fraction of computational cost while maintaining high
controllability on the downscaling task. Further, our method generalizes to
climate states unseen during training without explicitly formulated physical
constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Symmetries via Weight-Sharing with Doubly Stochastic Tensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Putri A. van der Linden, Alejandro García-Castellanos, Sharvaree Vadgama, Thijs P. Kuipers, Erik J. Bekkers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group equivariance has emerged as a valuable inductive bias in deep learning,
enhancing generalization, data efficiency, and robustness. Classically, group
equivariant methods require the groups of interest to be known beforehand,
which may not be realistic for real-world data. Additionally, baking in fixed
group equivariance may impose overly restrictive constraints on model
architecture. This highlights the need for methods that can dynamically
discover and apply symmetries as soft constraints. For neural network
architectures, equivariance is commonly achieved through group transformations
of a canonical weight tensor, resulting in weight sharing over a given group
$G$. In this work, we propose to learn such a weight-sharing scheme by defining
a collection of learnable doubly stochastic matrices that act as soft
permutation matrices on canonical weight tensors, which can take regular group
representations as a special case. This yields learnable kernel transformations
that are jointly optimized with downstream tasks. We show that when the dataset
exhibits strong symmetries, the permutation matrices will converge to regular
group representations and our weight-sharing networks effectively become
regular group convolutions. Additionally, the flexibility of the method enables
it to effectively pick up on partial symmetries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TextureCrop: Enhancing Synthetic Image Detection through Texture-based
  Cropping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15500v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15500v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Despina Konstantinidou, Christos Koutlis, Symeon Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI technologies produce increasingly realistic imagery, which,
despite its potential for creative applications, can also be misused to produce
misleading and harmful content. This renders Synthetic Image Detection (SID)
methods essential for identifying AI-generated content online. State-of-the-art
SID methods typically resize or center-crop input images due to architectural
or computational constraints, which hampers the detection of artifacts that
appear in high-resolution images. To address this limitation, we propose
TextureCrop, an image pre-processing component that can be plugged in any
pre-trained SID model to improve its performance. By focusing on high-frequency
image parts where generative artifacts are prevalent, TextureCrop enhances SID
performance with manageable memory requirements. Experimental results
demonstrate a consistent improvement in AUC across various detectors by 6.1%
compared to center cropping and by 15% compared to resizing, across
high-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.
Code available at https : //github.com/mever-team/texture-crop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformers and Large Language Models for Efficient Intrusion Detection
  Systems: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Kheddar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With significant advancements in Transformers LLMs, NLP has extended its
reach into many research fields due to its enhanced capabilities in text
generation and user interaction. One field benefiting greatly from these
advancements is cybersecurity. In cybersecurity, many parameters that need to
be protected and exchanged between senders and receivers are in the form of
text and tabular data, making NLP a valuable tool in enhancing the security
measures of communication protocols. This survey paper provides a comprehensive
analysis of the utilization of Transformers and LLMs in cyber-threat detection
systems. The methodology of paper selection and bibliometric analysis is
outlined to establish a rigorous framework for evaluating existing research.
The fundamentals of Transformers are discussed, including background
information on various cyber-attacks and datasets commonly used in this field.
The survey explores the application of Transformers in IDSs, focusing on
different architectures such as Attention-based models, LLMs like BERT and GPT,
CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.
Furthermore, it explores the diverse environments and applications where
Transformers and LLMs-based IDS have been implemented, including computer
networks, IoT devices, critical infrastructure protection, cloud computing,
SDN, as well as in autonomous vehicles. The paper also addresses research
challenges and future directions in this area, identifying key issues such as
interpretability, scalability, and adaptability to evolving threats, and more.
Finally, the conclusion summarizes the findings and highlights the significance
of Transformers and LLMs in enhancing cyber-threat detection capabilities,
while also outlining potential avenues for further research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.04760 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Decoders for Transformer-based Semantic Segmentation: A
  Compression Perspective <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qishuai Wen, Chun-Guang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for Transformer-based semantic segmentation
typically adopt Transformer decoders that are used to extract additional
embeddings from image embeddings via cross-attention, refine either or both
types of embeddings via self-attention, and project image embeddings onto the
additional embeddings via dot-product. Despite their remarkable success, these
empirical designs still lack theoretical justifications or interpretations,
thus hindering potentially principled improvements. In this paper, we argue
that there are fundamental connections between semantic segmentation and
compression, especially between the Transformer decoders and Principal
Component Analysis (PCA). From such a perspective, we derive a white-box, fully
attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the
interpretations as follows: 1) the self-attention operator refines image
embeddings to construct an ideal principal subspace that aligns with the
supervision and retains most information; 2) the cross-attention operator seeks
to find a low-rank approximation of the refined image embeddings, which is
expected to be a set of orthonormal bases of the principal subspace and
corresponds to the predefined classes; 3) the dot-product operation yields
compact representation for image embeddings as segmentation masks. Experiments
conducted on dataset ADE20K find that DEPICT consistently outperforms its
black-box counterpart, Segmenter, and it is light weight and more robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Masked Image Modeling to Avoid Model Collapse on Multi-modal
  MRI <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10377v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10377v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linxuan Han, Sa Xiao, Zimeng Li, Haidong Li, Xiuchao Zhao, Yeqing Han, Fumin Guo, Xin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal magnetic resonance imaging (MRI) provides information of lesions
for computer-aided diagnosis from different views. Deep learning algorithms are
suitable for identifying specific anatomical structures, segmenting lesions,
and classifying diseases. Manual labels are limited due to the high expense,
which hinders further improvement of accuracy. Self-supervised learning,
particularly masked image modeling (MIM), has shown promise in utilizing
unlabeled data. However, we spot model collapse when applying MIM to
multi-modal MRI datasets. The performance of downstream tasks does not see any
improvement following the collapsed model. To solve model collapse, we analyze
and address it in two types: complete collapse and dimensional collapse. We
find complete collapse occurs because the collapsed loss value in multi-modal
MRI datasets falls below the normally converged loss value. Based on this, the
hybrid mask pattern (HMP) masking strategy is introduced to elevate the
collapsed loss above the normally converged loss value and avoid complete
collapse. Additionally, we reveal that dimensional collapse stems from
insufficient feature uniformity in MIM. We mitigate dimensional collapse by
introducing the pyramid barlow twins (PBT) module as an explicit regularization
method. Overall, we construct the enhanced MIM (E-MIM) with HMP and PBT module
to avoid model collapse multi-modal MRI. Experiments are conducted on three
multi-modal MRI datasets to validate the effectiveness of our approach in
preventing both types of model collapse. By preventing model collapse, the
training of the model becomes more stable, resulting in a decent improvement in
performance for segmentation and classification tasks. The code is available at
https://github.com/LinxuanHan/E-MIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perception Matters: Enhancing Embodied AI with Uncertainty-Aware
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Prasanna, Daniel Honerkamp, Kshitij Sirohi, Tim Welschehold, Wolfram Burgard, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied AI has made significant progress acting in unexplored environments.
However, tasks such as object search have largely focused on efficient policy
learning. In this work, we identify several gaps in current search methods:
They largely focus on dated perception models, neglect temporal aggregation,
and transfer from ground truth directly to noisy perception at test time,
without accounting for the resulting overconfidence in the perceived state. We
address the identified problems through calibrated perception probabilities and
uncertainty across aggregation and found decisions, thereby adapting the models
for sequential tasks. The resulting methods can be directly integrated with
pretrained models across a wide family of existing search approaches at no
additional training cost. We perform extensive evaluations of aggregation
methods across both different semantic perception models and policies,
confirming the importance of calibrated uncertainties in both the aggregation
and found decisions. We make the code and trained models available at
https://semantic-search.cs.uni-freiburg.de.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TextureDiffusion: Target <span class="highlight-title">Prompt</span> Disentangled Editing for Various Texture
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Su, Junhao Zhuang, Chun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-guided image editing has achieved significant success.
However, existing methods can only apply simple textures like wood or gold when
changing the texture of an object. Complex textures such as cloud or fire pose
a challenge. This limitation stems from that the target prompt needs to contain
both the input image content and <texture>, restricting the texture
representation. In this paper, we propose TextureDiffusion, a tuning-free image
editing method applied to various texture transfer. Initially, the target
prompt is directly set to "<texture>", making the texture disentangled from the
input image content to enhance texture representation. Subsequently, query
features in self-attention and features in residual blocks are utilized to
preserve the structure of the input image. Finally, to maintain the background,
we introduce an edit localization technique which blends the self-attention
results and the intermediate latents. Comprehensive experiments demonstrate
that TextureDiffusion can harmoniously transfer various textures with excellent
structure and background preservation. Code is publicly available at
https://github.com/THU-CVML/TextureDiffusion
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ONER: Online Experience Replay for Incremental Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Jin, Jiahui Zhu, Guodong Wang, Shiwei Li, Jinjin Zhang, Qingjie Liu, Xinyue Liu, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental anomaly detection sequentially recognizes abnormal regions in
novel categories for dynamic industrial scenarios. This remains highly
challenging due to knowledge overwriting and feature conflicts, leading to
catastrophic forgetting. In this work, we propose ONER, an end-to-end ONline
Experience Replay method, which efficiently mitigates catastrophic forgetting
while adapting to new tasks with minimal cost. Specifically, our framework
utilizes two types of experiences from past tasks: decomposed prompts and
semantic prototypes, addressing both model parameter updates and feature
optimization. The decomposed prompts consist of learnable components that
assemble to produce attention-conditioned prompts. These prompts reuse
previously learned knowledge, enabling model to learn novel tasks effectively.
The semantic prototypes operate at both pixel and image levels, performing
regularization in the latent feature space to prevent forgetting across various
tasks. Extensive experiments demonstrate that our method achieves
state-of-the-art performance in incremental anomaly detection with
significantly reduced forgetting, as well as efficiently adapting to new
categories with minimal costs. These results confirm the efficiency and
stability of ONER, making it a powerful solution for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyFusion: Enhanced Reception Field Transformer for Hyperspectral Image
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04665v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04665v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Ming Lee, Yu-Fan Lin, Yu-Hao Ho, Li-Wei Kang, Chih-Chung Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) fusion addresses the challenge of reconstructing
High-Resolution HSIs (HR-HSIs) from High-Resolution Multispectral images
(HR-MSIs) and Low-Resolution HSIs (LR-HSIs), a critical task given the high
costs and hardware limitations associated with acquiring high-quality HSIs.
While existing methods leverage spatial and spectral relationships, they often
suffer from limited receptive fields and insufficient feature utilization,
leading to suboptimal performance. Furthermore, the scarcity of high-quality
HSI data highlights the importance of efficient data utilization to maximize
reconstruction quality. To address these issues, we propose HyFusion, a novel
Dual-Coupled Network (DCN) framework designed to enhance cross-domain feature
extraction and enable effective feature map reusing. The framework first
processes HR-MSI and LR-HSI inputs through specialized subnetworks that
mutually enhance each other during feature extraction, preserving complementary
spatial and spectral details. At its core, HyFusion utilizes an Enhanced
Reception Field Block (ERFB), which combines shifting-window attention and
dense connections to expand the receptive field, effectively capturing
long-range dependencies while minimizing information loss. Extensive
experiments demonstrate that HyFusion achieves state-of-the-art performance in
HR-MSI/LR-HSI fusion, significantly improving reconstruction quality while
maintaining a compact model size and computational efficiency. By integrating
enhanced receptive fields and feature map reusing into a coupled network
architecture, HyFusion provides a practical and effective solution for HSI
fusion in resource-constrained scenarios, setting a new benchmark in
hyperspectral imaging. Our code will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IGARSS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge-Guided <span class="highlight-title">Prompt</span> Learning for Deepfake Facial Image Detection <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Cheng Deng, Zhidong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent generative models demonstrate impressive performance on synthesizing
photographic images, which makes humans hardly to distinguish them from
pristine ones, especially on realistic-looking synthetic facial images.
Previous works mostly focus on mining discriminative artifacts from vast amount
of visual data. However, they usually lack the exploration of prior knowledge
and rarely pay attention to the domain shift between training categories (e.g.,
natural and indoor objects) and testing ones (e.g., fine-grained human facial
images), resulting in unsatisfactory detection performance. To address these
issues, we propose a novel knowledge-guided prompt learning method for deepfake
facial image detection. Specifically, we retrieve forgery-related prompts from
large language models as expert knowledge to guide the optimization of
learnable prompts. Besides, we elaborate test-time prompt tuning to alleviate
the domain shift, achieving significant performance improvement and boosting
the application in real-world scenarios. Extensive experiments on
DeepFakeFaceForensics dataset show that our proposed approach notably
outperforms state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11421v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11421v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wu, Fan Xu, Chong Chen, Xian-Sheng Hua, Xiao Luo, Haixin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the challenge of spatio-temporal video
prediction task, which involves generating future video frames based on
historical spatio-temporal observation streams. Existing approaches typically
utilize external information such as semantic maps to improve video prediction
accuracy, which often neglect the inherent physical knowledge embedded within
videos. Worse still, their high computational costs could impede their
applications for high-resolution videos. To address these constraints, we
introduce a novel framework called \underline{P}hysics-\underline{a}ssisted
\underline{S}patio-\underline{t}emporal \underline{Net}work (PastNet) for
high-quality video prediction. The core of PastNet lies in incorporating a
spectral convolution operator in the Fourier domain, which efficiently
introduces inductive biases from the underlying physical laws. Additionally, we
employ a memory bank with the estimated intrinsic dimensionality to discretize
local features during the processing of complex spatio-temporal signals,
thereby reducing computational costs and facilitating efficient high-resolution
video prediction. Extensive experiments on various widely-used spatio-temporal
video benchmarks demonstrate the effectiveness and efficiency of the proposed
PastNet compared with a range of state-of-the-art methods, particularly in
high-resolution scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DehazeGS: Seeing Through Fog with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current novel view synthesis tasks primarily rely on high-quality and clear
images. However, in foggy scenes, scattering and attenuation can significantly
degrade the reconstruction and rendering quality. Although NeRF-based dehazing
reconstruction algorithms have been developed, their use of deep fully
connected neural networks and per-ray sampling strategies leads to high
computational costs. Moreover, NeRF's implicit representation struggles to
recover fine details from hazy scenes. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly
modeling point clouds into 3D Gaussians. In this paper, we propose leveraging
the explicit Gaussian representation to explain the foggy image formation
process through a physically accurate forward rendering process. We introduce
DehazeGS, a method capable of decomposing and rendering a fog-free background
from participating media using only muti-view foggy images as input. We model
the transmission within each Gaussian distribution to simulate the formation of
fog. During this process, we jointly learn the atmospheric light and scattering
coefficient while optimizing the Gaussian representation of the hazy scene. In
the inference stage, we eliminate the effects of scattering and attenuation on
the Gaussians and directly project them onto a 2D plane to obtain a clear view.
Experiments on both synthetic and real-world foggy datasets demonstrate that
DehazeGS achieves state-of-the-art performance in terms of both rendering
quality and computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spacewalker: Traversing Representation Spaces for Fast Interactive
  Exploration and Annotation of Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Heine, Fabian Hörst, Jana Fragemann, Gijs Luijten, Jan Egger, Fin Bahnsen, M. Saquib Sarfraz, Jens Kleesiek, Constantin Seibold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industries such as healthcare, finance, and manufacturing, analysis of
unstructured textual data presents significant challenges for analysis and
decision making. Uncovering patterns within large-scale corpora and
understanding their semantic impact is critical, but depends on domain experts
or resource-intensive manual reviews. In response, we introduce Spacewalker in
this system demonstration paper, an interactive tool designed to analyze,
explore, and annotate data across multiple modalities. It allows users to
extract data representations, visualize them in low-dimensional spaces and
traverse large datasets either exploratory or by querying regions of interest.
We evaluated Spacewalker through extensive experiments and annotation studies,
assessing its efficacy in improving data integrity verification and annotation.
We show that Spacewalker reduces time and effort compared to traditional
methods. The code of this work is open-source and can be found at:
https://github.com/code-lukas/Spacewalker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06664v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06664v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Zhang, Xuechao Zou, Kai Li, Congyan Lang, Shiying Wang, Pin Tao, Tengfei Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained remote sensing image segmentation is essential for accurately
identifying detailed objects in remote sensing images. Recently, vision
transformer models (VTMs) pre-trained on large-scale datasets have demonstrated
strong zero-shot generalization. However, directly applying them to specific
tasks may lead to domain shift. We introduce a novel end-to-end learning
paradigm combining knowledge guidance with domain refinement to enhance
performance. We present two key components: the Feature Alignment Module (FAM)
and the Feature Modulation Module (FMM). FAM aligns features from a CNN-based
backbone with those from the pretrained VTM's encoder using channel
transformation and spatial interpolation, and transfers knowledge via KL
divergence and L2 normalization constraint. FMM further adapts the knowledge to
the specific domain to address domain shift. We also introduce a fine-grained
grass segmentation dataset and demonstrate, through experiments on two
datasets, that our method achieves a significant improvement of 2.57 mIoU on
the grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the
potential of combining knowledge transfer and domain adaptation to overcome
domain-related challenges and data limitations. The project page is available
at https://xavierjiezou.github.io/KTDA/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edicho: Consistent Image Editing in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21079v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21079v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyan Bai, Hao Ouyang, Yinghao Xu, Qiuyu Wang, Ceyuan Yang, Ka Leong Cheng, Yujun Shen, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a verified need, consistent editing across in-the-wild images remains a
technical challenge arising from various unmanageable factors, like object
poses, lighting conditions, and photography environments. Edicho steps in with
a training-free solution based on diffusion models, featuring a fundamental
design principle of using explicit image correspondence to direct editing.
Specifically, the key components include an attention manipulation module and a
carefully refined classifier-free guidance (CFG) denoising strategy, both of
which take into account the pre-estimated correspondence. Such an
inference-time algorithm enjoys a plug-and-play nature and is compatible to
most diffusion-based editing methods, such as ControlNet and BrushNet.
Extensive results demonstrate the efficacy of Edicho in consistent cross-image
editing under diverse settings. We will release the code to facilitate future
studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ant-research.github.io/edicho/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoPE: Mixture of <span class="highlight-title">Prompt</span> Experts for Parameter-Efficient and Scalable
  Multimodal Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10568v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10568v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Jiang, Lingbo Liu, Changwen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the demonstrated parameter efficiency of prompt-based multimodal
fusion methods, their limited adaptivity and expressiveness often result in
suboptimal performance compared to other tuning approaches. In this paper, we
introduce the Mixture of Prompt Experts (MoPE), the first technique designed to
overcome these limitations by decomposing standard prompts to capture
instance-level features adaptively. Building on this decomposition, MoPE
enhances prompt fusion's expressiveness by leveraging multimodal pairing priors
to route the most effective prompt for each instance dynamically. Compared to
vanilla prompting, our MoPE-based fusion method exhibits greater
expressiveness, scaling more effectively with the training data and the overall
number of trainable parameters. We also investigate regularization terms for
expert routing, which lead to emergent expert specialization with enhanced
adaptiveness and interpretablity. Extensive experiments across six multimodal
datasets spanning four modalities demonstrate state-of-the-art performance for
prompt fusion, matching or even surpassing the performance of fine-tuning while
requiring only 0.8% of the trainable parameters. Project homepage:
https://github.com/songrise/MoPE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review, Extended version of arxiv:2312.03734</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BIOMEDICA: An Open Biomedical Image-Caption Archive, <span class="highlight-title">Dataset</span>, and
  Vision-Language Models Derived from Scientific Literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey J Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Austin Wolfgang Katzer, Collin Chiu, Anita Rau, Xiaohan Wang, Yuhui Zhang, Alfred Seunghoon Song, Robert Tibshirani, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of vision-language models (VLMs) is driven by large-scale and
diverse multimodal datasets. However, progress toward generalist biomedical
VLMs is limited by the lack of annotated, publicly accessible datasets across
biology and medicine. Existing efforts are restricted to narrow domains,
missing the full diversity of biomedical knowledge encoded in scientific
literature. To address this gap, we introduce BIOMEDICA, a scalable,
open-source framework to extract, annotate, and serialize the entirety of the
PubMed Central Open Access subset into an easy-to-use, publicly accessible
dataset. Our framework produces a comprehensive archive with over 24 million
unique image-text pairs from over 6 million articles. Metadata and
expert-guided annotations are also provided. We demonstrate the utility and
accessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style
models continuously pre-trained on the BIOMEDICA dataset via streaming,
eliminating the need to download 27 TB of data locally. On average, our models
achieve state-of-the-art performance across 40 tasks - spanning pathology,
radiology, ophthalmology, dermatology, surgery, molecular biology,
parasitology, and cell biology - excelling in zero-shot classification with a
6.56% average improvement (as high as 29.8% and 17.5% in dermatology and
ophthalmology, respectively), and stronger image-text retrieval, all while
using 10x less compute. To foster reproducibility and collaboration, we release
our codebase and dataset for the broader research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recognizing Artistic Style of Archaeological Image Fragments Using Deep
  Style Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gur Elkin, Ofir Itzhak Shahar, Yaniv Ohayon, Nadav Alali, Ohad Ben-Shahar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ancient artworks obtained in archaeological excavations usually suffer from a
certain degree of fragmentation and physical degradation. Often, fragments of
multiple artifacts from different periods or artistic styles could be found on
the same site. With each fragment containing only partial information about its
source, and pieces from different objects being mixed, categorizing broken
artifacts based on their visual cues could be a challenging task, even for
professionals. As classification is a common function of many machine learning
models, the power of modern architectures can be harnessed for efficient and
accurate fragment classification. In this work, we present a generalized
deep-learning framework for predicting the artistic style of image fragments,
achieving state-of-the-art results for pieces with varying styles and
geometries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the 27th International Conference on
  Human-Computer Interaction (HCII 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flash Window Attention: speedup the attention computation for Swin
  Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhendong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the high resolution of image pixels, the Swin Transformer
introduces window attention. This mechanism divides an image into
non-overlapping windows and restricts attention computation to within each
window, significantly enhancing computational efficiency. To further optimize
this process, one might consider replacing standard attention with flash
attention, which has proven to be more efficient in language models. However, a
direct substitution is ineffective. Flash attention is designed for long
sequences, whereas window attention deals with shorter sequences but must
handle numerous of them in parallel. In this report, we present an optimized
solution called Flash Window Attention, tailored specifically for window
attention. Flash Window Attention improves attention computation efficiency by
up to 300% and enhances end-to-end runtime efficiency by up to 30%. Our code is
available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Infrastructure LiDAR Placement with Realistic LiDAR Simulation
  Library <span class="chip">ICRA'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15975v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15975v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Cai, Wentao Jiang, Runsheng Xu, Wenquan Zhao, Jiaqi Ma, Si Liu, Yikang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Vehicle-to-Everything(V2X) cooperative perception has attracted
increasing attention. Infrastructure sensors play a critical role in this
research field; however, how to find the optimal placement of infrastructure
sensors is rarely studied. In this paper, we investigate the problem of
infrastructure sensor placement and propose a pipeline that can efficiently and
effectively find optimal installation positions for infrastructure sensors in a
realistic simulated environment. To better simulate and evaluate LiDAR
placement, we establish a Realistic LiDAR Simulation library that can simulate
the unique characteristics of different popular LiDARs and produce
high-fidelity LiDAR point clouds in the CARLA simulator. Through simulating
point cloud data in different LiDAR placements, we can evaluate the perception
accuracy of these placements using multiple detection models. Then, we analyze
the correlation between the point cloud distribution and perception accuracy by
calculating the density and uniformity of regions of interest. Experiments show
that when using the same number and type of LiDAR, the placement scheme
optimized by our proposed method improves the average precision by 15%,
compared with the conventional placement scheme in the standard lane scene. We
also analyze the correlation between perception performance in the region of
interest and LiDAR point cloud distribution and validate that density and
uniformity can be indicators of performance. Both the RLS Library and related
code will be released at https://github.com/PJLab-ADG/PCSim.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, accepted to the IEEE International Conference on
  Robotics and Automation (ICRA'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Cascaded Dilated Convolution Approach for Mpox Lesion Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10106v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10106v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Deshmukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global outbreak of the Mpox virus, classified as a Public Health
Emergency of International Concern (PHEIC) by the World Health Organization,
presents significant diagnostic challenges due to its visual similarity to
other skin lesion diseases. Traditional diagnostic methods for Mpox, which rely
on clinical symptoms and laboratory tests, are slow and labor intensive. Deep
learning-based approaches for skin lesion classification offer a promising
alternative. However, developing a model that balances efficiency with accuracy
is crucial to ensure reliable and timely diagnosis without compromising
performance. This study introduces the Cascaded Atrous Group Attention (CAGA)
framework to address these challenges, combining the Cascaded Atrous Attention
module and the Cascaded Group Attention mechanism. The Cascaded Atrous
Attention module utilizes dilated convolutions and cascades the outputs to
enhance multi-scale representation. This is integrated into the Cascaded Group
Attention mechanism, which reduces redundancy in Multi-Head Self-Attention. By
integrating the Cascaded Atrous Group Attention module with EfficientViT-L1 as
the backbone architecture, this approach achieves state-of-the-art performance,
reaching an accuracy of 98% on the Mpox Close Skin Image (MCSI) dataset while
reducing model parameters by 37.5% compared to the original EfficientViT-L1.
The model's robustness is demonstrated through extensive validation on two
additional benchmark datasets, where it consistently outperforms existing
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, Submitted to Medical Imaging with Deep Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Neural Representations with Fourier Kolmogorov-Arnold Networks <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09323v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09323v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Mehrabian, Parsa Mojarad Adi, Moein Heidari, Ilker Hacihaliloglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representations (INRs) use neural networks to provide
continuous and resolution-independent representations of complex signals with a
small number of parameters. However, existing INR models often fail to capture
important frequency components specific to each task. To address this issue, in
this paper, we propose a Fourier Kolmogorov Arnold network (FKAN) for INRs. The
proposed FKAN utilizes learnable activation functions modeled as Fourier series
in the first layer to effectively control and learn the task-specific frequency
components. In addition, the activation functions with learnable Fourier
coefficients improve the ability of the network to capture complex patterns and
details, which is beneficial for high-resolution and high-dimensional data.
Experimental results show that our proposed FKAN model outperforms three
state-of-the-art baseline schemes, and improves the peak signal-to-noise ratio
(PSNR) and structural similarity index measure (SSIM) for the image
representation task and intersection over union (IoU) for the 3D occupancy
volume representation task, respectively. The code is available at
github.com/Ali-Meh619/FKAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Proc. IEEE ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient descent with generalized Newton's method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Bu, Shiyun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the generalized Newton's method (GeN) -- a Hessian-informed
approach that applies to any optimizer such as SGD and Adam, and covers the
Newton-Raphson method as a sub-case. Our method automatically and dynamically
selects the learning rate that accelerates the convergence, without the
intensive tuning of the learning rate scheduler. In practice, our method is
easily implementable, since it only requires additional forward passes with
almost zero computational overhead (in terms of training time and memory cost),
if the overhead is amortized over many iterations. We present extensive
experiments on language and vision tasks (e.g. GPT and ResNet) to showcase that
GeN optimizers match the state-of-the-art performance, which was achieved with
carefully tuned learning rate schedulers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaTrack: Exploiting Dual-Enhancement for Night UAV Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhui Zhang, Li Liu, Hao Wen, Xi Zhou, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Night unmanned aerial vehicle (UAV) tracking is impeded by the challenges of
poor illumination, with previous daylight-optimized methods demonstrating
suboptimal performance in low-light conditions, limiting the utility of UAV
applications. To this end, we propose an efficient mamba-based tracker,
leveraging dual enhancement techniques to boost night UAV tracking. The
mamba-based low-light enhancer, equipped with an illumination estimator and a
damage restorer, achieves global image enhancement while preserving the details
and structure of low-light images. Additionally, we advance a cross-modal mamba
network to achieve efficient interactive learning between vision and language
modalities. Extensive experiments showcase that our method achieves advanced
performance and exhibits significantly improved computation and memory
efficiency. For instance, our method is 2.8$\times$ faster than CiteTracker and
reduces 50.2$\%$ GPU memory. Our codes are available at
\url{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dissecting Query-Key Interaction in Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14880v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14880v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Pan, Aaron Philip, Ziqian Xie, Odelia Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-attention in vision transformers is often thought to perform perceptual
grouping where tokens attend to other tokens with similar embeddings, which
could correspond to semantically similar features of an object. However,
attending to dissimilar tokens can be beneficial by providing contextual
information. We propose to analyze the query-key interaction by the singular
value decomposition of the interaction matrix (i.e.
${\textbf{W}_q}^\top\textbf{W}_k$). We find that in many ViTs, especially those
with classification training objectives, early layers attend more to similar
tokens, while late layers show increased attention to dissimilar tokens,
providing evidence corresponding to perceptual grouping and contextualization,
respectively. Many of these interactions between features represented by
singular vectors are interpretable and semantic, such as attention between
relevant objects, between parts of an object, or between the foreground and
background. This offers a novel perspective on interpreting the attention
mechanism, which contributes to understanding how transformer models utilize
context and salient features when processing images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smartphone-based Eye Tracking System using Edge Intelligence and Model
  Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishan Gunawardena, Gough Yumu Lui, Jeewani Anupama Ginige, Bahman Javadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant limitation of current smartphone-based eye-tracking algorithms
is their low accuracy when applied to video-type visual stimuli, as they are
typically trained on static images. Also, the increasing demand for real-time
interactive applications like games, VR, and AR on smartphones requires
overcoming the limitations posed by resource constraints such as limited
computational power, battery life, and network bandwidth. Therefore, we
developed two new smartphone eye-tracking techniques for video-type visuals by
combining Convolutional Neural Networks (CNN) with two different Recurrent
Neural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent
Unit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean
Square Error of 0.955 cm and 1.091 cm, respectively. To address the
computational constraints of smartphones, we developed an edge intelligence
architecture to enhance the performance of smartphone-based eye tracking. We
applied various optimisation methods like quantisation and pruning to deep
learning models for better energy, CPU, and memory usage on edge devices,
focusing on real-time processing. Using model quantisation, the model inference
time in the CNN+LSTM and CNN+GRU models was reduced by 21.72% and 19.50%,
respectively, on edge devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I have included the three papers as reference, which are closely
  related. We have expanded the future work section to provide a more thorough
  discussion of the concepts of "varying lighting conditions" and "dynamic user
  environments." We have added a note below Table 4 to clarify the
  abbreviations' meaning. Elaborated the role of the Domain Expert within the
  presentation layer in Section 4.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Collection of a Human Robot Collaboration <span class="highlight-title">Dataset</span> for Cooperative
  Assembly in Glovebox Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivansh Sharma, Mathew Huang, Sanat Nair, Alan Wen, Christina Petlowany, Juston Moore, Selma Wanna, Mitch Pryor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industry 4.0 introduced AI as a transformative solution for modernizing
manufacturing processes. Its successor, Industry 5.0, envisions humans as
collaborators and experts guiding these AI-driven manufacturing solutions.
Developing these techniques necessitates algorithms capable of safe, real-time
identification of human positions in a scene, particularly their hands, during
collaborative assembly. Although substantial efforts have curated datasets for
hand segmentation, most focus on residential or commercial domains. Existing
datasets targeting industrial settings predominantly rely on synthetic data,
which we demonstrate does not effectively transfer to real-world operations.
Moreover, these datasets lack uncertainty estimations critical for safe
collaboration. Addressing these gaps, we present HAGS: Hand and Glove
Segmentation Dataset. This dataset provides challenging examples to build
applications toward hand and glove segmentation in industrial human-robot
collaboration scenarios as well as assess out-of-distribution images,
constructed via green screen augmentations, to determine ML-classifier
robustness. We study state-of-the-art, real-time segmentation models to
evaluate existing methods. Our dataset and baselines are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>draft paper to be submitted to IJRR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A systematic <span class="highlight-title">review</span> of the use of Deep Learning in Satellite Imagery for
  Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01272v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01272v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Victor, Zhen He, Aiden Nibali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agricultural research is essential for increasing food production to meet the
requirements of an increasing population in the coming decades. Recently,
satellite technology has been improving rapidly and deep learning has seen much
success in generic computer vision tasks and many application areas which
presents an important opportunity to improve analysis of agricultural land.
Here we present a systematic review of 150 studies to find the current uses of
deep learning on satellite imagery for agricultural research. Although we
identify 5 categories of agricultural monitoring tasks, the majority of the
research interest is in crop segmentation and yield prediction. We found that,
when used, modern deep learning methods consistently outperformed traditional
machine learning across most tasks; the only exception was that Long Short-Term
Memory (LSTM) Recurrent Neural Networks did not consistently outperform Random
Forests (RF) for yield prediction. The reviewed studies have largely adopted
methodologies from generic computer vision, except for one major omission:
benchmark datasets are not utilised to evaluate models across studies, making
it difficult to compare results. Additionally, some studies have specifically
utilised the extra spectral resolution available in satellite imagery, but
other divergent properties of satellite images - such as the hugely different
scales of spatial patterns - are not being taken advantage of in the reviewed
studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures and 10 tables in main paper. Final version, as
  submitted and accepted at JSTARS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot 3D Segmentation of Abdominal Organs in CT Scans Using Segment
  Anything Model 2: Adapting Video Tracking Capabilities for 3D Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06170v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06170v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yosuke Yamagishi, Shouhei Hanaoka, Tomohiro Kikuchi, Takahiro Nakao, Yuta Nakamura, Yukihiro Nomura, Soichiro Miki, Takeharu Yoshikawa, Osamu Abe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objectives: To evaluate the zero-shot performance of Segment Anything Model 2
(SAM 2) in 3D segmentation of abdominal organs in CT scans, and to investigate
the effects of prompt settings on segmentation results.
  Materials and Methods: In this retrospective study, we used a subset of the
TotalSegmentator CT dataset from eight institutions to assess SAM 2's ability
to segment eight abdominal organs. Segmentation was initiated from three
different z-coordinate levels (caudal, mid, and cranial levels) of each organ.
Performance was measured using the Dice similarity coefficient (DSC). We also
analyzed the impact of "negative prompts," which explicitly exclude certain
regions from the segmentation process, on accuracy.
  Results: 123 patients (mean age, 60.7 \pm 15.5 years; 63 men, 60 women) were
evaluated. As a zero-shot approach, larger organs with clear boundaries
demonstrated high segmentation performance, with mean DSCs as follows: liver
0.821 \pm 0.192, right kidney 0.862 \pm 0.212, left kidney 0.870 \pm 0.154, and
spleen 0.891 \pm 0.131. Smaller organs showed lower performance: gallbladder
0.531 \pm 0.291, pancreas 0.361 \pm 0.197, and adrenal glands, right 0.203 \pm
0.222, left 0.308 \pm 0.234. The initial slice for segmentation and the use of
negative prompts significantly influenced the results. By removing negative
prompts from the input, the DSCs significantly decreased for six organs.
  Conclusion: SAM 2 demonstrated promising zero-shot performance in segmenting
certain abdominal organs in CT scans, particularly larger organs. Performance
was significantly influenced by input negative prompts and initial slice
selection, highlighting the importance of optimizing these factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures (including 2 supplemental figure), 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XVertNet: Unsupervised Contrast Enhancement of Vertebral Structures with
  Dynamic Self-Tuning Guidance and Multi-Stage Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ella Eidlin, Assaf Hoogi, Hila Rozen, Mohammad Badarne, Nathan S. Netanyahu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-rays remain the primary diagnostic tool in emergency medicine, yet
their limited ability to capture fine anatomical details can result in missed
or delayed diagnoses. To address this, we introduce XVertNet, a novel
deep-learning framework designed to enhance vertebral structure visualization
in X-ray images significantly. Our framework introduces two key innovations:
(1) An unsupervised learning architecture that eliminates reliance on manually
labeled training data a persistent bottleneck in medical imaging, and (2) a
dynamic self-tuned internal guidance mechanism featuring an adaptive feedback
loop for real-time image optimization. Extensive validation across four major
public datasets revealed that XVertNet outperforms state-of-the-art enhancement
methods, as demonstrated by improvements in entropy scores, Tenengrad criterion
values, the local phase coherence sharpness index (LPC-SI), and thetone mapped
image quality index (TMQI). Furthermore, clinical validation conducted with two
board-certified radiologists confirmed that the enhanced images enabled more
sensitive detection of subtle vertebral fractures and degenerative changes. The
unsupervised nature of XVertNet facilitates immediate clinical deployment
without requiring additional training overhead. This innovation represents a
transformative advancement in emergency radiology, providing a scalable and
time-efficient solution to enhance diagnostic accuracy in high-pressure
clinical environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expressive Text-to-Image Generation with Rich Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06720v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06720v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songwei Ge, Taesung Park, Jun-Yan Zhu, Jia-Bin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plain text has become a prevalent interface for text-to-image synthesis.
However, its limited customization options hinder users from accurately
describing desired outputs. For example, plain text makes it hard to specify
continuous quantities, such as the precise RGB color value or importance of
each word. Furthermore, creating detailed text prompts for complex scenes is
tedious for humans to write and challenging for text encoders to interpret. To
address these challenges, we propose using a rich-text editor supporting
formats such as font style, size, color, and footnote. We extract each word's
attributes from rich text to enable local style control, explicit token
reweighting, precise color rendering, and detailed region synthesis. We achieve
these capabilities through a region-based diffusion process. We first obtain
each word's region based on attention maps of a diffusion process using plain
text. For each region, we enforce its text attributes by creating
region-specific detailed prompts and applying region-specific guidance, and
maintain its fidelity against plain-text generation through region-based
injections. We present various examples of image generation from rich text and
demonstrate that our method outperforms strong baselines with quantitative
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://rich-text-to-image.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09349v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09349v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Hao Lin, Bohan Liu, Yi-Ting Chen, Kuan-Sheng Chen, David Forsyth, Jia-Bin Huang, Anand Bhattad, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present UrbanIR (Urban Scene Inverse Rendering), a new inverse graphics
model that enables realistic, free-viewpoint renderings of scenes under various
lighting conditions with a single video. It accurately infers shape, albedo,
visibility, and sun and sky illumination from wide-baseline videos, such as
those from car-mounted cameras, differing from NeRF's dense view settings. In
this context, standard methods often yield subpar geometry and material
estimates, such as inaccurate roof representations and numerous 'floaters'.
UrbanIR addresses these issues with novel losses that reduce errors in inverse
graphics inference and rendering artifacts. Its techniques allow for precise
shadow volume estimation in the original scene. The model's outputs support
controllable editing, enabling photorealistic free-viewpoint renderings of
night simulations, relit scenes, and inserted objects, marking a significant
improvement over existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://urbaninverserendering.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SYNAPSE: SYmbolic Neural-Aided Preference Synthesis Engine <span class="chip">AAAI 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of preference learning, which aims to align
robot behaviors through learning user specific preferences (e.g. "good
pull-over location") from visual demonstrations. Despite its similarity to
learning factual concepts (e.g. "red door"), preference learning is a
fundamentally harder problem due to its subjective nature and the paucity of
person-specific training data. We address this problem using a novel framework
called SYNAPSE, which is a neuro-symbolic approach designed to efficiently
learn preferential concepts from limited data. SYNAPSE represents preferences
as neuro-symbolic programs, facilitating inspection of individual parts for
alignment, in a domain-specific language (DSL) that operates over images and
leverages a novel combination of visual parsing, large language models, and
program synthesis to learn programs representing individual preferences. We
perform extensive evaluations on various preferential concepts as well as user
case studies demonstrating its ability to align well with dissimilar user
preferences. Our method significantly outperforms baselines, especially when it
comes to out of distribution generalization. We show the importance of the
design choices in the framework through multiple ablation studies. Code,
additional results, and supplementary material can be found on the website:
https://amrl.cs.utexas.edu/synapse
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted (oral) at AAAI 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Performance of Point Cloud Completion Networks with
  Consistency Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07298v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07298v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Tirta Wijaya, Christofel Rio Goenawan, Seung-Hyun Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud completion networks are conventionally trained to minimize the
disparities between the completed point cloud and the ground-truth counterpart.
However, an incomplete object-level point cloud can have multiple valid
completion solutions when it is examined in isolation. This one-to-many mapping
issue can cause contradictory supervision signals to the network because the
loss function may produce different values for identical input-output pairs of
the network. In many cases, this issue could adversely affect the network
optimization process. In this work, we propose to enhance the conventional
learning objective using a novel completion consistency loss to mitigate the
one-to-many mapping problem. Specifically, the proposed consistency loss ensure
that a point cloud completion network generates a coherent completion solution
for incomplete objects originating from the same source point cloud.
Experimental results across multiple well-established datasets and benchmarks
demonstrated the proposed completion consistency loss have excellent capability
to enhance the completion performance of various existing networks without any
modification to the design of the networks. The proposed consistency loss
enhances the performance of the point completion network without affecting the
inference speed, thereby increasing the accuracy of point cloud completion.
Notably, a state-of-the-art point completion network trained with the proposed
consistency loss can achieve state-of-the-art accuracy on the challenging new
MVP dataset. The code and result of experiment various point completion models
using proposed consistency loss will be available at:
https://github.com/kaist-avelab/ConsistencyLoss .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First version of Paper "Enhancing Performance of Point Cloud
  Completion Networks with Consistency Loss" by Kevin Tirta Wijaya and
  Christofel Rio Goenawan. In process submission to Neurocomputing Journal 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Hu, Rong Liu, Meida Chen, Peter Beerel, Andrew Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving high-fidelity 3D reconstruction from monocular video remains
challenging due to the inherent limitations of traditional methods like
Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene
details. While differentiable rendering techniques such as Neural Radiance
Fields (NeRF) address some of these challenges, their high computational costs
make them unsuitable for real-time applications. Additionally, existing 3D
Gaussian Splatting (3DGS) methods often focus on photometric consistency,
neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and
pose updates for scene refinement. We propose a framework integrating dense
SLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach
introduces SLAM-Informed Adaptive Densification, which dynamically updates and
densifies the Gaussian model by leveraging dense point clouds from SLAM.
Additionally, we incorporate Geometry-Guided Optimization, which combines
edge-aware geometric constraints and photometric consistency to jointly
optimize the appearance and geometry of the 3DGS scene representation, enabling
detailed and accurate SLAM mapping reconstruction. Experiments on the Replica
and TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving
state-of-the-art results among monocular systems. Specifically, our method
achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,
representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the
previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by
10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the
potential of our framework in bridging the gap between photometric and
geometric dense 3D scene representations, paving the way for practical and
efficient monocular dense reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Geometry of Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Randall Balestriero, Ahmed Imtiaz Humayun, Richard Baraniuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we overview one promising avenue of progress at the
mathematical foundation of deep learning: the connection between deep networks
and function approximation by affine splines (continuous piecewise linear
functions in multiple dimensions). In particular, we will overview work over
the past decade on understanding certain geometrical properties of a deep
network's affine spline mapping, in particular how it tessellates its input
space. As we will see, the affine spline connection and geometrical viewpoint
provide a powerful portal through which to view, analyze, and improve the inner
workings of a deep network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at 'Notices of the American Mathematical
  Society'</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriMod Fusion for Multimodal Named Entity Recognition in Social Media <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mosab Alfaqeeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms serve as invaluable sources of user-generated content,
offering insights into various aspects of human behavior. Named Entity
Recognition (NER) plays a crucial role in analyzing such content by identifying
and categorizing named entities into predefined classes. However, traditional
NER models often struggle with the informal, contextually sparse, and ambiguous
nature of social media language. To address these challenges, recent research
has focused on multimodal approaches that leverage both textual and visual cues
for enhanced entity recognition. Despite advances, existing methods face
limitations in capturing nuanced mappings between visual objects and textual
entities and addressing distributional disparities between modalities. In this
paper, we propose a novel approach that integrates textual, visual, and hashtag
features (TriMod), utilizing Transformer-attention for effective modality
fusion. The improvements exhibited by our model suggest that named entities can
greatly benefit from the auxiliary context provided by multiple modalities,
enabling more accurate recognition. Through the experiments on a multimodal
social media dataset, we demonstrate the superiority of our approach over
existing state-of-the-art methods, achieving significant improvements in
precision, recall, and F1 score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CASCON</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eliciting In-context <span class="highlight-title">Retrie</span>val and Reasoning for Long-context Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context language models (LCLMs) promise to
transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With
their expanded context windows, LCLMs can process entire knowledge bases and
perform retrieval and reasoning directly -- a capability we define as
In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like
LOFT often overestimate LCLM performance by providing overly simplified
contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs
in more realistic scenarios by including confounding passages retrieved with
strong retrievers. We then propose three methods to enhance LCLM performance:
(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which
uses attention heads to filter and de-noise long contexts during decoding, and
(3) joint retrieval head training alongside the generation head. Our evaluation
of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with
our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on
LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised
fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks
despite being a much smaller model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Query Routing for <span class="highlight-title">Retrie</span>val Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiteng Mu, Liwen Zhang, Yong Jiang, Wenjie Li, Zhen Zhang, Pengjun Xie, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query routing for retrieval-augmented generation aims to assign an input
query to the most suitable search engine. Existing works rely heavily on
supervised datasets that require extensive manual annotation, resulting in high
costs and limited scalability, as well as poor generalization to
out-of-distribution scenarios. To address these challenges, we introduce a
novel unsupervised method that constructs the "upper-bound" response to
evaluate the quality of retrieval-augmented responses. This evaluation enables
the decision of the most suitable search engine for a given query. By
eliminating manual annotations, our approach can automatically process
large-scale real user queries and create training data. We conduct extensive
experiments across five datasets, demonstrating that our method significantly
enhances scalability and generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mathematical Information <span class="highlight-title">Retrie</span>val: Search and Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11646v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11646v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Zanibbi, Behrooz Mansouri, Anurag Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical information is essential for technical work, but its creation,
interpretation, and search are challenging. To help address these challenges,
researchers have developed multimodal search engines and mathematical question
answering systems. This book begins with a simple framework characterizing the
information tasks that people and systems perform as we work to answer
math-related questions. The framework is used to organize and relate the other
core topics of the book, including interactions between people and systems,
representing math formulas in sources, and evaluation. We close by addressing
some key questions and presenting directions for future work. This book is
intended for students, instructors, and researchers interested in systems that
help us find and use mathematical information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[DRAFT] Revised (3rd) draft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spacewalker: Traversing Representation Spaces for Fast Interactive
  Exploration and Annotation of Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Heine, Fabian Hörst, Jana Fragemann, Gijs Luijten, Jan Egger, Fin Bahnsen, M. Saquib Sarfraz, Jens Kleesiek, Constantin Seibold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industries such as healthcare, finance, and manufacturing, analysis of
unstructured textual data presents significant challenges for analysis and
decision making. Uncovering patterns within large-scale corpora and
understanding their semantic impact is critical, but depends on domain experts
or resource-intensive manual reviews. In response, we introduce Spacewalker in
this system demonstration paper, an interactive tool designed to analyze,
explore, and annotate data across multiple modalities. It allows users to
extract data representations, visualize them in low-dimensional spaces and
traverse large datasets either exploratory or by querying regions of interest.
We evaluated Spacewalker through extensive experiments and annotation studies,
assessing its efficacy in improving data integrity verification and annotation.
We show that Spacewalker reduces time and effort compared to traditional
methods. The code of this work is open-source and can be found at:
https://github.com/code-lukas/Spacewalker
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">160</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Equilibrium in Online Learning: Theory and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios N. Angelopoulos, Michael I. Jordan, Ryan J. Tibshirani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new perspective on online learning that we refer to as gradient
equilibrium: a sequence of iterates achieves gradient equilibrium if the
average of gradients of losses along the sequence converges to zero. In
general, this condition is not implied by nor implies sublinear regret. It
turns out that gradient equilibrium is achievable by standard online learning
methods such as gradient descent and mirror descent with constant step sizes
(rather than decaying step sizes, as is usually required for no regret).
Further, as we show through examples, gradient equilibrium translates into an
interpretable and meaningful property in online prediction problems spanning
regression, classification, quantile estimation, and others. Notably, we show
that the gradient equilibrium framework can be used to develop a debiasing
scheme for black-box predictions under arbitrary distribution shift, based on
simple post hoc online descent updates. We also show that post hoc gradient
updates can be used to calibrate predicted quantiles under distribution shift,
and that the framework leads to unbiased Elo scores for pairwise preference
prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at
  https://github.com/aangelopoulos/gradient-equilibrium/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Similarity Measure Between Functions with Applications to Statistical
  Learning and Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengpiao Huang, Kaizheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this note, we present a novel measure of similarity between two functions.
It quantifies how the sub-optimality gaps of two functions convert to each
other, and unifies several existing notions of functional similarity. We show
that it has convenient operation rules, and illustrate its use in empirical
risk minimization and non-stationary online optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Adversarial Post-Training for One-Step Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion models are widely used for image and video generation, but
their iterative generation process is slow and expansive. While existing
distillation approaches have demonstrated the potential for one-step generation
in the image domain, they still suffer from significant quality degradation. In
this work, we propose Adversarial Post-Training (APT) against real data
following diffusion pre-training for one-step video generation. To improve the
training stability and quality, we introduce several improvements to the model
architecture and training procedures, along with an approximated R1
regularization objective. Empirically, our experiments show that our
adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,
24fps videos in real time using a single forward evaluation step. Additionally,
our model is capable of generating 1024px images in a single step, achieving
quality comparable to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Loss Prediction Using Machine Learning with Extended Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Ethier, Mathieu Chateauvert, Ryan G. Dempsey, Alexis Bose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wireless communications rely on path loss modeling, which is most effective
when it includes the physical details of the propagation environment. Acquiring
this data has historically been challenging, but geographic information system
data is becoming increasingly available with higher resolution and accuracy.
Access to such details enables propagation models to more accurately predict
coverage and minimize interference in wireless deployments. Machine
learning-based modeling can significantly support this effort, with
feature-based approaches allowing for accurate, efficient, and scalable
propagation modeling. Building on previous work, we introduce an extended set
of features that improves prediction accuracy while, most importantly,
maintaining model generalization across a broad range of environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures, conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking <span class="highlight-title">Graph</span> Representations and <span class="highlight-title">Graph</span> Neural Networks for
  Multivariate Time Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wennuo Yang, Shiling Wu, Yuzhi Zhou, Weicheng Xie, Linlin Shen, Siyang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate Time Series Classification (MTSC) enables the analysis if
complex temporal data, and thus serves as a cornerstone in various real-world
applications, ranging from healthcare to finance. Since the relationship among
variables in MTS usually contain crucial cues, a large number of graph-based
MTSC approaches have been proposed, as the graph topology and edges can
explicitly represent relationships among variables (channels), where not only
various MTS graph representation learning strategies but also different Graph
Neural Networks (GNNs) have been explored. Despite such progresses, there is no
comprehensive study that fairly benchmarks and investigates the performances of
existing widely-used graph representation learning strategies/GNN classifiers
in the application of different MTSC tasks. In this paper, we present the first
benchmark which systematically investigates the effectiveness of the
widely-used three node feature definition strategies, four edge feature
learning strategies and five GNN architecture, resulting in 60 different
variants for graph-based MTSC. These variants are developed and evaluated with
a standardized data pipeline and training/validation/testing strategy on 26
widely-used suspensor MTSC datasets. Our experiments highlight that node
features significantly influence MTSC performance, while the visualization of
edge features illustrates why adaptive edge learning outperforms other edge
feature learning methods. The code of the proposed benchmark is publicly
available at
\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polynomial Threshold Functions of Bounded Tree-Width: Some
  Explainability and Complexity Aspects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karine Chubarian, Johnny Joyce, Gyorgy Turan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tree-width of a multivariate polynomial is the tree-width of the
hypergraph with hyperedges corresponding to its terms. Multivariate polynomials
of bounded tree-width have been studied by Makowsky and Meer as a new sparsity
condition that allows for polynomial solvability of problems which are
intractable in general. We consider a variation on this theme for Boolean
variables. A representation of a Boolean function as the sign of a polynomial
is called a polynomial threshold representation. We discuss Boolean functions
representable as polynomial threshold functions of bounded tree-width and
present two applications to Bayesian network classifiers, a probabilistic
graphical model. Both applications are in Explainable Artificial Intelligence
(XAI), the research area dealing with the black-box nature of many recent
machine learning models. We also give a separation result between the
representational power of positive and general polynomial threshold functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 figures. To be published in Festschrift in honor of
  Johann A. Makowsky</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Avoiding subtraction and division of stochastic signals using
  normalizing flows: NFdeconvolve 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Pessoa, Max Schweiger, Lance W. Q. Xu, Tristan Manha, Ayush Saurabh, Julian Antolin Camarena, Steve Pressé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Across the scientific realm, we find ourselves subtracting or dividing
stochastic signals. For instance, consider a stochastic realization, $x$,
generated from the addition or multiplication of two stochastic signals $a$ and
$b$, namely $x=a+b$ or $x = ab$. For the $x=a+b$ example, $a$ can be
fluorescence background and $b$ the signal of interest whose statistics are to
be learned from the measured $x$. Similarly, when writing $x=ab$, $a$ can be
thought of as the illumination intensity and $b$ the density of fluorescent
molecules of interest. Yet dividing or subtracting stochastic signals amplifies
noise, and we ask instead whether, using the statistics of $a$ and the
measurement of $x$ as input, we can recover the statistics of $b$. Here, we
show how normalizing flows can generate an approximation of the probability
distribution over $b$, thereby avoiding subtraction or division altogether.
This method is implemented in our software package, NFdeconvolve, available on
GitHub with a tutorial linked in the main text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Bayesian Neural Networks Explicitly Model Input Uncertainty? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Valdenegro-Toro, Marco Zullich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inputs to machine learning models can have associated noise or uncertainties,
but they are often ignored and not modelled. It is unknown if Bayesian Neural
Networks and their approximations are able to consider uncertainty in their
inputs. In this paper we build a two input Bayesian Neural Network (mean and
standard deviation) and evaluate its capabilities for input uncertainty
estimation across different methods like Ensembles, MC-Dropout, and Flipout.
Our results indicate that only some uncertainty estimation methods for
approximate Bayesian NNs can model input uncertainty, in particular Ensembles
and Flipout.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures, VISAPP 2025 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding Interpre<span class="highlight-title">table</span> Logic Rules from Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuqin Geng, Xiaojie Xu, Zhaoyue Wang, Ziyu Zhao, Xujie Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep neural networks continue to excel across various domains, their
black-box nature has raised concerns about transparency and trust. In
particular, interpretability has become increasingly essential for applications
that demand high safety and knowledge rigor, such as drug discovery, autonomous
driving, and genomics. However, progress in understanding even the simplest
deep neural networks - such as fully connected networks - has been limited,
despite their role as foundational elements in state-of-the-art models like
ResNet and Transformer. In this paper, we address this challenge by introducing
NeuroLogic, a novel approach for decoding interpretable logic rules from neural
networks. NeuroLogic leverages neural activation patterns to capture the
model's critical decision-making processes, translating them into logical rules
represented by hidden predicates. Thanks to its flexible design in the
grounding phase, NeuroLogic can be adapted to a wide range of neural networks.
For simple fully connected neural networks, hidden predicates can be grounded
in certain split patterns of original input features to derive
decision-tree-like rules. For large, complex vision neural networks, NeuroLogic
grounds hidden predicates into high-level visual concepts that are
understandable to humans. Our empirical study demonstrates that NeuroLogic can
extract global and interpretable rules from state-of-the-art models such as
ResNet, a task at which existing work struggles. We believe NeuroLogic can help
pave the way for understanding the black-box nature of neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Driven Water Segmentation with deep learning models for Enhanced
  Flood Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjida Afrin Mou, Tasfia Noor Chowdhury, Adib Ibn Mannan, Sadia Nourin Mim, Lubana Tarannum, Tasrin Noman, Jamal Uddin Ahamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flooding is a major natural hazard causing significant fatalities and
economic losses annually, with increasing frequency due to climate change.
Rapid and accurate flood detection and monitoring are crucial for mitigating
these impacts. This study compares the performance of three deep learning
models UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid in
flood detection, utilizing images from drones, in field observations, and
social media. This study involves creating a new dataset that augments
wellknown benchmark datasets with flood-specific images, enhancing the
robustness of the models. The UNet, ResNet, and DeepLab v3 architectures are
tested to determine their effectiveness in various environmental conditions and
geographical locations, and the strengths and limitations of each model are
also discussed here, providing insights into their applicability in different
scenarios by predicting image segmentation masks. This fully automated approach
allows these models to isolate flooded areas in images, significantly reducing
processing time compared to traditional semi-automated methods. The outcome of
this study is to predict segmented masks for each image effected by a flood
disaster and the validation accuracy of these models. This methodology
facilitates timely and continuous flood monitoring, providing vital data for
emergency response teams to reduce loss of life and economic damages. It offers
a significant reduction in the time required to generate flood maps, cutting
down the manual processing time. Additionally, we present avenues for future
research, including the integration of multimodal data sources and the
development of robust deep learning architectures tailored specifically for
flood detection tasks. Overall, our work contributes to the advancement of
flood management strategies through innovative use of deep learning
technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiplayer Federated Learning: Reaching Equilibrium with Less
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        TaeHo Yoon, Sayantan Choudhury, Nicolas Loizou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Federated Learning (FL) approaches assume collaborative clients
with aligned objectives working towards a shared global model. However, in many
real-world scenarios, clients act as rational players with individual
objectives and strategic behaviors, a concept that existing FL frameworks are
not equipped to adequately address. To bridge this gap, we introduce
Multiplayer Federated Learning (MpFL), a novel framework that models the
clients in the FL environment as players in a game-theoretic context, aiming to
reach an equilibrium. In this scenario, each player tries to optimize their own
utility function, which may not align with the collective goal. Within MpFL, we
propose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithm
in which each player/client performs local updates independently and
periodically communicates with other players. We theoretically analyze
PEARL-SGD and prove that it reaches a neighborhood of equilibrium with less
communication in the stochastic setup compared to its non-local counterpart.
Finally, we verify our theoretical findings through numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FDPP: Fine-tune Diffusion Policy with Human Preference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Chen, Devesh K. Jha, Masayoshi Tomizuka, Diego Romeres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from human demonstrations enables robots to perform
complex manipulation tasks and has recently witnessed huge success. However,
these techniques often struggle to adapt behavior to new preferences or changes
in the environment. To address these limitations, we propose Fine-tuning
Diffusion Policy with Human Preference (FDPP). FDPP learns a reward function
through preference-based learning. This reward is then used to fine-tune the
pre-trained policy with reinforcement learning (RL), resulting in alignment of
pre-trained policy with new human preferences while still solving the original
task. Our experiments across various robotic tasks and preferences demonstrate
that FDPP effectively customizes policy behavior without compromising
performance. Additionally, we show that incorporating Kullback-Leibler (KL)
regularization during fine-tuning prevents over-fitting and helps maintain the
competencies of the initial policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eliciting In-context <span class="highlight-title">Retrie</span>val and Reasoning for Long-context Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context language models (LCLMs) promise to
transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With
their expanded context windows, LCLMs can process entire knowledge bases and
perform retrieval and reasoning directly -- a capability we define as
In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like
LOFT often overestimate LCLM performance by providing overly simplified
contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs
in more realistic scenarios by including confounding passages retrieved with
strong retrievers. We then propose three methods to enhance LCLM performance:
(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which
uses attention heads to filter and de-noise long contexts during decoding, and
(3) joint retrieval head training alongside the generation head. Our evaluation
of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with
our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on
LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised
fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks
despite being a much smaller model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful
  Behaviors with Proximity Constraints <span class="chip">AAAI 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Nöther, Adish Singla, Goran Radanović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has proposed automated red-teaming methods for testing the
vulnerabilities of a given target large language model (LLM). These methods use
red-teaming LLMs to uncover inputs that induce harmful behavior in a target
LLM. In this paper, we study red-teaming strategies that enable a targeted
security assessment. We propose an optimization framework for red-teaming with
proximity constraints, where the discovered prompts must be similar to
reference prompts from a given dataset. This dataset serves as a template for
the discovered prompts, anchoring the search for test-cases to specific topics,
writing styles, or types of harmful behavior. We show that established
auto-regressive model architectures do not perform well in this setting. We
therefore introduce a black-box red-teaming method inspired by text-diffusion
models: Diffusion for Auditing and Red-Teaming (DART). DART modifies the
reference prompt by perturbing it in the embedding space, directly controlling
the amount of change introduced. We systematically evaluate our method by
comparing its effectiveness with established methods based on model fine-tuning
and zero- and few-shot prompting. Our results show that DART is significantly
more effective at discovering harmful inputs in close proximity to the
reference prompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of a paper published at AAAI 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Deep Active Learning for Medical Imaging: Replay-Base
  Architecture for Context Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Daniel, M. Rita Verdelho, Catarina Barata, Carlos Santiago
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning for medical imaging faces challenges in adapting and
generalizing to new contexts. Additionally, it often lacks sufficient labeled
data for specific tasks requiring significant annotation effort. Continual
Learning (CL) tackles adaptability and generalizability by enabling lifelong
learning from a data stream while mitigating forgetting of previously learned
knowledge. Active Learning (AL) reduces the number of required annotations for
effective training. This work explores both approaches (CAL) to develop a novel
framework for robust medical image analysis. Based on the automatic recognition
of shifts in image characteristics, Replay-Base Architecture for Context
Adaptation (RBACA) employs a CL rehearsal method to continually learn from
diverse contexts, and an AL component to select the most informative instances
for annotation. A novel approach to evaluate CAL methods is established using a
defined metric denominated IL-Score, which allows for the simultaneous
assessment of transfer learning, forgetting, and final model performance. We
show that RBACA works in domain and class-incremental learning scenarios, by
assessing its IL-Score on the segmentation and diagnosis of cardiac images. The
results show that RBACA outperforms a baseline framework without CAL, and a
state-of-the-art CAL method across various memory sizes and annotation budgets.
Our code is available in https://github.com/RuiDaniel/RBACA .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kannan Parthasarathy, Karthik Vaidhyanathan, Rudra Dhar, Venkat Krishnamachari, Basil Muhammed, Adyansh Kakran, Sreemaee Akshathala, Shrikara Arun, Sumant Dubey, Mohan Veerubhotla, Amey Karan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud Operations (CloudOps) is a rapidly growing field focused on the
automated management and optimization of cloud infrastructure which is
essential for organizations navigating increasingly complex cloud environments.
MontyCloud Inc. is one of the major companies in the CloudOps domain that
leverages autonomous bots to manage cloud compliance, security, and continuous
operations. To make the platform more accessible and effective to the
customers, we leveraged the use of GenAI.
  Developing a GenAI-based solution for autonomous CloudOps for the existing
MontyCloud system presented us with various challenges such as i) diverse data
sources; ii) orchestration of multiple processes; and iii) handling complex
workflows to automate routine tasks. To this end, we developed MOYA, a
multi-agent framework that leverages GenAI and balances autonomy with the
necessary human control. This framework integrates various internal and
external systems and is optimized for factors like task orchestration,
security, and error mitigation while producing accurate, reliable, and relevant
insights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our
multi-agent system with the help of practitioners as well as using automated
checks demonstrate enhanced accuracy, responsiveness, and effectiveness over
non-agentic approaches across complex workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted as full paper to CAIN 2025
  (https://conf.researchr.org/home/cain-2025), co-located with ICSE 2025
  (https://conf.researchr.org/home/icse-2025). The paper was submitted to CAIN
  for review on 9 November 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images
  using Choquet Integral and Differential Evolution Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Reza Takhsha, Maryam Rastgarpour, Mozhgan Naderi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has profoundly impacted billions globally. It
challenges public health and healthcare systems due to its rapid spread and
severe respiratory effects. An effective strategy to mitigate the COVID-19
pandemic involves integrating testing to identify infected individuals. While
RT-PCR is considered the gold standard for diagnosing COVID-19, it has some
limitations such as the risk of false negatives. To address this problem, this
paper introduces a novel Deep Learning Diagnosis System that integrates
pre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble
learning framework to achieve precise identification of COVID-19 cases from
Chest X-ray (CXR) images. We combine feature vectors from the final hidden
layers of pre-trained DCNNs using the Choquet integral to capture interactions
between different DCNNs that a linear approach cannot. We employed
Sugeno-$\lambda$ measure theory to derive fuzzy measures for subsets of
networks to enable aggregation. We utilized Differential Evolution to estimate
fuzzy densities. We developed a TensorFlow-based layer for Choquet operation to
facilitate efficient aggregation, due to the intricacies involved in
aggregating feature vectors. Experimental results on the COVIDx dataset show
that our ensemble model achieved 98\% accuracy in three-class classification
and 99.50\% in binary classification, outperforming its components-DenseNet-201
(97\% for three-class, 98.75\% for binary), Inception-v3 (96.25\% for
three-class, 98.50\% for binary), and Xception (94.50\% for three-class, 98\%
for binary)-and surpassing many previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Model and Preprocessing Verification for Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbiao Li, Anisa Halimi, Xiaoqian Jiang, Jaideep Vaidya, Erman Ayday
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework for privacy-preserving verification of
machine learning models, focusing on models trained on sensitive data.
Integrating Local Differential Privacy (LDP) with model explanations from LIME
and SHAP, our framework enables robust verification without compromising
individual privacy. It addresses two key tasks: binary classification, to
verify if a target model was trained correctly by applying the appropriate
preprocessing steps, and multi-class classification, to identify specific
preprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,
and Student Record-demonstrate that while the ML-based approach is particularly
effective in binary tasks, the threshold-based method performs comparably in
multi-class tasks. Results indicate that although verification accuracy varies
across datasets and noise levels, the framework provides effective detection of
preprocessing errors, strong privacy guarantees, and practical applicability
for safeguarding sensitive data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David Muñoz-Valero, Giovanni Montana, Luis Jimenez-Linares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses a critical challenge in the high-speed passenger railway
industry: designing effective dynamic pricing strategies in the context of
competing and cooperating operators. To address this, a multi-agent
reinforcement learning (MARL) framework based on a non-zero-sum Markov game is
proposed, incorporating random utility models to capture passenger decision
making. Unlike prior studies in areas such as energy, airlines, and mobile
networks, dynamic pricing for railway systems using deep reinforcement learning
has received limited attention. A key contribution of this paper is a
parametrisable and versatile reinforcement learning simulator designed to model
a variety of railway network configurations and demand patterns while enabling
realistic, microscopic modelling of user behaviour, called RailPricing-RL. This
environment supports the proposed MARL framework, which models heterogeneous
agents competing to maximise individual profits while fostering cooperative
behaviour to synchronise connecting services. Experimental results validate the
framework, demonstrating how user preferences affect MARL performance and how
pricing policies influence passenger choices, utility, and overall system
dynamics. This study provides a foundation for advancing dynamic pricing
strategies in railway systems, aligning profitability with system-wide
efficiency, and supporting future research on optimising pricing policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeineb Haouari, Jonas Weidner, Ivan Ezhov, Aswathi Varma, Daniel Rueckert, Bjoern Menze, Benedikt Wiestler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glioblastoma, a highly aggressive brain tumor, poses major challenges due to
its poor prognosis and high morbidity rates. Partial differential
equation-based models offer promising potential to enhance therapeutic outcomes
by simulating patient-specific tumor behavior for improved radiotherapy
planning. However, model calibration remains a bottleneck due to the high
computational demands of optimization methods like Monte Carlo sampling and
evolutionary algorithms. To address this, we recently introduced an approach
leveraging a neural forward solver with gradient-based optimization to
significantly reduce calibration time. This approach requires a highly accurate
and fully differentiable forward model. We investigate multiple architectures,
including (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a
3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best
overall results, excelling in both tumor outline matching and voxel-level
prediction of tumor cell concentration. It halved the MSE relative to the
baseline model and achieved the highest Dice score across all tumor cell
concentration thresholds. Our study demonstrates significant enhancement in
forward solver performance and outlines important future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Big Batch Bayesian Active Learning by Considering Predictive
  Probabilities <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian W. Ober, Samuel Power, Tom Diethe, Henry B. Moss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We observe that BatchBALD, a popular acquisition function for batch Bayesian
active learning for classification, can conflate epistemic and aleatoric
uncertainty, leading to suboptimal performance. Motivated by this observation,
we propose to focus on the predictive probabilities, which only exhibit
epistemic uncertainty. The result is an acquisition function that not only
performs better, but is also faster to evaluate, allowing for larger batches
than before.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures; presented as a lightning talk at the NeurIPS
  Workshop on Bayesian Decision-making and Uncertainty (BDU; 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Energy Efficiency and Performance Trade-offs in LLM
  Inference Across Tasks and DVFS Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown significant improvements in many
natural language processing (NLP) tasks, accelerating their rapid adoption
across many industries. These models are resource-intensive, requiring
extensive computational resources both during training and inference, leading
to increased energy consumption and negative environmental impact. As their
adoption accelerates, the sustainability of LLMs has become a critical issue,
necessitating strategies to optimize their runtime efficiency without
compromising performance. Hence, it is imperative to identify the parameters
that significantly influence the performance and energy efficiency of LLMs. To
that end, in this work, we investigate the effect of important parameters on
the performance and energy efficiency of LLMs during inference and examine
their trade-offs.
  First, we analyze how different types of models with varying numbers of
parameters and architectures perform on tasks like text generation, question
answering, and summarization by benchmarking LLMs such as Falcon-7B,
Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study
input and output sequence characteristics such as sequence length concerning
energy consumption, performance, and throughput. Finally, we explore the impact
of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency
Scaling (DVFS), on the models' latency and energy efficiency. Our extensive
benchmarking and statistical analysis reveal many interesting findings,
uncovering how specific optimizations can reduce energy consumption while
maintaining throughput and accuracy. This study provides actionable insights
for researchers and practitioners to design energy-efficient LLM inference
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Feature Maps for Quantum Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navneet Singh, Shiva Raj Pokhrel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Machine Learning (QML) offers significant potential for complex tasks
like genome sequence classification, but quantum noise on Noisy
Intermediate-Scale Quantum (NISQ) devices poses practical challenges. This
study systematically evaluates how various quantum noise models including
dephasing, amplitude damping, depolarizing, thermal noise, bit-flip, and
phase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and feature
mapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Results
indicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN are
more sensitive, particularly to depolarizing and amplitude-damping noise. The
PauliFeatureMap is especially vulnerable, highlighting difficulties in
maintaining accurate classification under noisy conditions. These findings
underscore the critical importance of feature map selection and noise
mitigation strategies in optimizing QML for genomic classification, with
promising implications for personalized medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven system identification using quadratic embeddings of
  nonlinear dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Klus, Joel-Pascal N'Konzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel data-driven method called QENDy (Quadratic Embedding of
Nonlinear Dynamics) that not only allows us to learn quadratic representations
of highly nonlinear dynamical systems, but also to identify the governing
equations. The approach is based on an embedding of the system into a
higher-dimensional feature space in which the dynamics become quadratic. Just
like SINDy (Sparse Identification of Nonlinear Dynamics), our method requires
trajectory data, time derivatives for the training data points, which can also
be estimated using finite difference approximations, and a set of preselected
basis functions, called dictionary. We illustrate the efficacy and accuracy of
QENDy with the aid of various benchmark problems and compare its performance
with SINDy and a deep learning method for identifying quadratic embeddings.
Furthermore, we analyze the convergence of QENDy and SINDy in the infinite data
limit, highlight their similarities and main differences, and compare the
quadratic embedding with linearization techniques based on the Koopman
operator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Globally Convergent Variational Inference <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Declan McNamara, Jackson Loper, Jeffrey Regier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In variational inference (VI), an approximation of the posterior distribution
is selected from a family of distributions through numerical optimization. With
the most common variational objective function, known as the evidence lower
bound (ELBO), only convergence to a local optimum can be guaranteed. In this
work, we instead establish the global convergence of a particular VI method.
This VI method, which may be considered an instance of neural posterior
estimation (NPE), minimizes an expectation of the inclusive (forward) KL
divergence to fit a variational distribution that is parameterized by a neural
network. Our convergence result relies on the neural tangent kernel (NTK) to
characterize the gradient dynamics that arise from considering the variational
objective in function space. In the asymptotic regime of a fixed,
positive-definite neural tangent kernel, we establish conditions under which
the variational objective admits a unique solution in a reproducing kernel
Hilbert space (RKHS). Then, we show that the gradient descent dynamics in
function space converge to this unique function. In ablation studies and
practical problems, we demonstrate that our results explain the behavior of NPE
in non-asymptotic finite-neuron settings, and show that NPE outperforms
ELBO-based optimization, which often converges to shallow local optima.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CWEval: Outcome-driven Evaluation on Functionality and Security of LLM
  Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have significantly aided developers by
generating or assisting in code writing, enhancing productivity across various
tasks. While identifying incorrect code is often straightforward, detecting
vulnerabilities in functionally correct code is more challenging, especially
for developers with limited security knowledge, which poses considerable
security risks of using LLM-generated code and underscores the need for robust
evaluation benchmarks that assess both functional correctness and security.
Current benchmarks like CyberSecEval and SecurityEval attempt to solve it but
are hindered by unclear and impractical specifications, failing to assess both
functionality and security accurately. To tackle these deficiencies, we
introduce CWEval, a novel outcome-driven evaluation framework designed to
enhance the evaluation of secure code generation by LLMs. This framework not
only assesses code functionality but also its security simultaneously with
high-quality task specifications and outcome-driven test oracles which provides
high accuracy. Coupled with CWEval-bench, a multilingual, security-critical
coding benchmark, CWEval provides a rigorous empirical security evaluation on
LLM-generated code, overcoming previous benchmarks' shortcomings. Through our
evaluations, CWEval reveals a notable portion of functional but insecure code
produced by LLMs, and shows a serious inaccuracy of previous evaluations,
ultimately contributing significantly to the field of secure code generation.
We open-source our artifact at: https://github.com/Co1lin/CWEval .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in LLM4Code 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Deep Hyperspectral Inpainting with the Plug and Play and
  Deep Image Prior Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Li, Mehrdad Yaghoobi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images are typically composed of hundreds of narrow and
contiguous spectral bands, each containing information regarding the material
composition of the imaged scene. However, these images can be affected by
various sources of noise, distortions, or data loss, which can significantly
degrade their quality and usefulness. This paper introduces a convergent
guaranteed algorithm, LRS-PnP-DIP(1-Lip), which successfully addresses the
instability issue of DHP that has been reported before. The proposed algorithm
extends the successful joint low-rank and sparse model to further exploit the
underlying data structures beyond the conventional and sometimes restrictive
unions of subspace models. A stability analysis guarantees the convergence of
the proposed algorithm under mild assumptions , which is crucial for its
application in real-world scenarios. Extensive experiments demonstrate that the
proposed solution consistently delivers visually and quantitatively superior
inpainting results, establishing state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 9 Figures, 7 Tables. arXiv admin note: text overlap with
  arXiv:2306.08128</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Quantum Machine Learning for Genomic Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navneet Singh, Shiva Raj Pokhrel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Machine Learning (QML) continues to evolve, unlocking new
opportunities for diverse applications. In this study, we investigate and
evaluate the applicability of QML models for binary classification of genome
sequence data by employing various feature mapping techniques. We present an
open-source, independent Qiskit-based implementation to conduct experiments on
a benchmark genomic dataset. Our simulations reveal that the interplay between
feature mapping techniques and QML algorithms significantly influences
performance. Notably, the Pegasos Quantum Support Vector Classifier
(Pegasos-QSVC) exhibits high sensitivity, particularly excelling in recall
metrics, while Quantum Neural Networks (QNN) achieve the highest training
accuracy across all feature maps. However, the pronounced variability in
classifier performance, dependent on feature mapping, highlights the risk of
overfitting to localized output distributions in certain scenarios. This work
underscores the transformative potential of QML for genomic data classification
while emphasizing the need for continued advancements to enhance the robustness
and accuracy of these methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Critical Synthesis of Uncertainty Quantification and <span class="highlight-title">Foundation</span> Models
  in Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Landgraf, Rongjun Qin, Markus Ulrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent foundation models have enabled significant breakthroughs in
monocular depth estimation, a clear path towards safe and reliable deployment
in the real-world remains elusive. Metric depth estimation, which involves
predicting absolute distances, poses particular challenges, as even the most
advanced foundation models remain prone to critical errors. Since quantifying
the uncertainty has emerged as a promising endeavor to address these
limitations and enable trustworthy deployment, we fuse five different
uncertainty quantification methods with the current state-of-the-art
DepthAnythingV2 foundation model. To cover a wide range of metric depth
domains, we evaluate their performance on four diverse datasets. Our findings
identify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a
particularly promising approach, offering reliable uncertainty estimates while
maintaining predictive performance and computational efficiency on par with the
baseline, encompassing both training and inference time. By fusing uncertainty
quantification and foundation models within the context of monocular depth
estimation, this paper lays a critical foundation for future research aimed at
improving not only model performance but also its explainability. Extending
this critical synthesis of uncertainty quantification and foundation models
into other crucial tasks, such as semantic segmentation and pose estimation,
presents exciting opportunities for safer and more reliable machine vision
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction
  Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Fang, Xinle Deng, Kangwei Liu, Ningyu Zhang, Jingyang Qian, Penghui Yang, Xiaohui Fan, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models excel at interpreting complex natural language
instructions, enabling them to perform a wide range of tasks. In the life
sciences, single-cell RNA sequencing (scRNA-seq) data serves as the "language
of cellular biology", capturing intricate gene expression patterns at the
single-cell level. However, interacting with this "language" through
conventional tools is often inefficient and unintuitive, posing challenges for
researchers. To address these limitations, we present InstructCell, a
multi-modal AI copilot that leverages natural language as a medium for more
direct and flexible single-cell analysis. We construct a comprehensive
multi-modal instruction dataset that pairs text-based instructions with
scRNA-seq profiles from diverse tissues and species. Building on this, we
develop a multi-modal cell language architecture capable of simultaneously
interpreting and processing both modalities. InstructCell empowers researchers
to accomplish critical tasks-such as cell type annotation, conditional
pseudo-cell generation, and drug sensitivity prediction-using straightforward
natural language commands. Extensive evaluations demonstrate that InstructCell
consistently meets or exceeds the performance of existing single-cell
foundation models, while adapting to diverse experimental conditions. More
importantly, InstructCell provides an accessible and intuitive tool for
exploring complex single-cell data, lowering technical barriers and enabling
deeper biological insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell;
  Models: https://huggingface.co/zjunlp/Instructcell-chat,
  https://huggingface.co/zjunlp/InstructCell-instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Zeng, Jie Song, Han Zheng, Hao Jiang, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved cutting-edge performance in image generation.
However, their lengthy denoising process and computationally intensive score
estimation network impede their scalability in low-latency and
resource-constrained scenarios. Post-training quantization (PTQ) compresses and
accelerates diffusion models without retraining, but it inevitably introduces
additional quantization noise, resulting in mean and variance deviations. In
this work, we propose D2-DPM, a dual denoising mechanism aimed at precisely
mitigating the adverse effects of quantization noise on the noise estimation
network. Specifically, we first unravel the impact of quantization noise on the
sampling equation into two components: the mean deviation and the variance
deviation. The mean deviation alters the drift coefficient of the sampling
equation, influencing the trajectory trend, while the variance deviation
magnifies the diffusion coefficient, impacting the convergence of the sampling
trajectory. The proposed D2-DPM is thus devised to denoise the quantization
noise at each time step, and then denoise the noisy sample through the inverse
diffusion iterations. Experimental results demonstrate that D2-DPM achieves
superior generation quality, yielding a 1.42 lower FID than the full-precision
model while achieving 3.99x compression and 11.67x bit-operation acceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, acceptted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revolutionizing Communication with Deep Learning and XAI for Enhanced
  Arabic Sign Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazen Balat, Rewaa Awaad, Ahmed B. Zaky, Salah A. Aly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces an integrated approach to recognizing Arabic Sign
Language (ArSL) using state-of-the-art deep learning models such as
MobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced
by explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and
RGB Arabic Alphabets Sign Language (AASL) datasets are employed, with
EfficientNet-B2 achieving peak accuracies of 99.48\% and 98.99\%, respectively.
Key innovations include sophisticated data augmentation methods to mitigate
class imbalance, implementation of stratified 5-fold cross-validation for
better generalization, and the use of Grad-CAM for clear model decision
transparency. The proposed system not only sets new benchmarks in recognition
accuracy but also emphasizes interpretability, making it suitable for
applications in healthcare, education, and inclusive communication
technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 25 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference-Time-Compute: More Faithful? A Research Note 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Chua, Owain Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained specifically to generate long Chains of Thought (CoTs) have
recently achieved impressive results. We refer to these models as
Inference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful
compared to traditional non-ITC models? We evaluate two ITC models (based on
Qwen-2.5 and Gemini-2) on an existing test of faithful CoT To measure
faithfulness, we test if models articulate cues in their prompt that influence
their answers to MMLU questions. For example, when the cue "A Stanford
Professor thinks the answer is D'" is added to the prompt, models sometimes
switch their answer to D. In such cases, the Gemini ITC model articulates the
cue 54% of the time, compared to 14% for the non-ITC Gemini.
  We evaluate 7 types of cue, such as misleading few-shot examples and
anchoring on past responses. ITC models articulate cues that influence them
much more reliably than all the 6 non-ITC models tested, such as
Claude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time.
  However, our study has important limitations. We evaluate only two ITC models
-- we cannot evaluate OpenAI's SOTA o1 model. We also lack details about the
training of these ITC models, making it hard to attribute our findings to
specific processes.
  We think faithfulness of CoT is an important property for AI Safety. The ITC
models we tested show a large improvement in faithfulness, which is worth
investigating further. To speed up this investigation, we release these early
results as a research note.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurit Cohen-Inger, Lior Rokach, Bracha Shapira, Seffi Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic decision-making has become deeply ingrained in many domains, yet
biases in machine learning models can still produce discriminatory outcomes,
often harming unprivileged groups. Achieving fair classification is inherently
challenging, requiring a careful balance between predictive performance and
ethical considerations. We present FairTTTS, a novel post-processing bias
mitigation method inspired by the Tree Test Time Simulation (TTTS) method.
Originally developed to enhance accuracy and robustness against adversarial
inputs through probabilistic decision-path adjustments, TTTS serves as the
foundation for FairTTTS. By building on this accuracy-enhancing technique,
FairTTTS mitigates bias and improves predictive performance. FairTTTS uses a
distance-based heuristic to adjust decisions at protected attribute nodes,
ensuring fairness for unprivileged samples. This fairness-oriented adjustment
occurs as a post-processing step, allowing FairTTTS to be applied to
pre-trained models, diverse datasets, and various fairness metrics without
retraining. Extensive evaluation on seven benchmark datasets shows that
FairTTTS outperforms traditional methods in fairness improvement, achieving a
20.96% average increase over the baseline compared to 18.78% for related work,
and further enhances accuracy by 0.55%. In contrast, competing methods
typically reduce accuracy by 0.42%. These results confirm that FairTTTS
effectively promotes more equitable decision-making while simultaneously
improving predictive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple-Input Variational Auto-Encoder for Anomaly Detection in
  Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phai Vu Dinh, Diep N. Nguyen, Dinh Thai Hoang, Quang Uy Nguyen, Eryk Dutkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection (AD) plays a pivotal role in AI applications, e.g., in
classification, and intrusion/threat detection in cybersecurity. However, most
existing methods face challenges of heterogeneity amongst feature subsets posed
by non-independent and identically distributed (non-IID) data. We propose a
novel neural network model called Multiple-Input Auto-Encoder for AD (MIAEAD)
to address this. MIAEAD assigns an anomaly score to each feature subset of a
data sample to indicate its likelihood of being an anomaly. This is done by
using the reconstruction error of its sub-encoder as the anomaly score. All
sub-encoders are then simultaneously trained using unsupervised learning to
determine the anomaly scores of feature subsets. The final AUC of MIAEAD is
calculated for each sub-dataset, and the maximum AUC obtained among the
sub-datasets is selected. To leverage the modelling of the distribution of
normal data to identify anomalies of the generative models, we develop a novel
neural network architecture/model called Multiple-Input Variational
Auto-Encoder (MIVAE). MIVAE can process feature subsets through its
sub-encoders before learning distribution of normal data in the latent space.
This allows MIVAE to identify anomalies that deviate from the learned
distribution. We theoretically prove that the difference in the average anomaly
score between normal samples and anomalies obtained by the proposed MIVAE is
greater than that of the Variational Auto-Encoder (VAEAD), resulting in a
higher AUC for MIVAE. Extensive experiments on eight real-world anomaly
datasets demonstrate the superior performance of MIAEAD and MIVAE over
conventional methods and the state-of-the-art unsupervised models, by up to 6%
in terms of AUC score. Alternatively, MIAEAD and MIVAE have a high AUC when
applied to feature subsets with low heterogeneity based on the coefficient of
variation (CV) score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bootstrapping Corner Cases: High-Resolution Inpainting for Safety
  Critical Detect and Avoid for Automated Flying 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Lyhs, Lars Hinneburg, Michael Fischer, Florian Ölsner, Stefan Milz, Jeremy Tschirner, Patrick Mäder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning techniques have shown tremendous potential,
especially for object detection on camera images. For this reason, they are
also used to enable safety-critical automated processes such as autonomous
drone flights. We present a study on object detection for Detect and Avoid, a
safety critical function for drones that detects air traffic during automated
flights for safety reasons. An ill-posed problem is the generation of good and
especially large data sets, since detection itself is the corner case. Most
models suffer from limited ground truth in raw data, \eg recorded air traffic
or frontal flight with a small aircraft. It often leads to poor and critical
detection rates. We overcome this problem by using inpainting methods to
bootstrap the dataset such that it explicitly contains the corner cases of the
raw data. We provide an overview of inpainting methods and generative models
and present an example pipeline given a small annotated dataset. We validate
our method by generating a high-resolution dataset, which we make publicly
available and present it to an independent object detector that was fully
trained on real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EEG-ReMinD: Enhancing Neurodegenerative EEG Decoding through
  <span class="highlight-title">Self-Supervised</span> State Reconstruction-Primed Riemannian Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Wang, Zhenxi Song, Yi Guo, Yuxin Liu, Guoyang Xu, Min Zhang, Zhiguo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of EEG decoding algorithms confronts challenges such as data
sparsity, subject variability, and the need for precise annotations, all of
which are vital for advancing brain-computer interfaces and enhancing the
diagnosis of diseases. To address these issues, we propose a novel two-stage
approach named Self-Supervised State Reconstruction-Primed Riemannian Dynamics
(EEG-ReMinD) , which mitigates reliance on supervised learning and integrates
inherent geometric features. This approach efficiently handles EEG data
corruptions and reduces the dependency on labels. EEG-ReMinD utilizes
self-supervised and geometric learning techniques, along with an attention
mechanism, to analyze the temporal dynamics of EEG features within the
framework of Riemannian geometry, referred to as Riemannian dynamics.
Comparative analyses on both intact and corrupted datasets from two different
neurodegenerative disorders underscore the enhanced performance of EEG-ReMinD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Wall-Pressure Spectrum Model for Aeroacoustic Predictions
  Based on Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Botero Bolívar, David Huergo, Fernanda L. dos Santos, Cornelis H. Venner, Leandro D. de Santana, Esteban Ferrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast-turn around methods to predict airfoil trailing-edge noise are crucial
for incorporating noise limitations into design optimization loops of several
applications. Among these aeroacoustic predictive models, Amiet's theory offers
the best balance between accuracy and simplicity. The accuracy of the model
relies heavily on precise wall-pressure spectrum predictions, which are often
based on single-equation formulations with adjustable parameters. These
parameters are calibrated for particular airfoils and flow conditions and
consequently tend to fail when applied outside their calibration range. This
paper introduces a new wall-pressure spectrum empirical model designed to
enhance the robustness and accuracy of current state-of-the-art predictions
while widening the range of applicability of the model to different airfoils
and flow conditions. The model is developed using AI-based symbolic regression
via a genetic-algorithm-based approach, and applied to a dataset of
wall-pressure fluctuations measured on NACA 0008 and NACA 63018 airfoils at
multiple angles of attack and inflow velocities, covering turbulent boundary
layers with both adverse and favorable pressure gradients. Validation against
experimental data (outside the training dataset) demonstrates the robustness of
the model compared to well-accepted semi-empirical models. Finally, the model
is integrated with Amiet's theory to predict the aeroacoustic noise of a
full-scale wind turbine, showing good agreement with experimental measurements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoHan: Robust Hand Detection in Operation Room 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roi Papo, Sapir Gershov, Tom Friedman, Itay Or, Gil Bolotin, Shlomi Laufer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-specific localization has garnered significant interest within the
computer vision community. Although there are numerous datasets with hand
annotations from various angles and settings, domain transfer techniques
frequently struggle in surgical environments. This is mainly due to the limited
availability of gloved hand instances and the unique challenges of operating
rooms (ORs). Thus, hand-detection models tailored to OR settings require
extensive training and expensive annotation processes. To overcome these
challenges, we present "RoHan" - a novel approach for robust hand detection in
the OR, leveraging advanced semi-supervised domain adaptation techniques to
tackle the challenges of varying recording conditions, diverse glove colors,
and occlusions common in surgical settings. Our methodology encompasses two
main stages: (1) data augmentation strategy that utilizes "Artificial Gloves,"
a method for augmenting publicly available hand datasets with synthetic images
of hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that
improves detection performance in real-world OR settings through iterative
prediction refinement and efficient frame filtering. We evaluate our method
using two datasets: simulated enterotomy repair and saphenous vein graft
harvesting. "RoHan" substantially reduces the need for extensive labeling and
model training, paving the way for the practical implementation of hand
detection technologies in medical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven inventory management for new products: A warm-start and
  adjusted Dyna-$Q$ approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Qu, Longxiao Liu, Wenjie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel reinforcement learning algorithm for
inventory management of newly launched products with no or limited historical
demand information. The algorithm follows the classic Dyna-$Q$ structure,
balancing the model-based and model-free approaches, while accelerating the
training process of Dyna-$Q$ and mitigating the model discrepancy generated by
the model-based feedback. Warm-start information from the demand data of
existing similar products can be incorporated into the algorithm to further
stabilize the early-stage training and reduce the variance of the estimated
optimal policy. Our approach is validated through a case study of bakery
inventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\%
reduction in average daily cost compared with $Q$-learning, and up to a 77.5\%
reduction in training time within the same horizon compared with classic
Dyna-$Q$. By incorporating the warm-start information, it can be found that the
adjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and
relatively low shortage percentages among all the algorithms under a 30-day
testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smooth Handovers via Smoothed Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Kalntis, Andra Lutu, Jesús Omaña Iglesias, Fernando A. Kuipers, George Iosifidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With users demanding seamless connectivity, handovers (HOs) have become a
fundamental element of cellular networks. However, optimizing HOs is a
challenging problem, further exacerbated by the growing complexity of mobile
networks. This paper presents the first countrywide study of HO optimization,
through the prism of Smoothed Online Learning (SOL). We first analyze an
extensive dataset from a commercial mobile network operator (MNO) in Europe
with more than 40M users, to understand and reveal important features and
performance impacts on HOs. Our findings highlight a correlation between HO
failures/delays, and the characteristics of radio cells and end-user devices,
showcasing the impact of heterogeneity in mobile networks nowadays. We
subsequently model UE-cell associations as dynamic decisions and propose a
realistic system model for smooth and accurate HOs that extends existing
approaches by (i) incorporating device and cell features on HO optimization,
and (ii) eliminating (prior) strong assumptions about requiring future signal
measurements and knowledge of end-user mobility. Our algorithm, aligned with
the O-RAN paradigm, provides robust dynamic regret guarantees, even in
challenging environments, and shows superior performance in multiple scenarios
with real-world and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Action Based Reinforcement Learning for Multi-Objective
  Compatible Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guizhe Jin, Zhuoren Li, Bo Leng, Wei Han, Lu Xiong, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has shown excellent performance in solving
decision-making and control problems of autonomous driving, which is
increasingly applied in diverse driving scenarios. However, driving is a
multi-attribute problem, leading to challenges in achieving multi-objective
compatibility for current RL methods, especially in both policy execution and
policy iteration. On the one hand, the common action space structure with
single action type limits driving flexibility or results in large behavior
fluctuations during policy execution. On the other hand, the multi-attribute
weighted single reward function result in the agent's disproportionate
attention to certain objectives during policy iterations. To this end, we
propose a Multi-objective Ensemble-Critic reinforcement learning method with
Hybrid Parametrized Action for multi-objective compatible autonomous driving.
Specifically, a parameterized action space is constructed to generate hybrid
driving actions, combining both abstract guidance and concrete control
commands. A multi-objective critics architecture is constructed considering
multiple attribute rewards, to ensure simultaneously focusing on different
driving objectives. Additionally, uncertainty-based exploration strategy is
introduced to help the agent faster approach viable driving policy. The
experimental results in both the simulated traffic environment and the HighD
dataset demonstrate that our method can achieve multi-objective compatible
autonomous driving in terms of driving efficiency, action consistency, and
safety. It enhances the general performance of the driving while significantly
increasing training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention
  for Enabled Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Lee, Singh Suniljit, Yong Siang Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the development of a multimodal sentiment analysis model
that integrates text, audio, and visual data to enhance sentiment
classification. The goal is to improve emotion detection by capturing the
complex interactions between these modalities, thereby enabling more accurate
and nuanced sentiment interpretation. The study evaluates three feature fusion
strategies -- late stage fusion, early stage fusion, and multi-headed attention
-- within a transformer-based architecture. Experiments were conducted using
the CMU-MOSEI dataset, which includes synchronized text, audio, and visual
inputs labeled with sentiment scores. Results show that early stage fusion
significantly outperforms late stage fusion, achieving an accuracy of 71.87\%,
while the multi-headed attention approach offers marginal improvement, reaching
72.39\%. The findings suggest that integrating modalities early in the process
enhances sentiment classification, while attention mechanisms may have limited
impact within the current framework. Future work will focus on refining feature
fusion techniques, incorporating temporal data, and exploring dynamic feature
weighting to further improve model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CuAsmRL: Optimizing GPU SASS Schedules via Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoliang He, Eiko Yoneki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are remarked by their substantial computational
requirements. To mitigate the cost, researchers develop specialized CUDA
kernels, which often fuse several tensor operations to maximize the utilization
of GPUs as much as possible. However, those specialized kernels may still leave
performance on the table as CUDA assembly experts show that manual optimization
of GPU SASS schedules can lead to better performance, and trial-and-error is
largely employed to manually find the best GPU SASS schedules.
  In this work, we employ an automatic approach to optimize GPU SASS schedules,
which thus can be integrated into existing compiler frameworks. The key to
automatic optimization is training an RL agent to mimic how human experts
perform manual scheduling. To this end, we formulate an assembly game, where RL
agents can play to find the best GPU SASS schedules. The assembly game starts
from a \textit{-O3} optimized SASS schedule, and the RL agents can iteratively
apply actions to mutate the current schedules. Positive rewards are generated
if the mutated schedules get higher throughput by executing on GPUs.
Experiments show that CuAsmRL can further improve the performance of existing
specialized CUDA kernels transparently by up to $26\%$, and on average $9\%$.
Moreover, it is used as a tool to reveal potential optimization moves learned
automatically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>cgo 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Policy Adaptation under Covariate Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueqing Liu, Qinwei Yang, Zhaoqing Tian, Ruocheng Guo, Peng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning of prediction models has been extensively studied, while
the corresponding policy learning approaches are rarely discussed. In this
paper, we propose principled approaches for learning the optimal policy in the
target domain by leveraging two datasets: one with full information from the
source domain and the other from the target domain with only covariates. First,
under the setting of covariate shift, we formulate the problem from a
perspective of causality and present the identifiability assumptions for the
reward induced by a given policy. Then, we derive the efficient influence
function and the semiparametric efficiency bound for the reward. Based on this,
we construct a doubly robust and semiparametric efficient estimator for the
reward and then learn the optimal policy by optimizing the estimated reward.
Moreover, we theoretically analyze the bias and the generalization error bound
for the learned policy. Furthermore, in the presence of both covariate and
concept shifts, we propose a novel sensitivity analysis method to evaluate the
robustness of the proposed policy learning approach. Extensive experiments
demonstrate that the approach not only estimates the reward more accurately but
also yields a policy that closely approximates the theoretically optimal
policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the use of Statistical Learning Theory for model selection in
  Structural Health Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. A. Lindley, N. Dervilis, K. Worden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whenever data-based systems are employed in engineering applications,
defining an optimal statistical representation is subject to the problem of
model selection. This paper focusses on how well models can generalise in
Structural Health Monitoring (SHM). Although statistical model validation in
this field is often performed heuristically, it is possible to estimate
generalisation more rigorously using the bounds provided by Statistical
Learning Theory (SLT). Therefore, this paper explores the selection process of
a kernel smoother for modelling the impulse response of a linear oscillator
from the perspective of SLT. It is demonstrated that incorporating domain
knowledge into the regression problem yields a lower guaranteed risk, thereby
enhancing generalisation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Attentive Spatio-Temporal Calibration for Precise Intermediate
  Layer Matching in ANN-to-SNN Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Hong, Yueming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) are promising for low-power computation due to
their event-driven mechanism but often suffer from lower accuracy compared to
Artificial Neural Networks (ANNs). ANN-to-SNN knowledge distillation can
improve SNN performance, but previous methods either focus solely on label
information, missing valuable intermediate layer features, or use a layer-wise
approach that neglects spatial and temporal semantic inconsistencies, leading
to performance degradation.To address these limitations, we propose a novel
method called self-attentive spatio-temporal calibration (SASTC). SASTC uses
self-attention to identify semantically aligned layer pairs between ANN and
SNN, both spatially and temporally. This enables the autonomous transfer of
relevant semantic information. Extensive experiments show that SASTC
outperforms existing methods, effectively solving the mismatching problem.
Superior accuracy results include 95.12% on CIFAR-10, 79.40% on CIFAR-100 with
2 time steps, and 68.69% on ImageNet with 4 time steps for static datasets, and
97.92% on DVS-Gesture and 83.60% on DVS-CIFAR10 for neuromorphic datasets. This
marks the first time SNNs have outperformed ANNs on both CIFAR-10 and
CIFAR-100, shedding the new light on the potential applications of SNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone
  Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikko Heikkinen, Archontis Politis, Konstantinos Drossos, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using deep neural networks (DNNs) for encoding of microphone array (MA)
signals to the Ambisonics spatial audio format can surpass certain limitations
of established conventional methods, but existing DNN-based methods need to be
trained separately for each MA. This paper proposes a DNN-based method for
Ambisonics encoding that can generalize to arbitrary MA geometries unseen
during training. The method takes as inputs the MA geometry and MA signals and
uses a multi-level encoder consisting of separate paths for geometry and signal
data, where geometry features inform the signal encoder at each level. The
method is validated in simulated anechoic and reverberant conditions with one
and two sources. The results indicate improvement over conventional encoding
across the whole frequency range for dry scenes, while for reverberant scenes
the improvement is frequency-dependent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Proceedings of the 2025 IEEE
  International Conference on Acoustics, Speech and Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UF<span class="highlight-title">Graph</span>FR: An attempt at a federated <span class="highlight-title">recommend</span>ation system based on user
  text characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has become an important research area in 'private
computing' due to the 'useable invisibility' of data during training. Inspired
by Federated learning, the federated recommendation system has gradually become
a new recommendation service architecture that can protect users' privacy. The
use of user diagrams to enhance federated recommendations is a promising topic.
How to use user diagrams to enhance federated recommendations is a promising
research topic. However, it's a great challenge to construct a user diagram
without compromising privacy in a federated learning scenario. Inspired by the
simple idea that similar users often have the same attribute characteristics,
we propose a personalized federated recommendation algorithm based on the user
relationship graph constructed by the user text characteristics(Graph
Federation Recommendation System based on User Text description Features,
UFGraphFR). The method uses the embedding layer weight of the user's text
feature description to construct the user relationship graph. It introduces the
Transformer mechanism to capture the sequence modeling of the user's historical
interaction sequence. Without access to user history interactions and specific
user attributes, the federal learning privacy protection of data 'useable
invisibility' is embodied. Preliminary experiments on some benchmark datasets
demonstrate the superior performance of UFGraphFR. Our experiments show that
this model can protect user privacy to some extent without affecting the
performance of the recommendation system. The code will be easily available on
https://github.com/trueWangSyutung/UFGraphFR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware
  Structured Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Andronic, Jiawen Li, George A. Constantinides
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard deep neural network inference involves the computation of
interleaved linear maps and nonlinear activation functions. Prior work for
ultra-low latency implementations has hardcoded these operations inside FPGA
lookup tables (LUTs). However, FPGA LUTs can implement a much greater variety
of functions. In this paper, we propose a novel approach to training DNNs for
FPGA deployment using multivariate polynomials as the basic building block. Our
method takes advantage of the flexibility offered by the soft logic, hiding the
polynomial evaluation inside the LUTs with minimal overhead. By using
polynomial building blocks, we achieve the same accuracy using considerably
fewer layers of soft logic than by using linear functions, leading to
significant latency and area improvements. LUT-based implementations also face
a significant challenge: the LUT size grows exponentially with the number of
inputs. Prior work relies on a priori fixed sparsity, with results heavily
dependent on seed selection. To address this, we propose a structured pruning
strategy using a bespoke hardware-aware group regularizer that encourages a
particular sparsity pattern that leads to a small number of inputs per neuron.
We demonstrate the effectiveness of PolyLUT on three tasks: network intrusion
detection, jet identification at the CERN Large Hadron Collider, and MNIST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2309.02334</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence Analysis of Real-time Recurrent Learning (RTRL) for a class
  of Recurrent Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Chun-Hei Lam, Justin Sirignano, Konstantinos Spiliopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks (RNNs) are commonly trained with the truncated
backpropagation-through-time (TBPTT) algorithm. For the purposes of
computational tractability, the TBPTT algorithm truncates the chain rule and
calculates the gradient on a finite block of the overall data sequence. Such
approximation could lead to significant inaccuracies, as the block length for
the truncated backpropagation is typically limited to be much smaller than the
overall sequence length. In contrast, Real-time recurrent learning (RTRL) is an
online optimization algorithm which asymptotically follows the true gradient of
the loss on the data sequence as the number of sequence time steps $t
\rightarrow \infty$. RTRL forward propagates the derivatives of the RNN
hidden/memory units with respect to the parameters and, using the forward
derivatives, performs online updates of the parameters at each time step in the
data sequence. RTRL's online forward propagation allows for exact optimization
over extremely long data sequences, although it can be computationally costly
for models with large numbers of parameters. We prove convergence of the RTRL
algorithm for a class of RNNs. The convergence analysis establishes a fixed
point for the joint distribution of the data sequence, RNN hidden layer, and
the RNN hidden layer forward derivatives as the number of data samples from the
sequence and the number of training steps tend to infinity. We prove
convergence of the RTRL algorithm to a stationary point of the loss. Numerical
studies illustrate our theoretical results. One potential application area for
RTRL is the analysis of financial data, which typically involve long time
series and models with small to medium numbers of parameters. This makes RTRL
computationally tractable and a potentially appealing optimization method for
training models. Thus, we include an example of RTRL applied to limit order
book data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced SPS Velocity-adaptive Scheme: Access Fariness in 5G NR V2I
  Networks <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Qiong Wu, Pingyi Fan, Kezhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-Infrastructure (V2I) technology enables information exchange
between vehicles and road infrastructure. Specifically, when a vehicle
approaches a roadside unit (RSU), it can exchange information with the RSU to
obtain accurate data that assists in driving. With the release of the 3rd
Generation Partnership Project (3GPP) Release 16, which includes the 5G New
Radio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt
mode-2 communication using sensing-based semi-persistent scheduling (SPS) for
resource allocation. In this approach, vehicles identify candidate resources
within a selection window and exclude ineligible resources based on information
from a sensing window. However, vehicles often drive at different speeds,
resulting in varying amounts of data transmission with RSUs as they pass by,
which leads to unfair access. Therefore, it is essential to design an access
scheme that accounts for different vehicle speeds to achieve fair access across
the network. This paper formulates an optimization problem for vehicular
networks and proposes a multi-objective optimization scheme to address it by
adjusting the selection window in the SPS mechanism of 5G NR V2I mode-2.
Simulation results demonstrate the effectiveness of the proposed scheme
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to IEEE Journal. The source code has
  been released at:
  https://github.com/qiongwu86/Enhanced-SPS-Velocity-adaptiveScheme-Access-Fariness-in-5G-NR-V2I-Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-driven framework for rapid and localized optimizations of urban
  open spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pegah Eshraghi, Arman Nikkhah Dehnavi, Maedeh Mirdamadi, Riccardo Talami, Zahra-Sadat Zomorodian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As urbanization accelerates, open spaces are increasingly recognized for
their role in enhancing sustainability and well-being, yet they remain
underexplored compared to built spaces. This study introduces an AI-driven
framework that integrates machine learning models (MLMs) and explainable AI
techniques to optimize Sky View Factor (SVF) and visibility, key spatial
metrics influencing thermal comfort and perceived safety in urban spaces.
Unlike global optimization methods, which are computationally intensive and
impractical for localized adjustments, this framework supports incremental
design improvements with lower computational costs and greater flexibility. The
framework employs SHapley Adaptive Explanations (SHAP) to analyze feature
importance and Counterfactual Explanations (CFXs) to propose minimal design
changes. Simulations tested five MLMs, identifying XGBoost as the most
accurate, with building width, park area, and heights of surrounding buildings
as critical for SVF, and distances from southern buildings as key for
visibility. Compared to Genetic Algorithms, which required approximately 15/30
minutes across 3/4 generations to converge, the tested CFX approach achieved
optimized results in 1 minute with a 5% RMSE error, demonstrating significantly
faster performance and suitability for scalable retrofitting strategies. This
interpretable and computationally efficient framework advances urban
performance optimization, providing data-driven insights and practical
retrofitting solutions for enhancing usability and environmental quality across
diverse urban contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximizing Uncertainty for Federated learning via Bayesian
  Optimisation-based Model Poisoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marios Aristodemou, Xiaolan Liu, Yuan Wang, Konstantinos G. Kyriakopoulos, Sangarapillai Lambotharan, Qingsong Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As we transition from Narrow Artificial Intelligence towards Artificial Super
Intelligence, users are increasingly concerned about their privacy and the
trustworthiness of machine learning (ML) technology. A common denominator for
the metrics of trustworthiness is the quantification of uncertainty inherent in
DL algorithms, and specifically in the model parameters, input data, and model
predictions. One of the common approaches to address privacy-related issues in
DL is to adopt distributed learning such as federated learning (FL), where
private raw data is not shared among users. Despite the privacy-preserving
mechanisms in FL, it still faces challenges in trustworthiness. Specifically,
the malicious users, during training, can systematically create malicious model
parameters to compromise the models predictive and generative capabilities,
resulting in high uncertainty about their reliability. To demonstrate malicious
behaviour, we propose a novel model poisoning attack method named Delphi which
aims to maximise the uncertainty of the global model output. We achieve this by
taking advantage of the relationship between the uncertainty and the model
parameters of the first hidden layer of the local model. Delphi employs two
types of optimisation , Bayesian Optimisation and Least Squares Trust Region,
to search for the optimal poisoned model parameters, named as Delphi-BO and
Delphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise
the distance of the predictive probability distribution towards an uncertain
distribution of model output. Furthermore, we establish a mathematical proof
for the attack effectiveness demonstrated in FL. Numerical results demonstrate
that Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR
highlighting vulnerability of FL systems to model poisoning attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Feature Construction for Anomaly Detection in Time Series
  -- An Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marine Hamon, Vincent Lemaire, Nour Eddine Yassine Nair-Benrekia, Samuel Berlemont, Julien Cumin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To detect anomalies with precision and without prior knowledge in time
series, is it better to build a detector from the initial temporal
representation, or to compute a new (tabular) representation using an existing
automatic variable construction library? In this article, we address this
question by conducting an in-depth experimental study for two popular detectors
(Isolation Forest and Local Outlier Factor). The obtained results, for 5
different datasets, show that the new representation, computed using the
tsfresh library, allows Isolation Forest to significantly improve its
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward Compatibility: A Framework for Inverse RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Lazzati, Mirco Mutti, Alberto Metelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide an original theoretical study of Inverse Reinforcement Learning
(IRL) through the lens of reward compatibility, a novel framework to quantify
the compatibility of a reward with the given expert's demonstrations.
Intuitively, a reward is more compatible with the demonstrations the closer the
performance of the expert's policy computed with that reward is to the optimal
performance for that reward. This generalizes the notion of feasible reward
set, the most common framework in the theoretical IRL literature, for which a
reward is either compatible or not compatible. The grayscale introduced by the
reward compatibility is the key to extend the realm of provably efficient IRL
far beyond what is attainable with the feasible reward set: from tabular to
large-scale MDPs. We analyze the IRL problem across various settings, including
optimal and suboptimal expert's demonstrations and both online and offline data
collection. For all of these dimensions, we provide a tractable algorithm and
corresponding sample complexity analysis, as well as various insights on reward
compatibility and how the framework can pave the way to yet more general
problem settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining imaging and shape features for prediction tasks of Alzheimer's
  disease classification and brain age regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nairouz Shehata, Carolina Piçarra, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate combining imaging and shape features extracted from MRI for
the clinically relevant tasks of brain age prediction and Alzheimer's disease
classification. Our proposed model fuses ResNet-extracted image embeddings with
shape embeddings from a bespoke graph neural network. The shape embeddings are
derived from surface meshes of 15 brain structures, capturing detailed
geometric information. Combined with the appearance features from T1-weighted
images, we observe improvements in the prediction performance on both tasks,
with substantial gains for classification. We evaluate the model using public
datasets, including CamCAN, IXI, and OASIS3, demonstrating the effectiveness of
fusing imaging and shape features for brain analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHEQ-ing the Box: Safe Variable Impedance Learning for Robotic Polishing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Cramer, Lukas Jäschke, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic systems are increasingly employed for industrial automation, with
contact-rich tasks like polishing requiring dexterity and compliant behaviour.
These tasks are difficult to model, making classical control challenging. Deep
reinforcement learning (RL) offers a promising solution by enabling the
learning of models and control policies directly from data. However, its
application to real-world problems is limited by data inefficiency and unsafe
exploration. Adaptive hybrid RL methods blend classical control and RL
adaptively, combining the strengths of both: structure from control and
learning from RL. This has led to improvements in data efficiency and
exploration safety. However, their potential for hardware applications remains
underexplored, with no evaluations on physical systems to date. Such
evaluations are critical to fully assess the practicality and effectiveness of
these methods in real-world settings. This work presents an experimental
demonstration of the hybrid RL algorithm CHEQ for robotic polishing with
variable impedance, a task requiring precise force and velocity tracking. In
simulation, we show that variable impedance enhances polishing performance. We
compare standalone RL with adaptive hybrid RL, demonstrating that CHEQ achieves
effective learning while adhering to safety constraints. On hardware, CHEQ
achieves effective polishing behaviour, requiring only eight hours of training
and incurring just five failures. These results highlight the potential of
adaptive hybrid RL for real-world, contact-rich tasks trained directly on
hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Derivation of Output Correlation Inferences for Multi-Output (aka
  Multi-Task) Gaussian Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhei Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian process (GP) is arguably one of the most widely used machine
learning algorithms in practice. One of its prominent applications is Bayesian
optimization (BO). Although the vanilla GP itself is already a powerful tool
for BO, it is often beneficial to be able to consider the dependencies of
multiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not
trivial to fully understand the derivations of its formulations and their
gradients from the previous literature. This paper serves friendly derivations
of the MTGP formulations and their gradients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Guide Dog: Egocentric Path Prediction on Smartphone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Jadhav, Jeffery Cao, Abhishree Shetty, Urvashi Priyam Kumar, Aditi Sharma, Ben Sukboontip, Jayant Sravan Tamarapalli, Jingyi Zhang, Anirudh Koul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces AI Guide Dog (AIGD), a lightweight egocentric
navigation assistance system for visually impaired individuals, designed for
real-time deployment on smartphones. AIGD addresses key challenges in blind
navigation by employing a vision-only, multi-label classification approach to
predict directional commands, ensuring safe traversal across diverse
environments. We propose a novel technique to enable goal-based outdoor
navigation by integrating GPS signals and high-level directions, while also
addressing uncertain multi-path predictions for destination-free indoor
navigation. Our generalized model is the first navigation assistance system to
handle both goal-oriented and exploratory navigation scenarios across indoor
and outdoor settings, establishing a new state-of-the-art in blind navigation.
We present methods, datasets, evaluations, and deployment insights to encourage
further innovations in assistive navigation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gandalf the Red: Adaptive Security for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Pfister, Václav Volhejn, Manuel Knott, Santiago Arias, Julia Bazińska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Damián Pascual-Ortiz, Jakub Podolak, Adrià Romero-López, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, Mateo Rojas-Carulla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current evaluations of defenses against prompt attacks in large language
model (LLM) applications often overlook two critical factors: the dynamic
nature of adversarial behavior and the usability penalties imposed on
legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security
Utility Threat Model), which explicitly separates attackers from legitimate
users, models multi-step interactions, and rigorously expresses the
security-utility in an optimizable form. We further address the shortcomings in
existing evaluations by introducing Gandalf, a crowd-sourced, gamified
red-teaming platform designed to generate realistic, adaptive attack datasets.
Using Gandalf, we collect and release a dataset of 279k prompt attacks.
Complemented by benign user data, our analysis reveals the interplay between
security and utility, showing that defenses integrated in the LLM (e.g., system
prompts) can degrade usability even without blocking requests. We demonstrate
that restricted application domains, defense-in-depth, and adaptive defenses
are effective strategies for building secure and useful LLM applications. Code
is available at
\href{https://github.com/lakeraai/dsec-gandalf}{\texttt{https://github.com/lakeraai/dsec-gandalf}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Niklas Pfister, V\'aclav Volhejn and Manuel Knott contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phase of Flight Classification in Aviation Safety using LSTM, GRU, and
  BiLSTM: A Case Study with ASN <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziida Nanyonga, Hassan Wasswa, Graham Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety is the main concern in the aviation industry, where even minor
operational issues can lead to serious consequences. This study addresses the
need for comprehensive aviation accident analysis by leveraging natural
language processing (NLP) and advanced AI models to classify the phase of
flight from unstructured aviation accident analysis narratives. The research
aims to determine whether the phase of flight can be inferred from narratives
of post-accident events using NLP techniques. The classification performance of
various deep learning models was evaluated. For single RNN-based models, LSTM
achieved an accuracy of 63%, precision 60%, and recall 61%. BiLSTM recorded an
accuracy of 64%, precision 63%, and a recall of 64%. GRU exhibited balanced
performance with an accuracy and recall of 60% and a precision of 63%. Joint
RNN-based models further enhanced predictive capabilities. GRU-LSTM,
LSTM-BiLSTM, and GRU-BiLSTM demonstrated accuracy rates of 62%, 67%, and 60%,
respectively, showcasing the benefits of combining these architectures. To
provide a comprehensive overview of model performance, single and combined
models were compared in terms of the various metrics. These results underscore
the models' capacity to classify the phase of flight from raw text narratives,
equipping aviation industry stakeholders with valuable insights for proactive
decision-making. Therefore, this research signifies a substantial advancement
in the application of NLP and deep learning models to enhance aviation safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Aviation Safety, Deep learning algorithms, Flight phase, NLP, ASN,
  and Classification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aviation Safety Enhancement via NLP & Deep Learning: Classifying Flight
  Phases in ATSB Safety Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aziida Nanyonga, Hassan Wasswa, Graham Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aviation safety is paramount, demanding precise analysis of safety
occurrences during different flight phases. This study employs Natural Language
Processing (NLP) and Deep Learning models, including LSTM, CNN, Bidirectional
LSTM (BLSTM), and simple Recurrent Neural Networks (sRNN), to classify flight
phases in safety reports from the Australian Transport Safety Bureau (ATSB).
The models exhibited high accuracy, precision, recall, and F1 scores, with LSTM
achieving the highest performance of 87%, 88%, 87%, and 88%, respectively. This
performance highlights their effectiveness in automating safety occurrence
analysis. The integration of NLP and Deep Learning technologies promises
transformative enhancements in aviation safety analysis, enabling targeted
safety measures and streamlined report handling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NLP, Aviation Safety, ATSB, Deep learning, Flight phase. arXiv admin
  note: substantial text overlap with arXiv:2501.01694</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence
  Modeling for Resource-Constrained Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed A. Taha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-range sequence modeling is a crucial aspect of natural language
processing and time series analysis. However, traditional models like Recurrent
Neural Networks (RNNs) and Transformers suffer from computational and memory
inefficiencies, especially when dealing with long sequences. This paper
introduces Logarithmic Memory Networks (LMNs), a novel architecture that
leverages a hierarchical logarithmic tree structure to efficiently store and
retrieve past information. LMNs dynamically summarize historical context,
significantly reducing the memory footprint and computational complexity of
attention mechanisms from O(n2) to O(log(n)). The model employs a
single-vector, targeted attention mechanism to access stored information, and
the memory block construction worker (summarizer) layer operates in two modes:
a parallel execution mode during training for efficient processing of
hierarchical tree structures and a sequential execution mode during inference,
which acts as a memory management system. It also implicitly encodes positional
information, eliminating the need for explicit positional encodings. These
features make LMNs a robust and scalable solution for processing long-range
sequences in resource-constrained environments, offering practical improvements
in efficiency and scalability. The code is publicly available under the MIT
License on GitHub: https://github.com/AhmedBoin/LogarithmicMemory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Classification Trees for Continuous Feature Data Using Dynamic
  Programming with Branch-and-Bound <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catalin E. Brita, Jacobus G. M. van der Linden, Emir Demirović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computing an optimal classification tree that provably maximizes training
performance within a given size limit, is NP-hard, and in practice, most
state-of-the-art methods do not scale beyond computing optimal trees of depth
three. Therefore, most methods rely on a coarse binarization of continuous
features to maintain scalability. We propose a novel algorithm that optimizes
trees directly on the continuous feature data using dynamic programming with
branch-and-bound. We develop new pruning techniques that eliminate many
sub-optimal splits in the search when similar to previously computed splits and
we provide an efficient subroutine for computing optimal depth-two trees. Our
experiments demonstrate that these techniques improve runtime by one or more
orders of magnitude over state-of-the-art optimal methods and improve test
accuracy by 5% over greedy heuristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the proceedings of AAAI-25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Label Refinement Matters More than Preference Optimization
  under Weak Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowen Ye, Cassidy Laidlaw, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) post-training relies on two stages of human supervision:
task demonstrations for supervised finetuning (SFT), followed by preference
comparisons for reinforcement learning from human feedback (RLHF). As LMs
become more capable, the tasks they are given become harder to supervise. Will
post-training remain effective under unreliable supervision? To test this, we
simulate unreliable demonstrations and comparison feedback using small LMs and
time-constrained humans. We find that in the presence of unreliable
supervision, SFT still retains some effectiveness, but DPO (a common RLHF
algorithm) fails to improve the model beyond SFT. To address this, we propose
iterative label refinement (ILR) as an alternative to RLHF. ILR improves the
SFT data by using comparison feedback to decide whether human demonstrations
should be replaced by model-generated alternatives, then retrains the model via
SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with
unreliable supervision (math, coding, and safe instruction-following). Our
findings suggest that as LMs are used for complex tasks where human supervision
is unreliable, RLHF may no longer be the best use of human comparison feedback;
instead, it is better to direct feedback towards improving the training data
rather than continually training the model. Our code and data are available at
https://github.com/helloelwin/iterative-label-refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Algorithmic Bias in Multiclass CNN Classifications Using
  Causal Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Sik Byun, Wendy Wan Yee Hui, Wai Kwong Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study describes a procedure for applying causal modeling to detect and
mitigate algorithmic bias in a multiclass classification problem. The dataset
was derived from the FairFace dataset, supplemented with emotional labels
generated by the DeepFace pre-trained model. A custom Convolutional Neural
Network (CNN) was developed, consisting of four convolutional blocks, followed
by fully connected layers and dropout layers to mitigate overfitting. Gender
bias was identified in the CNN model's classifications: Females were more
likely to be classified as "happy" or "sad," while males were more likely to be
classified as "neutral." To address this, the one-vs-all (OvA) technique was
applied. A causal model was constructed for each emotion class to adjust the
CNN model's predicted class probabilities. The adjusted probabilities for the
various classes were then aggregated by selecting the class with the highest
probability. The resulting debiased classifications demonstrated enhanced
gender fairness across all classes, with negligible impact--or even a slight
improvement--on overall accuracy. This study highlights that algorithmic
fairness and accuracy are not necessarily trade-offs. All data and code for
this study are publicly available for download.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MD-Syn: Synergistic drug combination prediction based on the
  multidimensional feature fusion method and attention mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        XinXin Ge, Yi-Ting Lee, Shan-Ju Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug combination therapies have shown promising therapeutic efficacy in
complex diseases and have demonstrated the potential to reduce drug resistance.
However, the huge number of possible drug combinations makes it difficult to
screen them all in traditional experiments. In this study, we proposed MD-Syn,
a computational framework, which is based on the multidimensional feature
fusion method and multi-head attention mechanisms. Given drug pair-cell line
triplets, MD-Syn considers one-dimensional and two-dimensional feature spaces
simultaneously. It consists of a one-dimensional feature embedding module
(1D-FEM), a two-dimensional feature embedding module (2D-FEM), and a deep
neural network-based classifier for synergistic drug combination prediction.
MD-Syn achieved the AUROC of 0.919 in 5-fold cross-validation, outperforming
the state-of-the-art methods. Further, MD-Syn showed comparable results over
two independent datasets. In addition, the multi-head attention mechanisms not
only learn embeddings from different feature aspects but also focus on
essential interactive feature elements, improving the interpretability of
MD-Syn. In summary, MD-Syn is an interpretable framework to prioritize
synergistic drug combination pairs with chemicals and cancer cell line gene
expression profiles. To facilitate broader community access to this model, we
have developed a web portal (https://labyeh104-2.life.nthu.edu.tw/) that
enables customized predictions of drug combination synergy effects based on
user-specified compounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Nonparametric Estimation: from Sparse to Dense Samples per
  Terminal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deheng Yuan, Tao Guo, Zhongyi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider the communication-constrained problem of nonparametric function
estimation, in which each distributed terminal holds multiple i.i.d. samples.
Under certain regularity assumptions, we characterize the minimax optimal rates
for all regimes, and identify phase transitions of the optimal rates as the
samples per terminal vary from sparse to dense. This fully solves the problem
left open by previous works, whose scopes are limited to regimes with either
dense samples or a single sample per terminal. To achieve the optimal rates, we
design a layered estimation protocol by exploiting protocols for the parametric
density estimation problem. We show the optimality of the protocol using
information-theoretic methods and strong data processing inequalities, and
incorporating the classic balls and bins model. The optimal rates are immediate
for various special cases such as density estimation, Gaussian, binary, Poisson
and heteroskedastic regression models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ deepTerra -- AI Land Classification Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Keith Wilkinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  deepTerra is a comprehensive platform designed to facilitate the
classification of land surface features using machine learning and satellite
imagery. The platform includes modules for data collection, image augmentation,
training, testing, and prediction, streamlining the entire workflow for image
classification tasks. This paper presents a detailed overview of the
capabilities of deepTerra, shows how it has been applied to various research
areas, and discusses the future directions it might take.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State-of-the-Art Transformer Models for Image Super-Resolution:
  Techniques, Challenges, and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasish Dutta, Deepjyoti Chetia, Neeharika Sonowal, Sanjib Kr Kalita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Super-Resolution (SR) aims to recover a high-resolution image from its
low-resolution counterpart, which has been affected by a specific degradation
process. This is achieved by enhancing detail and visual quality. Recent
advancements in transformer-based methods have remolded image super-resolution
by enabling high-quality reconstructions surpassing previous deep-learning
approaches like CNN and GAN-based. This effectively addresses the limitations
of previous methods, such as limited receptive fields, poor global context
capture, and challenges in high-frequency detail recovery. Additionally, the
paper reviews recent trends and advancements in transformer-based SR models,
exploring various innovative techniques and architectures that combine
transformers with traditional networks to balance global and local contexts.
These neoteric methods are critically analyzed, revealing promising yet
unexplored gaps and potential directions for future research. Several
visualizations of models and techniques are included to foster a holistic
understanding of recent trends. This work seeks to offer a structured roadmap
for researchers at the forefront of deep learning, specifically exploring the
impact of transformers on super-resolution techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Intra- and Cross-frame Topological Consistency Scheme for
  Semi-supervised Atherosclerotic Coronary Plaque Segmentation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Zhang, Zihan Li, Dandan Shan, Yuehui Qiu, Qingqi Hong, Qingqiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the precision of segmenting coronary atherosclerotic plaques from
CT Angiography (CTA) images is pivotal for advanced Coronary Atherosclerosis
Analysis (CAA), which distinctively relies on the analysis of vessel
cross-section images reconstructed via Curved Planar Reformation. This task
presents significant challenges due to the indistinct boundaries and structures
of plaques and blood vessels, leading to the inadequate performance of current
deep learning models, compounded by the inherent difficulty in annotating such
complex data. To address these issues, we propose a novel dual-consistency
semi-supervised framework that integrates Intra-frame Topological Consistency
(ITC) and Cross-frame Topological Consistency (CTC) to leverage labeled and
unlabeled data. ITC employs a dual-task network for simultaneous segmentation
mask and Skeleton-aware Distance Transform (SDT) prediction, achieving similar
prediction of topology structure through consistency constraint without
additional annotations. Meanwhile, CTC utilizes an unsupervised estimator for
analyzing pixel flow between skeletons and boundaries of adjacent frames,
ensuring spatial continuity. Experiments on two CTA datasets show that our
method surpasses existing semi-supervised methods and approaches the
performance of supervised methods on CAA. In addition, our method also performs
better than other methods on the ACDC dataset, demonstrating its
generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flow: A Modular Approach to Automated Agentic Workflow Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boye Niu, Yiliao Song, Kai Lian, Yifan Shen, Yu Yao, Kun Zhang, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent frameworks powered by large language models (LLMs) have
demonstrated great success in automated planning and task execution. However,
the effective adjustment of Agentic workflows during execution has not been
well-studied. A effective workflow adjustment is crucial, as in many real-world
scenarios, the initial plan must adjust to unforeseen challenges and changing
conditions in real-time to ensure the efficient execution of complex tasks. In
this paper, we define workflows as an activity-on-vertex (AOV) graphs. We
continuously refine the workflow by dynamically adjusting task allocations
based on historical performance and previous AOV with LLM agents. To further
enhance system performance, we emphasize modularity in workflow design based on
measuring parallelism and dependence complexity. Our proposed multi-agent
framework achieved efficient sub-task concurrent execution, goal achievement,
and error tolerance. Empirical results across different practical tasks
demonstrate dramatic improvements in the efficiency of multi-agent frameworks
through dynamic workflow updating and modularization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction Interval Construction Method for Electricity Prices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of electricity prices plays an essential role in the
electricity market. To reflect the uncertainty of electricity prices, price
intervals are predicted. This paper proposes a novel prediction interval
construction method. A conditional generative adversarial network is first
presented to generate electricity price scenarios, with which the prediction
intervals can be constructed. Then, different generated scenarios are stacked
to obtain the probability densities, which can be applied to accurately reflect
the uncertainty of electricity prices. Furthermore, a reinforced prediction
mechanism based on the volatility level of weather factors is introduced to
address the spikes or volatile prices. A case study is conducted to verify the
effectiveness of the proposed novel prediction interval construction method.
The method can also provide the probability density of each price scenario
within the prediction interval and has the superiority to address the volatile
prices and price spikes with a reinforced prediction mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Verification and Refinement of Language Model Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonho Ko, Jinheon Baek, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performance across a wide
range of natural language tasks. However, a critical challenge remains in that
they sometimes generate factually incorrect answers. To address this, while
many previous work has focused on identifying errors in their generation and
further refining them, they are slow in deployment since they are designed to
verify the response from LLMs only after their entire generation (from the
first to last tokens) is done. Further, we observe that once LLMs generate
incorrect tokens early on, there is a higher likelihood that subsequent tokens
will also be factually incorrect. To this end, in this work, we propose
Streaming-VR (Streaming Verification and Refinement), a novel approach designed
to enhance the efficiency of verification and refinement of LLM outputs.
Specifically, the proposed Streaming-VR enables on-the-fly verification and
correction of tokens as they are being generated, similar to a streaming
process, ensuring that each subset of tokens is checked and refined in
real-time by another LLM as the LLM constructs its response. Through
comprehensive evaluations on multiple datasets, we demonstrate that our
approach not only enhances the factual accuracy of LLMs, but also offers a more
efficient solution compared to prior refinement methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Among parameter-efficient fine-tuning methods, freezing has emerged as a
popular strategy for speeding up training, reducing catastrophic forgetting,
and improving downstream performance. We investigate the impact of freezing the
decoder in a multi-task setup comprising diverse natural language tasks, aiming
to reduce deployment overhead and enhance portability to novel tasks. Our
experiments, conducted by fine-tuning both individual and multi-task setups on
the AlexaTM model, reveal that freezing decoders is highly effective for tasks
with natural language outputs and mitigates catastrophic forgetting in
multilingual tasks. However, we find that pairing frozen decoders with a larger
model can effectively maintain or even enhance performance in structured and QA
tasks, making it a viable strategy for a broader range of task types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STTS-EAD: Improving Spatio-Temporal Learning Based Time Series
  Prediction via 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Liang, Tianhao Zhang, Tingyu Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling anomalies is a critical preprocessing step in multivariate time
series prediction. However, existing approaches that separate anomaly
preprocessing from model training for multivariate time series prediction
encounter significant limitations. Specifically, these methods fail to utilize
auxiliary information crucial for identifying latent anomalies associated with
spatiotemporal factors during the preprocessing stage. Instead, they rely
solely on data distribution for anomaly detection, which can result in the
incorrect processing of numerous samples that could otherwise contribute
positively to model training. To address this, we propose STTS-EAD, an
end-to-end method that seamlessly integrates anomaly detection into the
training process of multivariate time series forecasting and aims to improve
Spatio-Temporal learning based Time Series prediction via Embedded Anomaly
Detection. Our proposed STTS-EAD leverages spatio-temporal information for
forecasting and anomaly detection, with the two parts alternately executed and
optimized for each other. To the best of our knowledge, STTS-EAD is the first
to integrate anomaly detection and forecasting tasks in the training phase for
improving the accuracy of multivariate time series forecasting. Extensive
experiments on a public stock dataset and two real-world sales datasets from a
renowned coffee chain enterprise show that our proposed method can effectively
process detected anomalies in the training stage to improve forecasting
performance in the inference stage and significantly outperform baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal mapping Coordinates Physics-Informed Neural Networks
  (CoCo-PINNs): learning neural networks for designing neutral inclusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daehee Cho, Hyeonmin Yun, Jaeyong Lee, Mikyoung Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on designing and solving the neutral inclusion problem via neural
networks. The neutral inclusion problem has a long history in the theory of
composite materials, and it is exceedingly challenging to identify the precise
condition that precipitates a general-shaped inclusion into a neutral
inclusion. Physics-informed neural networks (PINNs) have recently become a
highly successful approach to addressing both forward and inverse problems
associated with partial differential equations. We found that traditional PINNs
perform inadequately when applied to the inverse problem of designing neutral
inclusions with arbitrary shapes. In this study, we introduce a novel approach,
Conformal mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs),
which integrates complex analysis techniques into PINNs. This method exhibits
strong performance in solving forward-inverse problems to construct neutral
inclusions of arbitrary shapes in two dimensions, where the imperfect interface
condition on the inclusion's boundary is modeled by training neural networks.
Notably, we mathematically prove that training with a single linear field is
sufficient to achieve neutrality for untrained linear fields in arbitrary
directions, given a minor assumption. We demonstrate that CoCo-PINNs offer
enhanced performances in terms of credibility, consistency, and stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnoosh Koleini, Muhammad Usama Saleem, Pu Wang, Hongfei Xue, Ahmed Helmy, Abbey Fenwick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D human pose estimation from single-camera images and
videos have relied on parametric models, like SMPL. However, these models
oversimplify anatomical structures, limiting their accuracy in capturing true
joint locations and movements, which reduces their applicability in
biomechanics, healthcare, and robotics. Biomechanically accurate pose
estimation, on the other hand, typically requires costly marker-based motion
capture systems and optimization techniques in specialized labs. To bridge this
gap, we propose BioPose, a novel learning-based framework for predicting
biomechanically accurate 3D human pose directly from monocular videos. BioPose
includes three key components: a Multi-Query Human Mesh Recovery model
(MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose
refinement technique. MQ-HMR leverages a multi-query deformable transformer to
extract multi-scale fine-grained image features, enabling precise human mesh
recovery. NeurIK treats the mesh vertices as virtual markers, applying a
spatial-temporal network to regress biomechanically accurate 3D poses under
anatomical constraints. To further improve 3D pose estimations, a 2D-informed
refinement step optimizes the query tokens during inference by aligning the 3D
structure with 2D pose observations. Experiments on benchmark datasets
demonstrate that BioPose significantly outperforms state-of-the-art methods.
Project website:
\url{https://m-usamasaleem.github.io/publication/BioPose/BioPose.html}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linearly Convergent Mixup Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gakuto Obi, Ayato Saito, Yuto Sasaki, Tsuyoshi Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning in the reproducing kernel Hilbert space (RKHS) such as the support
vector machine has been recognized as a promising technique. It continues to be
highly effective and competitive in numerous prediction tasks, particularly in
settings where there is a shortage of training data or computational
limitations exist. These methods are especially valued for their ability to
work with small datasets and their interpretability. To address the issue of
limited training data, mixup data augmentation, widely used in deep learning,
has remained challenging to apply to learning in RKHS due to the generation of
intermediate class labels. Although gradient descent methods handle these
labels effectively, dual optimization approaches are typically not directly
applicable. In this study, we present two novel algorithms that extend to a
broader range of binary classification models. Unlike gradient-based
approaches, our algorithms do not require hyperparameters like learning rates,
simplifying their implementation and optimization. Both the number of
iterations to converge and the computational cost per iteration scale linearly
with respect to the dataset size. The numerical experiments demonstrate that
our algorithms achieve faster convergence to the optimal solution compared to
gradient descent approaches, and that mixup data augmentation consistently
improves the predictive performance across various loss functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>none</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Indoor Localization: Advanced Transformer Architecture for
  NLOS Dominated Wireless Environments with Distributed Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Masrur,  Jung-Fu,  Cheng, Atieh R. Khamesi, Ismail Guvenc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indoor localization in challenging non-line-of-sight (NLOS) environments
often leads to mediocre accuracy with traditional approaches. Deep learning
(DL) has been applied to tackle these challenges; however, many DL approaches
overlook computational complexity, especially for floating-point operations
(FLOPs), making them unsuitable for resource-limited devices. Transformer-based
models have achieved remarkable success in natural language processing (NLP)
and computer vision (CV) tasks, motivating their use in wireless applications.
However, their use in indoor localization remains nascent, and directly
applying Transformers for indoor localization can be both computationally
intensive and exhibit limitations in accuracy. To address these challenges, in
this work, we introduce a novel tokenization approach, referred to as Sensor
Snapshot Tokenization (SST), which preserves variable-specific representations
of power delay profile (PDP) and enhances attention mechanisms by effectively
capturing multi-variate correlation. Complementing this, we propose a
lightweight Swish-Gated Linear Unit-based Transformer (L-SwiGLU Transformer)
model, designed to reduce computational complexity without compromising
localization accuracy. Together, these contributions mitigate the computational
burden and dependency on large datasets, making Transformer models more
efficient and suitable for resource-constrained scenarios. The proposed
tokenization method enables the Vanilla Transformer to achieve a 90th
percentile positioning error of 0.388 m in a highly NLOS indoor factory,
surpassing conventional tokenization methods. The L-SwiGLU ViT further reduces
the error to 0.355 m, achieving an 8.51% improvement. Additionally, the
proposed model outperforms a 14.1 times larger model with a 46.13% improvement,
underscoring its computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been submitted to IEEE Transactions on Machine Learning
  in Communications and Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetry-Aware Generative Modeling through Learned Canonicalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kusha Sareen, Daniel Levy, Arnab Kumar Mondal, Sékou-Oumar Kaba, Tara Akhound-Sadegh, Siamak Ravanbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling of symmetric densities has a range of applications in AI
for science, from drug discovery to physics simulations. The existing
generative modeling paradigm for invariant densities combines an invariant
prior with an equivariant generative process. However, we observe that this
technique is not necessary and has several drawbacks resulting from the
limitations of equivariant networks. Instead, we propose to model a learned
slice of the density so that only one representative element per orbit is
learned. To accomplish this, we learn a group-equivariant canonicalization
network that maps training samples to a canonical pose and train a
non-equivariant generative model over these canonicalized samples. We implement
this idea in the context of diffusion models. Our preliminary experimental
results on molecular modeling are promising, demonstrating improved sample
quality and faster inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurReps 2024 Workshop Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BMIP: Bi-directional Modality Interaction <span class="highlight-title">Prompt</span> Learning for VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song-Lin Lv, Yu-Yang Chen, Zhi Zhou, Ming Yang, Lan-Zhe Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have exhibited remarkable generalization
capabilities, and prompt learning for VLMs has attracted great attention for
the ability to adapt pre-trained VLMs to specific downstream tasks. However,
existing studies mainly focus on single-modal prompts or uni-directional
modality interaction, overlooking the powerful alignment effects resulting from
the interaction between the vision and language modalities. To this end, we
propose a novel prompt learning method called
$\underline{\textbf{B}}i-directional \underline{\textbf{M}}odality
\underline{\textbf{I}}nteraction \underline{\textbf{P}}rompt (BMIP)$, which
dynamically weights bi-modal information through learning the information of
the attention layer, enhancing trainability and inter-modal consistency
compared to simple information aggregation methods. To evaluate the
effectiveness of prompt learning methods, we propose a more realistic
evaluation paradigm called open-world generalization complementing the widely
adopted cross-dataset transfer and domain generalization tasks. Comprehensive
experiments on various datasets reveal that BMIP not only outperforms current
state-of-the-art methods across all three evaluation paradigms but is also
flexible enough to be combined with other prompt-based methods for consistent
performance enhancement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying the Importance of Data Alignment in Downstream Model
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krrish Chawla, Aryan Sahai, Mario DePavia, Sudharsan Sundar, Brando Miranda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrary to the conventional emphasis on dataset size, we explore the role of
data alignment -- an often overlooked aspect of data quality -- in training
capable Large Language Models (LLMs). To do so, we use the Task2Vec-based
alignment coefficient, a quantitative measure of the similarity between two
datasets, to quantify the impact of alignment between training data and
evaluation data on downstream performance. In particular, we conduct controlled
\textit{interventional} experiments for two settings: 1. the impact of
increased alignment coefficients between various pre-training (pt) against
evaluation datasets, and 2. the impact of increased alignment coefficients
between domain specific fine-tuning (ft) against domain specific evaluation.
The domain specific task we explore is Autoformalization -- the machine
translation task between natural language and code for formal verification. In
both settings, we find a strong, predictable negative correlation between the
alignment coefficient of a model's training and evaluation data and the model's
loss/perplexity on the respective downstream task. These findings suggest a
re-evaluation of LLM training approaches, demonstrating the relevance of data
alignment compared to data quantity, especially in specialized downstream tasks
such as Autoformalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLAVARS: A Multimodal <span class="highlight-title">Foundation</span>al Language and Vision Alignment Model
  for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Corley, Simone Fobi Nsutezo, Anthony Ortiz, Caleb Robinson, Rahul Dodhia, Juan M. Lavista Ferres, Peyman Najafirad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing imagery is dense with objects and contextual visual
information. There is a recent trend to combine paired satellite images and
text captions for pretraining performant encoders for downstream tasks.
However, while contrastive image-text methods like CLIP enable vision-language
alignment and zero-shot classification ability, vision-only downstream
performance tends to degrade compared to image-only pretraining, such as MAE.
In this paper, we propose FLAVARS, a pretraining method that combines the best
of both contrastive learning and masked modeling, along with geospatial
alignment via contrastive location encoding. We find that FLAVARS significantly
outperforms a baseline of SkyCLIP for vision-only tasks such as KNN
classification and semantic segmentation, +6\% mIOU on SpaceNet1, while
retaining the ability to perform zero-shot classification, unlike MAE
pretrained methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time series forecasting for multidimensional telemetry data using GAN
  and BiLSTM in a Digital Twin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Carmo de Almeida Neto, Claudio Miceli de Farias, Leandro Santiago de Araujo, Leopoldo Andre Dutra Lusquino Filho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research related to digital twins has been increasing in recent years.
Besides the mirroring of the physical word into the digital, there is the need
of providing services related to the data collected and transferred to the
virtual world. One of these services is the forecasting of physical part future
behavior, that could lead to applications, like preventing harmful events or
designing improvements to get better performance. One strategy used to predict
any system operation it is the use of time series models like ARIMA or LSTM,
and improvements were implemented using these algorithms. Recently, deep
learning techniques based on generative models such as Generative Adversarial
Networks (GANs) have been proposed to create time series and the use of LSTM
has gained more relevance in time series forecasting, but both have limitations
that restrict the forecasting results. Another issue found in the literature is
the challenge of handling multivariate environments/applications in time series
generation. Therefore, new methods need to be studied in order to fill these
gaps and, consequently, provide better resources for creating useful digital
twins. In this proposal, it is going to be studied the integration of a BiLSTM
layer with a time series obtained by GAN in order to improve the forecasting of
all the features provided by the dataset in terms of accuracy and,
consequently, improving behaviour prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Head Motion Degrades Machine Learning Classification of Alzheimer's
  Disease from Positron Emission Tomo<span class="highlight-title">graph</span>y 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eléonore V. Lieffrig, Takuya Toyonaga, Jiazhen Zhang, John A. Onofrey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain positron emission tomography (PET) imaging is broadly used in research
and clinical routines to study, diagnose, and stage Alzheimer's disease (AD).
However, its potential cannot be fully exploited yet due to the lack of
portable motion correction solutions, especially in clinical settings. Head
motion during data acquisition has indeed been shown to degrade image quality
and induces tracer uptake quantification error. In this study, we demonstrate
that it also biases machine learning-based AD classification. We start by
proposing a binary classification algorithm solely based on PET images. We find
that it reaches a high accuracy in classifying motion corrected images into
cognitive normal or AD. We demonstrate that the classification accuracy
substantially decreases when images lack motion correction, thereby limiting
the algorithm's effectiveness and biasing image interpretation. We validate
these findings in cohorts of 128 $^{11}$C-UCB-J and 173 $^{18}$F-FDG scans, two
tracers highly relevant to the study of AD. Classification accuracies decreased
by 10% and 5% on 20 $^{18}$F-FDG and 20 $^{11}$C-UCB-J testing cases,
respectively. Our findings underscore the critical need for efficient motion
correction methods to make the most of the diagnostic capabilities of PET-based
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models For Text Classification: Case Study And
  Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arina Kostina, Marios D. Dikaiakos, Dimosthenis Stefanidis, George Pallis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlocking the potential of Large Language Models (LLMs) in data
classification represents a promising frontier in natural language processing.
In this work, we evaluate the performance of different LLMs in comparison with
state-of-the-art deep-learning and machine-learning models, in two different
classification scenarios: i) the classification of employees' working locations
based on job reviews posted online (multiclass classification), and 2) the
classification of news articles as fake or not (binary classification). Our
analysis encompasses a diverse range of language models differentiating in
size, quantization, and architecture. We explore the impact of alternative
prompting techniques and evaluate the models based on the weighted F1-score.
Also, we examine the trade-off between performance (F1-score) and time
(inference response time) for each language model to provide a more nuanced
understanding of each model's practical applicability. Our work reveals
significant variations in model responses based on the prompting strategies. We
find that LLMs, particularly Llama3 and GPT-4, can outperform traditional
methods in complex classification tasks, such as multiclass classification,
though at the cost of longer inference times. In contrast, simpler ML models
offer better performance-to-time trade-offs in simpler binary classification
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Rate-In: Information-Driven Adaptive Dropout Rates for Improved
  Inference-Time Uncertainty Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tal Zeevi, Ravid Shwartz-Ziv, <span class="highlight-author">Yann LeCun</span>, Lawrence H. Staib, John A. Onofrey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty estimation is crucial for deploying neural networks in
risk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a
widely used technique for approximating predictive uncertainty by performing
stochastic forward passes with dropout during inference. However, using static
dropout rates across all layers and inputs can lead to suboptimal uncertainty
estimates, as it fails to adapt to the varying characteristics of individual
inputs and network layers. Existing approaches optimize dropout rates during
training using labeled data, resulting in fixed inference-time parameters that
cannot adjust to new data distributions, compromising uncertainty estimates in
Monte Carlo simulations.
  In this paper, we propose Rate-In, an algorithm that dynamically adjusts
dropout rates during inference by quantifying the information loss induced by
dropout in each layer's feature maps. By treating dropout as controlled noise
injection and leveraging information-theoretic principles, Rate-In adapts
dropout rates per layer and per input instance without requiring ground truth
labels. By quantifying the functional information loss in feature maps, we
adaptively tune dropout rates to maintain perceptual quality across diverse
medical imaging tasks and architectural configurations. Our extensive empirical
study on synthetic data and real-world medical imaging tasks demonstrates that
Rate-In improves calibration and sharpens uncertainty estimates compared to
fixed or heuristic dropout rates without compromising predictive performance.
Rate-In offers a practical, unsupervised, inference-time approach to optimizing
dropout for more reliable predictive uncertainty estimation in critical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated author affiliation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Distribution Matching of Representations via Noise-Injected
  Deep InfoMax 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Butakov, Alexander Semenenko, Alexander Tolmachev, Andrey Gladkov, Marina Munkhoeva, Alexey Frolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep InfoMax (DIM) is a well-established method for self-supervised
representation learning (SSRL) based on maximization of the mutual information
between the input and the output of a deep neural network encoder. Despite the
DIM and contrastive SSRL in general being well-explored, the task of learning
representations conforming to a specific distribution (i.e., distribution
matching, DM) is still under-addressed. Motivated by the importance of DM to
several downstream tasks (including generative modeling, disentanglement,
outliers detection and other), we enhance DIM to enable automatic matching of
learned representations to a selected prior distribution. To achieve this, we
propose injecting an independent noise into the normalized outputs of the
encoder, while keeping the same InfoMax training objective. We show that such
modification allows for learning uniformly and normally distributed
representations, as well as representations of other absolutely continuous
distributions. Our approach is tested on various downstream tasks. The results
indicate a moderate trade-off between the performance on the downstream tasks
and quality of DM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 7 fugures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic <span class="highlight-title">Prompt</span>
  Optimization for Text Generation <span class="chip">AAAI-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02748v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02748v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing automatic prompt engineering methods are typically designed for
discriminative tasks, where new task prompts are iteratively refined with
limited feedback from a single metric reflecting a single aspect. However,
these approaches are suboptimal for generative tasks, which require more
nuanced guidance beyond a single numeric metric to improve the prompt and
optimize multiple aspects of the generated text. To address these challenges,
we propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt
Optimization (CriSPO) approach. CriSPO introduces a critique-suggestion module
as its core component. This module spontaneously discovers aspects, and
compares generated and reference texts across these aspects, providing specific
suggestions for prompt modification. These clear critiques and actionable
suggestions guide a receptive optimizer module to make more substantial
changes, exploring a broader and more effective search space. To further
improve CriSPO with multi-metric optimization, we introduce an Automatic Suffix
Tuning (AST) extension to enhance the performance of task prompts across
multiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4
summarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score
improvement on summarization and substantial improvement of various metrics on
QA. Code available at https://github.com/amazon-science/crispo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Detection and Analysis of Minor Deformations in Flat Walls Due
  to Railway Vibrations Using LiDAR and Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surjo Dey, Ankit Sharma, Hritu Raj, Susham Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces an advanced methodology for automatically identifying
minor deformations in flat walls caused by vibrations from nearby railway
tracks. It leverages high-density Terrestrial Laser Scanner (TLS) LiDAR surveys
and AI/ML techniques to collect and analyze data. The scan data is processed
into a detailed point cloud, which is segmented to distinguish ground points,
trees, buildings, and other objects. The analysis focuses on identifying
sections along flat walls and estimating their deformations relative to the
ground orientation.
  Findings from the study, conducted at the RGIPT campus, reveal significant
deformations in walls close to the railway corridor, with the highest
deformations ranging from 7 to 8 cm and an average of 3 to 4 cm. In contrast,
walls further from the corridor show negligible deformations. The developed
automated process for feature extraction and deformation monitoring
demonstrates potential for structural health monitoring. By integrating LiDAR
data with machine learning, the methodology provides an efficient system for
identifying and analyzing structural deformations, highlighting the importance
of continuous monitoring for ensuring structural integrity and public safety in
urban infrastructure. This approach represents a substantial advancement in
automated feature extraction and deformation analysis, contributing to more
effective management of urban infrastructure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I am requesting the withdrawal of my paper due to the need for
  significant revisions to ensure the accuracy and integrity of the presented
  findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Agnostic Modeling of Source Reliability on Wikipedia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacopo D'Ignazi, Andreas Kaltenbrunner, Yelena Mejova, Michele Tizzani, Kyriaki Kalimeri, Mariano Beiró, Pablo Aragón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last few years, content verification through reliable sources has
become a fundamental need to combat disinformation. Here, we present a
language-agnostic model designed to assess the reliability of sources across
multiple language editions of Wikipedia. Utilizing editorial activity data, the
model evaluates source reliability within different articles of varying
controversiality such as Climate Change, COVID-19, History, Media, and Biology
topics. Crafting features that express domain usage across articles, the model
effectively predicts source reliability, achieving an F1 Macro score of
approximately 0.80 for English and other high-resource languages. For
mid-resource languages, we achieve 0.65 while the performance of low-resource
languages varies; in all cases, the time the domain remains present in the
articles (which we dub as permanence) is one of the most predictive features.
We highlight the challenge of maintaining consistent model performance across
languages of varying resource levels and demonstrate that adapting models from
higher-resource languages can improve performance. This work contributes not
only to Wikipedia's efforts in ensuring content verifiability but in ensuring
reliability across diverse user-generated content in various language
communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of <span class="highlight-title">Foundation</span> Models in Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) are large-scale deep learning models that are
developed using large datasets and self-supervised learning methods. These
models serve as a base for different downstream tasks, including healthcare.
FMs have been adopted with great success across various domains within
healthcare. Existing healthcare-based surveys have not yet included all of
these domains. Therefore, we provide a detailed survey of FMs in healthcare. We
focus on the history, learning strategies, flagship models, applications, and
challenges of FMs. We explore how FMs such as the BERT and GPT families are
reshaping various healthcare domains, including clinical large language models,
medical image analysis, and omics. Furthermore, we provide a detailed taxonomy
of healthcare applications facilitated by FMs, such as clinical NLP, medical
computer vision, graph learning, and other biology-related tasks. Despite the
promising opportunities FMs provide, they also have several associated
challenges, which are explained in detail. We also outline open research issues
and potential lessons learned to provide researchers and practitioners with
insights into the capabilities of FMs in healthcare to advance their deployment
and mitigate associated risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pareto Set Learning for Multi-Objective Reinforcement Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erlong Liu, Yu-Chang Wu, Xiaobin Huang, Chengrui Gao, Ren-Jian Wang, Ke Xue, Chao Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective decision-making problems have emerged in numerous real-world
scenarios, such as video games, navigation and robotics. Considering the clear
advantages of Reinforcement Learning (RL) in optimizing decision-making
processes, researchers have delved into the development of Multi-Objective RL
(MORL) methods for solving multi-objective decision problems. However, previous
methods either cannot obtain the entire Pareto front, or employ only a single
policy network for all the preferences over multiple objectives, which may not
produce personalized solutions for each preference. To address these
limitations, we propose a novel decomposition-based framework for MORL, Pareto
Set Learning for MORL (PSL-MORL), that harnesses the generation capability of
hypernetwork to produce the parameters of the policy network for each
decomposition weight, generating relatively distinct policies for various
scalarized subproblems with high efficiency. PSL-MORL is a general framework,
which is compatible for any RL algorithm. The theoretical result guarantees the
superiority of the model capacity of PSL-MORL and the optimality of the
obtained policy network. Through extensive experiments on diverse benchmarks,
we demonstrate the effectiveness of PSL-MORL in achieving dense coverage of the
Pareto front, significantly outperforming state-of-the-art MORL methods in the
hypervolume and sparsity indicators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025 Accept</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feedback-driven object detection and iterative model improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated object detection has become increasingly valuable across diverse
applications, yet efficient, high-quality annotation remains a persistent
challenge. In this paper, we present the development and evaluation of a
platform designed to interactively improve object detection models. The
platform allows uploading and annotating images as well as fine-tuning object
detection models. Users can then manually review and refine annotations,
further creating improved snapshots that are used for automatic object
detection on subsequent image uploads - a process we refer to as semi-automatic
annotation resulting in a significant gain in annotation efficiency.
  Whereas iterative refinement of model results to speed up annotation has
become common practice, we are the first to quantitatively evaluate its
benefits with respect to time, effort, and interaction savings. Our
experimental results show clear evidence for a significant time reduction of up
to 53% for semi-automatic compared to manual annotation. Importantly, these
efficiency gains did not compromise annotation quality, while matching or
occasionally even exceeding the accuracy of manual annotations. These findings
demonstrate the potential of our lightweight annotation platform for creating
high-quality object detection datasets and provide best practices to guide
future development of annotation platforms.
  The platform is open-source, with the frontend and backend repositories
available on GitHub (https://github.com/ml-lab-htw/iterative-annotate). To
support the understanding of our labeling process, we have created an
explanatory video demonstrating the methodology using microscopy images of E.
coli bacteria as an example. The video is available on YouTube
(https://www.youtube.com/watch?v=CM9uhE8NN5E).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AI4EA24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark
  Detection <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jui-Che Chiang, Hou-Ning Hu, Bo-Syuan Hou, Chia-Yu Tseng, Yu-Lun Liu, Min-Hung Chen, Yen-Yu Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although facial landmark detection (FLD) has gained significant progress,
existing FLD methods still suffer from performance drops on partially
non-visible faces, such as faces with occlusions or under extreme lighting
conditions or poses. To address this issue, we introduce ORFormer, a novel
transformer-based method that can detect non-visible regions and recover their
missing features from visible parts. Specifically, ORFormer associates each
image patch token with one additional learnable token called the messenger
token. The messenger token aggregates features from all but its patch. This
way, the consensus between a patch and other patches can be assessed by
referring to the similarity between its regular and messenger embeddings,
enabling non-visible region identification. Our method then recovers occluded
patches with features aggregated by the messenger tokens. Leveraging the
recovered features, ORFormer compiles high-quality heatmaps for the downstream
FLD task. Extensive experiments show that our method generates heatmaps
resilient to partial occlusions. By integrating the resultant heatmaps into
existing FLD methods, our method performs favorably against the state of the
arts on challenging datasets such as WFLW and COFW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025 Project Link: https://ben0919.github.io/ORFormer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WINE: Wavelet-Guided GAN Inversion and Editing for High-Fidelity
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaewon Kim, Seung-Jun Moon, Gyeong-Moon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advanced GAN inversion models aim to convey high-fidelity information
from original images to generators through methods using generator tuning or
high-dimensional feature learning. Despite these efforts, accurately
reconstructing image-specific details remains as a challenge due to the
inherent limitations both in terms of training and structural aspects, leading
to a bias towards low-frequency information. In this paper, we look into the
widely used pixel loss in GAN inversion, revealing its predominant focus on the
reconstruction of low-frequency features. We then propose WINE, a
Wavelet-guided GAN Inversion aNd Editing model, which transfers the
high-frequency information through wavelet coefficients via newly proposed
wavelet loss and wavelet fusion scheme. Notably, WINE is the first attempt to
interpret GAN inversion in the frequency domain. Our experimental results
showcase the precision of WINE in preserving high-frequency details and
enhancing image quality. Even in editing scenarios, WINE outperforms existing
state-of-the-art GAN inversion models with a fine balance between editability
and reconstruction quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out
  Context Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengyuan Liu, Nikhil Kandpal, Colin Raffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The influence of contextual input on the behavior of large language models
(LLMs) has prompted the development of context attribution methods that aim to
quantify each context span's effect on an LLM's generations. The leave-one-out
(LOO) error, which measures the change in the likelihood of the LLM's response
when a given span of the context is removed, provides a principled way to
perform context attribution, but can be prohibitively expensive to compute for
large models. In this work, we introduce AttriBoT, a series of novel techniques
for efficiently computing an approximation of the LOO error for context
attribution. Specifically, AttriBoT uses cached activations to avoid redundant
operations, performs hierarchical attribution to reduce computation, and
emulates the behavior of large target models with smaller proxy models. Taken
together, AttriBoT can provide a >300x speedup while remaining more faithful to
a target model's LOO error than prior context attribution methods. This stark
increase in performance makes computing context attributions for a given
response 30x faster than generating the response itself, empowering real-world
applications that require computing attributions at scale. We release a
user-friendly and efficient implementation of AttriBoT to enable efficient LLM
interpretability as well as encourage future development of efficient context
attribution methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Electricity Price Prediction Using Multi-Kernel Gaussian Process
  Regression Combined with Kernel-Based Support Vector Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Das, Stephan Schlüter, Lorenz Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new hybrid model for predicting German electricity
prices. The algorithm is based on combining Gaussian Process Regression (GPR)
and Support Vector Regression (SVR). While GPR is a competent model for
learning the stochastic pattern within the data and interpolation, its
performance for out-of-sample data is not very promising. By choosing a
suitable data-dependent covariance function, we can enhance the performance of
GPR for the tested German hourly power prices. However, since the out-of-sample
prediction depends on the training data, the prediction is vulnerable to noise
and outliers. To overcome this issue, a separate prediction is made using SVR,
which applies margin-based optimization, having an advantage in dealing with
non-linear processes and outliers, since only certain necessary points (support
vectors) in the training data are responsible for regression. Both individual
predictions are later combined using the performance-based weight assignment
method. A test on historic German power prices shows that this approach
outperforms its chosen benchmarks such as the autoregressive exogenous model,
the naive approach, as well as the long short-term memory approach of
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Set-based Neural Network Encoding Without Weight Tying 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16625v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16625v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Andreis, Soro Bedionita, Philip H. S. Torr, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a neural network weight encoding method for network property
prediction that utilizes set-to-set and set-to-vector functions to efficiently
encode neural network parameters. Our approach is capable of encoding neural
networks in a model zoo of mixed architecture and different parameter sizes as
opposed to previous approaches that require custom encoding models for
different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural
network \textbf{E}ncoder (SNE) takes into consideration the hierarchical
computational structure of neural networks. To respect symmetries inherent in
network weight space, we utilize Logit Invariance to learn the required minimal
invariance properties. Additionally, we introduce a \textit{pad-chunk-encode}
pipeline to efficiently encode neural network layers that is adjustable to
computational and memory constraints. We also introduce two new tasks for
neural network property prediction: cross-dataset and cross-architecture. In
cross-dataset property prediction, we evaluate how well property predictors
generalize across model zoos trained on different datasets but of the same
architecture. In cross-architecture property prediction, we evaluate how well
property predictors transfer to model zoos of different architecture not seen
during training. We show that SNE outperforms the relevant baselines on
standard benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximation Rates in Fréchet Metrics: Barron Spaces, Paley-Wiener
  Spaces, and Fourier Multipliers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdeljawad, Thomas Dittrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operator learning is a recent development in the simulation of Partial
Differential Equations (PDEs) by means of neural networks. The idea behind this
approach is to learn the behavior of an operator, such that the resulting
neural network is an (approximate) mapping in infinite-dimensional spaces that
is capable of (approximately) simulating the solution operator governed by the
PDE. In our work, we study some general approximation capabilities for linear
differential operators by approximating the corresponding symbol in the Fourier
domain. Analogous to the structure of the class of H\"ormander-Symbols, we
consider the approximation with respect to a topology that is induced by a
sequence of semi-norms. In that sense, we measure the approximation error in
terms of a Fr\'echet metric, and our main result identifies sufficient
conditions for achieving a predefined approximation error. Secondly, we then
focus on a natural extension of our main theorem, in which we manage to reduce
the assumptions on the sequence of semi-norms. Based on existing approximation
results for the exponential spectral Barron space, we then present a concrete
example of symbols that can be approximated well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Federated <span class="highlight-title">Graph</span> Learning in One-shot Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11304v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11304v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guochen Yan, Xunkai Li, Luyuan Xie, Wentao Zhang, Qingni Shen, Yuejian Fang, Zhonghai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Graph Learning (FGL) has emerged as a promising paradigm for
breaking data silos among distributed private graphs. In practical scenarios
involving heterogeneous distributed graph data, personalized Federated Graph
Learning (pFGL) aims to enhance model utility by training personalized models
tailored to client needs. However, existing pFGL methods often require numerous
communication rounds under heterogeneous graphs, leading to significant
communication overhead and security concerns. While One-shot Federated Learning
(OFL) enables collaboration in a single round, existing OFL methods are
designed for image-centric tasks and ineffective for graph data, leaving a
critical gap in the field. Additionally, personalized models derived from
existing methods suffer from bias, failing to effectively generalize to the
minority. To address these challenges, we propose the first $\textbf{O}$ne-shot
$\textbf{p}$ersonalized $\textbf{F}$ederated $\textbf{G}$raph
$\textbf{L}$earning method ($\textbf{O-pFGL}$) for node classification,
compatible with Secure Aggregation protocols for privacy preservation.
Specifically, for effective graph learning in one communication round, our
method estimates and aggregates class-wise feature distribution statistics to
construct a global pseudo-graph on the server, facilitating the training of a
global graph model. To mitigate bias, we introduce a two-stage personalized
training approach that adaptively balances local personal information and
global insights from the pseudo-graph, improving both personalization and
generalization. Extensive experiments on 12 multi-scale graph datasets
demonstrate that our method significantly outperforms state-of-the-art
baselines across various settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Sub-<span class="highlight-title">graph</span> Distillation for Robust Semi-supervised Continual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Fan, Yu Wang, Pengfei Zhu, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) has shown promising results and comparable
performance to learning at once in a fully supervised manner. However, CL
strategies typically require a large number of labeled samples, making their
real-life deployment challenging. In this work, we focus on semi-supervised
continual learning (SSCL), where the model progressively learns from partially
labeled data with unknown categories. We provide a comprehensive analysis of
SSCL and demonstrate that unreliable distributions of unlabeled data lead to
unstable training and refinement of the progressing stages. This problem
severely impacts the performance of SSCL. To address the limitations, we
propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for
semi-supervised continual learning, which leverages both semantic and
structural information to achieve more stable knowledge distillation on
unlabeled data and exhibit robustness against distribution bias. Firstly, we
formalize a general model of structural distillation and design a dynamic graph
construction for the continual learning progress. Next, we define a structure
distillation vector and design a dynamic sub-graph distillation algorithm,
which enables end-to-end training and adaptability to scale up tasks. The
entire proposed method is adaptable to various CL methods and supervision
settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,
and ImageNet-100, with varying supervision ratios, demonstrate the
effectiveness of our proposed approach in mitigating the catastrophic
forgetting problem in semi-supervised continual learning scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balanced Neural ODEs: nonlinear model order reduction and Koopman
  operator approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10174v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10174v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Aka, Johannes Brunnemann, Jörg Eiden, Arne Speerforck, Lars Mikelsons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational Autoencoders (VAEs) are a powerful framework for learning latent
representations of reduced dimensionality, while Neural ODEs excel in learning
transient system dynamics. This work combines the strengths of both to generate
fast surrogate models with adjustable complexity reacting on time-varying
inputs signals. By leveraging the VAE's dimensionality reduction using a
nonhierarchical prior, our method adaptively assigns stochastic noise,
naturally complementing known NeuralODE training enhancements and enabling
probabilistic time series modeling. We show that standard Latent ODEs struggle
with dimensionality reduction in systems with time-varying inputs. Our approach
mitigates this by continuously propagating variational parameters through time,
establishing fixed information channels in latent space. This results in a
flexible and robust method that can learn different system complexities, e.g.
deep neural networks or linear matrices. Hereby, it enables efficient
approximation of the Koopman operator without the need for predefining its
dimensionality. As our method balances dimensionality reduction and
reconstruction accuracy, we call it Balanced Neural ODE (B-NODE). We
demonstrate the effectiveness of this methods on several academic and
real-world test cases, e.g. a power plant or MuJoCo data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference paper under review, after revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spurious Feature Eraser: Stabilizing Test-Time Adaptation for
  Vision-Language <span class="highlight-title">Foundation</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language foundation models have exhibited remarkable success across a
multitude of downstream tasks due to their scalability on extensive image-text
paired data. However, these models also display significant limitations when
applied to downstream tasks, such as fine-grained image classification, as a
result of ``decision shortcuts'' that hinder their generalization capabilities.
In this work, we find that the CLIP model possesses a rich set of features,
encompassing both \textit{desired invariant causal features} and
\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP
on downstream tasks originates from its inability to effectively utilize
pre-trained features in accordance with specific task requirements. To address
this challenge, we propose a simple yet effective method, Spurious Feature
Eraser (SEraser), to alleviate the decision shortcuts by erasing the spurious
features. Specifically, we introduce a test-time prompt tuning paradigm that
optimizes a learnable prompt, thereby compelling the model to exploit invariant
features while disregarding decision shortcuts during the inference phase. The
proposed method effectively alleviates excessive dependence on potentially
misleading spurious information. We conduct comparative analysis of the
proposed method against various approaches which validates the significant
superiority.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImagiNet: A Multi-Content Benchmark for Synthetic Image Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delyan Boychev, Radostin Cholakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent generative models produce images with a level of authenticity that
makes them nearly indistinguishable from real photos and artwork. Potential
harmful use cases of these models, necessitate the creation of robust synthetic
image detectors. However, current datasets in the field contain generated
images with questionable quality or have examples from one predominant content
type which leads to poor generalizability of the underlying detectors. We find
that the curation of a balanced amount of high-resolution generated images
across various content types is crucial for the generalizability of detectors,
and introduce ImagiNet, a dataset of 200K examples, spanning four categories:
photos, paintings, faces, and miscellaneous. Synthetic images in ImagiNet are
produced with both open-source and proprietary generators, whereas real
counterparts for each content type are collected from public datasets. The
structure of ImagiNet allows for a two-track evaluation system: i)
classification as real or synthetic and ii) identification of the generative
model. To establish a strong baseline, we train a ResNet-50 model using a
self-supervised contrastive objective (SelfCon) for each track which achieves
evaluation AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%,
even under conditions that involve compression and resizing. The provided model
is generalizable enough to achieve zero-shot state-of-the-art performance on
previous synthetic detection benchmarks. We provide ablations to demonstrate
the importance of content types and publish code and data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Datasets and Evaluators of AI Safety, AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Agent: Leveraging LLMs For Audio Generation, Editing and
  Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Wang, Chi-Keung Tang, Yu-Wing Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Audio-Agent, a multimodal framework for audio generation,
editing and composition based on text or video inputs. Conventional approaches
for text-to-audio (TTA) tasks often make single-pass inferences from text
descriptions. While straightforward, this design struggles to produce
high-quality audio when given complex text conditions. In our method, we
utilize a pre-trained TTA diffusion network as the audio generation agent to
work in tandem with GPT-4, which decomposes the text condition into atomic,
specific instructions and calls the agent for audio generation. In doing so,
Audio-Agent can generate high-quality audio that is closely aligned with the
provided text or video exhibiting complex and multiple events, while supporting
variable-length and variable-volume generation. For video-to-audio (VTA) tasks,
most existing methods require training a timestamp detector to synchronize
video events with the generated audio, a process that can be tedious and
time-consuming. Instead, we propose a simpler approach by fine-tuning a
pre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both
semantic and temporal conditions that bridge the video and audio modality.
Consequently, our framework contributes a comprehensive solution for both TTA
and VTA tasks without substantial computational overhead in training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correlation-Aware <span class="highlight-title">Graph</span> Convolutional Networks for Multi-Label Node
  Classification <span class="chip">KDD2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchen Bei, Weizhi Chen, Hao Chen, Sheng Zhou, Carl Yang, Jiapei Fan, Longtao Huang, Jiajun Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label node classification is an important yet under-explored domain in
graph mining as many real-world nodes belong to multiple categories rather than
just a single one. Although a few efforts have been made by utilizing Graph
Convolution Networks (GCNs) to learn node representations and model
correlations between multiple labels in the embedding space, they still suffer
from the ambiguous feature and ambiguous topology induced by multiple labels,
which reduces the credibility of the messages delivered in graphs and overlooks
the label correlations on graph data. Therefore, it is crucial to reduce the
ambiguity and empower the GCNs for accurate classification. However, this is
quite challenging due to the requirement of retaining the distinctiveness of
each label while fully harnessing the correlation between labels
simultaneously. To address these issues, in this paper, we propose a
Correlation-aware Graph Convolutional Network (CorGCN) for multi-label node
classification. By introducing a novel Correlation-Aware Graph Decomposition
module, CorGCN can learn a graph that contains rich label-correlated
information for each label. It then employs a Correlation-Enhanced Graph
Convolution to model the relationships between labels during message passing to
further bolster the classification process. Extensive experiments on five
datasets demonstrate the effectiveness of our proposed CorGCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted by KDD2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Lebeau, Florent Chatelain, Romain Couillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a comprehensive understanding of the estimation of a
planted low-rank signal from a general spiked tensor model near the
computational threshold. Relying on standard tools from the theory of large
random matrices, we characterize the large-dimensional spectral behavior of the
unfoldings of the data tensor and exhibit relevant signal-to-noise ratios
governing the detectability of the principal directions of the signal. These
results allow to accurately predict the reconstruction performance of truncated
multilinear SVD (MLSVD) in the non-trivial regime. This is particularly
important since it serves as an initialization of the higher-order orthogonal
iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank
approximation depends entirely on its initialization. We give a sufficient
condition for the convergence of HOOI and show that the number of iterations
before convergence tends to $1$ in the large-dimensional limit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System
  Model Fields with Generative Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Hess, Michael Aich, Baoxiang Pan, Niklas Boers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and high-resolution Earth system model (ESM) simulations are
essential to assess the ecological and socio-economic impacts of anthropogenic
climate change, but are computationally too expensive to be run at sufficiently
high spatial resolution. Recent machine learning approaches have shown
promising results in downscaling ESM simulations, outperforming
state-of-the-art statistical approaches. However, existing methods require
computationally costly retraining for each ESM and extrapolate poorly to
climates unseen during training. We address these shortcomings by learning a
consistency model (CM) that efficiently and accurately downscales arbitrary ESM
simulations without retraining in a zero-shot manner. Our approach yields
probabilistic downscaled fields at a resolution only limited by the
observational reference data. We show that the CM outperforms state-of-the-art
diffusion models at a fraction of computational cost while maintaining high
controllability on the downscaling task. Further, our method generalizes to
climate states unseen during training without explicitly formulated physical
constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Symmetries via Weight-Sharing with Doubly Stochastic Tensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Putri A. van der Linden, Alejandro García-Castellanos, Sharvaree Vadgama, Thijs P. Kuipers, Erik J. Bekkers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group equivariance has emerged as a valuable inductive bias in deep learning,
enhancing generalization, data efficiency, and robustness. Classically, group
equivariant methods require the groups of interest to be known beforehand,
which may not be realistic for real-world data. Additionally, baking in fixed
group equivariance may impose overly restrictive constraints on model
architecture. This highlights the need for methods that can dynamically
discover and apply symmetries as soft constraints. For neural network
architectures, equivariance is commonly achieved through group transformations
of a canonical weight tensor, resulting in weight sharing over a given group
$G$. In this work, we propose to learn such a weight-sharing scheme by defining
a collection of learnable doubly stochastic matrices that act as soft
permutation matrices on canonical weight tensors, which can take regular group
representations as a special case. This yields learnable kernel transformations
that are jointly optimized with downstream tasks. We show that when the dataset
exhibits strong symmetries, the permutation matrices will converge to regular
group representations and our weight-sharing networks effectively become
regular group convolutions. Additionally, the flexibility of the method enables
it to effectively pick up on partial symmetries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable and Resource-Efficient Second-Order Federated Learning via
  Over-the-Air Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulmomen Ghalkha, Chaouki Ben Issaid, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Second-order federated learning (FL) algorithms offer faster convergence than
their first-order counterparts by leveraging curvature information. However,
they are hindered by high computational and storage costs, particularly for
large-scale models. Furthermore, the communication overhead associated with
large models and digital transmission exacerbates these challenges, causing
communication bottlenecks. In this work, we propose a scalable second-order FL
algorithm using a sparse Hessian estimate and leveraging over-the-air
aggregation, making it feasible for larger models. Our simulation results
demonstrate more than $67\%$ of communication resources and energy savings
compared to other first and second-order baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 4 subfigures, letter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Decoders for Transformer-based Semantic Segmentation: A
  Compression Perspective <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qishuai Wen, Chun-Guang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for Transformer-based semantic segmentation
typically adopt Transformer decoders that are used to extract additional
embeddings from image embeddings via cross-attention, refine either or both
types of embeddings via self-attention, and project image embeddings onto the
additional embeddings via dot-product. Despite their remarkable success, these
empirical designs still lack theoretical justifications or interpretations,
thus hindering potentially principled improvements. In this paper, we argue
that there are fundamental connections between semantic segmentation and
compression, especially between the Transformer decoders and Principal
Component Analysis (PCA). From such a perspective, we derive a white-box, fully
attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the
interpretations as follows: 1) the self-attention operator refines image
embeddings to construct an ideal principal subspace that aligns with the
supervision and retains most information; 2) the cross-attention operator seeks
to find a low-rank approximation of the refined image embeddings, which is
expected to be a set of orthonormal bases of the principal subspace and
corresponds to the predefined classes; 3) the dot-product operation yields
compact representation for image embeddings as segmentation masks. Experiments
conducted on dataset ADE20K find that DEPICT consistently outperforms its
black-box counterpart, Segmenter, and it is light weight and more robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning
  Algorithms Based on Reduced Order Markov Decision Process Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehua Zhou, Xuan Xie, Jiayang Song, Zhan Shu, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe Reinforcement Learning (SRL) aims to realize a safe learning process for
Deep Reinforcement Learning (DRL) algorithms by incorporating safety
constraints. However, the efficacy of SRL approaches often relies on accurate
function approximations, which are notably challenging to achieve in the early
learning stages due to data insufficiency. To address this issue, we introduce
in this work a novel Generalizable Safety enhancer (GenSafe) that is able to
overcome the challenge of data insufficiency and enhance the performance of SRL
approaches. Leveraging model order reduction techniques, we first propose an
innovative method to construct a Reduced Order Markov Decision Process (ROMDP)
as a low-dimensional approximator of the original safety constraints. Then, by
solving the reformulated ROMDP-based constraints, GenSafe refines the actions
of the agent to increase the possibility of constraint satisfaction.
Essentially, GenSafe acts as an additional safety layer for SRL algorithms. We
evaluate GenSafe on multiple SRL approaches and benchmark problems. The results
demonstrate its capability to improve safety performance, especially in the
early learning phases, while maintaining satisfactory task performance. Our
proposed GenSafe not only offers a novel measure to augment existing SRL
methods but also shows broad compatibility with various SRL algorithms, making
it applicable to a wide range of systems and SRL problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair CoVariance Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Cavallo, Madeline Navarro, Santiago Segarra, Elvin Isufi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Covariance-based data processing is widespread across signal processing and
machine learning applications due to its ability to model data
interconnectivities and dependencies. However, harmful biases in the data may
become encoded in the sample covariance matrix and cause data-driven methods to
treat different subpopulations unfairly. Existing works such as fair principal
component analysis (PCA) mitigate these effects, but remain unstable in low
sample regimes, which in turn may jeopardize the fairness goal. To address both
biases and instability, we propose Fair coVariance Neural Networks (FVNNs),
which perform graph convolutions on the covariance matrix for both fair and
accurate predictions. Our FVNNs provide a flexible model compatible with
several existing bias mitigation techniques. In particular, FVNNs allow for
mitigating the bias in two ways: first, they operate on fair covariance
estimates that remove biases from their principal components; second, they are
trained in an end-to-end fashion via a fairness regularizer in the loss
function so that the model parameters are tailored to solve the task directly
in a fair manner. We prove that FVNNs are intrinsically fairer than analogous
PCA approaches thanks to their stability in low sample regimes. We validate the
robustness and fairness of our model on synthetic and real-world data,
showcasing the flexibility of FVNNs along with the tradeoff between fair and
accurate performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Attention as a Parametric Endofunctor: A Categorical Framework for
  Transformer Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles O'Neill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-attention mechanisms have revolutionised deep learning architectures,
yet their core mathematical structures remain incompletely understood. In this
work, we develop a category-theoretic framework focusing on the linear
components of self-attention. Specifically, we show that the query, key, and
value maps naturally define a parametric 1-morphism in the 2-category
$\mathbf{Para(Vect)}$. On the underlying 1-category $\mathbf{Vect}$, these maps
induce an endofunctor whose iterated composition precisely models multi-layer
attention. We further prove that stacking multiple self-attention layers
corresponds to constructing the free monad on this endofunctor. For positional
encodings, we demonstrate that strictly additive embeddings correspond to
monoid actions in an affine sense, while standard sinusoidal encodings, though
not additive, retain a universal property among injective (faithful)
position-preserving maps. We also establish that the linear portions of
self-attention exhibit natural equivariance to permutations of input tokens,
and show how the "circuits" identified in mechanistic interpretability can be
interpreted as compositions of parametric 1-morphisms. This categorical
perspective unifies geometric, algebraic, and interpretability-based approaches
to transformer analysis, making explicit the underlying structures of
attention. We restrict to linear maps throughout, deferring the treatment of
nonlinearities such as softmax and layer normalisation, which require more
advanced categorical constructions. Our results build on and extend recent work
on category-theoretic foundations for deep learning, offering deeper insights
into the algebraic structure of attention mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private Collaborative Edge Inference via Over-the-Air Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selim F. Yilmaz, Burak Hasircioglu, Li Qiao, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider collaborative inference at the wireless edge, where each client's
model is trained independently on its local dataset. Clients are queried in
parallel to make an accurate decision collaboratively. In addition to
maximizing the inference accuracy, we also want to ensure the privacy of local
models. To this end, we leverage the superposition property of the multiple
access channel to implement bandwidth-efficient multi-user inference methods.
We propose different methods for ensemble and multi-view classification that
exploit over-the-air computation (OAC). We show that these schemes perform
better than their orthogonal counterparts with statistically significant
differences while using fewer resources and providing privacy guarantees. We
also provide experimental results verifying the benefits of the proposed OAC
approach to multi-user inference, and perform an ablation study to demonstrate
the effectiveness of our design choices. We share the source code of the
framework publicly on Github to facilitate further research and
reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures. This work extends from our preliminary study
  presented at the 2022 IEEE International Symposium on Information Theory [1].
  arXiv admin note: text overlap with arXiv:2202.03129</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of
  Large Language Models in Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangru Lin, Shaoguang Mao, Emanuele La Malfa, Valentin Hofmann, Adrian de Wynter, Xun Wang, Si-Qing Chen, Michael Wooldridge, Janet B. Pierrehumbert, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language is not monolithic. While benchmarks, including those designed for
multiple languages, are often used as proxies to evaluate the performance of
Large Language Models (LLMs), they tend to overlook the nuances of
within-language variation, and thus fail to model the experience of speakers of
non-standard dialects. Focusing on African American Vernacular English (AAVE),
we present the first study aimed at objectively assessing the fairness and
robustness of LLMs in handling dialects in canonical reasoning tasks, including
algorithm, math, logic, and integrated reasoning. We introduce \textbf{ReDial}
(\textbf{Re}asoning with \textbf{Dial}ect Queries), a benchmark containing
1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE
speakers, including experts with computer science backgrounds, to rewrite seven
popular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate
widely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model
families. Our findings reveal that \textbf{almost all of these widely used
models show significant brittleness and unfairness to queries in AAVE}. Our
work establishes a systematic and objective framework for analyzing LLM bias in
dialectal queries. Moreover, it highlights how mainstream LLMs provide unfair
service to dialect speakers in reasoning tasks, laying a critical foundation
for relevant future research. Code and data can be accessed at
https://github.com/fangru-lin/redial_dialect_robustness_fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesis and Analysis of Data as Probability Measures with
  Entropy-Regularized Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brendan Mallery, James M. Murphy, Shuchin Aeron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider synthesis and analysis of probability measures using the
entropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn
divergence. The synthesis problem consists of computing the barycenter, with
respect to these costs, of $m$ reference measures given a set of coefficients
belonging to the $m$-dimensional simplex. The analysis problem consists of
finding the coefficients for the closest barycenter in the Wasserstein-2
distance to a given measure $\mu$. Under the weakest assumptions on the
measures thus far in the literature, we compute the derivative of the
entropy-regularized Wasserstein-2 cost. We leverage this to establish a
characterization of regularized barycenters as solutions to a fixed-point
equation for the average of the entropic maps from the barycenter to the
reference measures. This characterization yields a finite-dimensional, convex,
quadratic program for solving the analysis problem when $\mu$ is a barycenter.
It is shown that these coordinates, as well as the value of the barycenter
functional, can be estimated from samples with dimension-independent rates of
convergence, a hallmark of entropy-regularized optimal transport, and we verify
these rates experimentally. We also establish that barycentric coordinates are
stable with respect to perturbations in the Wasserstein-2 metric, suggesting a
robustness of these coefficients to corruptions. We employ the barycentric
coefficients as features for classification of corrupted point cloud data, and
show that compared to neural network baselines, our approach is more efficient
in small training data regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages. Code to reproduce experiments:
  https://github.com/brendanmallery9/Entropic-Barycenters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KAN KAN Buff Signed <span class="highlight-title">Graph</span> Neural Networks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhieddine Shebaro, Jelena Tešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Representation Learning focuses on creating embeddings for nodes and
edges that capture their features and connections. Graph Neural Networks (GNNs)
use neural networks to model complex graph relationships. The Kolmogorov-Arnold
Neural Network (KAN) has recently emerged as an alternative to the Multi-Layer
Perceptron (MLP), offering better accuracy and interpretability with fewer
parameters. KANs have been applied to GNN tasks. This paper introduces the
integration of KANs into Signed Graph Convolutional Networks (SGCNs). We
evaluate KAN-enhanced SGCNs (KASGCN) on signed community detection and link
sign prediction tasks to improve embedding quality in signed networks. While
the results show some variability, KASGCN performs competitively with or
similarly to the standard SGCN in the functions tested. Its effectiveness
depends on the specific context, such as the signed graph and parameter
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Artificial Intelligence Methods for Lead Time Prediction
  in Non-Cycled Areas of Automotive Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cornelius Hake, Jonas Weigele, Frederik Reichert, Christian Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The present study examines the effectiveness of applying Artificial
Intelligence methods in an automotive production environment to predict unknown
lead times in a non-cycle-controlled production area. Data structures are
analyzed to identify contextual features and then preprocessed using one-hot
encoding. Methods selection focuses on supervised machine learning techniques.
In supervised learning methods, regression and classification methods are
evaluated. Continuous regression based on target size distribution is not
feasible. Classification methods analysis shows that Ensemble Learning and
Support Vector Machines are the most suitable. Preliminary study results
indicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost
yield the best results. After further testing and extensive hyperparameter
optimization, the final method choice is the LightGBM algorithm. Depending on
feature availability and prediction interval granularity, relative prediction
accuracies of up to 90% can be achieved. Further tests highlight the importance
of periodic retraining of AI models to accurately represent complex production
processes using the database. The research demonstrates that AI methods can be
effectively applied to highly variable production data, adding business value
by providing an additional metric for various control tasks while outperforming
current non AI-based systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Set-Based Training for Neural Network Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14961v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14961v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Koller, Tobias Ladner, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are vulnerable to adversarial attacks, i.e., small input
perturbations can significantly affect the outputs of a neural network.
Therefore, to ensure safety of safety-critical environments, the robustness of
a neural network must be formally verified against input perturbations, e.g.,
from noisy sensors. To improve the robustness of neural networks and thus
simplify the formal verification, we present a novel set-based training
procedure in which we compute the set of possible outputs given the set of
possible inputs and compute for the first time a gradient set, i.e., each
possible output has a different gradient. Therefore, we can directly reduce the
size of the output enclosure by choosing gradients toward its center. Small
output enclosures increase the robustness of a neural network and, at the same
time, simplify its formal verification. The latter benefit is due to the fact
that a larger size of propagated sets increases the conservatism of most
verification methods. Our extensive evaluation demonstrates that set-based
training produces robust neural networks with competitive performance, which
can be verified using fast (polynomial-time) verification algorithms due to the
reduced output set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COOL: Efficient and Reliable Chain-Oriented Objective Logic with Neural
  Networks Feedback Control for Program Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jipeng Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program synthesis methods, whether formal or neural-based, lack fine-grained
control and flexible modularity, which limits their adaptation to complex
software development. These limitations stem from rigid Domain-Specific
Language (DSL) frameworks and neural network incorrect predictions. To this
end, we propose the Chain of Logic (CoL), which organizes the synthesis process
into an activity flow and provides heuristic control to guide the process.
Furthermore, by integrating neural networks with libraries and introducing a
Neural Network Feedback Control (NNFC) mechanism, our approach modularizes
synthesis and mitigates the impact of neural network mispredictions.
Experiments on relational and symbolic synthesis tasks show that CoL
significantly enhances the efficiency and reliability of DSL program synthesis
across multiple metrics. Specifically, CoL improves accuracy by 70% while
reducing tree operations by 91% and time by 95%. Additionally, NNFC further
boosts accuracy by 6%, with a 64% reduction in tree operations under
challenging conditions such as insufficient training data, increased
difficulty, and multidomain synthesis. These improvements confirm COOL as a
highly efficient and reliable program synthesis framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoPE: Mixture of <span class="highlight-title">Prompt</span> Experts for Parameter-Efficient and Scalable
  Multimodal Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10568v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10568v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Jiang, Lingbo Liu, Changwen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the demonstrated parameter efficiency of prompt-based multimodal
fusion methods, their limited adaptivity and expressiveness often result in
suboptimal performance compared to other tuning approaches. In this paper, we
introduce the Mixture of Prompt Experts (MoPE), the first technique designed to
overcome these limitations by decomposing standard prompts to capture
instance-level features adaptively. Building on this decomposition, MoPE
enhances prompt fusion's expressiveness by leveraging multimodal pairing priors
to route the most effective prompt for each instance dynamically. Compared to
vanilla prompting, our MoPE-based fusion method exhibits greater
expressiveness, scaling more effectively with the training data and the overall
number of trainable parameters. We also investigate regularization terms for
expert routing, which lead to emergent expert specialization with enhanced
adaptiveness and interpretablity. Extensive experiments across six multimodal
datasets spanning four modalities demonstrate state-of-the-art performance for
prompt fusion, matching or even surpassing the performance of fine-tuning while
requiring only 0.8% of the trainable parameters. Project homepage:
https://github.com/songrise/MoPE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review, Extended version of arxiv:2312.03734</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer-Adaptive State Pruning for Deep State Space Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseon Gwak, Seongrok Moon, Joohwan Ko, PooGyeon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the lack of state dimension optimization methods, deep state space
models (SSMs) have sacrificed model capacity, training search space, or
stability to alleviate computational costs caused by high state dimensions. In
this work, we provide a structured pruning method for SSMs, Layer-Adaptive
STate pruning (LAST), which reduces the state dimension of each layer in
minimizing model-level output energy loss by extending modal truncation for a
single system. LAST scores are evaluated using the $\mathcal{H}_{\infty}$ norms
of subsystems and layer-wise energy normalization. The scores serve as global
pruning criteria, enabling cross-layer comparison of states and layer-adaptive
pruning. Across various sequence benchmarks, LAST optimizes previous SSMs,
revealing the redundancy and compressibility of their state spaces. Notably, we
demonstrate that, on average, pruning 33% of states still maintains performance
with 0.52% accuracy loss in multi-input multi-output SSMs without retraining.
Code is available at https://github.com/msgwak/LAST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Bayesian State Estimation with Compressed Measurement of
  Model-free Process using Semi-supervised Learning <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anubhab Ghosh, Yonina C. Eldar, Saikat Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research topic is: data-driven Bayesian state estimation with compressed
measurement (BSCM) of model-free process, say for a (causal) tracking
application. The dimension of the temporal measurement vector is lower than the
dimension of the temporal state vector to be estimated. Hence the state
estimation problem is an underdetermined inverse problem. The underlying
dynamical model of the states is assumed to be unknown and hence, we use the
terminology 'model-free process'. In absence of the dynamical model, we can not
employ traditional model-driven methods like Kalman Filter (KF) and Particle
Filter (PF), and instead require data-driven methods. We first experimentally
show that two existing unsupervised learning-based data-driven methods fail to
address the BSCM problem for model-free process; they are - data-driven
nonlinear state estimation (DANSE) method and deep Markov model (DMM) method.
The unsupervised learning uses unlabelled data comprised of only noisy, linear
measurements. While DANSE provides a good predictive / forecasting performance
to model the temporal measurement data as time-series, its unsupervised
learning lacks a regularization for state estimation. We then investigate the
use of a semi-supervised learning approach, and develop a semi-supervised
learning-based DANSE method, referred to as SemiDANSE. In SemiDANSE, we use a
limited amount of labelled data along-with a large amount of unlabelled data,
and that helps to bring the desired regularization for addressing the BSCM
problem. The labelled data means pairwise measurement-and-state data. Using
three chaotic dynamical systems (or processes) with nonlinear dynamical models
as benchmark, we show that the data-driven SemiDANSE provides competitive
performance for BSCM against a hybrid method called KalmanNet and two
model-driven methods -- an extended KF (EKF) and an unscented KF (UKF).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, under review at IEEE TSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoMo: A <span class="highlight-title">Foundation</span> Model for Mobile Traffic Forecasting with Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoye Chai, Xiaoqian Qi, Shiyuan Zhang, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile traffic forecasting allows operators to anticipate network dynamics
and performance in advance, offering substantial potential for enhancing
service quality and improving user experience. However, existing models are
often task-oriented and are trained with tailored data, which limits their
effectiveness in diverse mobile network tasks of Base Station (BS) deployment,
resource allocation, energy optimization, etc. and hinders generalization
across different urban environments. Foundation models have made remarkable
strides across various domains of NLP and CV due to their multi-tasking
adaption and zero/few-shot learning capabilities. In this paper, we propose an
innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to
handle diverse forecasting tasks of short/long-term predictions and
distribution generation across multiple cities to support network planning and
optimization. FoMo combines diffusion models and transformers, where various
spatio-temporal masks are proposed to enable FoMo to learn intrinsic features
of different tasks, and a contrastive learning strategy is developed to capture
the correlations between mobile traffic and urban contexts, thereby improving
its transfer learning capability. Extensive experiments on 9 real-world
datasets demonstrate that FoMo outperforms current models concerning diverse
forecasting tasks and zero/few-shot learning, showcasing a strong universality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Less Certain Adversarial Examples Improves Robust
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04539v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04539v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minxing Zhang, Michael Backes, Xiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the robust overfitting phenomenon of adversarial
training. Observing that models with better robust generalization performance
are less certain in predicting adversarially generated training inputs, we
argue that overconfidence in predicting adversarial examples is a potential
cause. Therefore, we hypothesize that generating less certain adversarial
examples improves robust generalization, and propose a formal definition of
adversarial certainty that captures the variance of the model's predicted
logits on adversarial examples. Our theoretical analysis of synthetic
distributions characterizes the connection between adversarial certainty and
robust generalization. Accordingly, built upon the notion of adversarial
certainty, we develop a general method to search for models that can generate
training-time adversarial inputs with reduced certainty, while maintaining the
model's capability in distinguishing adversarial examples. Extensive
experiments on image benchmarks demonstrate that our method effectively learns
models with consistently improved robustness and mitigates robust overfitting,
confirming the importance of generating less certain adversarial examples for
robust generalization. Our implementations are available as open-source code
at: https://github.com/TrustMLRG/AdvCertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A User's Guide to $\texttt{KSig}$: GPU-Accelerated Computation of the
  Signature Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Csaba Tóth, Danilo Jr Dela Cruz, Harald Oberhauser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The signature kernel is a positive definite kernel for sequential and
temporal data that has become increasingly popular in machine learning
applications due to powerful theoretical guarantees, strong empirical
performance, and recently introduced various scalable variations. In this
chapter, we give a short introduction to $\texttt{KSig}$, a
$\texttt{Scikit-Learn}$ compatible Python package that implements various
GPU-accelerated algorithms for computing signature kernels, and performing
downstream learning tasks. We also introduce a new algorithm based on tensor
sketches which gives strong performance compared to existing algorithms. The
package is available at https://github.com/tgcsaba/ksig.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Gradient Subspaces: Addressing and Overcoming LoRA's
  Limitations in Federated Fine-Tuning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23111v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23111v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navyansh Mahla, Kshitij Sharad Jadhav, Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various domains, particularly in task generalization for both text and vision
data. While fine-tuning these models can significantly enhance their
performance on specific downstream tasks, it often requires high-quality data
that cannot be shared due to privacy concerns. Federated Learning (FL) offers a
promising solution for collaborative training without direct data sharing.
However, many parameter-efficient fine-tuning strategies for LLMs in FL,
particularly those based on Low-Rank Adaptation (LoRA), face limitations. In
this paper, we critically analyze the convergence and performance guarantees of
popular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to
constrained subspace learning of low-rank matrices. This limitation hinders
effective fine-tuning of LLMs in federated settings. Through rigorous
analytical and empirical evaluations, we demonstrate that direct weight
averaging outperforms LoRA-based strategies, leading to superior performance
for fine-tuned models. Our comprehensive comparison unmasks inefficiencies in
LoRA approaches and underscores the advantages of direct weight aggregation. We
extend our analysis to low-rank gradient-based optimizers, such as GaLore, used
during local training steps. Our findings show that GaLore along with
direct-weight aggregation is a more effective approach, outperforming federated
LoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.
While privacy remains paramount in FL discourse, our focus is on assessing
performance outcomes of federated fine-tuned models and evaluating various FL
frameworks from both theoretical and empirical perspectives. Our findings
advocate reassessing the reliance on LoRA within FL contexts, paving the way
for more efficient training methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Policy Enables In-Context Reinforcement Learning within Trust
  Horizons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqin Chen, Santiago Paternain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained foundation models have exhibited extraordinary in-context learning
performance, allowing zero-shot generalization to new tasks not encountered
during pretraining. In the case of reinforcement learning (RL), in-context RL
(ICRL) emerges when pretraining FMs on decision-making problems in an
autoregressive-supervised manner. Nevertheless, current state-of-the-art ICRL
algorithms, like Algorithm Distillation, Decision Pretrained Transformer and
Decision Importance Transformer, impose stringent requirements on the
pretraining dataset concerning the source policies, context information, and
action labels. Notably, these algorithms either demand optimal policies or
require varying degrees of well-trained behavior policies for all pretraining
environments. This significantly hinders the application of ICRL to real-world
scenarios, where acquiring optimal or well-trained policies for a substantial
volume of real-world training environments can be intractable. To overcome this
challenge, we introduce a novel approach, termed State-Action Distillation
(SAD), that allows to generate an effective pretraining dataset guided solely
by random policies. In particular, SAD selects query states and corresponding
action labels by distilling outstanding state-action pairs from the entire
state and action spaces by using random policies within a trust horizon, and
then inherits the classical autoregressive-supervised mechanism during
pretraining. To the best of our knowledge, this is the first work that enables
effective ICRL under random policies and random contexts. We also establish
quantitative analysis of the trustworthiness as well as the performance
guarantees of SAD. Moreover, our empirical results across multiple popular ICRL
benchmark environments demonstrate that, on average, SAD outperforms the best
baseline by 236.3% in the offline evaluation and by 135.2% in the online
evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doubly-Bounded Queue for Constrained Online Learning: Keeping Pace with
  Dynamics of Both Loss and Constraint <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juncheng Wang, Bingjie Yan, Yituo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider online convex optimization with time-varying constraints and
conduct performance analysis using two stringent metrics: dynamic regret with
respect to the online solution benchmark, and hard constraint violation that
does not allow any compensated violation over time. We propose an efficient
algorithm called Constrained Online Learning with Doubly-bounded Queue (COLDQ),
which introduces a novel virtual queue that is both lower and upper bounded,
allowing tight control of the constraint violation without the need for the
Slater condition. We prove via a new Lyapunov drift analysis that COLDQ
achieves $O(T^\frac{1+V_x}{2})$ dynamic regret and $O(T^{V_g})$ hard constraint
violation, where $V_x$ and $V_g$ capture the dynamics of the loss and
constraint functions. For the first time, the two bounds smoothly approach to
the best-known $O(T^\frac{1}{2})$ regret and $O(1)$ violation, as the dynamics
of the losses and constraints diminish. For strongly convex loss functions,
COLDQ matches the best-known $O(\log{T})$ static regret while maintaining the
$O(T^{V_g})$ hard constraint violation. We further introduce an expert-tracking
variation of COLDQ, which achieves the same performance bounds without any
prior knowledge of the system dynamics. Simulation results demonstrate that
COLDQ outperforms the state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-matrix Factorization Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose novel attention architectures, Multi-matrix Factorization
Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard
Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain
as strong performance under stringent Key-Value cache (KV cache) constraints.
MFA enhances model capacity by efficiently scaling up both the number and
dimension of attention heads through low-rank matrix factorization in the
Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory
requirements by repurposing the key cache as value through value projection
re-parameterization. MFA's design enables strong model capacity when working
under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache
limits with minor performance trade-off. Notably, in our extensive and
large-scale experiments, the proposed architecture outperforms MLA and performs
comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaSociety: An Adaptive Environment with Social Structures for
  Multi-Agent Decision-Making <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03865v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03865v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional interactive environments limit agents' intelligence growth with
fixed tasks. Recently, single-agent environments address this by generating new
tasks based on agent actions, enhancing task diversity. We consider the
decision-making problem in multi-agent settings, where tasks are further
influenced by social connections, affecting rewards and information access.
However, existing multi-agent environments lack a combination of adaptive
physical surroundings and social connections, hindering the learning of
intelligent behaviors. To address this, we introduce AdaSociety, a customizable
multi-agent environment featuring expanding state and action spaces, alongside
explicit and alterable social structures. As agents progress, the environment
adaptively generates new tasks with social structures for agents to undertake.
In AdaSociety, we develop three mini-games showcasing distinct social
structures and tasks. Initial results demonstrate that specific social
structures can promote both individual and collective benefits, though current
reinforcement learning and LLM-based algorithms show limited effectiveness in
leveraging social structures to enhance performance. Overall, AdaSociety serves
as a valuable research platform for exploring intelligence in diverse physical
and social settings. The code is available at
https://github.com/bigai-ai/AdaSociety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS D&B 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lean Attention: Hardware-Aware Scalable Attention Mechanism for the
  Decode-Phase of Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor Rühle, Saravan Rajmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have emerged as one of the most widely used
architectures for natural language processing, natural language generation, and
image generation. The size of the state-of-the-art models has increased
steadily reaching billions of parameters. These huge models are memory hungry
and incur significant inference latency even on cutting edge AI-accelerators,
such as GPUs. Specifically, the time and memory complexity of the attention
operation is quadratic in terms of the total context length, i.e., prompt and
output tokens. Thus, several optimizations such as key-value tensor caching and
FlashAttention computation have been proposed to deliver the low latency
demands of applications relying on such large models. However, these techniques
do not cater to the computationally distinct nature of different phases during
inference.
  To that end, we propose LeanAttention, a scalable technique of computing
self-attention for the token-generation phase (decode-phase) of decoder-only
transformer models. LeanAttention enables scaling the attention mechanism
implementation for the challenging case of long context lengths by re-designing
the execution flow for the decode-phase. We identify that the associative
property of online softmax can be treated as a reduction operation thus
allowing us to parallelize the attention computation over these large context
lengths. We extend the "stream-K" style reduction of tiled calculation to
self-attention to enable parallel computation resulting in an average of 2.6x
attention execution speedup over FlashAttention-2 and up to 8.33x speedup for
512k context lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Poisoning Attacks on Federated Learning-based Wireless Traffic
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Zhang, Minghong Fang, Jiayuan Huang, Yuchen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a distributed framework to train a global
control model across multiple base stations without compromising the privacy of
their local network data. This makes it ideal for applications like wireless
traffic prediction (WTP), which plays a crucial role in optimizing network
resources, enabling proactive traffic flow management, and enhancing the
reliability of downstream communication-aided applications, such as IoT
devices, autonomous vehicles, and industrial automation systems. Despite its
promise, the security aspects of FL-based distributed wireless systems,
particularly in regression-based WTP problems, remain inadequately
investigated. In this paper, we introduce a novel fake traffic injection (FTI)
attack, designed to undermine the FL-based WTP system by injecting fabricated
traffic distributions with minimal knowledge. We further propose a defense
mechanism, termed global-local inconsistency detection (GLID), which
strategically removes abnormal model parameters that deviate beyond a specific
percentile range estimated through statistical methods in each dimension.
Extensive experimental evaluations, performed on real-world wireless traffic
datasets, demonstrate that both our attack and defense strategies significantly
outperform existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IFIP/IEEE Networking 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Radar Signal Recognition through <span class="highlight-title">Self-Supervised</span> Learning and Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Huang, Simon Denman, Akila Pemasiri, Clinton Fookes, Terrence Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic radar signal recognition (RSR) plays a pivotal role in electronic
warfare (EW), as accurately classifying radar signals is critical for informing
decision-making processes. Recent advances in deep learning have shown
significant potential in improving RSR performance in domains with ample
annotated data. However, these methods fall short in EW scenarios where
annotated RF data are scarce or impractical to obtain. To address these
challenges, we introduce a self-supervised learning (SSL) method which utilises
masked signal modelling and RF domain adaption to enhance RSR performance in
environments with limited RF samples and labels. Specifically, we investigate
pre-training masked autoencoders (MAE) on baseband in-phase and quadrature
(I/Q) signals from various RF domains and subsequently transfer the learned
representation to the radar domain, where annotated data are limited. Empirical
results show that our lightweight self-supervised ResNet model with domain
adaptation achieves up to a 17.5% improvement in 1-shot classification accuracy
when pre-trained on in-domain signals (i.e., radar signals) and up to a 16.31%
improvement when pre-trained on out-of-domain signals (i.e., comm signals),
compared to its baseline without SSL. We also provide reference results for
several MAE designs and pre-training strategies, establishing a new benchmark
for few-shot radar signal classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactually Fair Reinforcement Learning via Sequential Data
  Preprocessing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jitao Wang, Chengchun Shi, John D. Piette, Joshua R. Loftus, Donglin Zeng, Zhenke Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When applied in healthcare, reinforcement learning (RL) seeks to dynamically
match the right interventions to subjects to maximize population benefit.
However, the learned policy may disproportionately allocate efficacious actions
to one subpopulation, creating or exacerbating disparities in other
socioeconomically-disadvantaged subgroups. These biases tend to occur in
multi-stage decision making and can be self-perpetuating, which if unaccounted
for could cause serious unintended consequences that limit access to care or
treatment benefit. Counterfactual fairness (CF) offers a promising statistical
tool grounded in causal inference to formulate and study fairness. In this
paper, we propose a general framework for fair sequential decision making. We
theoretically characterize the optimal CF policy and prove its stationarity,
which greatly simplifies the search for optimal CF policies by leveraging
existing RL algorithms. The theory also motivates a sequential data
preprocessing algorithm to achieve CF decision making under an additive noise
assumption. We prove and then validate our policy learning approach in
controlling unfairness and attaining optimal value through simulations.
Analysis of a digital health dataset designed to reduce opioid misuse shows
that our proposal greatly enhances fair access to counseling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational and Statistical Asymptotic Analysis of the JKO Scheme for
  Iterative Algorithms to update distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang Wu, Yazhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The seminal paper of Jordan, Kinderlehrer, and Otto introduced what is now
widely known as the JKO scheme, an iterative algorithmic framework for
computing distributions. This scheme can be interpreted as a Wasserstein
gradient flow and has been successfully applied in machine learning contexts,
such as deriving policy solutions in reinforcement learning. In this paper, we
extend the JKO scheme to accommodate models with unknown parameters.
Specifically, we develop statistical methods to estimate these parameters and
adapt the JKO scheme to incorporate the estimated values. To analyze the
adopted statistical JKO scheme, we establish an asymptotic theory via
stochastic partial differential equations that describes its limiting dynamic
behavior. Our framework allows both the sample size used in parameter
estimation and the number of algorithmic iterations to go to infinity. This
study offers a unified framework for joint computational and statistical
asymptotic analysis of the statistical JKO scheme. On the computational side,
we examine the scheme's dynamic behavior as the number of iterations increases,
while on the statistical side, we investigate the large-sample behavior of the
resulting distributions computed through the scheme. We conduct numerical
simulations to evaluate the finite-sample performance of the proposed methods
and validate the developed asymptotic theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELDER: Enhancing Lifelong Model Editing with Mixture-of-LoRA <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11869v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11869v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaang Li, Quan Wang, Zhongnan Wang, Yongdong Zhang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) require model editing to efficiently update
specific knowledge within them and avoid factual errors. Most model editing
methods are solely designed for single-time use and result in a significant
forgetting effect in lifelong editing scenarios, where sequential edits are
conducted over time. Previous approaches manage sequential edits by freezing
original parameters and discretely allocating new parameters for each knowledge
update. However, these methods lack robustness to minor input variations due to
the discrete mapping between data and parameters. To overcome this challenge,
we propose ELDER, a novel approach to create a continuous association between
data and adapters. ELDER integrates multiple LoRAs through a router network and
is trained to establish a smooth data-adapter association, thereby enhancing
the edit robustness and generalization of semantically equivalent inputs. To
ensure inputs containing the same knowledge will be processed by the same
LoRAs, we design a novel loss to guide the model link LoRA allocations with
edit knowledge. Furthermore, we propose a deferral mechanism to retain the
original LLM capabilities post-edit. Extensive experiments on GPT-2 XL and
LLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong
setting, outperforming eight baselines while exhibiting strong scalability and
preserving LLMs' general abilities on downstream tasks. Our code is available
at https://github.com/JiaangL/ELDER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Retrie</span>val-Reasoning Large Language Model-based Synthetic Clinical Trial
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Xu, Fang Wu, Yuanyuan Zhang, Yue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) exhibits promise in the clinical domain. However, it is
constrained by data scarcity and ethical considerations, as the generation of
clinical trials presents significant challenges due to stringent privacy
regulations, high costs, and the extended duration required for conducting
studies with human participants. Despite the advancements of large language
models (LLMs) in general generation tasks, their potential in facilitating the
generation of synthetic clinical trials is under-explored. To address this gap,
we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs
to generate artificial yet realistic and diverse clinical trials with binary
success/failure labels. Experiments conducted on real clinical trials from the
\url{ClinicalTrials.gov} database demonstrate that our synthetic data can
effectively augment real datasets. Furthermore, by fine-tuning a pre-trained
model as a binary classifier on synthetic clinical trial datasets, we
demonstrate that this augmentation enhances model training for downstream tasks
such as trial outcome prediction. Our findings suggest that LLMs for synthetic
clinical trial generation hold promise for accelerating clinical research and
upholding ethical standards for patient privacy. The code is publicly available
at
https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI <span class="highlight-title">Foundation</span> Models for Wearable Movement Data in Mental Health
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15240v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15240v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franklin Y. Ruan, Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, Nicholas C. Jacobson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained foundation models and transformer architectures have driven the
success of large language models (LLMs) and other modern AI breakthroughs.
However, similar advancements in health data modeling remain limited due to the
need for innovative adaptations. Wearable movement data offers a valuable
avenue for exploration, as it's a core feature in nearly all commercial
smartwatches, well established in clinical and mental health research, and the
sequential nature of the data shares similarities to language. We introduce the
Pretrained Actigraphy Transformer (PAT), the first open source foundation model
designed for time-series wearable movement data. Leveraging transformer-based
architectures and novel techniques, such as patch embeddings, and pretraining
on data from 29,307 participants in a national U.S. sample, PAT achieves
state-of-the-art performance in several mental health prediction tasks. PAT is
also lightweight and easily interpretable, making it a robust tool for mental
health research.
  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Efficient Split Learning for Fine-Tuning Large Language Models in
  Edge Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuguang Li, Shaohua Wu, Liang Li, Songge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we propose an energy-efficient split learning (SL) framework
for fine-tuning large language models (LLMs) using geo-distributed personal
data at the network edge, where LLMs are split and alternately across massive
mobile devices and an edge server. Considering the device heterogeneity and
channel dynamics in edge networks, a \underline{C}ut l\underline{A}yer and
computing \underline{R}esource \underline{D}ecision (CARD) algorithm is
developed to minimize training delay and energy consumption. Simulation results
demonstrate that the proposed approach reduces the average training delay and
server's energy consumption by 70.8% and 53.1%, compared to the benchmarks,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Go AIs be adversarially robust? <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Tseng, Euan McLean, Kellin Pelrine, Tony T. Wang, Adam Gleave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work found that superhuman Go AIs can be defeated by simple adversarial
strategies, especially "cyclic" attacks. In this paper, we study whether adding
natural countermeasures can achieve robustness in Go, a favorable domain for
robustness since it benefits from incredible average-case capability and a
narrow, innately adversarial setting. We test three defenses: adversarial
training on hand-constructed positions, iterated adversarial training, and
changing the network architecture. We find that though some of these defenses
protect against previously discovered attacks, none withstand freshly trained
adversaries. Furthermore, most of the reliably effective attacks these
adversaries discover are different realizations of the same overall class of
cyclic attacks. Our results suggest that building robust AI systems is
challenging even with extremely superhuman systems in some of the most
tractable settings, and highlight two key gaps: efficient generalization of
defenses, and diversity in training. For interactive examples of attacks and a
link to our codebase, see https://goattack.far.ai.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages, AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physically Guided Deep Unsupervised Inversion for 1D Magnetotelluric
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15274v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15274v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Goyes-Peñafiel, Umair bin Waheed, Henry Arguello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global demand for unconventional energy sources such as geothermal energy
and white hydrogen requires new exploration techniques for precise subsurface
structure characterization and potential reservoir identification. The
Magnetotelluric (MT) method is crucial for these tasks, providing critical
information on the distribution of subsurface electrical resistivity at depths
ranging from hundreds to thousands of meters. However, traditional iterative
algorithm-based inversion methods require the adjustment of multiple
parameters, demanding time-consuming and exhaustive tuning processes to achieve
proper cost function minimization. Recent advances have incorporated deep
learning algorithms for MT inversion, primarily based on supervised learning,
and large labeled datasets are needed for training. This work utilizes
TensorFlow operations to create a differentiable forward MT operator,
leveraging its automatic differentiation capability. Moreover, instead of
solving for the subsurface model directly, as classical algorithms perform,
this paper presents a new deep unsupervised inversion algorithm guided by
physics to estimate 1D MT models. Instead of using datasets with the observed
data and their respective model as labels during training, our method employs a
differentiable modeling operator that physically guides the cost function
minimization, making the proposed method solely dependent on observed data.
Therefore, the optimization algorithm updates the network weights to minimize
the data misfit. We test the proposed method with field and synthetic data at
different acquisition frequencies, demonstrating that the resistivity models
obtained are more accurate than those calculated using other techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, github repository, submitted to IEEE-GRSL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\text{Transformer}^2$: Self-adaptive LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Sun, Edoardo Cetin, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-adaptive large language models (LLMs) aim to solve the challenges posed
by traditional fine-tuning methods, which are often computationally intensive
and static in their ability to handle diverse tasks. We introduce
$\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for
unseen tasks in real-time by selectively adjusting only the singular components
of their weight matrices. During inference, $\text{Transformer}^2$ employs a
two-pass mechanism: first, a dispatch system identifies the task properties,
and then task-specific "expert" vectors, trained using reinforcement learning,
are dynamically mixed to obtain targeted behavior for the incoming prompt. Our
method outperforms ubiquitous approaches such as LoRA, with fewer parameters
and greater efficiency. $\text{Transformer}^2$ demonstrates versatility across
different LLM architectures and modalities, including vision-language tasks.
$\text{Transformer}^2$ represents a significant leap forward, offering a
scalable, efficient solution for enhancing the adaptability and task-specific
performance of LLMs, paving the way for truly dynamic, self-organizing AI
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 panges, 11 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E2ESlack: An End-to-End <span class="highlight-title">Graph</span>-Based Framework for Pre-Routing Slack
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Bodhe, Zhanguang Zhang, Atia Hamidizadeh, Shixiong Kai, Yingxue Zhang, Mingxuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-routing slack prediction remains a critical area of research in
Electronic Design Automation (EDA). Despite numerous machine learning-based
approaches targeting this task, there is still a lack of a truly end-to-end
framework that engineers can use to obtain TNS/WNS metrics from raw circuit
data at the placement stage. Existing works have demonstrated effectiveness in
Arrival Time (AT) prediction but lack a mechanism for Required Arrival Time
(RAT) prediction, which is essential for slack prediction and obtaining TNS/WNS
metrics. In this work, we propose E2ESlack, an end-to-end graph-based framework
for pre-routing slack prediction. The framework includes a TimingParser that
supports DEF, SDF and LIB files for feature extraction and graph construction,
an arrival time prediction model and a fast RAT estimation module. To the best
of our knowledge, this is the first work capable of predicting path-level
slacks at the pre-routing stage. We perform extensive experiments and
demonstrate that our proposed RAT estimation method outperforms the SOTA
ML-based prediction method and also pre-routing STA tool. Additionally, the
proposed E2ESlack framework achieves TNS/WNS values comparable to post-routing
STA results while saving up to 23x runtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient descent with generalized Newton's method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Bu, Shiyun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the generalized Newton's method (GeN) -- a Hessian-informed
approach that applies to any optimizer such as SGD and Adam, and covers the
Newton-Raphson method as a sub-case. Our method automatically and dynamically
selects the learning rate that accelerates the convergence, without the
intensive tuning of the learning rate scheduler. In practice, our method is
easily implementable, since it only requires additional forward passes with
almost zero computational overhead (in terms of training time and memory cost),
if the overhead is amortized over many iterations. We present extensive
experiments on language and vision tasks (e.g. GPT and ResNet) to showcase that
GeN optimizers match the state-of-the-art performance, which was achieved with
carefully tuned learning rate schedulers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can AI Help with Your Personal Finances? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19784v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19784v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oudom Hean, Utsha Saha, Binita Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have emerged as a
transformative development in artificial intelligence (AI), drawing significant
attention from industry and academia. Trained on vast datasets, these
sophisticated AI systems exhibit impressive natural language processing and
content generation capabilities. This paper explores the potential of LLMs to
address key challenges in personal finance, focusing on the United States. We
evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,
Anthropic's Claude, and Meta's Llama, to assess their effectiveness in
providing accurate financial advice on topics such as mortgages, taxes, loans,
and investments. Our findings show that while these models achieve an average
accuracy rate of approximately 70%, they also display notable limitations in
certain areas. Specifically, LLMs struggle to provide accurate responses for
complex financial queries, with performance varying significantly across
different topics. Despite these limitations, the analysis reveals notable
improvements in newer versions of these models, highlighting their growing
utility for individuals and financial advisors. As these AI systems continue to
evolve, their potential for advancing AI-driven applications in personal
finance becomes increasingly promising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smartphone-based Eye Tracking System using Edge Intelligence and Model
  Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishan Gunawardena, Gough Yumu Lui, Jeewani Anupama Ginige, Bahman Javadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant limitation of current smartphone-based eye-tracking algorithms
is their low accuracy when applied to video-type visual stimuli, as they are
typically trained on static images. Also, the increasing demand for real-time
interactive applications like games, VR, and AR on smartphones requires
overcoming the limitations posed by resource constraints such as limited
computational power, battery life, and network bandwidth. Therefore, we
developed two new smartphone eye-tracking techniques for video-type visuals by
combining Convolutional Neural Networks (CNN) with two different Recurrent
Neural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent
Unit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean
Square Error of 0.955 cm and 1.091 cm, respectively. To address the
computational constraints of smartphones, we developed an edge intelligence
architecture to enhance the performance of smartphone-based eye tracking. We
applied various optimisation methods like quantisation and pruning to deep
learning models for better energy, CPU, and memory usage on edge devices,
focusing on real-time processing. Using model quantisation, the model inference
time in the CNN+LSTM and CNN+GRU models was reduced by 21.72% and 19.50%,
respectively, on edge devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I have included the three papers as reference, which are closely
  related. We have expanded the future work section to provide a more thorough
  discussion of the concepts of "varying lighting conditions" and "dynamic user
  environments." We have added a note below Table 4 to clarify the
  abbreviations' meaning. Elaborated the role of the Domain Expert within the
  presentation layer in Section 4.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACPO: AI-Enabled Compiler Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09982v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09982v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir H. Ashouri, Muhammad Asif Manzoor, Duc Minh Vu, Raymond Zhang, Colin Toft, Ziwen Wang, Angel Zhang, Bryan Chan, Tomasz S. Czajkowski, Yaoqing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key to performance optimization of a program is to decide correctly when
a certain transformation should be applied by a compiler. This is an ideal
opportunity to apply machine-learning models to speed up the tuning process;
while this realization has been around since the late 90s, only recent
advancements in ML enabled a practical application of ML to compilers as an
end-to-end framework.
  This paper presents ACPO: An AI-Enabled Compiler Framework, a novel framework
that provides LLVM with simple and comprehensive tools to benefit from
employing ML models for different optimization passes. We first showcase the
high-level view, class hierarchy, and functionalities of ACPO and subsequently,
demonstrate \taco{a couple of use cases of ACPO by ML-enabling the Loop Unroll
and Function Inlining passes used in LLVM's O3. and finally, describe how ACPO
can be leveraged to optimize other passes. Experimental results reveal that the
ACPO model for Loop Unroll can gain on average 4%, 3%, 5.4%, and 0.2% compared
to LLVM's vanilla O3 optimization when deployed on Polybench, Coral-2,
CoreMark, and Graph-500, respectively. Furthermore, by including both Function
Inlining and Loop Unroll models, ACPO can provide a combined speedup of 4.5% on
Polybench and 2.4% on Cbench when compared with LLVM's O3, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACPO (12 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EPIC: Effective <span class="highlight-title">Prompt</span>ing for Imbalanced-Class Data Synthesis in <span class="highlight-title">Tabular</span>
  Data Classification via Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12404v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12404v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhee Kim, Taesung Kim, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable in-context learning
capabilities across diverse applications. In this work, we explore the
effectiveness of LLMs for generating realistic synthetic tabular data,
identifying key prompt design elements to optimize performance. We introduce
EPIC, a novel approach that leverages balanced, grouped data samples and
consistent formatting with unique variable mapping to guide LLMs in generating
accurate synthetic data across all classes, even for imbalanced datasets.
Evaluations on real-world datasets show that EPIC achieves state-of-the-art
machine learning classification performance, significantly improving generation
efficiency. These findings highlight the effectiveness of EPIC for synthetic
tabular data generation, particularly in addressing class imbalance. Our source
code for our work is available at:
https://seharanul17.github.io/project-synthetic-tabular-llm/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A systematic <span class="highlight-title">review</span> of the use of Deep Learning in Satellite Imagery for
  Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01272v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01272v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Victor, Zhen He, Aiden Nibali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agricultural research is essential for increasing food production to meet the
requirements of an increasing population in the coming decades. Recently,
satellite technology has been improving rapidly and deep learning has seen much
success in generic computer vision tasks and many application areas which
presents an important opportunity to improve analysis of agricultural land.
Here we present a systematic review of 150 studies to find the current uses of
deep learning on satellite imagery for agricultural research. Although we
identify 5 categories of agricultural monitoring tasks, the majority of the
research interest is in crop segmentation and yield prediction. We found that,
when used, modern deep learning methods consistently outperformed traditional
machine learning across most tasks; the only exception was that Long Short-Term
Memory (LSTM) Recurrent Neural Networks did not consistently outperform Random
Forests (RF) for yield prediction. The reviewed studies have largely adopted
methodologies from generic computer vision, except for one major omission:
benchmark datasets are not utilised to evaluate models across studies, making
it difficult to compare results. Additionally, some studies have specifically
utilised the extra spectral resolution available in satellite imagery, but
other divergent properties of satellite images - such as the hugely different
scales of spatial patterns - are not being taken advantage of in the reviewed
studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures and 10 tables in main paper. Final version, as
  submitted and accepted at JSTARS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double Equivariance for Inductive Link Prediction for Both New Nodes and
  New Relation Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01313v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01313v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhou, Yucheng Zhang, Jianfei Gao, Yangze Zhou, Bruno Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of fully inductive link prediction in knowledge graphs has gained
significant attention, with various graph neural networks being proposed to
address it. This task presents greater challenges than traditional inductive
link prediction tasks with only new nodes, as models must be capable of
zero-shot generalization to both unseen nodes and unseen relation types in the
inference graph. Despite the development of novel models, a unifying
theoretical understanding of their success remains elusive, and the limitations
of these methods are not well-studied. In this work, we introduce the concept
of double permutation-equivariant representations and demonstrate its necessity
for effective performance in this task. We show that many existing models,
despite their diverse architectural designs, conform to this framework.
However, we also identify inherent limitations in double
permutation-equivariant representations, which restrict these models's ability
to learn effectively on datasets with varying characteristics. Our findings
suggest that while double equivariance is necessary for meta-learning across
knowledge graphs from different domains, it is not sufficient. There remains a
fundamental gap between double permutation-equivariant models and the concept
of foundation models designed to learn patterns across all domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-dimensional learning of narrow neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have been marked with the fast-pace diversification and
increasing ubiquity of machine learning applications. Yet, a firm theoretical
understanding of the surprising efficiency of neural networks to learn from
high-dimensional data still proves largely elusive. In this endeavour, analyses
inspired by statistical physics have proven instrumental, enabling the tight
asymptotic characterization of the learning of neural networks in high
dimensions, for a broad class of solvable models. This manuscript reviews the
tools and ideas underlying recent progress in this line of work. We introduce a
generic model -- the sequence multi-index model -- which encompasses numerous
previously studied models as special instances. This unified framework covers a
broad class of machine learning architectures with a finite number of hidden
units, including multi-layer perceptrons, autoencoders, attention mechanisms;
and tasks, including (un)supervised learning, denoising, contrastive learning,
in the limit of large data dimension, and comparably large number of samples.
We explicate in full detail the analysis of the learning of sequence
multi-index models, using statistical physics techniques such as the replica
method and approximate message-passing algorithms. This manuscript thus
provides a unified presentation of analyses reported in several previous works,
and a detailed overview of central techniques in the field of statistical
physics of machine learning. This review should be a useful primer for machine
learning theoreticians curious of statistical physics approaches; it should
also be of value to statistical physicists interested in the transfer of such
ideas to the study of neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expressive Text-to-Image Generation with Rich Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06720v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06720v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songwei Ge, Taesung Park, Jun-Yan Zhu, Jia-Bin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plain text has become a prevalent interface for text-to-image synthesis.
However, its limited customization options hinder users from accurately
describing desired outputs. For example, plain text makes it hard to specify
continuous quantities, such as the precise RGB color value or importance of
each word. Furthermore, creating detailed text prompts for complex scenes is
tedious for humans to write and challenging for text encoders to interpret. To
address these challenges, we propose using a rich-text editor supporting
formats such as font style, size, color, and footnote. We extract each word's
attributes from rich text to enable local style control, explicit token
reweighting, precise color rendering, and detailed region synthesis. We achieve
these capabilities through a region-based diffusion process. We first obtain
each word's region based on attention maps of a diffusion process using plain
text. For each region, we enforce its text attributes by creating
region-specific detailed prompts and applying region-specific guidance, and
maintain its fidelity against plain-text generation through region-based
injections. We present various examples of image generation from rich text and
demonstrate that our method outperforms strong baselines with quantitative
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://rich-text-to-image.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network Emulator for Atmospheric Chemical ODE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01829v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01829v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Song Liu, Petri Clusius, Michael Boy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling atmospheric chemistry is complex and computationally intense. Given
the recent success of Deep neural networks in digital signal processing, we
propose a Neural Network Emulator for fast chemical concentration modeling. We
consider atmospheric chemistry as a time-dependent Ordinary Differential
Equation. To extract the hidden correlations between initial states and future
time evolution, we propose ChemNNE, an Attention based Neural Network Emulator
(NNE) that can model the atmospheric chemistry as a neural ODE process. To
efficiently simulate the chemical changes, we propose the sinusoidal time
embedding to estimate the oscillating tendency over time. More importantly, we
use the Fourier neural operator to model the ODE process for efficient
computation. We also propose three physical-informed losses to supervise the
training optimization. To evaluate our model, we propose a large-scale chemical
dataset that can be used for neural network training and evaluation. The
extensive experiments show that our approach achieves state-of-the-art
performance in modeling accuracy and computational speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MassSpecGym: A benchmark for the discovery and identification of
  molecules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Bushuiev, Anton Bushuiev, Niek F. de Jonge, Adamo Young, Fleming Kretschmer, Raman Samusevich, Janne Heirman, Fei Wang, Luke Zhang, Kai Dührkop, Marcus Ludwig, Nils A. Haupt, Apurva Kalia, Corinna Brungs, Robin Schmid, Russell Greiner, Bo Wang, David S. Wishart, Li-Ping Liu, Juho Rousu, Wout Bittremieux, Hannes Rost, Tytus D. Mak, Soha Hassoun, Florian Huber, Justin J. J. van der Hooft, Michael A. Stravs, Sebastian Böcker, Josef Sivic, Tomáš Pluskal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery and identification of molecules in biological and environmental
samples is crucial for advancing biomedical and chemical sciences. Tandem mass
spectrometry (MS/MS) is the leading technique for high-throughput elucidation
of molecular structures. However, decoding a molecular structure from its mass
spectrum is exceptionally challenging, even when performed by human experts. As
a result, the vast majority of acquired MS/MS spectra remain uninterpreted,
thereby limiting our understanding of the underlying (bio)chemical processes.
Despite decades of progress in machine learning applications for predicting
molecular structures from MS/MS spectra, the development of new methods is
severely hindered by the lack of standard datasets and evaluation protocols. To
address this problem, we propose MassSpecGym -- the first comprehensive
benchmark for the discovery and identification of molecules from MS/MS data.
Our benchmark comprises the largest publicly available collection of
high-quality labeled MS/MS spectra and defines three MS/MS annotation
challenges: \textit{de novo} molecular structure generation, molecule
retrieval, and spectrum simulation. It includes new evaluation metrics and a
generalization-demanding data split, therefore standardizing the MS/MS
annotation tasks and rendering the problem accessible to the broad machine
learning community. MassSpecGym is publicly available at
\url{https://github.com/pluskal-lab/MassSpecGym}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Parameter-Efficient Quantum Anomaly Detection Method on a
  Superconducting Quantum Processor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maida Wang, Jinyang Jiang, Peter V. Coveney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning has gained attention for its potential to address
computational challenges. However, whether those algorithms can effectively
solve practical problems and outperform their classical counterparts,
especially on current quantum hardware, remains a critical question. In this
work, we propose a novel quantum machine learning method, called Quantum
Support Vector Data Description (QSVDD), for practical image anomaly detection,
which aims to achieve both parameter efficiency and superior accuracy compared
to classical models. Emulation results indicate that QSVDD demonstrates
favourable recognition capabilities compared to classical baselines, achieving
an average accuracy of over 90% on benchmarks with significantly fewer
trainable parameters. Theoretical analysis confirms that QSVDD has a comparable
expressivity to classical counterparts while requiring only a fraction of the
parameters. Furthermore, we demonstrate the first implementation of a quantum
anomaly detection method for general image datasets on a superconducting
quantum processor. Specifically, we achieve an accuracy of over 80% with only
16 parameters on the device, providing initial evidence of QSVDD's practical
viability in the noisy intermediate-scale quantum era and highlighting its
significant reduction in parameter requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-visual Deepfake Detection With Local Temporal Inconsistencies <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcella Astrid, Enjie Ghorbel, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an audio-visual deepfake detection approach that aims to
capture fine-grained temporal inconsistencies between audio and visual
modalities. To achieve this, both architectural and data synthesis strategies
are introduced. From an architectural perspective, a temporal distance map,
coupled with an attention mechanism, is designed to capture these
inconsistencies while minimizing the impact of irrelevant temporal
subsequences. Moreover, we explore novel pseudo-fake generation techniques to
synthesize local inconsistencies. Our approach is evaluated against
state-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating
its effectiveness in detecting audio-visual deepfakes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Video Moment <span class="highlight-title">Retrie</span>val via Off-the-shelf Multimodal Large
  Language Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifang Xu, Yunzhuo Sun, Benxiang Zhai, Ming Li, Wenxin Liang, Yang Li, Sidan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The target of video moment retrieval (VMR) is predicting temporal spans
within a video that semantically match a given linguistic query. Existing VMR
methods based on multimodal large language models (MLLMs) overly rely on
expensive high-quality datasets and time-consuming fine-tuning. Although some
recent studies introduce a zero-shot setting to avoid fine-tuning, they
overlook inherent language bias in the query, leading to erroneous
localization. To tackle the aforementioned challenges, this paper proposes
Moment-GPT, a tuning-free pipeline for zero-shot VMR utilizing frozen MLLMs.
Specifically, we first employ LLaMA-3 to correct and rephrase the query to
mitigate language bias. Subsequently, we design a span generator combined with
MiniGPT-v2 to produce candidate spans adaptively. Finally, to leverage the
video comprehension capabilities of MLLMs, we apply VideoChatGPT and span
scorer to select the most appropriate spans. Our proposed method substantially
outperforms the state-ofthe-art MLLM-based and zero-shot models on several
public datasets, including QVHighlights, ActivityNet-Captions, and
Charades-STA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORD: Co-design of Resource Allocation and Deadline Decomposition with
  Generative Profiling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Gifford, Abby Eisenklam, Georgiy A. Bondar, Yifan Cai, Tushar Sial, Linh Thi Xuan Phan, Abhishek Halder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As multicore hardware is becoming increasingly common in real-time systems,
traditional scheduling techniques that assume a single worst-case execution
time for a task are no longer adequate, since they ignore the impact of shared
resources on execution time. When tasks execute concurrently on different
cores, their execution times often vary substantially with their allocated
budgets of shared resources, such as cache and memory bandwidth. Even under a
specific resource allocation, the resource use pattern of a task also changes
with time during a job execution. It is therefore important to consider the
relationship between multicore resources and execution time in task modeling
and scheduling algorithm design.
  In this paper, we propose a much more precise execution model for DAG-based
real-time tasks that captures the time-varying resource use characteristics of
a task under different budgets of shared resources. We present a generative
resource profiling algorithm that efficiently predicts, from limited
measurement data, the resource profile of a task at any time during its
execution under a given resource budget. The generative profiles can then be
used to construct the execution models for tasks, using which one can make
informed resource allocation decisions. We further introduce a multicore
resource allocation and deadline decomposition co-design technique for
DAG-based tasks that leverages the generated execution models to jointly
allocate resources and deadlines to subtasks, to maximize resource efficiency
and schedulability. Our evaluation results show that our generative profiling
algorithm achieves high accuracy while being efficient, and that our
co-allocation technique substantially improves schedulability compared to a
state-of-the-art deadline decomposition method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smartphone-based Eye Tracking System using Edge Intelligence and Model
  Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishan Gunawardena, Gough Yumu Lui, Jeewani Anupama Ginige, Bahman Javadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant limitation of current smartphone-based eye-tracking algorithms
is their low accuracy when applied to video-type visual stimuli, as they are
typically trained on static images. Also, the increasing demand for real-time
interactive applications like games, VR, and AR on smartphones requires
overcoming the limitations posed by resource constraints such as limited
computational power, battery life, and network bandwidth. Therefore, we
developed two new smartphone eye-tracking techniques for video-type visuals by
combining Convolutional Neural Networks (CNN) with two different Recurrent
Neural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent
Unit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean
Square Error of 0.955 cm and 1.091 cm, respectively. To address the
computational constraints of smartphones, we developed an edge intelligence
architecture to enhance the performance of smartphone-based eye tracking. We
applied various optimisation methods like quantisation and pruning to deep
learning models for better energy, CPU, and memory usage on edge devices,
focusing on real-time processing. Using model quantisation, the model inference
time in the CNN+LSTM and CNN+GRU models was reduced by 21.72% and 19.50%,
respectively, on edge devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I have included the three papers as reference, which are closely
  related. We have expanded the future work section to provide a more thorough
  discussion of the concepts of "varying lighting conditions" and "dynamic user
  environments." We have added a note below Table 4 to clarify the
  abbreviations' meaning. Elaborated the role of the Domain Expert within the
  presentation layer in Section 4.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACPO: AI-Enabled Compiler Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09982v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09982v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir H. Ashouri, Muhammad Asif Manzoor, Duc Minh Vu, Raymond Zhang, Colin Toft, Ziwen Wang, Angel Zhang, Bryan Chan, Tomasz S. Czajkowski, Yaoqing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key to performance optimization of a program is to decide correctly when
a certain transformation should be applied by a compiler. This is an ideal
opportunity to apply machine-learning models to speed up the tuning process;
while this realization has been around since the late 90s, only recent
advancements in ML enabled a practical application of ML to compilers as an
end-to-end framework.
  This paper presents ACPO: An AI-Enabled Compiler Framework, a novel framework
that provides LLVM with simple and comprehensive tools to benefit from
employing ML models for different optimization passes. We first showcase the
high-level view, class hierarchy, and functionalities of ACPO and subsequently,
demonstrate \taco{a couple of use cases of ACPO by ML-enabling the Loop Unroll
and Function Inlining passes used in LLVM's O3. and finally, describe how ACPO
can be leveraged to optimize other passes. Experimental results reveal that the
ACPO model for Loop Unroll can gain on average 4%, 3%, 5.4%, and 0.2% compared
to LLVM's vanilla O3 optimization when deployed on Polybench, Coral-2,
CoreMark, and Graph-500, respectively. Furthermore, by including both Function
Inlining and Loop Unroll models, ACPO can provide a combined speedup of 4.5% on
Polybench and 2.4% on Cbench when compared with LLVM's O3, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACPO (12 pages)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Database <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Dataframe Systems: Lazy Fat Pandas on a Diet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhushan Pal Singh, Priyesh Kumar, Chiranmoy Bhattacharya, S. Sudarshan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pandas is widely used for data science applications, but users often run into
problems when datasets are larger than memory. There are several frameworks
based on lazy evaluation that handle large datasets, but the programs have to
be rewritten to suit the framework, and the presence of multiple frameworks
complicates the life of a programmer. In this paper we present a framework that
allows programmers to code in plain Pandas; with just two lines of code changed
by the user, our system optimizes the program using a combination of
just-in-time static analysis, and runtime optimization based on a lazy
dataframe wrapper framework. Moreover, our system allows the programmer to
choose the backend. It works seamlessly with Pandas, Dask, and Modin, allowing
the choice of the best-suited backend for an application based on factors such
as data size. Performance results on a variety of programs show the benefits of
our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Evaluation of Serverless Cloud Infrastructure for
  Large-Scale Data Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Bodner, Theo Radig, David Justen, Daniel Ritter, Tilmann Rabl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data processing systems are increasingly deployed in the cloud. While
monolithic systems run fully on virtual servers, recent systems embrace cloud
infrastructure and utilize the disaggregation of compute and storage to scale
them independently. The introduction of serverless compute services, such as
AWS Lambda, enables finer-grained and elastic scalability within these systems.
Prior work shows the viability of serverless infrastructure for scalable data
processing yet also sees limitations due to variable performance and cost
overhead, in particular for networking and storage.
  In this paper, we perform a detailed analysis of the performance and cost
characteristics of serverless infrastructure in the data processing context. We
base our analysis on a large series of micro-benchmarks across different
compute and storage services, as well as end-to-end workloads. To enable our
analysis, we propose the Skyrise serverless evaluation platform. For the widely
used serverless infrastructure of AWS, our analysis reveals distinct boundaries
for performance variability in serverless networks and storage. We further
present cost break-even points for serverless compute and storage. These
insights provide guidance on when and how serverless infrastructure can be
efficiently used for data processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skyrise: Exploiting Serverless Cloud Infrastructure for Elastic Data
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Bodner, Daniel Ritter, Martin Boissier, Tilmann Rabl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing offers elasticity unmatched by conventional server-based
cloud infrastructure. Although modern data processing systems embrace
serverless storage, such as Amazon S3, they continue to manage their compute
resources as servers. This is challenging for unpredictable workloads, leaving
clusters often underutilized. Recent research shows the potential of serverless
compute resources, such as cloud functions, for elastic data processing, but
also sees limitations in performance robustness and cost efficiency for long
running workloads. These challenges require holistic approaches across the
system stack. However, to the best of our knowledge, there is no end-to-end
data processing system built entirely on serverless infrastructure. In this
paper, we present Skyrise, our effort towards building the first fully
serverless SQL query processor. Skyrise exploits the elasticity of its
underlying infrastructure, while alleviating the inherent limitations with a
number of adaptive and cost-aware techniques. We show that both Skyrise's
performance and cost are competitive to other cloud data systems for
terabyte-scale queries of the analytical TPC-H benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-variable Quantification of BDDs in External Memory using Nested
  Sweeping (Extended Paper) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steffan Christ Sølvsten, Jaco van de Pol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research on the Adiar BDD package has been successful at designing
algorithms capable of handling large Binary Decision Diagrams (BDDs) stored in
external memory. To do so, it uses consecutive sweeps through the BDDs to
resolve computations. Yet, this approach has kept algorithms for multi-variable
quantification, the relational product, and variable reordering out of its
scope.
  In this work, we address this by introducing the nested sweeping framework.
Here, multiple concurrent sweeps pass information between eachother to compute
the result. We have implemented the framework in Adiar and used it to create a
new external memory multi-variable quantification algorithm. Compared to
conventional depth-first implementations, Adiar with nested sweeping is able to
solve more instances of our benchmarks and/or solve them faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 14 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QMDB: Quick Merkle Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Zhang, Ryan Zarick, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain
state management by integrating key-value (KV) and Merkle tree storage into a
single unified architecture. QMDB delivers a significant throughput improvement
over existing architectures, achieving up to 6X over the widely used RocksDB
and 8X over NOMT, a leading verifiable database. Its novel append-only
twig-based design enables one SSD read per state access, O(1) IOs for updates,
and in-memory Merkleization on a memory footprint as small as 2.3 bytes per
entry, enabling it to run on even modest consumer-grade PCs. QMDB scales
seamlessly across both commodity and enterprise hardware, achieving up to 2.28
million state updates per second. This performance enables support for 1
million token transfers per second (TPS), marking QMDB as the first solution
achieving such a milestone. QMDB has been benchmarked with workloads exceeding
15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to
scale to 280 billion entries on a single server. Furthermore, QMDB introduces
historical proofs, unlocking the ability to query its blockchain's historical
state at the latest block. QMDB not only meets the demands of current
blockchains but also provides a robust foundation for building scalable,
efficient, and verifiable decentralized applications across diverse use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-13T00:00:00Z">2025-01-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WebWalker: Benchmarking LLMs in Web Traversal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou, Pengjun Xie, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) demonstrates remarkable performance
across tasks in open-domain question-answering. However, traditional search
engines may retrieve shallow content, limiting the ability of LLMs to handle
complex, multi-layered information. To address it, we introduce WebWalkerQA, a
benchmark designed to assess the ability of LLMs to perform web traversal. It
evaluates the capacity of LLMs to traverse a website's subpages to extract
high-quality data systematically. We propose WebWalker, which is a multi-agent
framework that mimics human-like web navigation through an explore-critic
paradigm. Extensive experimental results show that WebWalkerQA is challenging
and demonstrates the effectiveness of RAG combined with WebWalker, through the
horizontal and vertical integration in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal
  Aspects in Video Editing <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Biyyala, Bharat Chanderprakash Kathuria, Jialu Li, Youshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video editing models have advanced significantly, but evaluating their
performance remains challenging. Traditional metrics, such as CLIP text and
image scores, often fall short: text scores are limited by inadequate training
data and hierarchical dependencies, while image scores fail to assess temporal
consistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation
Metric), a novel evaluation framework that leverages modern Vision-Language
Models (VLMs), Object Detection, and Temporal Consistency checks. SST-EM
comprises four components: (1) semantic extraction from frames using a VLM, (2)
primary object tracking with Object Detection, (3) focused object refinement
via an LLM agent, and (4) temporal consistency assessment using a Vision
Transformer (ViT). These components are integrated into a unified metric with
weights derived from human evaluations and regression analysis. The name SST-EM
reflects its focus on Semantic, Spatial, and Temporal aspects of video
evaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and
temporal smoothness in video editing. The source code is available in the
\textbf{\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub
Repository}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imagine while Reasoning in Space: Multimodal Visualization-of-Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) prompting has proven highly effective for enhancing
complex reasoning in Large Language Models (LLMs) and Multimodal Large Language
Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks.
Nonetheless, human cognition extends beyond language alone, enabling the
remarkable capability to think in both words and images. Inspired by this
mechanism, we propose a new reasoning paradigm, Multimodal
Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by
generating image visualizations of their reasoning traces. To ensure
high-quality visualization, we introduce token discrepancy loss into
autoregressive MLLMs. This innovation significantly improves both visual
coherence and fidelity. We validate this approach through several dynamic
spatial reasoning tasks. Experimental results reveal that MVoT demonstrates
competitive performance across tasks. Moreover, it exhibits robust and reliable
improvements in the most challenging scenarios where CoT fails. Ultimately,
MVoT establishes new possibilities for complex reasoning tasks where visual
thinking can effectively complement verbal reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables
  including references and appendices)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Large Language Models in Inferring Personality Traits from
  User Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng Zhu, Ruoming Jin, Karin G. Coifman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are demonstrating remarkable human like
capabilities across diverse domains, including psychological assessment. This
study evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer
Big Five personality traits and generate Big Five Inventory-10 (BFI-10) item
scores from user conversations under zero-shot prompting conditions. Our
findings reveal that incorporating an intermediate step--prompting for BFI-10
item scores before calculating traits--enhances accuracy and aligns more
closely with the gold standard than direct trait inference. This structured
approach underscores the importance of leveraging psychological frameworks in
improving predictive precision. Additionally, a group comparison based on
depressive symptom presence revealed differential model performance.
Participants were categorized into two groups: those experiencing at least one
depressive symptom and those without symptoms. GPT-4o mini demonstrated
heightened sensitivity to depression-related shifts in traits such as
Neuroticism and Conscientiousness within the symptom-present group, whereas
GPT-4o exhibited strengths in nuanced interpretation across groups. These
findings underscore the potential of LLMs to analyze real-world psychological
data effectively, offering a valuable foundation for interdisciplinary research
at the intersection of artificial intelligence and psychology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Key-Value Cache Fusion for Position Invariant RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philhoon Oh, Jinwoo Shin, James Thorne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) underscore the necessity
of Retrieval Augmented Generation (RAG) to leverage external information.
However, LLMs are sensitive to the position of relevant information within
contexts and tend to generate incorrect responses when such information is
placed in the middle, known as `Lost in the Middle' phenomenon. In this paper,
we introduce a framework that generates consistent outputs for decoder-only
models, irrespective of the input context order. Experimental results for three
open domain question answering tasks demonstrate position invariance, where the
model is not sensitive to input context order, and superior robustness to
irrelevent passages compared to prevailing approaches for RAG pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Heterogeneous Multimodal <span class="highlight-title">Graph</span> Learning Framework for Recognizing User
  Emotions in Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sree Bhattacharyya, Shuhua Yang, James Z. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of social media platforms has provided unprecedented
access to massive amounts of multimodal user-generated content. Comprehending
user emotions can provide valuable insights for improving communication and
understanding of human behaviors. Despite significant advancements in Affective
Computing, the diverse factors influencing user emotions in social networks
remain relatively understudied. Moreover, there is a notable lack of deep
learning-based methods for predicting user emotions in social networks, which
could be addressed by leveraging the extensive multimodal data available. This
work presents a novel formulation of personalized emotion prediction in social
networks based on heterogeneous graph learning. Building upon this formulation,
we design HMG-Emo, a Heterogeneous Multimodal Graph Learning Framework that
utilizes deep learning-based features for user emotion recognition.
Additionally, we include a dynamic context fusion module in HMG-Emo that is
capable of adaptively integrating the different modalities in social media
data. Through extensive experiments, we demonstrate the effectiveness of
HMG-Emo and verify the superiority of adopting a graph neural network-based
approach, which outperforms existing baselines that use rich hand-crafted
features. To the best of our knowledge, HMG-Emo is the first multimodal and
deep-learning-based approach to predict personalized emotions within online
social networks. Our work highlights the significance of exploiting advanced
deep learning techniques for less-explored problems in Affective Computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Student Writing Through Automated Syntax Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamyar Zeinalipour, Mehak Mehak, Fatemeh Parsamotamed, Marco Maggini, Marco Gori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study underscores the pivotal role of syntax feedback in augmenting the
syntactic proficiency of students. Recognizing the challenges faced by learners
in mastering syntactic nuances, we introduce a specialized dataset named
Essay-Syntax-Instruct designed to enhance the understanding and application of
English syntax among these students. Leveraging the capabilities of Large
Language Models (LLMs) such as GPT3.5-Turbo, Llama-2-7b-chat-hf,
Llama-2-13b-chat-hf, and Mistral-7B-Instruct-v0.2, this work embarks on a
comprehensive fine-tuning process tailored to the syntax improvement task.
Through meticulous evaluation, we demonstrate that the fine-tuned LLMs exhibit
a marked improvement in addressing syntax-related challenges, thereby serving
as a potent tool for students to identify and rectify their syntactic errors.
The findings not only highlight the effectiveness of the proposed dataset in
elevating the performance of LLMs for syntax enhancement but also illuminate a
promising path for utilizing advanced language models to support language
acquisition efforts. This research contributes to the broader field of language
learning technology by showcasing the potential of LLMs in facilitating the
linguistic development of Students.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at AIEER 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the encoding of linguistic representations in the
  Fully-Connected Layer of generative CNNs for Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Ferenc Šegedin, Gasper Beguš
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability work on the convolutional layers of CNNs has primarily
focused on computer vision, but some studies also explore correspondences
between the latent space and the output in the audio domain. However, it has
not been thoroughly examined how acoustic and linguistic information is
represented in the fully connected (FC) layer that bridges the latent space and
convolutional layers. The current study presents the first exploration of how
the FC layer of CNNs for speech synthesis encodes linguistically relevant
information. We propose two techniques for exploration of the fully connected
layer. In Experiment 1, we use weight matrices as inputs into convolutional
layers. In Experiment 2, we manipulate the FC layer to explore how
symbolic-like representations are encoded in CNNs. We leverage the fact that
the FC layer outputs a feature map and that variable-specific weight matrices
are temporally structured to (1) demonstrate how the distribution of learned
weights varies between latent variables in systematic ways and (2) demonstrate
how manipulating the FC layer while holding constant subsequent model
parameters affects the output. We ultimately present an FC manipulation that
can output a single segment. Using this technique, we show that lexically
specific latent codes in generative CNNs (ciwGAN) have shared lexically
invariant sublexical representations in the FC-layer weights, showing that
ciwGAN encodes lexical information in a linguistically principled manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESURF: Simple and Effective EDU Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Sediqin, Shlomo Engelson Argamon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting text into Elemental Discourse Units (EDUs) is a fundamental task
in discourse parsing. We present a new simple method for identifying EDU
boundaries, and hence segmenting them, based on lexical and character n-gram
features, using random forest classification. We show that the method, despite
its simplicity, outperforms other methods both for segmentation and within a
state of the art discourse parser. This indicates the importance of such
features for identifying basic discourse elements, pointing towards potentially
more training-efficient methods for discourse analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quilt-1M: One Million Image-Text Pairs for Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11207v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11207v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, Linda Shapiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent accelerations in multi-modal applications have been made possible with
the plethora of image and text data available online. However, the scarcity of
analogous data in the medical field, specifically in histopathology, has slowed
comparable progress. To enable similar representation learning for
histopathology, we turn to YouTube, an untapped resource of videos, offering
$1,087$ hours of valuable educational histopathology videos from expert
clinicians. From YouTube, we curate QUILT: a large-scale vision-language
dataset consisting of $802, 144$ image and text pairs. QUILT was automatically
curated using a mixture of models, including large language models, handcrafted
algorithms, human knowledge databases, and automatic speech recognition. In
comparison, the most comprehensive datasets curated for histopathology amass
only around $200$K samples. We combine QUILT with datasets from other sources,
including Twitter, research papers, and the internet in general, to create an
even larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it
as the largest vision-language histopathology dataset to date. We demonstrate
the value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model
outperforms state-of-the-art models on both zero-shot and linear probing tasks
for classifying new histopathology images across $13$ diverse patch-level
datasets of $8$ different sub-pathologies and cross-modal retrieval tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Attention Vectors: Generative Multimodal Model Features Are
  Discriminative Vision-Language Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chancharik Mitra, Brandon Huang, Tianning Chai, Zhiqiu Lin, Assaf Arbelle, Rogerio Feris, Leonid Karlinsky, Trevor Darrell, Deva Ramanan, Roei Herzig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a
wide variety of vision-language (VL) tasks such as image captioning or visual
question answering. Despite strong performance, LMMs are not directly suited
for foundational discriminative vision-language tasks (i.e., tasks requiring
discrete label predictions) such as image classification and multiple-choice
VQA. One key challenge in utilizing LMMs for discriminative tasks is the
extraction of useful features from generative models. To overcome this issue,
we propose an approach for finding features in the model's latent space to more
effectively leverage LMMs for discriminative tasks. Toward this end, we present
Sparse Attention Vectors (SAVs) -- a finetuning-free method that leverages
sparse attention head activations (fewer than 1\% of the heads) in LMMs as
strong features for VL tasks. With only few-shot examples, SAVs demonstrate
state-of-the-art performance compared to a variety of few-shot and finetuned
baselines on a collection of discriminative tasks. Our experiments also imply
that SAVs can scale in performance with additional examples and generalize to
similar tasks, establishing SAVs as both effective and robust multimodal
feature representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Divergences between Language Models and Human Brains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09308v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09308v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Zhou, Emmy Liu, Graham Neubig, Michael J. Tarr, Leila Wehbe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do machines and humans process language in similar ways? Recent research has
hinted at the affirmative, showing that human neural activity can be
effectively predicted using the internal representations of language models
(LMs). Although such results are thought to reflect shared computational
principles between LMs and human brains, there are also clear differences in
how LMs and humans represent and use language. In this work, we systematically
explore the divergences between human and machine language processing by
examining the differences between LM representations and human brain responses
to language as measured by Magnetoencephalography (MEG) across two datasets in
which subjects read and listened to narrative stories. Using an LLM-based
data-driven approach, we identify two domains that LMs do not capture well:
social/emotional intelligence and physical commonsense. We validate these
findings with human behavioral experiments and hypothesize that the gap is due
to insufficient representations of social/emotional and physical knowledge in
LMs. Our results show that fine-tuning LMs on these domains can improve their
alignment with human brain responses.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">115</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> Distillation via Committee Voting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Cui, Zhaoyi Li, Xiaochen Ma, Xinyue Bi, Yaxin Luo, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation aims to synthesize a smaller, representative dataset
that preserves the essential properties of the original data, enabling
efficient model training with reduced computational resources. Prior work has
primarily focused on improving the alignment or matching process between
original and synthetic data, or on enhancing the efficiency of distilling large
datasets. In this work, we introduce ${\bf C}$ommittee ${\bf V}$oting for ${\bf
D}$ataset ${\bf D}$istillation (CV-DD), a novel and orthogonal approach that
leverages the collective wisdom of multiple models or experts to create
high-quality distilled datasets. We start by showing how to establish a strong
baseline that already achieves state-of-the-art accuracy through leveraging
recent advancements and thoughtful adjustments in model design and optimization
processes. By integrating distributions and predictions from a committee of
models while generating high-quality soft labels, our method captures a wider
spectrum of data features, reduces model-specific biases and the adverse
effects of distribution shifts, leading to significant improvements in
generalization. This voting-based strategy not only promotes diversity and
robustness within the distilled dataset but also significantly reduces
overfitting, resulting in improved performance on post-eval tasks. Extensive
experiments across various datasets and IPCs (images per class) demonstrate
that Committee Voting leads to more reliable and adaptable distilled data
compared to single/multi-model distillation methods, demonstrating its
potential for efficient and accurate dataset distillation. Code is available
at: https://github.com/Jiacheng8/CV-DD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at: https://github.com/Jiacheng8/CV-DD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnCommon Objects in 3D 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Liu, Piyush Tayal, Jianyuan Wang, Jesus Zarzar, Tom Monnier, Konstantinos Tertikas, Jiali Duan, Antoine Toisoul, Jason Y. Zhang, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, David Novotny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for
3D deep learning and 3D generative AI. uCO3D is the largest publicly-available
collection of high-resolution videos of objects with 3D annotations that
ensures full-360$^{\circ}$ coverage. uCO3D is significantly more diverse than
MVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of
higher quality, due to extensive quality checks of both the collected videos
and the 3D annotations. Similar to analogous datasets, uCO3D contains
annotations for 3D camera poses, depth maps and sparse point clouds. In
addition, each object is equipped with a caption and a 3D Gaussian Splat
reconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D
and obtain superior results using the latter, showing that uCO3D is better for
learning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Free Motion-Guided Video Generation with Enhanced Temporal
  Consistency Using Motion Consistency Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Zicheng Duan, Dong Gong, Lingqiao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of generating temporally consistent
videos with motion guidance. While many existing methods depend on additional
control modules or inference-time fine-tuning, recent studies suggest that
effective motion guidance is achievable without altering the model architecture
or requiring extra training. Such approaches offer promising compatibility with
various video generation foundation models. However, existing training-free
methods often struggle to maintain consistent temporal coherence across frames
or to follow guided motion accurately. In this work, we propose a simple yet
effective solution that combines an initial-noise-based approach with a novel
motion consistency loss, the latter being our key innovation. Specifically, we
capture the inter-frame feature correlation patterns of intermediate features
from a video diffusion model to represent the motion pattern of the reference
video. We then design a motion consistency loss to maintain similar feature
correlation patterns in the generated video, using the gradient of this loss in
the latent space to guide the generation process for precise motion control.
This approach improves temporal consistency across various motion control tasks
while preserving the benefits of a training-free setup. Extensive experiments
show that our method sets a new standard for efficient, temporally coherent
video generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://zhangxinyu-xyz.github.io/SimulateMotion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MatchAnything: Universal Cross-Modality Image Matching with Large-Scale
  <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyi He, Hao Yu, Sida Peng, Dongli Tan, Zehong Shen, Hujun Bao, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image matching, which aims to identify corresponding pixel locations between
images, is crucial in a wide range of scientific disciplines, aiding in image
registration, fusion, and analysis. In recent years, deep learning-based image
matching algorithms have dramatically outperformed humans in rapidly and
accurately finding large amounts of correspondences. However, when dealing with
images captured under different imaging modalities that result in significant
appearance changes, the performance of these algorithms often deteriorates due
to the scarcity of annotated cross-modal training data. This limitation hinders
applications in various fields that rely on multiple image modalities to obtain
complementary information. To address this challenge, we propose a large-scale
pre-training framework that utilizes synthetic cross-modal training signals,
incorporating diverse data from various sources, to train models to recognize
and match fundamental structures across images. This capability is transferable
to real-world, unseen cross-modality image matching tasks. Our key finding is
that the matching model trained with our framework achieves remarkable
generalizability across more than eight unseen cross-modality registration
tasks using the same network weight, substantially outperforming existing
methods, whether designed for generalization or tailored for specific tasks.
This advancement significantly enhances the applicability of image matching
technologies across various scientific disciplines and paves the way for new
applications in multi-modality human and artificial intelligence analysis and
beyond.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/MatchAnything/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal
  Aspects in Video Editing <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Biyyala, Bharat Chanderprakash Kathuria, Jialu Li, Youshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video editing models have advanced significantly, but evaluating their
performance remains challenging. Traditional metrics, such as CLIP text and
image scores, often fall short: text scores are limited by inadequate training
data and hierarchical dependencies, while image scores fail to assess temporal
consistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation
Metric), a novel evaluation framework that leverages modern Vision-Language
Models (VLMs), Object Detection, and Temporal Consistency checks. SST-EM
comprises four components: (1) semantic extraction from frames using a VLM, (2)
primary object tracking with Object Detection, (3) focused object refinement
via an LLM agent, and (4) temporal consistency assessment using a Vision
Transformer (ViT). These components are integrated into a unified metric with
weights derived from human evaluations and regression analysis. The name SST-EM
reflects its focus on Semantic, Spatial, and Temporal aspects of video
evaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and
temporal smoothness in video editing. The source code is available in the
\textbf{\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub
Repository}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imagine while Reasoning in Space: Multimodal Visualization-of-Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) prompting has proven highly effective for enhancing
complex reasoning in Large Language Models (LLMs) and Multimodal Large Language
Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks.
Nonetheless, human cognition extends beyond language alone, enabling the
remarkable capability to think in both words and images. Inspired by this
mechanism, we propose a new reasoning paradigm, Multimodal
Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by
generating image visualizations of their reasoning traces. To ensure
high-quality visualization, we introduce token discrepancy loss into
autoregressive MLLMs. This innovation significantly improves both visual
coherence and fidelity. We validate this approach through several dynamic
spatial reasoning tasks. Experimental results reveal that MVoT demonstrates
competitive performance across tasks. Moreover, it exhibits robust and reliable
improvements in the most challenging scenarios where CoT fails. Ultimately,
MVoT establishes new possibilities for complex reasoning tasks where visual
thinking can effectively complement verbal reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables
  including references and appendices)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confident Pseudo-labeled Diffusion Augmentation for Canine Cardiomegaly
  Detection <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiman Zhang, Lakshmikar Reddy Polamreddy, Youshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Canine cardiomegaly, marked by an enlarged heart, poses serious health risks
if undetected, requiring accurate diagnostic methods. Current detection models
often rely on small, poorly annotated datasets and struggle to generalize
across diverse imaging conditions, limiting their real-world applicability. To
address these issues, we propose a Confident Pseudo-labeled Diffusion
Augmentation (CDA) model for identifying canine cardiomegaly. Our approach
addresses the challenge of limited high-quality training data by employing
diffusion models to generate synthetic X-ray images and annotate Vertebral
Heart Score key points, thereby expanding the dataset. We also employ a
pseudo-labeling strategy with Monte Carlo Dropout to select high-confidence
labels, refine the synthetic dataset, and improve accuracy. Iteratively
incorporating these labels enhances the model's performance, overcoming the
limitations of existing approaches. Experimental results show that the CDA
model outperforms traditional methods, achieving state-of-the-art accuracy in
canine cardiomegaly detection. The code implementation is available at
https://github.com/Shira7z/CDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion <span class="chip">WACV-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tharun Anand, Aryan Garg, Kaushik Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial video editing has become increasingly important for content creators,
enabling the manipulation of facial expressions and attributes. However,
existing models encounter challenges such as poor editing quality, high
computational costs and difficulties in preserving facial identity across
diverse edits. Additionally, these models are often constrained to editing
predefined facial attributes, limiting their flexibility to diverse editing
prompts. To address these challenges, we propose a novel facial video editing
framework that leverages the rich latent space of pre-trained text-to-image
(T2I) diffusion models and fine-tune them specifically for facial video editing
tasks. Our approach introduces a targeted fine-tuning scheme that enables high
quality, localized, text-driven edits while ensuring identity preservation
across video frames. Additionally, by using pre-trained T2I models during
inference, our approach significantly reduces editing time by 80%, while
maintaining temporal consistency throughout the video sequence. We evaluate the
effectiveness of our approach through extensive testing across a wide range of
challenging scenarios, including varying head poses, complex action sequences,
and diverse facial expressions. Our method consistently outperforms existing
techniques, demonstrating superior performance across a broad set of metrics
and benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV-25 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RadAlign: Advancing Radiology Report Generation with Vision-Language
  Concept Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Difei Gu, Yunhe Gao, Yang Zhou, Mu Zhou, Dimitris Metaxas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated chest radiographs interpretation requires both accurate disease
classification and detailed radiology report generation, presenting a
significant challenge in the clinical workflow. Current approaches either focus
on classification accuracy at the expense of interpretability or generate
detailed but potentially unreliable reports through image captioning
techniques. In this study, we present RadAlign, a novel framework that combines
the predictive accuracy of vision-language models (VLMs) with the reasoning
capabilities of large language models (LLMs). Inspired by the radiologist's
workflow, RadAlign first employs a specialized VLM to align visual features
with key medical concepts, achieving superior disease classification with an
average AUC of 0.885 across multiple diseases. These recognized medical
conditions, represented as text-based concepts in the aligned visual-language
space, are then used to prompt LLM-based report generation. Enhanced by a
retrieval-augmented generation mechanism that grounds outputs in similar
historical cases, RadAlign delivers superior report quality with a GREEN score
of 0.678, outperforming state-of-the-art methods' 0.634. Our framework
maintains strong clinical interpretability while reducing hallucinations,
advancing automated medical imaging and report analysis through integrated
predictive and generative AI. Code is available at
https://github.com/difeigu/RadAlign.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three-view Focal Length Recovery From Homo<span class="highlight-title">graph</span>ies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaqing Ding, Viktor Kocur, Zuzana Berger Haladová, Qianliang Wu, Shen Cai, Jian Yang, Zuzana Kukelova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach for recovering focal lengths from
three-view homographies. By examining the consistency of normal vectors between
two homographies, we derive new explicit constraints between the focal lengths
and homographies using an elimination technique. We demonstrate that three-view
homographies provide two additional constraints, enabling the recovery of one
or two focal lengths. We discuss four possible cases, including three cameras
having an unknown equal focal length, three cameras having two different
unknown focal lengths, three cameras where one focal length is known, and the
other two cameras have equal or different unknown focal lengths. All the
problems can be converted into solving polynomials in one or two unknowns,
which can be efficiently solved using Sturm sequence or hidden variable
technique. Evaluation using both synthetic and real data shows that the
proposed solvers are both faster and more accurate than methods relying on
existing two-view solvers. The code and data are available on
https://github.com/kocurvik/hf
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/kocurvik/hf Dataset available
  at: https://doi.org/10.5281/zenodo.14638904</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal
  Violence Detection Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenping Jin, Li Zhu, Jing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised violence detection refers to the technique of training
models to identify violent segments in videos using only video-level labels.
Among these approaches, multimodal violence detection, which integrates
modalities such as audio and optical flow, holds great potential. Existing
methods in this domain primarily focus on designing multimodal fusion models to
address modality discrepancies. In contrast, we take a different approach;
leveraging the inherent discrepancies across modalities in violence event
representation to propose a novel multimodal semantic feature alignment method.
This method sparsely maps the semantic features of local, transient, and less
informative modalities ( such as audio and optical flow ) into the more
informative RGB semantic feature space. Through an iterative process, the
method identifies the suitable no-zero feature matching subspace and aligns the
modality-specific event representations based on this subspace, enabling the
full exploitation of information from all modalities during the subsequent
modality fusion stage. Building on this, we design a new weakly supervised
violence detection framework that consists of unimodal multiple-instance
learning for extracting unimodal semantic features, multimodal alignment,
multimodal fusion, and final detection. Experimental results on benchmark
datasets demonstrate the effectiveness of our method, achieving an average
precision (AP) of 86.07% on the XD-Violence dataset. Our code is available at
https://github.com/xjpp2016/MAVD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point
  Cloud or Mesh 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lewis A G Stuart, Michael P Pound
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D
reconstructions, but these scenes often require specialised renderers for
effective visualisation. In contrast, point clouds are a widely used 3D
representation and are compatible with most popular 3D processing software, yet
converting 3DGS scenes into point clouds is a complex challenge. In this work
we introduce 3DGS-to-PC, a flexible and highly customisable framework that is
capable of transforming 3DGS scenes into dense, high-accuracy point clouds. We
sample points probabilistically from each Gaussian as a 3D density function. We
additionally threshold new points using the Mahalanobis distance to the
Gaussian centre, preventing extreme outliers. The result is a point cloud that
closely represents the shape encoded into the 3D Gaussian scene. Individual
Gaussians use spherical harmonics to adapt colours depending on view, and each
point may contribute only subtle colour hints to the resulting rendered scene.
To avoid spurious or incorrect colours that do not fit with the final point
cloud, we recalculate Gaussian colours via a customised image rendering
approach, assigning each Gaussian the colour of the pixel to which it
contributes most across all views. 3DGS-to-PC also supports mesh generation
through Poisson Surface Reconstruction, applied to points sampled from
predicted surface Gaussians. This allows coloured meshes to be generated from
3DGS scenes without the need for re-training. This package is highly
customisable and capability of simple integration into existing 3DGS pipelines.
3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud
and surface-based formats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Dynamic Neural Networks: from Computer Vision to Multi-modal
  Sensor Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Montello, Ronja Güldenring, Simone Scardapane, Lazaros Nalpantidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model compression is essential in the deployment of large Computer Vision
models on embedded devices. However, static optimization techniques (e.g.
pruning, quantization, etc.) neglect the fact that different inputs have
different complexities, thus requiring different amount of computations.
Dynamic Neural Networks allow to condition the number of computations to the
specific input. The current literature on the topic is very extensive and
fragmented. We present a comprehensive survey that synthesizes and unifies
existing Dynamic Neural Networks research in the context of Computer Vision.
Additionally, we provide a logical taxonomy based on which component of the
network is adaptive: the output, the computation graph or the input.
Furthermore, we argue that Dynamic Neural Networks are particularly beneficial
in the context of Sensor Fusion for better adaptivity, noise reduction and
information prioritization. We present preliminary works in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at International Journal of Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrecipDiff: Leveraging image diffusion models to enhance satellite-based
  precipitation observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Yu Dai, Hayato Ushijima-Mwesigwa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent report from the World Meteorological Organization (WMO) highlights
that water-related disasters have caused the highest human losses among natural
disasters over the past 50 years, with over 91\% of deaths occurring in
low-income countries. This disparity is largely due to the lack of adequate
ground monitoring stations, such as weather surveillance radars (WSR), which
are expensive to install. For example, while the US and Europe combined possess
over 600 WSRs, Africa, despite having almost one and half times their landmass,
has fewer than 40. To address this issue, satellite-based observations offer a
global, near-real-time monitoring solution. However, they face several
challenges like accuracy, bias, and low spatial resolution. This study
leverages the power of diffusion models and residual learning to address these
limitations in a unified framework. We introduce the first diffusion model for
correcting the inconsistency between different precipitation products. Our
method demonstrates the effectiveness in downscaling satellite precipitation
estimates from 10 km to 1 km resolution. Extensive experiments conducted in the
Seattle region demonstrate significant improvements in accuracy, bias
reduction, and spatial detail. Importantly, our approach achieves these results
using only precipitation data, showcasing the potential of a purely computer
vision-based approach for enhancing satellite precipitation products and paving
the way for further advancements in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided SAM: Label-Efficient Part Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. B. van Rooij, G. J. Burghouts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localizing object parts precisely is essential for tasks such as object
recognition and robotic manipulation. Recent part segmentation methods require
extensive training data and labor-intensive annotations. Segment-Anything Model
(SAM) has demonstrated good performance on a wide range of segmentation
problems, but requires (manual) positional prompts to guide it where to
segment. Furthermore, since it has been trained on full objects instead of
object parts, it is prone to over-segmentation of parts. To address this, we
propose a novel approach that guides SAM towards the relevant object parts. Our
method learns positional prompts from coarse patch annotations that are easier
and cheaper to acquire. We train classifiers on image patches to identify part
classes and aggregate patches into regions of interest (ROIs) with positional
prompts. SAM is conditioned on these ROIs and prompts. This approach, termed
`Guided SAM', enhances efficiency and reduces manual effort, allowing effective
part segmentation with minimal labeled data. We demonstrate the efficacy of
Guided SAM on a dataset of car parts, improving the average IoU on state of the
art models from 0.37 to 0.49 with annotations that are on average five times
more efficient to acquire.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for
  Volume-to-Volume Medical Image Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyue Zhu, Dou Hoon Kwark, Ruike Zhu, Kaiwen Hong, Yiqi Tao, Shirui Luo, Yudu Li, Zhi-Pei Liang, Volodymyr Kindratenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite success in volume-to-volume translations in medical images, most
existing models struggle to effectively capture the inherent volumetric
distribution using 3D representations. The current state-of-the-art approach
combines multiple 2D-based networks through weighted averaging, thereby
neglecting the 3D spatial structures. Directly training 3D models in medical
imaging presents significant challenges due to high computational demands and
the need for large-scale datasets. To address these challenges, we introduce
Diff-Ensembler, a novel hybrid 2D-3D model for efficient and effective
volumetric translations by ensembling perpendicularly trained 2D diffusion
models with a 3D network in each diffusion step. Moreover, our model can
naturally be used to ensemble diffusion models conditioned on different
modalities, allowing flexible and accurate fusion of input conditions.
Extensive experiments demonstrate that Diff-Ensembler attains superior accuracy
and volumetric realism in 3D medical image super-resolution and modality
translation. We further demonstrate the strength of our model's volumetric
realism using tumor segmentation as a downstream task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCORD: Open-Campus Object Removal <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Zhang, Runpu Wei, Kongming Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in generative models, particularly diffusion-based
techniques, have revolutionized image inpainting tasks by enabling the
generation of high-fidelity and diverse content. However, object removal
remains under-explored as a specific subset of inpainting, facing challenges
such as inadequate semantic understanding and the unintended generation of
artifacts. Existing datasets for object removal often rely on synthetic data,
which fails to align with real-world scenarios, limiting model performance.
Although some real-world datasets address these issues partially, they suffer
from scalability, annotation inefficiencies, and limited realism in physical
phenomena such as lighting and shadows. To address these limitations, this
paper introduces a novel approach to object removal by constructing a
high-resolution real-world dataset through long-duration video capture with
fixed camera settings. Leveraging advanced tools such as Grounding-DINO,
Segment-Anything-Model, and MASA for automated annotation, we provides image,
background, and mask pairs while significantly reducing annotation time and
labor. With our efficient annotation pipeline, we release the first fully open,
high-resolution real-world dataset for object removal, and improved performance
in object removal tasks through fine-tuning of pre-trained diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Scene Understanding for Automatic Target Recognition Using
  Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasiru Ranasinghe, Vibashan VS, James Uplinger, Celso De Melo, Vishal M. Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic target recognition (ATR) plays a critical role in tasks such as
navigation and surveillance, where safety and accuracy are paramount. In
extreme use cases, such as military applications, these factors are often
challenged due to the presence of unknown terrains, environmental conditions,
and novel object categories. Current object detectors, including open-world
detectors, lack the ability to confidently recognize novel objects or operate
in unknown environments, as they have not been exposed to these new conditions.
However, Large Vision-Language Models (LVLMs) exhibit emergent properties that
enable them to recognize objects in varying conditions in a zero-shot manner.
Despite this, LVLMs struggle to localize objects effectively within a scene. To
address these limitations, we propose a novel pipeline that combines the
detection capabilities of open-world detectors with the recognition confidence
of LVLMs, creating a robust system for zero-shot ATR of novel classes and
unknown domains. In this study, we compare the performance of various LVLMs for
recognizing military vehicles, which are often underrepresented in training
datasets. Additionally, we examine the impact of factors such as distance
range, modality, and prompting methods on the recognition performance,
providing insights into the development of more reliable ATR systems for novel
conditions and classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kolmogorov-Arnold Network for Remote Sensing Image Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianping Ma, Ziyao Wang, Yin Hu, Xiaokang Zhang, Man-On Pun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation plays a crucial role in remote sensing applications,
where the accurate extraction and representation of features are essential for
high-quality results. Despite the widespread use of encoder-decoder
architectures, existing methods often struggle with fully utilizing the
high-dimensional features extracted by the encoder and efficiently recovering
detailed information during decoding. To address these problems, we propose a
novel semantic segmentation network, namely DeepKANSeg, including two key
innovations based on the emerging Kolmogorov Arnold Network (KAN). Notably, the
advantage of KAN lies in its ability to decompose high-dimensional complex
functions into univariate transformations, enabling efficient and flexible
representation of intricate relationships in data. First, we introduce a
KAN-based deep feature refinement module, namely DeepKAN to effectively capture
complex spatial and rich semantic relationships from high-dimensional features.
Second, we replace the traditional multi-layer perceptron (MLP) layers in the
global-local combined decoder with KAN-based linear layers, namely GLKAN. This
module enhances the decoder's ability to capture fine-grained details during
decoding. To evaluate the effectiveness of the proposed method, experiments are
conducted on two well-known fine-resolution remote sensing benchmark datasets,
namely ISPRS Vaihingen and ISPRS Potsdam. The results demonstrate that the
KAN-enhanced segmentation model achieves superior performance in terms of
accuracy compared to state-of-the-art methods. They highlight the potential of
KANs as a powerful alternative to traditional architectures in semantic
segmentation tasks. Moreover, the explicit univariate decomposition provides
improved interpretability, which is particularly beneficial for applications
requiring explainable learning in remote sensing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Deng, Zhe Xu, Tsuyoshi Isshiki, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is challenging due to the diversity of medical
images and the lack of labeled data, which motivates recent developments in
federated semi-supervised learning (FSSL) to leverage a large amount of
unlabeled data from multiple centers for model training without sharing raw
data. However, what remains under-explored in FSSL is the domain shift problem
which may cause suboptimal model aggregation and low effectivity of the
utilization of unlabeled data, eventually leading to unsatisfactory performance
in unseen domains. In this paper, we explore this previously ignored scenario,
namely domain generalized federated semi-supervised learning (FedSemiDG), which
aims to learn a model in a distributed manner from multiple domains with
limited labeled data and abundant unlabeled data such that the model can
generalize well to unseen domains. We present a novel framework, Federated
Generalization-Aware SemiSupervised Learning (FGASL), to address the challenges
in FedSemiDG by effectively tackling critical issues at both global and local
levels. Globally, we introduce Generalization-Aware Aggregation (GAA),
assigning adaptive weights to local models based on their generalization
performance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement
(DR) strategy to combine global and domain-specific knowledge, generating more
reliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA)
enforces feature consistency under perturbations, promoting domain-invariant
learning. Extensive experiments on three medical segmentation tasks (cardiac
MRI, spine MRI and bladder cancer MRI) demonstrate that our method
significantly outperforms state-of-the-art FSSL and domain generalization
approaches, achieving robust generalization on unseen domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimberVision: A Multi-Task <span class="highlight-title">Dataset</span> and Framework for Log-Component
  Segmentation and Tracking in Autonomous Forestry Operations <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Steininger, Julia Simon, Andreas Trondl, Markus Murschitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timber represents an increasingly valuable and versatile resource. However,
forestry operations such as harvesting, handling and measuring logs still
require substantial human labor in remote environments posing significant
safety risks. Progressively automating these tasks has the potential of
increasing their efficiency as well as safety, but requires an accurate
detection of individual logs as well as live trees and their context. Although
initial approaches have been proposed for this challenging application domain,
specialized data and algorithms are still too scarce to develop robust
solutions. To mitigate this gap, we introduce the TimberVision dataset,
consisting of more than 2k annotated RGB images containing a total of 51k trunk
components including cut and lateral surfaces, thereby surpassing any existing
dataset in this domain in terms of both quantity and detail by a large margin.
Based on this data, we conduct a series of ablation experiments for oriented
object detection and instance segmentation and evaluate the influence of
multiple scene parameters on model performance. We introduce a generic
framework to fuse the components detected by our models for both tasks into
unified trunk representations. Furthermore, we automatically derive geometric
properties and apply multi-object tracking to further enhance robustness. Our
detection and tracking approach provides highly descriptive and accurate trunk
representations solely from RGB image data, even under challenging
environmental conditions. Our solution is suitable for a wide range of
application scenarios and can be readily combined with other sensor modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Winter Conference on Applications of Computer Vision
  (WACV) 2025. Code and dataset available at
  https://github.com/timbervision/timbervision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A method for estimating roadway billboard salience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuzana Berger Haladova, Michal Zrubec, Zuzana Cernekova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Roadside billboards and other forms of outdoor advertising play a crucial
role in marketing initiatives; however, they can also distract drivers,
potentially contributing to accidents. This study delves into the significance
of roadside advertising in images captured from a driver's perspective.
Firstly, it evaluates the effectiveness of neural networks in detecting
advertising along roads, focusing on the YOLOv5 and Faster R-CNN models.
Secondly, the study addresses the determination of billboard significance using
methods for saliency extraction. The UniSal and SpectralResidual methods were
employed to create saliency maps for each image. The study establishes a
database of eye tracking sessions captured during city highway driving to
assess the saliency models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anonymization of Documents for Law Enforcement with Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Eberhardinger, Patrick Takenaka, Daniel Grießhaber, Johannes Maucher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The steadily increasing utilization of data-driven methods and approaches in
areas that handle sensitive personal information such as in law enforcement
mandates an ever increasing effort in these institutions to comply with data
protection guidelines. In this work, we present a system for automatically
anonymizing images of scanned documents, reducing manual effort while ensuring
data protection compliance. Our method considers the viability of further
forensic processing after anonymization by minimizing automatically redacted
areas by combining automatic detection of sensitive regions with knowledge from
a manually anonymized reference document. Using a self-supervised image model
for instance retrieval of the reference document, our approach requires only
one anonymized example to efficiently redact all documents of the same type,
significantly reducing processing time. We show that our approach outperforms
both a purely automatic redaction system and also a naive copy-paste scheme of
the reference anonymization to other documents on a hand-crafted dataset of
ground truth redactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Symposium on CI in Security, Defence and Biometrics
  2025 (IEEE CISDB)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localization-Aware Multi-Scale Representation Learning for Repetitive
  Action Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujia Wang, Xiangwei Shen, Yansong Tang, Xin Dong, Wenjia Geng, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Repetitive action counting (RAC) aims to estimate the number of
class-agnostic action occurrences in a video without exemplars. Most current
RAC methods rely on a raw frame-to-frame similarity representation for period
prediction. However, this approach can be significantly disrupted by common
noise such as action interruptions and inconsistencies, leading to sub-optimal
counting performance in realistic scenarios. In this paper, we introduce a
foreground localization optimization objective into similarity representation
learning to obtain more robust and efficient video features. We propose a
Localization-Aware Multi-Scale Representation Learning (LMRL) framework.
Specifically, we apply a Multi-Scale Period-Aware Representation (MPR) with a
scale-specific design to accommodate various action frequencies and learn more
flexible temporal correlations. Furthermore, we introduce the Repetition
Foreground Localization (RFL) method, which enhances the representation by
coarsely identifying periodic actions and incorporating global semantic
information. These two modules can be jointly optimized, resulting in a more
discerning periodic action representation. Our approach significantly reduces
the impact of noise, thereby improving counting accuracy. Additionally, the
framework is designed to be scalable and adaptable to different types of video
content. Experimental results on the RepCountA and UCFRep datasets demonstrate
that our proposed method effectively handles repetitive action counting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE VCIP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Devil is in the Spurious Correlation: Boosting Moment <span class="highlight-title">Retrie</span>val via
  Temporal Dynamic Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyang Zhou, Fanyue Wei, Lixin Duan, Wen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a textual query along with a corresponding video, the objective of
moment retrieval aims to localize the moments relevant to the query within the
video. While commendable results have been demonstrated by existing
transformer-based approaches, predicting the accurate temporal span of the
target moment is currently still a major challenge. In this paper, we reveal
that a crucial reason stems from the spurious correlation between the text
queries and the moment context. Namely, the model may associate the textual
query with the background frames rather than the target moment. To address this
issue, we propose a temporal dynamic learning approach for moment retrieval,
where two strategies are designed to mitigate the spurious correlation. First,
we introduce a novel video synthesis approach to construct a dynamic context
for the relevant moment. With separate yet similar videos mixed up, the
synthesis approach empowers our model to attend to the target moment of the
corresponding query under various dynamic contexts. Second, we enhance the
representation by learning temporal dynamics. Besides the visual
representation, text queries are aligned with temporal dynamic representations,
which enables our model to establish a non-spurious correlation between the
query-related moment and context. With the aforementioned proposed method, the
spurious correlation issue in moment retrieval can be largely alleviated. Our
method establishes a new state-of-the-art performance on two popular benchmarks
of moment retrieval, \ie, QVHighlights and Charades-STA. In addition, the
detailed ablation analyses demonstrate the effectiveness of the proposed
strategies. Our code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code and Pixels: Multi-Modal Contrastive <span class="highlight-title">Pre-train</span>ing for Enhanced
  <span class="highlight-title">Tabular</span> Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kankana Roy, Lars Krämer, Sebastian Domaschke, Malik Haris, Roland Aydin, Fabian Isensee, Martin Held
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from tabular data is of paramount importance, as it complements the
conventional analysis of image and video data by providing a rich source of
structured information that is often critical for comprehensive understanding
and decision-making processes. We present Multi-task Contrastive Masked Tabular
Modeling (MT-CMTM), a novel method aiming to enhance tabular models by
leveraging the correlation between tabular data and corresponding images.
MT-CMTM employs a dual strategy combining contrastive learning with masked
tabular modeling, optimizing the synergy between these data modalities.
  Central to our approach is a 1D Convolutional Neural Network with residual
connections and an attention mechanism (1D-ResNet-CBAM), designed to
efficiently process tabular data without relying on images. This enables
MT-CMTM to handle purely tabular data for downstream tasks, eliminating the
need for potentially costly image acquisition and processing.
  We evaluated MT-CMTM on the DVM car dataset, which is uniquely suited for
this particular scenario, and the newly developed HIPMP dataset, which connects
membrane fabrication parameters with image data. Our MT-CMTM model outperforms
the proposed tabular 1D-ResNet-CBAM, which is trained from scratch, achieving a
relative 1.48% improvement in relative MSE on HIPMP and a 2.38% increase in
absolute accuracy on DVM. These results demonstrate MT-CMTM's robustness and
its potential to advance the field of multi-modal learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative analysis of optical character recognition methods for Sámi
  texts from the National Library of Norway 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tita Enstad, Trond Trosterud, Marie Iversdatter Røsok, Yngvil Beyer, Marie Roald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Character Recognition (OCR) is crucial to the National Library of
Norway's (NLN) digitisation process as it converts scanned documents into
machine-readable text. However, for the S\'ami documents in NLN's collection,
the OCR accuracy is insufficient. Given that OCR quality affects downstream
processes, evaluating and improving OCR for text written in S\'ami languages is
necessary to make these resources accessible. To address this need, this work
fine-tunes and evaluates three established OCR approaches, Transkribus,
Tesseract and TrOCR, for transcribing S\'ami texts from NLN's collection. Our
results show that Transkribus and TrOCR outperform Tesseract on this task,
while Tesseract achieves superior performance on an out-of-domain dataset.
Furthermore, we show that fine-tuning pre-trained models and supplementing
manual annotations with machine annotations and synthetic text images can yield
accurate OCR for S\'ami languages, even with a moderate amount of manually
annotated data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the 25th Nordic Conference on
  Computational Linguistics (NoDaLiDa)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Realistic Camouflaged Object Detection: Benchmarks and Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimeng Xin, Tianxu Wu, Shiming Chen, Shuo Ye, Zijing Xie, Yixiong Zou, Xinge You, Yufei Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflaged object detection (COD) primarily relies on semantic or instance
segmentation methods. While these methods have made significant advancements in
identifying the contours of camouflaged objects, they may be inefficient or
cost-effective for tasks that only require the specific location of the object.
Object detection algorithms offer an optimized solution for Realistic
Camouflaged Object Detection (RCOD) in such cases. However, detecting
camouflaged objects remains a formidable challenge due to the high degree of
similarity between the features of the objects and their backgrounds. Unlike
segmentation methods that perform pixel-wise comparisons to differentiate
between foreground and background, object detectors omit this analysis, further
aggravating the challenge. To solve this problem, we propose a camouflage-aware
feature refinement (CAFR) strategy. Since camouflaged objects are not rare
categories, CAFR fully utilizes a clear perception of the current object within
the prior knowledge of large models to assist detectors in deeply understanding
the distinctions between background and foreground. Specifically, in CAFR, we
introduce the Adaptive Gradient Propagation (AGP) module that fine-tunes all
feature extractor layers in large detection models to fully refine
class-specific features from camouflaged contexts. We then design the Sparse
Feature Refinement (SFR) module that optimizes the transformer-based feature
extractor to focus primarily on capturing class-specific features in
camouflaged scenarios. To facilitate the assessment of RCOD tasks, we manually
annotate the labels required for detection on three existing segmentation COD
datasets, creating a new benchmark for RCOD tasks. Code and datasets are
available at: https://github.com/zhimengXin/RCOD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-based Video Person Re-identification via Cross-Modality and
  Temporal Collaboration <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renkai Li, Xin Yuan, Wei Liu, Xin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-based person re-identification (ReID) has become increasingly important
due to its applications in video surveillance applications. By employing events
in video-based person ReID, more motion information can be provided between
continuous frames to improve recognition accuracy. Previous approaches have
assisted by introducing event data into the video person ReID task, but they
still cannot avoid the privacy leakage problem caused by RGB images. In order
to avoid privacy attacks and to take advantage of the benefits of event data,
we consider using only event data. To make full use of the information in the
event stream, we propose a Cross-Modality and Temporal Collaboration (CMTC)
network for event-based video person ReID. First, we design an event transform
network to obtain corresponding auxiliary information from the input of raw
events. Additionally, we propose a differential modality collaboration module
to balance the roles of events and auxiliaries to achieve complementary
effects. Furthermore, we introduce a temporal collaboration module to exploit
motion information and appearance cues. Experimental results demonstrate that
our method outperforms others in the task of event-based video person ReID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Liang, Naveed Akhtar, Jordan Vice, Xiangrui Kong, Ajmal Saeed Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D semantic scene completion is critical for multiple downstream tasks in
autonomous systems. It estimates missing geometric and semantic information in
the acquired scene data. Due to the challenging real-world conditions, this
task usually demands complex models that process multi-modal data to achieve
acceptable performance. We propose a unique neural model, leveraging advances
from the state space and diffusion generative modeling to achieve remarkable 3D
semantic scene completion performance with monocular image input. Our technique
processes the data in the conditioned latent space of a variational autoencoder
where diffusion modeling is carried out with an innovative state space
technique. A key component of our neural network is the proposed Skimba (Skip
Mamba) denoiser, which is adept at efficiently processing long-sequence data.
The Skimba diffusion model is integral to our 3D scene completion network,
incorporating a triple Mamba structure, dimensional decomposition residuals and
varying dilations along three directions. We also adopt a variant of this
network for the subsequent semantic segmentation stage of our method. Extensive
evaluation on the standard SemanticKITTI and SSCBench-KITTI360 datasets show
that our approach not only outperforms other monocular techniques by a large
margin, it also achieves competitive performance against stereo methods. The
code is available at https://github.com/xrkong/skimba
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EdgeTAM: On-Device Track Anything Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Zhou, Chenchen Zhu, Yunyang Xiong, Saksham Suri, Fanyi Xiao, Lemeng Wu, Raghuraman Krishnamoorthi, Bo Dai, Chen Change Loy, Vikas Chandra, Bilge Soran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On top of Segment Anything Model (SAM), SAM 2 further extends its capability
from image to video inputs through a memory bank mechanism and obtains a
remarkable performance compared with previous methods, making it a foundation
model for video segmentation task. In this paper, we aim at making SAM 2 much
more efficient so that it even runs on mobile devices while maintaining a
comparable performance. Despite several works optimizing SAM for better
efficiency, we find they are not sufficient for SAM 2 because they all focus on
compressing the image encoder, while our benchmark shows that the newly
introduced memory attention blocks are also the latency bottleneck. Given this
observation, we propose EdgeTAM, which leverages a novel 2D Spatial Perceiver
to reduce the computational cost. In particular, the proposed 2D Spatial
Perceiver encodes the densely stored frame-level memories with a lightweight
Transformer that contains a fixed set of learnable queries. Given that video
segmentation is a dense prediction task, we find preserving the spatial
structure of the memories is essential so that the queries are split into
global-level and patch-level groups. We also propose a distillation pipeline
that further improves the performance without inference overhead. As a result,
EdgeTAM achieves 87.7, 70.0, 72.3, and 71.7 J&F on DAVIS 2017, MOSE, SA-V val,
and SA-V test, while running at 16 FPS on iPhone 15 Pro Max.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be released at https://github.com/facebookresearch/EdgeTAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOS-Attack: A Scalable Multi-objective Adversarial Attack Framework <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Guo, Cheng Gong, Xi Lin, Fei Liu, Zhichao Lu, Qingfu Zhang, Zhenkun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crafting adversarial examples is crucial for evaluating and enhancing the
robustness of Deep Neural Networks (DNNs), presenting a challenge equivalent to
maximizing a non-differentiable 0-1 loss function.
  However, existing single objective methods, namely adversarial attacks focus
on a surrogate loss function, do not fully harness the benefits of engaging
multiple loss functions, as a result of insufficient understanding of their
synergistic and conflicting nature.
  To overcome these limitations, we propose the Multi-Objective Set-based
Attack (MOS Attack), a novel adversarial attack framework leveraging multiple
loss functions and automatically uncovering their interrelations.
  The MOS Attack adopts a set-based multi-objective optimization strategy,
enabling the incorporation of numerous loss functions without additional
parameters.
  It also automatically mines synergistic patterns among various losses,
facilitating the generation of potent adversarial attacks with fewer
objectives.
  Extensive experiments have shown that our MOS Attack outperforms
single-objective attacks. Furthermore, by harnessing the identified synergistic
patterns, MOS Attack continues to show superior results with a reduced number
of loss functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review of CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Neural Representations for Registration of Left Ventricle
  Myocardium During a Cardiac Cycle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Micheelsen Lowes, Jonas Jalili Pedersen, Bjørn S. Hansen, Klaus Fuglsang Kofoed, Maxime Sermesant, Rasmus R. Paulsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the movement of the left ventricle myocardium (LVmyo) during
the cardiac cycle is essential for assessing cardiac function. One way to model
this movement is through a series of deformable image registrations (DIRs) of
the LVmyo. Traditional deep learning methods for DIRs, such as those based on
convolutional neural networks, often require substantial memory and
computational resources. In contrast, implicit neural representations (INRs)
offer an efficient approach by operating on any number of continuous points.
This study extends the use of INRs for DIR to cardiac computed tomography (CT),
focusing on LVmyo registration. To enhance the precision of the registration
around the LVmyo, we incorporate the signed distance field of the LVmyo with
the Hounsfield Unit values from the CT frames. This guides the registration of
the LVmyo, while keeping the tissue information from the CT frames. Our
framework demonstrates high registration accuracy and provides a robust method
for temporal registration that facilitates further analysis of LVmyo motion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, STACOM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth and Image Fusion for Road Obstacle Detection Using Stereo Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Perezyabov, Mikhail Gavrilenkov, Ilya Afanasyev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is devoted to the detection of objects on a road, performed with a
combination of two methods based on both the use of depth information and video
analysis of data from a stereo camera. Since neither the time of the appearance
of an object on the road, nor its size and shape is known in advance,
ML/DL-based approaches are not applicable. The task becomes more complicated
due to variations in artificial illumination, inhomogeneous road surface
texture, and unknown character and features of the object. To solve this
problem we developed the depth and image fusion method that complements a
search of small contrast objects by RGB-based method, and obstacle detection by
stereo image-based approach with SLIC superpixel segmentation. We conducted
experiments with static and low speed obstacles in an underground parking lot
and demonstrated the successful work of the developed technique for detecting
and even tracking small objects, which can be parking infrastructure objects,
things left on the road, wheels, dropped boxes, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Vision-Language Models Evaluate Handwritten Math? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oikantik Nath, Hanani Bathina, Mohammed Safi Ur Rahman Khan, Mitesh M. Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Vision-Language Models (VLMs) have opened new
possibilities in automatic grading of handwritten student responses,
particularly in mathematics. However, a comprehensive study to test the ability
of VLMs to evaluate and reason over handwritten content remains absent. To
address this gap, we introduce FERMAT, a benchmark designed to assess the
ability of VLMs to detect, localize and correct errors in handwritten
mathematical content. FERMAT spans four key error dimensions - computational,
conceptual, notational, and presentation - and comprises over 2,200 handwritten
math solutions derived from 609 manually curated problems from grades 7-12 with
intentionally introduced perturbations. Using FERMAT we benchmark nine VLMs
across three tasks: error detection, localization, and correction. Our results
reveal significant shortcomings in current VLMs in reasoning over handwritten
text, with Gemini-1.5-Pro achieving the highest error correction rate (77%). We
also observed that some models struggle with processing handwritten content, as
their accuracy improves when handwritten inputs are replaced with printed text
or images. These findings highlight the limitations of current VLMs and reveal
new avenues for improvement. We release FERMAT and all the associated resources
in the open-source to drive further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSTA: Spatial-Temporal Causal Adaptive Learning for Exemplar-Free Video
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tieyuan Chen, Huabin Liu, Chern Hong Lim, John See, Xing Gao, Junhui Hou, Weiyao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning aims to acquire new knowledge while retaining past
information. Class-incremental learning (CIL) presents a challenging scenario
where classes are introduced sequentially. For video data, the task becomes
more complex than image data because it requires learning and preserving both
spatial appearance and temporal action involvement. To address this challenge,
we propose a novel exemplar-free framework that equips separate spatiotemporal
adapters to learn new class patterns, accommodating the incremental information
representation requirements unique to each class. While separate adapters are
proven to mitigate forgetting and fit unique requirements, naively applying
them hinders the intrinsic connection between spatial and temporal information
increments, affecting the efficiency of representing newly learned class
information. Motivated by this, we introduce two key innovations from a causal
perspective. First, a causal distillation module is devised to maintain the
relation between spatial-temporal knowledge for a more efficient
representation. Second, a causal compensation mechanism is proposed to reduce
the conflicts during increment and memorization between different types of
information. Extensive experiments conducted on benchmark datasets demonstrate
that our framework can achieve new state-of-the-art results, surpassing current
example-based methods by 4.2% in accuracy on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TCSVT Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MECD+: Unlocking Event-Level Causal <span class="highlight-title">Graph</span> Discovery for Video Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tieyuan Chen, Huabin Liu, Yi Wang, Yihang Chen, Tianyao He, Chaofan Gan, Huanyu He, Weiyao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video causal reasoning aims to achieve a high-level understanding of videos
from a causal perspective. However, it exhibits limitations in its scope,
primarily executed in a question-answering paradigm and focusing on brief video
segments containing isolated events and basic causal relations, lacking
comprehensive and structured causality analysis for videos with multiple
interconnected events. To fill this gap, we introduce a new task and dataset,
Multi-Event Causal Discovery (MECD). It aims to uncover the causal relations
between events distributed chronologically across long videos. Given visual
segments and textual descriptions of events, MECD identifies the causal
associations between these events to derive a comprehensive and structured
event-level video causal graph explaining why and how the result event
occurred. To address the challenges of MECD, we devise a novel framework
inspired by the Granger Causality method, incorporating an efficient mask-based
event prediction model to perform an Event Granger Test. It estimates causality
by comparing the predicted result event when premise events are masked versus
unmasked. Furthermore, we integrate causal inference techniques such as
front-door adjustment and counterfactual inference to mitigate challenges in
MECD like causality confounding and illusory causality. Additionally, context
chain reasoning is introduced to conduct more robust and generalized reasoning.
Experiments validate the effectiveness of our framework in reasoning complete
causal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,
respectively. Further experiments demonstrate that causal relation graphs can
also contribute to downstream video understanding tasks such as video question
answering and video event prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TPAMI Submission. arXiv admin note: substantial text overlap
  with arXiv:2409.17647</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Use of Contrastive Language-Image <span class="highlight-title">Pre-Train</span>ing for Human
  Posture Classification: Insights from Yoga Pose Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrzej D. Dobrzycki, Ana M. Bernardos, Luca Bergesio, Andrzej Pomirski, Daniel Sáez-Trigueros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate human posture classification in images and videos is crucial for
automated applications across various fields, including work safety, physical
rehabilitation, sports training, or daily assisted living. Recently, multimodal
learning methods, such as Contrastive Language-Image Pretraining (CLIP), have
advanced significantly in jointly understanding images and text. This study
aims to assess the effectiveness of CLIP in classifying human postures,
focusing on its application in yoga. Despite the initial limitations of the
zero-shot approach, applying transfer learning on 15,301 images (real and
synthetic) with 82 classes has shown promising results. The article describes
the full procedure for fine-tuning, including the choice for image description
syntax, models and hyperparameters adjustment. The fine-tuned CLIP model,
tested on 3826 images, achieves an accuracy of over 85%, surpassing the current
state-of-the-art of previous works on the same dataset by approximately 6%, its
training time being 3.5 times lower than what is needed to fine-tune a
YOLOv8-based model. For more application-oriented scenarios, with smaller
datasets of six postures each, containing 1301 and 401 training images, the
fine-tuned models attain an accuracy of 98.8% and 99.1%, respectively.
Furthermore, our experiments indicate that training with as few as 20 images
per pose can yield around 90% accuracy in a six-class dataset. This study
demonstrates that this multimodal technique can be effectively used for yoga
pose classification, and possibly for human posture classification, in general.
Additionally, CLIP inference time (around 7 ms) supports that the model can be
integrated into automated systems for posture evaluation, e.g., for developing
a real-time personal yoga assistant for performance assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimeLogic: A Temporal Logic Benchmark for Video QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirnam Swetha, Hilde Kuehne, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal logical understanding, a core facet of human cognition, plays a
pivotal role in capturing complex sequential events and their temporal
relationships within videos. This capability is particularly crucial in tasks
like Video Question Answering (VideoQA), where the goal is to process visual
data over time together with textual data to provide coherent answers. However,
current VideoQA benchmarks devote little focus to evaluating this critical
skill due to the challenge of annotating temporal logic. Despite the
advancement of vision-language models, assessing their temporal logical
reasoning powers remains a challenge, primarily due to the lack QA pairs that
demand formal, complex temporal reasoning. To bridge this gap, we introduce the
TimeLogic QA (TLQA) framework to automatically generate the QA pairs,
specifically designed to evaluate the temporal logical understanding. To this
end, TLQA leverages temporal annotations from existing video datasets together
with temporal operators derived from logic theory to construct questions that
test understanding of event sequences and their temporal relationships. TLQA
framework is generic and scalable, capable of leveraging both, existing video
action datasets with temporal action segmentation annotations, or video
datasets with temporal scene graph annotations, to automatically generate
temporal logical questions. We leverage 4 datasets, STAR, Breakfast, AGQA, and
CrossTask, and generate two VideoQA dataset variants - small (TLQA-S) and large
(TLQA-L) - containing 2k and 10k QA pairs for each category, resulting in 32k
and 160k total pairs per dataset. We undertake a comprehensive evaluation of
leading-edge VideoQA models, employing the TLQA to benchmark their temporal
logical understanding capabilities. We assess the VideoQA model's temporal
reasoning performance on 16 categories of temporal logic with varying temporal
complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-face emotion detection for effective Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Ala Yahyaoui, Mouaad Oujabour, Leila Ben Letaifa, Amine Bohi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of dialogue interfaces in mobile devices has become
ubiquitous, providing a wide array of services. As technology progresses,
humanoid robots designed with human-like features to interact effectively with
people are gaining prominence, and the use of advanced human-robot dialogue
interfaces is continually expanding. In this context, emotion recognition plays
a crucial role in enhancing human-robot interaction by enabling robots to
understand human intentions. This research proposes a facial emotion detection
interface integrated into a mobile humanoid robot, capable of displaying
real-time emotions from multiple individuals on a user interface. To this end,
various deep neural network models for facial expression recognition were
developed and evaluated under consistent computer-based conditions, yielding
promising results. Afterwards, a trade-off between accuracy and memory
footprint was carefully considered to effectively implement this application on
a mobile humanoid robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures and 1 table. Accepted at the 17th International
  Conference on Agents and Artificial Intelligence (ICAART 2025), Porto,
  Portugal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaceOracle: Chat with a Face Image Oracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wassim Kabbani, Kiran Raja, Raghavendra Ramachandra, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A face image is a mandatory part of ID and travel documents. Obtaining
high-quality face images when issuing such documents is crucial for both human
examiners and automated face recognition systems. In several international
standards, face image quality requirements are intricate and defined in detail.
Identifying and understanding non-compliance or defects in the submitted face
images is crucial for both issuing authorities and applicants. In this work, we
introduce FaceOracle, an LLM-powered AI assistant that helps its users analyze
a face image in a natural conversational manner using standard compliant
algorithms. Leveraging the power of LLMs, users can get explanations of various
face image quality concepts as well as interpret the outcome of face image
quality assessment (FIQA) algorithms. We implement a proof-of-concept that
demonstrates how experts at an issuing authority could integrate FaceOracle
into their workflow to analyze, understand, and communicate their decisions
more efficiently, resulting in enhanced productivity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lung Cancer detection using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Chaudhari, Ankush Singh, Sanchi Gajbhiye, Pratham Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we discuss lung cancer detection using hybrid model of
Convolutional-Neural-Networks (CNNs) and Support-Vector-Machines-(SVMs) in
order to gain early detection of tumors, benign or malignant. The work uses
this hybrid model by training upon the Computed Tomography scans (CT scans) as
dataset. Using deep learning for detecting lung cancer early is a cutting-edge
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VAGeo: View-specific Attention for Cross-View Object Geo-Localization <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyang Li, Xin Yuan, Wei Liu, Xin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-view object geo-localization (CVOGL) aims to locate an object of
interest in a captured ground- or drone-view image within the satellite image.
However, existing works treat ground-view and drone-view query images
equivalently, overlooking their inherent viewpoint discrepancies and the
spatial correlation between the query image and the satellite-view reference
image. To this end, this paper proposes a novel View-specific Attention
Geo-localization method (VAGeo) for accurate CVOGL. Specifically, VAGeo
contains two key modules: view-specific positional encoding (VSPE) module and
channel-spatial hybrid attention (CSHA) module. In object-level, according to
the characteristics of different viewpoints of ground and drone query images,
viewpoint-specific positional codings are designed to more accurately identify
the click-point object of the query image in the VSPE module. In feature-level,
a hybrid attention in the CSHA module is introduced by combining channel
attention and spatial attention mechanisms simultaneously for learning
discriminative features. Extensive experimental results demonstrate that the
proposed VAGeo gains a significant performance improvement, i.e., improving
acc@0.25/acc@0.5 on the CVOGL dataset from 45.43%/42.24% to 48.21%/45.22% for
ground-view, and from 61.97%/57.66% to 66.19%/61.87% for drone-view.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A4O: All Trigger for One sample 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc Anh Vu, Anh Tuan Tran, Cong Tran, Cuong Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks have become a critical threat to deep neural networks
(DNNs), drawing many research interests. However, most of the studied attacks
employ a single type of trigger. Consequently, proposed backdoor defenders
often rely on the assumption that triggers would appear in a unified way. In
this paper, we show that this naive assumption can create a loophole, allowing
more sophisticated backdoor attacks to bypass. We design a novel backdoor
attack mechanism that incorporates multiple types of backdoor triggers,
focusing on stealthiness and effectiveness. Our journey begins with the
intriguing observation that the performance of a backdoor attack in deep
learning models, as well as its detectability and removability, are all
proportional to the magnitude of the trigger. Based on this correlation, we
propose reducing the magnitude of each trigger type and combining them to
achieve a strong backdoor relying on the combined trigger while still staying
safely under the radar of defenders. Extensive experiments on three standard
datasets demonstrate that our method can achieve high attack success rates
(ASRs) while consistently bypassing state-of-the-art defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Guarantees on Automated Precision Weeding using Conformal
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Melki, Lionel Bombrun, Boubacar Diallo, Jérôme Dias, Jean-Pierre da Costa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision agriculture in general, and precision weeding in particular, have
greatly benefited from the major advancements in deep learning and computer
vision. A large variety of commercial robotic solutions are already available
and deployed. However, the adoption by farmers of such solutions is still low
for many reasons, an important one being the lack of trust in these systems.
This is in great part due to the opaqueness and complexity of deep neural
networks and the manufacturers' inability to provide valid guarantees on their
performance. Conformal prediction, a well-established methodology in the
machine learning community, is an efficient and reliable strategy for providing
trustworthy guarantees on the predictions of any black-box model under very
minimal constraints. Bridging the gap between the safe machine learning and
precision agriculture communities, this article showcases conformal prediction
in action on the task of precision weeding through deep learning-based image
classification. After a detailed presentation of the conformal prediction
methodology and the development of a precision spraying pipeline based on a
''conformalized'' neural network and well-defined spraying decision rules, the
article evaluates this pipeline on two real-world scenarios: one under
in-distribution conditions, the other reflecting a near out-of-distribution
setting. The results show that we are able to provide formal, i.e. certifiable,
guarantees on spraying at least 90% of the weeds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Radial Distortion in Face Images: Detection and Impact 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wassim Kabbani, Tristan Le Pessot, Kiran Raja, Raghavendra Ramachandra, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring face images of sufficiently high quality is important for online ID
and travel document issuance applications using face recognition systems (FRS).
Low-quality, manipulated (intentionally or unintentionally), or distorted
images degrade the FRS performance and facilitate documents' misuse. Securing
quality for enrolment images, especially in the unsupervised self-enrolment
scenario via a smartphone, becomes important to assure FRS performance. In this
work, we focus on the less studied area of radial distortion (a.k.a., the
fish-eye effect) in face images and its impact on FRS performance. We introduce
an effective radial distortion detection model that can detect and flag radial
distortion in the enrolment scenario. We formalize the detection model as a
face image quality assessment (FIQA) algorithm and provide a careful inspection
of the effect of radial distortion on FRS performance. Evaluation results show
excellent detection results for the proposed models, and the study on the
impact on FRS uncovers valuable insights into how to best use these models in
operational systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BIOMEDICA: An Open Biomedical Image-Caption Archive, <span class="highlight-title">Dataset</span>, and
  Vision-Language Models Derived from Scientific Literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey J Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Austin Wolfgang Katzer, Collin Chiu, Anita Rau, Xiaohan Wang, Yuhui Zhang, Alfred Seunghoon Song, Robert Tibshirani, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of vision-language models (VLMs) is driven by large-scale and
diverse multimodal datasets. However, progress toward generalist biomedical
VLMs is limited by the lack of annotated, publicly accessible datasets across
biology and medicine. Existing efforts are restricted to narrow domains,
missing the full diversity of biomedical knowledge encoded in scientific
literature. To address this gap, we introduce BIOMEDICA, a scalable,
open-source framework to extract, annotate, and serialize the entirety of the
PubMed Central Open Access subset into an easy-to-use, publicly accessible
dataset.Our framework produces a comprehensive archive with over 24 million
unique image-text pairs from over 6 million articles. Metadata and
expert-guided annotations are also provided. We demonstrate the utility and
accessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style
models continuously pre-trained on the BIOMEDICA dataset via streaming,
eliminating the need to download 27 TB of data locally.On average, our models
achieve state-of-the-art performance across 40 tasks - spanning pathology,
radiology, ophthalmology, dermatology, surgery, molecular biology,
parasitology, and cell biology - excelling in zero-shot classification with a
6.56% average improvement (as high as 29.8% and 17.5% in dermatology and
ophthalmology, respectively), and stronger image-text retrieval, all while
using 10x less compute. To foster reproducibility and collaboration, we release
our codebase and dataset for the broader research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Noise-Tolerant Network for Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike image classification and annotation, for which deep network models
have achieved dominating superior performances compared to traditional computer
vision algorithms, deep learning for automatic image segmentation still faces
critical challenges. One of such hurdles is to obtain ground-truth
segmentations as the training labels for deep network training. Especially when
we study biomedical images, such as histopathological images (histo-images), it
is unrealistic to ask for manual segmentation labels as the ground truth for
training due to the fine image resolution as well as the large image size and
complexity. In this paper, instead of relying on clean segmentation labels, we
study whether and how integrating imperfect or noisy segmentation results from
off-the-shelf segmentation algorithms may help achieve better segmentation
results through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend
the noisy label deep learning to image segmentation with two novel aspects: (1)
multiple noisy labels can be integrated into one deep learning model; (2) noisy
segmentation modeling, including probabilistic parameters, is adaptive,
depending on the given testing image appearance. Implementation of the new ANTN
model on both the synthetic data and real-world histo-images demonstrates its
effectiveness and superiority over off-the-shelf and other existing
deep-learning-based image segmentation algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eye Sclera for Fair Face Image Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wassim Kabbani, Kiran Raja, Raghavendra Ramachandra, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fair operational systems are crucial in gaining and maintaining society's
trust in face recognition systems (FRS). FRS start with capturing an image and
assessing its quality before using it further for enrollment or verification.
Fair Face Image Quality Assessment (FIQA) schemes therefore become equally
important in the context of fair FRS. This work examines the sclera as a
quality assessment region for obtaining a fair FIQA. The sclera region is
agnostic to demographic variations and skin colour for assessing the quality of
a face image. We analyze three skin tone related ISO/IEC face image quality
assessment measures and assess the sclera region as an alternative area for
assessing FIQ. Our analysis of the face dataset of individuals from different
demographic groups representing different skin tones indicates sclera as an
alternative to measure dynamic range, over- and under-exposure of face using
sclera region alone. The sclera region being agnostic to skin tone, i.e.,
demographic factors, provides equal utility as a fair FIQA as shown by our
Error-vs-Discard Characteristic (EDC) curve analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Single Object Tracking in LiDAR Point Clouds under Adverse
  Weather Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiantong Zhao, Xiuping Liu, Shengjing Tian, Yinan Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D single object tracking (3DSOT) in LiDAR point clouds is a critical task
for outdoor perception, enabling real-time perception of object location,
orientation, and motion. Despite the impressive performance of current 3DSOT
methods, evaluating them on clean datasets inadequately reflects their
comprehensive performance, as the adverse weather conditions in real-world
surroundings has not been considered. One of the main obstacles is the lack of
adverse weather benchmarks for the evaluation of 3DSOT. To this end, this work
proposes a challenging benchmark for LiDAR-based 3DSOT in adverse weather,
which comprises two synthetic datasets (KITTI-A and nuScenes-A) and one
real-world dataset (CADC-SOT) spanning three weather types: rain, fog, and
snow. Based on this benchmark, five representative 3D trackers from different
tracking frameworks conducted robustness evaluation, resulting in significant
performance degradations. This prompts the question: What are the factors that
cause current advanced methods to fail on such adverse weather samples?
Consequently, we explore the impacts of adverse weather and answer the above
question from three perspectives: 1) target distance; 2) template shape
corruption; and 3) target shape corruption. Finally, based on domain
randomization and contrastive learning, we designed a dual-branch tracking
framework for adverse weather, named DRCT, achieving excellent performance in
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSV-Mamba: A Multiscale Vision Mamba Network for Echocardio<span class="highlight-title">graph</span>y
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxian Yang, Qi Wang, Kaiqi Zhang, Ke Wei, Jun Lyu, Lingchao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging frequently encounters challenges, such as those related to
elevated noise levels, diminished spatiotemporal resolution, and the complexity
of anatomical structures. These factors significantly hinder the model's
ability to accurately capture and analyze structural relationships and dynamic
patterns across various regions of the heart. Mamba, an emerging model, is one
of the most cutting-edge approaches that is widely applied to diverse vision
and language tasks. To this end, this paper introduces a U-shaped deep learning
model incorporating a large-window Mamba scale (LMS) module and a hierarchical
feature fusion approach for echocardiographic segmentation. First, a cascaded
residual block serves as an encoder and is employed to incrementally extract
multiscale detailed features. Second, a large-window multiscale mamba module is
integrated into the decoder to capture global dependencies across regions and
enhance the segmentation capability for complex anatomical structures.
Furthermore, our model introduces auxiliary losses at each decoder layer and
employs a dual attention mechanism to fuse multilayer features both spatially
and across channels. This approach enhances segmentation performance and
accuracy in delineating complex anatomical structures. Finally, the
experimental results using the EchoNet-Dynamic and CAMUS datasets demonstrate
that the model outperforms other methods in terms of both accuracy and
robustness. For the segmentation of the left ventricular endocardium
(${LV}_{endo}$), the model achieved optimal values of 95.01 and 93.36,
respectively, while for the left ventricular epicardium (${LV}_{epi}$), values
of 87.35 and 87.80, respectively, were achieved. This represents an improvement
ranging between 0.54 and 1.11 compared with the best-performing model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duplex: Dual Prototype Learning for Compositional Zero-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhong Peng, Yishi Xu, Gerong Wang, Wenchao Chen, Bo Chen, Jing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional Zero-Shot Learning (CZSL) aims to enable models to recognize
novel compositions of visual states and objects that were absent during
training. Existing methods predominantly focus on learning semantic
representations of seen compositions but often fail to disentangle the
independent features of states and objects in images, thereby limiting their
ability to generalize to unseen compositions. To address this challenge, we
propose Duplex, a novel dual-prototype learning method that integrates semantic
and visual prototypes through a carefully designed dual-branch architecture,
enabling effective representation learning for compositional tasks. Duplex
utilizes a Graph Neural Network (GNN) to adaptively update visual prototypes,
capturing complex interactions between states and objects. Additionally, it
leverages the strong visual-semantic alignment of pre-trained Vision-Language
Models (VLMs) and employs a multi-path architecture combined with prompt
engineering to align image and text representations, ensuring robust
generalization. Extensive experiments on three benchmark datasets demonstrate
that Duplex outperforms state-of-the-art methods in both closed-world and
open-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matching Free Depth Recovery from Structured Light 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuohang Yu, Kai Wang, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach for depth estimation from images captured by
structured light systems. Unlike many previous methods that rely on image
matching process, our approach uses a density voxel grid to represent scene
geometry, which is trained via self-supervised differentiable volume rendering.
Our method leverages color fields derived from projected patterns in structured
light systems during the rendering process, enabling the isolated optimization
of the geometry field. This contributes to faster convergence and high-quality
output. Additionally, we incorporate normalized device coordinates (NDC), a
distortion loss, and a novel surface-based color loss to enhance geometric
fidelity. Experimental results demonstrate that our method outperforms existing
matching-based techniques in geometric performance for few-shot scenarios,
achieving approximately a 60% reduction in average estimated depth errors on
synthetic scenes and about 30% on real-world captured scenes. Furthermore, our
approach delivers fast training, with a speed roughly three times faster than
previous matching-free methods that employ implicit representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multimodal Fusion via Meta-Learning Towards Micro-Video
  <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Liu, Yinwei Wei, Fan Liu, Wenjie Wang, Liqiang Nie, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal information (e.g., visual, acoustic, and textual) has been widely
used to enhance representation learning for micro-video recommendation. For
integrating multimodal information into a joint representation of micro-video,
multimodal fusion plays a vital role in the existing micro-video recommendation
approaches. However, the static multimodal fusion used in previous studies is
insufficient to model the various relationships among multimodal information of
different micro-videos. In this paper, we develop a novel meta-learning-based
multimodal fusion framework called Meta Multimodal Fusion (MetaMMF), which
dynamically assigns parameters to the multimodal fusion function for each
micro-video during its representation learning. Specifically, MetaMMF regards
the multimodal fusion of each micro-video as an independent task. Based on the
meta information extracted from the multimodal features of the input task,
MetaMMF parameterizes a neural network as the item-specific fusion function via
a meta learner. We perform extensive experiments on three benchmark datasets,
demonstrating the significant improvements over several state-of-the-art
multimodal recommendation models, like MMGCN, LATTICE, and InvRL. Furthermore,
we lighten our model by adopting canonical polyadic decomposition to improve
the training efficiency, and validate its effectiveness through experimental
results. Codes are available at https://github.com/hanliu95/MetaMMF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ACM Transactions on Information
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Quest for Visual Understanding: A Journey Through the Evolution of
  Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anupam Pandey, Deepjyoti Bodo, Arpan Phukan, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is an interdisciplinary field that bridges
the gap between computer vision (CV) and natural language processing(NLP),
enabling Artificial Intelligence(AI) systems to answer questions about images.
Since its inception in 2015, VQA has rapidly evolved, driven by advances in
deep learning, attention mechanisms, and transformer-based models. This survey
traces the journey of VQA from its early days, through major breakthroughs,
such as attention mechanisms, compositional reasoning, and the rise of
vision-language pre-training methods. We highlight key models, datasets, and
techniques that shaped the development of VQA systems, emphasizing the pivotal
role of transformer architectures and multimodal pre-training in driving recent
progress. Additionally, we explore specialized applications of VQA in domains
like healthcare and discuss ongoing challenges, such as dataset bias, model
interpretability, and the need for common-sense reasoning. Lastly, we discuss
the emerging trends in large multimodal language models and the integration of
external knowledge, offering insights into the future directions of VQA. This
paper aims to provide a comprehensive overview of the evolution of VQA,
highlighting both its current state and potential advancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular
  Video Based on Rectified Mesh-embedded Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce RMAvatar, a novel human avatar representation with Gaussian
splatting embedded on mesh to learn clothed avatar from a monocular video. We
utilize the explicit mesh geometry to represent motion and shape of a virtual
human and implicit appearance rendering with Gaussian Splatting. Our method
consists of two main modules: Gaussian initialization module and Gaussian
rectification module. We embed Gaussians into triangular faces and control
their motion through the mesh, which ensures low-frequency motion and surface
deformation of the avatar. Due to the limitations of LBS formula, the human
skeleton is hard to control complex non-rigid transformations. We then design a
pose-related Gaussian rectification module to learn fine-detailed non-rigid
deformations, further improving the realism and expressiveness of the avatar.
We conduct extensive experiments on public datasets, RMAvatar shows
state-of-the-art performance on both rendering quality and quantitative
evaluations. Please see our project page at https://rm-avatar.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVM2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Scale-aware Adaptive Masked Knowledge Distillation for Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ZhouRui Zhang, Jun Li, JiaYan Li, ZhiJian Wu, JianHua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent feature masking knowledge distillation methods make use of attention
mechanisms to identify either important spatial regions or channel clues for
discriminative feature reconstruction. However, most of existing strategies
perform global attention-guided feature masking distillation without delving
into fine-grained visual clues in feature maps. In particular, uncovering
locality-aware clues across different scales are conducive to reconstructing
region-aware features, thereby significantly benefiting distillation
performance. In this study, we propose a fine-grained adaptive feature masking
distillation framework for accurate object detection. Different from previous
methods in which global masking is performed on single-scale feature maps, we
explore the scale-aware feature masking by performing feature distillation
across various scales, such that the object-aware locality is encoded for
improved feature reconstruction. In addition, our fine-grained feature
distillation strategy is combined with a masking logits distillation scheme in
which logits difference between teacher and student networks is utilized to
guide the distillation process. Thus, it can help the student model to better
learn from the teacher counterpart with improved knowledge transfer. Extensive
experiments for detection task demonstrate the superiority of our method. For
example, when RetinaNet, RepPoints and Cascade Mask RCNN are used as teacher
detectors, the student network achieves mAP scores of 41.5\%, 42.9\%, and
42.6\%, respectively, outperforming state-of-the-art methods such as DMKD and
FreeKD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Learning for 3D Hand-Object Reconstruction and
  Compositional Action Recognition from Egocentric RGB Videos Using
  Superquadrics <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tze Ho Elden Tse, Runyang Feng, Linfang Zheng, Jiho Park, Yixing Gao, Jihie Kim, Ales Leonardis, Hyung Jin Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the availability of egocentric 3D hand-object interaction datasets,
there is increasing interest in developing unified models for hand-object pose
estimation and action recognition. However, existing methods still struggle to
recognise seen actions on unseen objects due to the limitations in representing
object shape and movement using 3D bounding boxes. Additionally, the reliance
on object templates at test time limits their generalisability to unseen
objects. To address these challenges, we propose to leverage superquadrics as
an alternative 3D object representation to bounding boxes and demonstrate their
effectiveness on both template-free object reconstruction and action
recognition tasks. Moreover, as we find that pure appearance-based methods can
outperform the unified methods, the potential benefits from 3D geometric
information remain unclear. Therefore, we study the compositionality of actions
by considering a more challenging task where the training combinations of verbs
and nouns do not overlap with the testing split. We extend H2O and FPHA
datasets with compositional splits and design a novel collaborative learning
framework that can explicitly reason about the geometric relations between
hands and the manipulated object. Through extensive quantitative and
qualitative evaluations, we demonstrate significant improvements over the
state-of-the-arts in (compositional) action recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Training of Neural Networks to Achieve Bayes Optimal
  Classification Accuracy <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Tavasoli Naeini, Ali Bereyhi, Morteza Noshad, Ben Liang, Alfred O. Hero III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work invokes the notion of $f$-divergence to introduce a novel upper
bound on the Bayes error rate of a general classification task. We show that
the proposed bound can be computed by sampling from the output of a
parameterized model. Using this practical interpretation, we introduce the
Bayes optimal learning threshold (BOLT) loss whose minimization enforces a
classification model to achieve the Bayes error rate. We validate the proposed
loss for image and text classification tasks, considering MNIST, Fashion-MNIST,
CIFAR-10, and IMDb datasets. Numerical experiments demonstrate that models
trained with BOLT achieve performance on par with or exceeding that of
cross-entropy, particularly on challenging datasets. This highlights the
potential of BOLT in improving generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Sclera Segmentation through Semi-supervised Learning with Fewer
  Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanjun Wang, Lu Wang, Ning Niu, Qiaoyi Yao, Yixuan Wang, Sufen Ren, Shengchao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sclera segmentation is crucial for developing automatic eye-related medical
computer-aided diagnostic systems, as well as for personal identification and
verification, because the sclera contains distinct personal features. Deep
learning-based sclera segmentation has achieved significant success compared to
traditional methods that rely on hand-crafted features, primarily because it
can autonomously extract critical output-related features without the need to
consider potential physical constraints. However, achieving accurate sclera
segmentation using these methods is challenging due to the scarcity of
high-quality, fully labeled datasets, which depend on costly, labor-intensive
medical acquisition and expertise. To address this challenge, this paper
introduces a novel sclera segmentation framework that excels with limited
labeled samples. Specifically, we employ a semi-supervised learning method that
integrates domain-specific improvements and image-based spatial transformations
to enhance segmentation performance. Additionally, we have developed a
real-world eye diagnosis dataset to enrich the evaluation process. Extensive
experiments on our dataset and two additional public datasets demonstrate the
effectiveness and superiority of our proposed method, especially with
significantly fewer labeled samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, 19 pages, 9 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Heterogeneous Multimodal <span class="highlight-title">Graph</span> Learning Framework for Recognizing User
  Emotions in Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sree Bhattacharyya, Shuhua Yang, James Z. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of social media platforms has provided unprecedented
access to massive amounts of multimodal user-generated content. Comprehending
user emotions can provide valuable insights for improving communication and
understanding of human behaviors. Despite significant advancements in Affective
Computing, the diverse factors influencing user emotions in social networks
remain relatively understudied. Moreover, there is a notable lack of deep
learning-based methods for predicting user emotions in social networks, which
could be addressed by leveraging the extensive multimodal data available. This
work presents a novel formulation of personalized emotion prediction in social
networks based on heterogeneous graph learning. Building upon this formulation,
we design HMG-Emo, a Heterogeneous Multimodal Graph Learning Framework that
utilizes deep learning-based features for user emotion recognition.
Additionally, we include a dynamic context fusion module in HMG-Emo that is
capable of adaptively integrating the different modalities in social media
data. Through extensive experiments, we demonstrate the effectiveness of
HMG-Emo and verify the superiority of adopting a graph neural network-based
approach, which outperforms existing baselines that use rich hand-crafted
features. To the best of our knowledge, HMG-Emo is the first multimodal and
deep-learning-based approach to predict personalized emotions within online
social networks. Our work highlights the significance of exploiting advanced
deep learning techniques for less-explored problems in Affective Computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixing the Scale and Shift in Monocular Depth For Camera Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaqing Ding, Václav Vávra, Viktor Kocur, Jian Yang, Torsten Sattler, Zuzana Kukelova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in monocular depth prediction have led to significantly
improved depth prediction accuracy. In turn, this enables various applications
to use such depth predictions. In this paper, we propose a novel framework for
estimating the relative pose between two cameras from point correspondences
with associated monocular depths. Since depth predictions are typically defined
up to an unknown scale and shift parameter, our solvers jointly estimate both
scale and shift parameters together with the camera pose. We derive efficient
solvers for three cases: (1) two calibrated cameras, (2) two uncalibrated
cameras with an unknown but shared focal length, and (3) two uncalibrated
cameras with unknown and different focal lengths. Experiments on synthetic and
real data, including experiments with depth maps estimated by 11 different
depth predictors, show the practical viability of our solvers. Compared to
prior work, our solvers achieve state-of-the-art results on two large-scale,
real-world datasets. The source code is available at
https://github.com/yaqding/pose_monodepth
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Democratizing Text-to-Image Masked Generative Models with Compact
  Text-Aware One-Dimensional Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, Liang-Chieh Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image tokenizers form the foundation of modern text-to-image generative
models but are notoriously difficult to train. Furthermore, most existing
text-to-image models rely on large-scale, high-quality private datasets, making
them challenging to replicate. In this work, we introduce Text-Aware
Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful
image tokenizer that can utilize either discrete or continuous 1-dimensional
tokens. TA-TiTok uniquely integrates textual information during the tokenizer
decoding stage (i.e., de-tokenization), accelerating convergence and enhancing
performance. TA-TiTok also benefits from a simplified, yet effective, one-stage
training process, eliminating the need for the complex two-stage distillation
used in previous 1-dimensional tokenizers. This design allows for seamless
scalability to large datasets. Building on this, we introduce a family of
text-to-image Masked Generative Models (MaskGen), trained exclusively on open
data while achieving comparable performance to models trained on private data.
We aim to release both the efficient, strong TA-TiTok tokenizers and the
open-data, open-weight MaskGen models to promote broader access and democratize
the field of text-to-image masked generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://tacju.github.io/projects/maskgen.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing Human-Hand Segmentation on In-Distribution and
  Out-of-Distribution Data in Human-Robot Interactions Using a Deep Ensemble
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Jalayer, Yuxin Chen, Masoud Jalayer, Carlotta Orsenigo, Masayoshi Tomizuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable detection and segmentation of human hands are critical for enhancing
safety and facilitating advanced interactions in human-robot collaboration.
Current research predominantly evaluates hand segmentation under
in-distribution (ID) data, which reflects the training data of deep learning
(DL) models. However, this approach fails to address out-of-distribution (OOD)
scenarios that often arise in real-world human-robot interactions. In this
study, we present a novel approach by evaluating the performance of pre-trained
DL models under both ID data and more challenging OOD scenarios. To mimic
realistic industrial scenarios, we designed a diverse dataset featuring simple
and cluttered backgrounds with industrial tools, varying numbers of hands (0 to
4), and hands with and without gloves. For OOD scenarios, we incorporated
unique and rare conditions such as finger-crossing gestures and motion blur
from fast-moving hands, addressing both epistemic and aleatoric uncertainties.
To ensure multiple point of views (PoVs), we utilized both egocentric cameras,
mounted on the operator's head, and static cameras to capture RGB images of
human-robot interactions. This approach allowed us to account for multiple
camera perspectives while also evaluating the performance of models trained on
existing egocentric datasets as well as static-camera datasets. For
segmentation, we used a deep ensemble model composed of UNet and RefineNet as
base learners. Performance evaluation was conducted using segmentation metrics
and uncertainty quantification via predictive entropy. Results revealed that
models trained on industrial datasets outperformed those trained on
non-industrial datasets, highlighting the importance of context-specific
training. Although all models struggled with OOD scenarios, those trained on
industrial datasets demonstrated significantly better generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pedestrian Trajectory Prediction Based on Social Interactions Learning
  With Random Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajia Xie, Sheng Zhang, Beihao Xia, Zhu Xiao, Hongbo Jiang, Siwang Zhou, Zheng Qin, Hongyang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrian trajectory prediction is a critical technology in the evolution of
self-driving cars toward complete artificial intelligence. Over recent years,
focusing on the trajectories of pedestrians to model their social interactions
has surged with great interest in more accurate trajectory predictions.
However, existing methods for modeling pedestrian social interactions rely on
pre-defined rules, struggling to capture non-explicit social interactions. In
this work, we propose a novel framework named DTGAN, which extends the
application of Generative Adversarial Networks (GANs) to graph sequence data,
with the primary objective of automatically capturing implicit social
interactions and achieving precise predictions of pedestrian trajectory. DTGAN
innovatively incorporates random weights within each graph to eliminate the
need for pre-defined interaction rules. We further enhance the performance of
DTGAN by exploring diverse task loss functions during adversarial training,
which yields improvements of 16.7\% and 39.3\% on metrics ADE and FDE,
respectively. The effectiveness and accuracy of our framework are verified on
two public datasets. The experimental results show that our proposed DTGAN
achieves superior performance and is well able to understand pedestrians'
intentions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,7 figures,Accepted to IEEE Transactions on Multimedia (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C2PD: Continuity-Constrained Pixelwise Deformation for Guided Depth
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Kang, Qing Cai, Runqing Tan, Yimei Liu, Zhi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guided depth super-resolution (GDSR) has demonstrated impressive performance
across a wide range of domains, with numerous methods being proposed. However,
existing methods often treat depth maps as images, where shading values are
computed discretely, making them struggle to effectively restore the continuity
inherent in the depth map. In this paper, we propose a novel approach that
maximizes the utilization of spatial characteristics in depth, coupled with
human abstract perception of real-world substance, by transforming the GDSR
issue into deformation of a roughcast with ideal plasticity, which can be
deformed by force like a continuous object. Specifically, we firstly designed a
cross-modal operation, Continuity-constrained Asymmetrical Pixelwise Operation
(CAPO), which can mimic the process of deforming an isovolumetrically flexible
object through external forces. Utilizing CAPO as the fundamental component, we
develop the Pixelwise Cross Gradient Deformation (PCGD), which is capable of
emulating operations on ideal plastic objects (without volume constraint).
Notably, our approach demonstrates state-of-the-art performance across four
widely adopted benchmarks for GDSR, with significant advantages in large-scale
tasks and generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> Distillation as Pushforward Optimal Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Ye Tan, Emma Slade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation aims to find a synthetic training set such that training
on the synthetic data achieves similar performance to training on real data,
with orders of magnitude less computational requirements. Existing methods can
be broadly categorized as either bi-level optimization problems that have
neural network training heuristics as the lower level problem, or disentangled
methods that bypass the bi-level optimization by matching distributions of
data. The latter method has the major advantages of speed and scalability in
terms of size of both training and distilled datasets. We demonstrate that when
equipped with an encoder-decoder structure, the empirically successful
disentangled methods can be reformulated as an optimal quantization problem,
where a finite set of points is found to approximate the underlying probability
measure by minimizing the expected projection distance. In particular, we link
existing disentangled dataset distillation methods to the classical optimal
quantization and Wasserstein barycenter problems, demonstrating consistency of
distilled datasets for diffusion-based generative priors. We propose a simple
extension of the state-of-the-art data distillation method D4M, achieving
better performance on the ImageNet-1K dataset with trivial additional
computation, and state-of-the-art performance in higher image-per-class
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixi Feng, Chao Liu, Sifei Liu, William Yang Wang, Arash Vahdat, Weili Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video generation models struggle to follow complex text prompts and
synthesize multiple objects, raising the need for additional grounding input
for improved controllability. In this work, we propose to decompose videos into
visual primitives - blob video representation, a general representation for
controllable video generation. Based on blob conditions, we develop a
blob-grounded video diffusion model named BlobGEN-Vid that allows users to
control object motions and fine-grained object appearance. In particular, we
introduce a masked 3D attention module that effectively improves regional
consistency across frames. In addition, we introduce a learnable module to
interpolate text embeddings so that users can control semantics in specific
frames and obtain smooth object transitions. We show that our framework is
model-agnostic and build BlobGEN-Vid based on both U-Net and DiT-based video
diffusion models. Extensive experimental results show that BlobGEN-Vid achieves
superior zero-shot video generation ability and state-of-the-art layout
controllability on multiple benchmarks. When combined with an LLM for layout
planning, our framework even outperforms proprietary text-to-video generators
in terms of compositional accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://blobgen-vid2.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Sound of Water: Inferring Physical Properties from Pouring Liquids <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush Bagad, Makarand Tapaswi, Cees G. M. Snoek, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the connection between audio-visual observations and the underlying
physics of a mundane yet intriguing everyday activity: pouring liquids. Given
only the sound of liquid pouring into a container, our objective is to
automatically infer physical properties such as the liquid level, the shape and
size of the container, the pouring rate and the time to fill. To this end, we:
(i) show in theory that these properties can be determined from the fundamental
frequency (pitch); (ii) train a pitch detection model with supervision from
simulated data and visual data with a physics-inspired objective; (iii)
introduce a new large dataset of real pouring videos for a systematic study;
(iv) show that the trained model can indeed infer these physical properties for
real data; and finally, (v) we demonstrate strong generalization to various
container shapes, other datasets, and in-the-wild YouTube videos. Our work
presents a keen understanding of a narrow yet rich problem at the intersection
of acoustics, physics, and learning. It opens up applications to enhance
multisensory perception in robotic pouring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://bpiyush.github.io/pouring-water-website.
  Short version accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robot Synesthesia: A Sound and Emotion Guided AI Painter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04850v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04850v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vihaan Misra, Peter Schaldenbrand, Jean Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  If a picture paints a thousand words, sound may voice a million. While recent
robotic painting and image synthesis methods have achieved progress in
generating visuals from text inputs, the translation of sound into images is
vastly unexplored. Generally, sound-based interfaces and sonic interactions
have the potential to expand accessibility and control for the user and provide
a means to convey complex emotions and the dynamic aspects of the real world.
In this paper, we propose an approach for using sound and speech to guide a
robotic painting process, known here as robot synesthesia. For general sound,
we encode the simulated paintings and input sounds into the same latent space.
For speech, we decouple speech into its transcribed text and the tone of the
speech. Whereas we use the text to control the content, we estimate the
emotions from the tone to guide the mood of the painting. Our approach has been
fully integrated with FRIDA, a robotic painting framework, adding sound and
speech to FRIDA's existing input modalities, such as text and style. In two
surveys, participants were able to correctly guess the emotion or natural sound
used to generate a given painting more than twice as likely as random chance.
On our sound-guided image manipulation and music-guided paintings, we discuss
the results qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quilt-1M: One Million Image-Text Pairs for Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11207v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11207v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, Linda Shapiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent accelerations in multi-modal applications have been made possible with
the plethora of image and text data available online. However, the scarcity of
analogous data in the medical field, specifically in histopathology, has slowed
comparable progress. To enable similar representation learning for
histopathology, we turn to YouTube, an untapped resource of videos, offering
$1,087$ hours of valuable educational histopathology videos from expert
clinicians. From YouTube, we curate QUILT: a large-scale vision-language
dataset consisting of $802, 144$ image and text pairs. QUILT was automatically
curated using a mixture of models, including large language models, handcrafted
algorithms, human knowledge databases, and automatic speech recognition. In
comparison, the most comprehensive datasets curated for histopathology amass
only around $200$K samples. We combine QUILT with datasets from other sources,
including Twitter, research papers, and the internet in general, to create an
even larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it
as the largest vision-language histopathology dataset to date. We demonstrate
the value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model
outperforms state-of-the-art models on both zero-shot and linear probing tasks
for classifying new histopathology images across $13$ diverse patch-level
datasets of $8$ different sub-pathologies and cross-modal retrieval tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhance Eye Disease Detection using Learnable Probabilistic Discrete
  Latents in Machine Learning Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16865v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16865v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Prabhakaran, YeKun Xiao, Ching-Yu Cheng, Dianbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ocular diseases, including diabetic retinopathy and glaucoma, present a
significant public health challenge due to their high prevalence and potential
for causing vision impairment. Early and accurate diagnosis is crucial for
effective treatment and management. In recent years, deep learning models have
emerged as powerful tools for analysing medical images, such as retina imaging.
However, challenges persist in model relibability and uncertainty estimation,
which are critical for clinical decision-making. This study leverages the
probabilistic framework of Generative Flow Networks (GFlowNets) to learn the
posterior distribution over latent discrete dropout masks for the
classification and analysis of ocular diseases using fundus images. We develop
a robust and generalizable method that utilizes GFlowOut integrated with
ResNet18 and ViT models as the backbone in identifying various ocular
conditions. This study employs a unique set of dropout masks - none, random,
bottomup, and topdown - to enhance model performance in analyzing these fundus
images. Our results demonstrate that our learnable probablistic latents
significantly improves accuracy, outperforming the traditional dropout
approach. We utilize a gradient map calculation method, Grad-CAM, to assess
model explainability, observing that the model accurately focuses on critical
image regions for predictions. The integration of GFlowOut in neural networks
presents a promising advancement in the automated diagnosis of ocular diseases,
with implications for improving clinical workflows and patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGB-D Indiscernible Object Counting in Underwater Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guolei Sun, Xiaogang Cheng, Zhaochong An, Xiaokang Wang, Yun Liu, Deng-Ping Fan, Ming-Ming Cheng, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, indiscernible/camouflaged scene understanding has attracted lots of
research attention in the vision community. We further advance the frontier of
this field by systematically studying a new challenge named indiscernible
object counting (IOC), the goal of which is to count objects that are blended
with respect to their surroundings. Due to a lack of appropriate IOC datasets,
we present a large-scale dataset IOCfish5K which contains a total of 5,637
high-resolution images and 659,024 annotated center points. Our dataset
consists of a large number of indiscernible objects (mainly fish) in underwater
scenes, making the annotation process all the more challenging. IOCfish5K is
superior to existing datasets with indiscernible scenes because of its larger
scale, higher image resolutions, more annotations, and denser scenes. All these
aspects make it the most challenging dataset for IOC so far, supporting
progress in this area. Benefiting from the recent advancements of depth
estimation foundation models, we construct high-quality depth maps for
IOCfish5K by generating pseudo labels using the Depth Anything V2 model. The
RGB-D version of IOCfish5K is named IOCfish5K-D. For benchmarking purposes on
IOCfish5K, we select 14 mainstream methods for object counting and carefully
evaluate them. For multimodal IOCfish5K-D, we evaluate other 4 popular
multimodal counting methods. Furthermore, we propose IOCFormer, a new strong
baseline that combines density and regression branches in a unified framework
and can effectively tackle object counting under concealed scenes. We also
propose IOCFormer-D to enable the effective usage of depth modality in helping
detect and count objects hidden in their environments. Experiments show that
IOCFormer and IOCFormer-D achieve state-of-the-art scores on IOCfish5K and
IOCfish5K-D, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal version. The resources are available at
  https://github.com/GuoleiSun/Indiscernible-Object-Counting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMAR-Net: Accurate Cross-Modal 3D SAR Reconstruction of Vehicle Targets
  with Sparse Multi-Baseline Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04158v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04158v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da Li, Guoqiang Zhao, Houjun Sun, Jiacheng Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-baseline Synthetic Aperture Radar (SAR) three-dimensional (3D)
tomography is a crucial remote sensing technique that provides 3D resolution
unavailable in conventional SAR imaging. However, achieving high-quality
imaging typically requires multi-angle or full-aperture data, resulting in
significant imaging costs. Recent advancements in sparse 3D SAR, which rely on
data from limited apertures, have gained attention as a cost-effective
alternative. Notably, deep learning techniques have markedly enhanced the
imaging quality of sparse 3D SAR. Despite these advancements, existing methods
primarily depend on high-resolution radar images for supervising the training
of deep neural networks (DNNs). This exclusive dependence on single-modal data
prevents the introduction of complementary information from other data sources,
limiting further improvements in imaging performance. In this paper, we
introduce a Cross-Modal 3D-SAR Reconstruction Network (CMAR-Net) to enhance 3D
SAR imaging by integrating heterogeneous information. Leveraging cross-modal
supervision from 2D optical images and error transfer guaranteed by
differentiable rendering, CMAR-Net achieves efficient training and reconstructs
highly sparse multi-baseline SAR data into visually structured and accurate 3D
images, particularly for vehicle targets. Extensive experiments on simulated
and real-world datasets demonstrate that CMAR-Net significantly outperforms
SOTA sparse reconstruction algorithms based on compressed sensing (CS) and deep
learning (DL). Furthermore, our method eliminates the need for time-consuming
full-aperture data preprocessing and relies solely on computer-rendered optical
images, significantly reducing dataset construction costs. This work highlights
the potential of deep learning for multi-baseline SAR 3D imaging and introduces
a novel framework for radar imaging research through cross-modal learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in
reconstructing detailed 3D scenes within multi-view setups and the emergence of
large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based
method utilizing a human face foundation model as guidance with just a single
image as input. To achieve that, we extend such a model for diverse-view human
head generation by fine-tuning on synthetic data and modifying its
conditioning. Our avatars maintain a dense correspondence with a human face
mesh template, allowing blendshape-based expression generation. This is
achieved through a modified 3DGS approach, connectivity regularizers, and a
strategic initialization tailored for our task. Additionally, we propose an
optional efficient SDS-based correction step to refine the blendshape
expressions, enhancing realism and diversity. Experiments demonstrate that
Arc2Avatar achieves state-of-the-art realism and identity preservation,
effectively addressing color issues by allowing the use of very low guidance,
enabled by our strong identity prior and initialization strategy, without
compromising detail. Please visit https://arc2avatar.github.io for more
resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page https://arc2avatar.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Pérez-García, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Matthew P. Lungren, Maria Wetscherek, Noel Codella, Stephanie L. Hyland, Javier Alvarez-Valle, Ozan Oktay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-supervised pre-training has proven to be a valuable method for
extracting semantically meaningful features from images, serving as a
foundational element in multimodal systems within the computer vision and
medical imaging domains. However, the computed features are limited by the
information contained in the text, which is particularly problematic in medical
imaging, where the findings described by radiologists focus on specific
observations. This challenge is compounded by the scarcity of paired
imaging-text data due to concerns over leakage of personal health information.
In this work, we fundamentally challenge the prevailing reliance on language
supervision for learning general-purpose biomedical imaging encoders. We
introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal
biomedical imaging data that obtains similar or greater performance than
state-of-the-art biomedical language-supervised models on a diverse range of
benchmarks. Specifically, the quality of learned representations is evaluated
on standard imaging tasks (classification and semantic segmentation), and a
vision-language alignment task (text report generation from images). To further
demonstrate the drawback of language supervision, we show that features from
RAD-DINO correlate with other medical records (e.g., sex or age) better than
language-supervised models, which are generally not mentioned in radiology
reports. Finally, we conduct a series of ablations determining the factors in
RAD-DINO's performance; notably, we observe that RAD-DINO's downstream
performance scales well with the quantity and diversity of training data,
demonstrating that image-only supervision is a scalable approach for training a
foundational biomedical image encoder. Model weights of RAD-DINO trained on
publicly available datasets are available at
https://huggingface.co/microsoft/rad-dino.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agentic Copyright Watermarking against Adversarial Evidence Forgery with
  Purification-Agnostic Curriculum Proxy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erjin Bao, Ching-Chun Chang, Hanrui Wang, Isao Echizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proliferation of AI agents in various domains, protecting the
ownership of AI models has become crucial due to the significant investment in
their development. Unauthorized use and illegal distribution of these models
pose serious threats to intellectual property, necessitating effective
copyright protection measures. Model watermarking has emerged as a key
technique to address this issue, embedding ownership information within models
to assert rightful ownership during copyright disputes. This paper presents
several contributions to model watermarking: a self-authenticating black-box
watermarking protocol using hash techniques, a study on evidence forgery
attacks using adversarial perturbations, a proposed defense involving a
purification step to counter adversarial attacks, and a purification-agnostic
curriculum proxy learning method to enhance watermark robustness and model
performance. Experimental results demonstrate the effectiveness of these
approaches in improving the security, reliability, and performance of
watermarked models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Decoders for Transformer-based Semantic Segmentation: A
  Compression Perspective <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qishuai Wen, Chun-Guang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for Transformer-based semantic segmentation
typically adopt Transformer decoders that are used to extract additional
embeddings from image embeddings via cross-attention, refine either or both
types of embeddings via self-attention, and project image embeddings onto the
additional embeddings via dot-product. Despite their remarkable success, these
empirical designs still lack theoretical justifications or interpretations,
thus hindering potentially principled improvements. In this paper, we argue
that there are fundamental connections between semantic segmentation and
compression, especially between the Transformer decoders and Principal
Component Analysis (PCA). From such a perspective, we derive a white-box, fully
attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the
interpretations as follows: 1) the self-attention operator refines image
embeddings to construct an ideal principal subspace that aligns with the
supervision and retains most information; 2) the cross-attention operator seeks
to find a low-rank approximation of the refined image embeddings, which is
expected to be a set of orthonormal bases of the principal subspace and
corresponds to the predefined classes; 3) the dot-product operation yields
compact representation for image embeddings as segmentation masks. Experiments
conducted on dataset ADE20K find that DEPICT consistently outperforms its
black-box counterpart, Segmenter, and it is light weight and more robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScVLM: Enhancing Vision-Language Model for Safety-Critical Event
  Understanding <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Shi, Boyu Jiang, Tong Zeng, Feng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately identifying, understanding and describing traffic safety-critical
events (SCEs), including crashes, tire strikes, and near-crashes, is crucial
for advanced driver assistance systems, automated driving systems, and traffic
safety. As SCEs are rare events, most general vision-language models (VLMs)
have not been trained sufficiently to link SCE videos and narratives, which
could lead to hallucinations and missing key safety characteristics. Here, we
introduce ScVLM, a novel hybrid methodology that integrates supervised and
contrastive learning techniques to classify the severity and types of SCEs, as
well as to generate narrative descriptions of SCEs. This approach utilizes
classification to enhance VLMs' comprehension of driving videos and improve the
rationality of event descriptions. The proposed approach is trained on and
evaluated by more than 8,600 SCEs from the Second Strategic Highway Research
Program Naturalistic Driving Study dataset, the largest publicly accessible
driving dataset with videos and SCE annotations. The results demonstrate the
superiority of the proposed approach in generating contextually accurate event
descriptions and mitigating VLM hallucinations. The code will be available at
https://github.com/datadrivenwheels/ScVLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automation of Quantum Dot Measurement Analysis via Explainable Machine
  Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13699v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13699v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of quantum dot (QD) devices for quantum computing has
necessitated more efficient and automated methods for device characterization
and tuning. This work demonstrates the feasibility and advantages of applying
explainable machine learning techniques to the analysis of quantum dot
measurements, paving the way for further advances in automated and transparent
QD device tuning. Many of the measurements acquired during the tuning process
come in the form of images that need to be properly analyzed to guide the
subsequent tuning steps. By design, features present in such images capture
certain behaviors or states of the measured QD devices. When considered
carefully, such features can aid the control and calibration of QD devices. An
important example of such images are so-called $\textit{triangle plots}$, which
visually represent current flow and reveal characteristics important for QD
device calibration. While image-based classification tools, such as
convolutional neural networks (CNNs), can be used to verify whether a given
measurement is $\textit{good}$ and thus warrants the initiation of the next
phase of tuning, they do not provide any insights into how the device should be
adjusted in the case of $\textit{bad}$ images. This is because CNNs sacrifice
prediction and model intelligibility for high accuracy. To ameliorate this
trade-off, a recent study introduced an image vectorization approach that
relies on the Gabor wavelet transform (Schug $\textit{et al.}$ 2024
$\textit{Proc. XAI4Sci: Explainable Machine Learning for Sciences Workshop
(AAAI 2024) (Vancouver, Canada)}$ pp 1-6). Here we propose an alternative
vectorization method that involves mathematical modeling of synthetic triangles
to mimic the experimental data. Using explainable boosting machines, we show
that this new method offers superior explainability of model prediction without
sacrificing accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, abbreviated version published in Proceedings of
  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,
  (Vancouver, Canada)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Class Distance Weighted Cross Entropy Loss for Classification of Disease
  Severity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01246v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01246v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gorkem Polat, Ümit Mert Çağlar, Alptekin Temizel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing disease severity with ordinal classes, where each class reflects
increasing severity levels, benefits from loss functions designed for this
ordinal structure. Traditional categorical loss functions, like Cross-Entropy
(CE), often perform suboptimally in these scenarios. To address this, we
propose a novel loss function, Class Distance Weighted Cross-Entropy (CDW-CE),
which penalizes misclassifications more severely when the predicted and actual
classes are farther apart. We evaluated CDW-CE using various deep
architectures, comparing its performance against several categorical and
ordinal loss functions. To assess the quality of latent representations, we
used t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold
approximation and projection (UMAP) visualizations, quantified the clustering
quality using the Silhouette Score, and compared Class Activation Maps (CAM)
generated by models trained with CDW-CE and CE loss. Feedback from domain
experts was incorporated to evaluate how well model attention aligns with
expert opinion. Our results show that CDW-CE consistently improves performance
in ordinal image classification tasks. It achieves higher Silhouette Scores,
indicating better class discrimination capability, and its CAM visualizations
show a stronger focus on clinically significant regions, as validated by domain
experts. Receiver operator characteristics (ROC) curves and the area under the
curve (AUC) scores highlight that CDW-CE outperforms other loss functions,
including prominent ordinal loss functions from the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FusionSORT: Fusion Methods for Online Multi-object Visual Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathanael L. Baisa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate four different fusion methods for associating
detections to tracklets in multi-object visual tracking. In addition to
considering strong cues such as motion and appearance information, we also
consider weak cues such as height intersection-over-union (height-IoU) and
tracklet confidence information in the data association using different fusion
methods. These fusion methods include minimum, weighted sum based on IoU,
Kalman filter (KF) gating, and hadamard product of costs due to the different
cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and
DanceTrack datasets, and find out that the choice of a fusion method is key for
data association in multi-object visual tracking. We hope that this
investigative work helps the computer vision research community to use the
right fusion method for data association in multi-object visual tracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Light Transport-aware Diffusion Posterior Sampling for Single-View
  Reconstruction of 3D Volumes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludwic Leonard, Nils Thuerey, Ruediger Westermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a single-view reconstruction technique of volumetric fields in
which multiple light scattering effects are omnipresent, such as in clouds. We
model the unknown distribution of volumetric fields using an unconditional
diffusion model trained on a novel benchmark dataset comprising 1,000
synthetically simulated volumetric density fields. The neural diffusion model
is trained on the latent codes of a novel, diffusion-friendly, monoplanar
representation. The generative model is used to incorporate a tailored
parametric diffusion posterior sampling technique into different reconstruction
tasks. A physically-based differentiable volume renderer is employed to provide
gradients with respect to light transport in the latent space. This stands in
contrast to classic NeRF approaches and makes the reconstructions better
aligned with observed data. Through various experiments, we demonstrate
single-view reconstruction of volumetric clouds at a previously unattainable
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08926v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08926v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Virmarie Maquiling, Sean Anthony Byrne, Diederick C. Niehorster, Marco Carminati, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the transformative potential of SAM 2, a vision foundation model,
in advancing gaze estimation and eye tracking technologies. By significantly
reducing annotation time, lowering technical barriers through its ease of
deployment, and enhancing segmentation accuracy, SAM 2 addresses critical
challenges faced by researchers and practitioners. Utilizing its zero-shot
segmentation capabilities with minimal user input-a single click per video-we
tested SAM 2 on over 14 million eye images from diverse datasets, including
virtual reality setups and the world's largest unified dataset recorded using
wearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches
the performance of domain-specific models trained solely on eye images,
achieving competitive mean Intersection over Union (mIoU) scores of up to 93%
without fine-tuning. Additionally, we provide our code and segmentation masks
for these widely used datasets to promote further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Virmarie Maquiling and Sean Anthony Byrne contributed equally to this
  paper, 8 pages, 3 figures, ETRA 2025, pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expanding Performance Boundaries of Open-Source Multimodal Models with
  Model, Data, and Test-Time Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05271v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05271v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)
series that builds upon InternVL 2.0, maintaining its core model architecture
while introducing significant enhancements in training and testing strategies
as well as data quality. In this work, we delve into the relationship between
model scaling and performance, systematically exploring the performance trends
in vision encoders, language models, dataset sizes, and test-time
configurations. Through extensive evaluations on a wide range of benchmarks,
including multi-discipline reasoning, document understanding, multi-image /
video understanding, real-world comprehension, multimodal hallucination
detection, visual grounding, multilingual capabilities, and pure language
processing, InternVL 2.5 exhibits competitive performance, rivaling leading
commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is
the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a
3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing
strong potential for test-time scaling. We hope this model contributes to the
open-source community by setting new standards for developing and applying
multimodal AI systems. HuggingFace demo see
https://huggingface.co/spaces/OpenGVLab/InternVL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BayesAdapter: enhanced uncertainty estimation in CLIP few-shot
  adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Morales-Álvarez, Stergios Christodoulidis, Maria Vakalopoulou, Pablo Piantanida, Jose Dolz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large pre-trained vision-language models (VLMs) represents a
paradigm shift in machine learning, with unprecedented results in a broad span
of visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited
remarkable zero-shot and transfer learning capabilities in classification. To
transfer CLIP to downstream tasks, adapters constitute a parameter-efficient
approach that avoids backpropagation through the large model (unlike related
prompt learning methods). However, CLIP adapters have been developed to target
discriminative performance, and the quality of their uncertainty estimates has
been overlooked. In this work we show that the discriminative performance of
state-of-the-art CLIP adapters does not always correlate with their uncertainty
estimation capabilities, which are essential for a safe deployment in
real-world scenarios. We also demonstrate that one of such adapters is obtained
through MAP inference from a more general probabilistic framework. Based on
this observation we introduce BayesAdapter, which leverages Bayesian inference
to estimate a full probability distribution instead of a single point, better
capturing the variability inherent in the parameter space. In a comprehensive
empirical evaluation we show that our approach obtains high quality uncertainty
estimates in the predictions, standing out in calibration and selective
classification. Our code will be publicly available upon acceptance of the
paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 5 figures, 23 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIM: A Million-scale Benchmark for Generative Image Manipulation
  Detection and Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yirui Chen, Xudong Huang, Quan Zhang, Wei Li, Mingjian Zhu, Qiangyu Yan, Simiao Li, Hanting Chen, Hailin Hu, Jie Yang, Wei Liu, Jie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraordinary ability of generative models emerges as a new trend in
image editing and generating realistic images, posing a serious threat to the
trustworthiness of multimedia data and driving the research of image
manipulation detection and location (IMDL). However, the lack of a large-scale
data foundation makes the IMDL task unattainable. In this paper, we build a
local manipulation data generation pipeline that integrates the powerful
capabilities of SAM, LLM, and generative models. Upon this basis, we propose
the GIM dataset, which has the following advantages: 1) Large scale, GIM
includes over one million pairs of AI-manipulated images and real images. 2)
Rich image content, GIM encompasses a broad range of image classes. 3) Diverse
generative manipulation, the images are manipulated images with
state-of-the-art generators and various manipulation tasks. The aforementioned
advantages allow for a more comprehensive evaluation of IMDL methods, extending
their applicability to diverse images. We introduce the GIM benchmark with two
settings to evaluate existing IMDL methods. In addition, we propose a novel
IMDL framework, termed GIMFormer, which consists of a ShadowTracer,
Frequency-Spatial block (FSB), and a Multi-Window Anomalous Modeling (MWAM)
module. Extensive experiments on the GIM demonstrate that GIMFormer surpasses
the previous state-of-the-art approach on two different benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code page: https://github.com/chenyirui/GIM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point-JEPA: A Joint Embedding Predictive Architecture for
  <span class="highlight-title">Self-Supervised</span> Learning on Point Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16432v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16432v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayumu Saito, Prachi Kudeshia, Jiju Poovvancheri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in self-supervised learning in the point cloud domain
have demonstrated significant potential. However, these methods often suffer
from drawbacks, including lengthy pre-training time, the necessity of
reconstruction in the input space, or the necessity of additional modalities.
In order to address these issues, we introduce Point-JEPA, a joint embedding
predictive architecture designed specifically for point cloud data. To this
end, we introduce a sequencer that orders point cloud patch embeddings to
efficiently compute and utilize their proximity based on the indices during
target and context selection. The sequencer also allows shared computations of
the patch embeddings' proximity between context and target selection, further
improving the efficiency. Experimentally, our method achieves competitive
results with state-of-the-art methods while avoiding the reconstruction in the
input space or additional modality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runci Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumors can result in neurological dysfunction, alterations in cognitive
and psychological states, increased intracranial pressure, and the occurrence
of seizures, thereby presenting a substantial risk to human life and health.
The You Only Look Once(YOLO) series models have demonstrated superior accuracy
in object detection for medical imaging. In this paper, we develop a novel
SCC-YOLO architecture by integrating the SCConv attention mechanism into
YOLOv9. The SCConv module reconstructs an efficient convolutional module by
reducing spatial and channel redundancy among features, thereby enhancing the
learning of image features. We investigate the impact of intergrating different
attention mechanisms with the YOLOv9 model on brain tumor image detection using
both the Br35H dataset and our self-made dataset(Brain_Tumor_Dataset).
Experimental results show that on the Br35H dataset, SCC-YOLO achieved a 0.3%
improvement in mAp50 compared to YOLOv9, while on our self-made dataset,
SCC-YOLO exhibited a 0.5% improvement over YOLOv9. SCC-YOLO has reached
state-of-the-art performance in brain tumor detection. Source code is available
at : https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Guided Coarse-to-Fine Fusion Network for Robust Remote Sensing
  Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Zhao, Changfu Zhou, Yu Zhang, Chenglong Li, Xiaoliang Ma, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote Sensing Visual Question Answering (RSVQA) has gained significant
research interest. However, current RSVQA methods are limited by the imaging
mechanisms of optical sensors, particularly under challenging conditions such
as cloud-covered and low-light scenarios. Given the all-time and all-weather
imaging capabilities of Synthetic Aperture Radar (SAR), it is crucial to
investigate the integration of optical-SAR images to improve RSVQA performance.
In this work, we propose a Text-guided Coarse-to-Fine Fusion Network (TGFNet),
which leverages the semantic relationships between question text and
multi-source images to guide the network toward complementary fusion at the
feature level. Specifically, we develop a Text-guided Coarse-to-Fine Attention
Refinement (CFAR) module to focus on key areas related to the question in
complex remote sensing images. This module progressively directs attention from
broad areas to finer details through key region routing, enhancing the model's
ability to focus on relevant regions. Furthermore, we propose an Adaptive
Multi-Expert Fusion (AMEF) module that dynamically integrates different
experts, enabling the adaptive fusion of optical and SAR features. In addition,
we create the first large-scale benchmark dataset for evaluating optical-SAR
RSVQA methods, comprising 6,008 well-aligned optical-SAR image pairs and
1,036,694 well-labeled question-answer pairs across 16 diverse question types,
including complex relational reasoning questions. Extensive experiments on the
proposed dataset demonstrate that our TGFNet effectively integrates
complementary information between optical and SAR images, significantly
improving the model's performance in challenging scenarios. The dataset is
available at: https://github.com/mmic-lcl/.
  Index Terms: Remote Sensing Visual Question Answering, Multi-source Data
Fusion, Multimodal, Remote Sensing, OPT-SAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Driven Early Mental Health Screening: Analyzing Selfies of Pregnant
  Women <span class="chip">ALT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo A. Basílio, Thiago B. Pereira, Alessandro L. Koerich, Hermano Tavares, Ludmila Dias, Maria das Graças da S. Teixeira, Rafael T. Sousa, Wilian H. Hisatugu, Amanda S. Mota, Anilton S. Garcia, Marco Aurélio K. Galletta, Thiago M. Paixão
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Major Depressive Disorder and anxiety disorders affect millions globally,
contributing significantly to the burden of mental health issues. Early
screening is crucial for effective intervention, as timely identification of
mental health issues can significantly improve treatment outcomes. Artificial
intelligence (AI) can be valuable for improving the screening of mental
disorders, enabling early intervention and better treatment outcomes. AI-driven
screening can leverage the analysis of multiple data sources, including facial
features in digital images. However, existing methods often rely on controlled
environments or specialized equipment, limiting their broad applicability. This
study explores the potential of AI models for ubiquitous depression-anxiety
screening given face-centric selfies. The investigation focuses on high-risk
pregnant patients, a population that is particularly vulnerable to mental
health issues. To cope with limited training data resulting from our clinical
setup, pre-trained models were utilized in two different approaches:
fine-tuning convolutional neural networks (CNNs) originally designed for facial
expression recognition and employing vision-language models (VLMs) for
zero-shot analysis of facial expressions. Experimental results indicate that
the proposed VLM-based method significantly outperforms CNNs, achieving an
accuracy of 77.6%. Although there is significant room for improvement, the
results suggest that VLMs can be a promising approach for mental health
screening.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication in HEALTHINF25 at the
  18th International Joint Conference on Biomedical Engineering Systems and
  Technologies (BIOSTEC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Forward Compatibility in Class Incremental Learning by
  Increasing Representation Rank and Feature Richness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeill Kim, Wonseok Lee, Moonjung Eo, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class Incremental Learning (CIL) constitutes a pivotal subfield within
continual learning, aimed at enabling models to progressively learn new
classification tasks while retaining knowledge obtained from prior tasks.
Although previous studies have predominantly focused on backward compatible
approaches to mitigate catastrophic forgetting, recent investigations have
introduced forward compatible methods to enhance performance on novel tasks and
complement existing backward compatible methods. In this study, we introduce an
effective-Rank based Feature Richness enhancement (RFR) method, designed for
improving forward compatibility. Specifically, this method increases the
effective rank of representations during the base session, thereby facilitating
the incorporation of more informative features pertinent to unseen novel tasks.
Consequently, RFR achieves dual objectives in backward and forward
compatibility: minimizing feature extractor modifications and enhancing novel
task performance, respectively. To validate the efficacy of our approach, we
establish a theoretical connection between effective rank and the Shannon
entropy of representations. Subsequently, we conduct comprehensive experiments
by integrating RFR into eleven well-known CIL methods. Our results demonstrate
the effectiveness of our approach in enhancing novel-task performance while
mitigating catastrophic forgetting. Furthermore, our method notably improves
the average incremental accuracy across all eleven cases examined.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Counterfactual Image Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20287v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20287v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez, Athanasios Vlontzos, Yannis Panagakis, Giorgos Papanastasiou, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has revolutionised visual content editing, empowering users to
effortlessly modify images and videos. However, not all edits are equal. To
perform realistic edits in domains such as natural image or medical imaging,
modifications must respect causal relationships inherent to the data generation
process. Such image editing falls into the counterfactual image generation
regime. Evaluating counterfactual image generation is substantially complex:
not only it lacks observable ground truths, but also requires adherence to
causal constraints. Although several counterfactual image generation methods
and evaluation metrics exist, a comprehensive comparison within a unified
setting is lacking. We present a comparison framework to thoroughly benchmark
counterfactual image generation methods. We integrate all models that have been
used for the task at hand and expand them to novel datasets and causal graphs,
demonstrating the superiority of Hierarchical VAEs across most datasets and
metrics. Our framework is implemented in a user-friendly Python package that
can be extended to incorporate additional SCMs, causal methods, generative
models, and datasets for the community to build on. Code:
https://github.com/gulnazaki/counterfactual-benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at NeurIPS 2024 Datasets and
  Benchmarks Track https://openreview.net/forum?id=0T8xRFrScB Project page:
  https://gulnazaki.github.io/counterfactual-benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Situational Scene <span class="highlight-title">Graph</span> for Structured Human-centric Situation
  Understanding <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinthani Sugandhika, Chen Li, Deepu Rajan, Basura Fernando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph based representation has been widely used in modelling spatio-temporal
relationships in video understanding. Although effective, existing graph-based
approaches focus on capturing the human-object relationships while ignoring
fine-grained semantic properties of the action components. These semantic
properties are crucial for understanding the current situation, such as where
does the action takes place, what tools are used and functional properties of
the objects. In this work, we propose a graph-based representation called
Situational Scene Graph (SSG) to encode both human-object relationships and the
corresponding semantic properties. The semantic details are represented as
predefined roles and values inspired by situation frame, which is originally
designed to represent a single action. Based on our proposed representation, we
introduce the task of situational scene graph generation and propose a
multi-stage pipeline Interactive and Complementary Network (InComNet) to
address the task. Given that the existing datasets are not applicable to the
task, we further introduce a SSG dataset whose annotations consist of semantic
role-value frames for human, objects and verb predicates of human-object
relations. Finally, we demonstrate the effectiveness of our proposed SSG
representation by testing on different downstream tasks. Experimental results
show that the unified representation can not only benefit predicate
classification and semantic role-value classification, but also benefit
reasoning tasks on human-centric situation understanding. We will release the
code and the dataset soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Head Explainer: A General Framework to Improve Explainability in
  CNNs and Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohang Sun, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and
modular framework that enhances both the explainability and accuracy of
Convolutional Neural Networks (CNNs) and Transformer-based models. MHEX
consists of three core components: an Attention Gate that dynamically
highlights task-relevant features, Deep Supervision that guides early layers to
capture fine-grained details pertinent to the target class, and an Equivalent
Matrix that unifies refined local and global representations to generate
comprehensive saliency maps. Our approach demonstrates superior compatibility,
enabling effortless integration into existing residual networks like ResNet and
Transformer architectures such as BERT with minimal modifications. Extensive
experiments on benchmark datasets in medical imaging and text classification
show that MHEX not only improves classification accuracy but also produces
highly interpretable and detailed saliency scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OCTolyzer: Fully automatic toolkit for segmentation and feature
  extracting in optical coherence tomo<span class="highlight-title">graph</span>y and scanning laser ophthalmoscopy
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Burke, Justin Engelmann, Samuel Gibbon, Charlene Hamid, Diana Moukaddem, Dan Pugh, Tariq Farrah, Niall Strang, Neeraj Dhaun, Tom MacGillivray, Stuart King, Ian J. C. MacCormick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical coherence tomography (OCT) and scanning laser ophthalmoscopy (SLO) of
the eye has become essential to ophthalmology and the emerging field of
oculomics, thus requiring a need for transparent, reproducible, and rapid
analysis of this data for clinical research and the wider research community.
Here, we introduce OCTolyzer, the first open-source toolkit for retinochoroidal
analysis in OCT/SLO data. It features two analysis suites for OCT and SLO data,
facilitating deep learning-based anatomical segmentation and feature extraction
of the cross-sectional retinal and choroidal layers and en face retinal
vessels. We describe OCTolyzer and evaluate the reproducibility of its OCT
choroid analysis. At the population level, metrics for choroid region thickness
were highly reproducible, with a mean absolute error (MAE)/Pearson correlation
for macular volume choroid thickness (CT) of 6.7$\mu$m/0.99, macular B-scan CT
of 11.6$\mu$m/0.99, and peripapillary CT of 5.0$\mu$m/0.99. Macular choroid
vascular index (CVI) also showed strong reproducibility, with MAE/Pearson for
volume CVI yielding 0.0271/0.97 and B-scan CVI 0.0130/0.91. At the eye level,
measurement noise for regional and vessel metrics was below 5% and 20% of the
population's variability, respectively. Outliers were caused by poor-quality
B-scans with thick choroids and invisible choroid-sclera boundary. Processing
times on a laptop CPU were under three seconds for macular/peripapillary
B-scans and 85 seconds for volume scans. OCTolyzer can convert OCT/SLO data
into reproducible and clinically meaningful retinochoroidal features and will
improve the standardisation of ocular measurements in OCT/SLO image analysis,
requiring no specialised training or proprietary software to be used. OCTolyzer
is freely available here: https://github.com/jaburke166/OCTolyzer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 15 pages, 9 figures, 3 tables. Supplementary material: 9
  pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VibrantVS: A high-resolution multi-task transformer for forest canopy
  height estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Chang, Kiarie Ndegwa, Andreas Gros, Vincent A. Landau, Luke J. Zachmann, Bogdan State, Mitchell A. Gritts, Colton W. Miller, Nathan E. Rutenbeck, Scott Conway, Guy Bayes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the application of a novel multi-task vision transformer
(ViT) model for the estimation of canopy height models (CHMs) using 4-band
National Agriculture Imagery Program (NAIP) imagery across the western United
States. We compare the effectiveness of this model in terms of accuracy and
precision aggregated across ecoregions and class heights versus three other
benchmark peer-reviewed models. Key findings suggest that, while other
benchmark models can provide high precision in localized areas, the VibrantVS
model has substantial advantages across a broad reach of ecoregions in the
western United States with higher accuracy, higher precision, the ability to
generate updated inference at a cadence of three years or less, and high
spatial resolution. The VibrantVS model provides significant value for
ecological monitoring and land management decisions for wildfire mitigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object
  Interaction Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkun He, Yun Liu, Ruitao Liu, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing realistic human-object interaction motions is a critical problem
in VR/AR and human animation. Unlike the commonly studied scenarios involving a
single human or hand interacting with one object, we address a more generic
multi-body setting with arbitrary numbers of humans, hands, and objects. This
complexity introduces significant challenges in synchronizing motions due to
the high correlations and mutual influences among bodies. To address these
challenges, we introduce SyncDiff, a novel method for multi-body interaction
synthesis using a synchronized motion diffusion strategy. SyncDiff employs a
single diffusion model to capture the joint distribution of multi-body motions.
To enhance motion fidelity, we propose a frequency-domain motion decomposition
scheme. Additionally, we introduce a new set of alignment scores to emphasize
the synchronization of different body motions. SyncDiff jointly optimizes both
data sample likelihood and alignment likelihood through an explicit
synchronization strategy. Extensive experiments across four datasets with
various multi-body configurations demonstrate the superiority of SyncDiff over
existing state-of-the-art motion synthesis methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PSA-VLM: Enhancing Vision-Language Model Safety through Progressive
  Concept-Bottleneck-Driven Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11543v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11543v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhendong Liu, Yuanbi Nie, Yingshui Tan, Jiaheng Liu, Xiangyu Yue, Qiushi Cui, Chongjun Wang, Xiaoyong Zhu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benefiting from the powerful capabilities of Large Language Models (LLMs),
pre-trained visual encoder models connected to LLMs form Vision Language Models
(VLMs). However, recent research shows that the visual modality in VLMs is
highly vulnerable, allowing attackers to bypass safety alignment in LLMs
through visually transmitted content, launching harmful attacks. To address
this challenge, we propose a progressive concept-based alignment strategy,
PSA-VLM, which incorporates safety modules as concept bottlenecks to enhance
visual modality safety alignment. By aligning model predictions with specific
safety concepts, we improve defenses against risky images, enhancing
explainability and controllability while minimally impacting general
performance. Our method is obtained through two-stage training. The low
computational cost of the first stage brings very effective performance
improvement, and the fine-tuning of the language model in the second stage
further improves the safety performance. Our method achieves state-of-the-art
results on popular VLM safety benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2405.13581</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Migician: Revealing the Magic of Free-Form Multi-Image Grounding in
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Li, Heyu Huang, Chi Chen, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancement of Multimodal Large Language Models (MLLMs) has
significantly improved their fine-grained perception of single images and
general comprehension across multiple images. However, existing MLLMs still
face challenges in achieving precise grounding in complex multi-image
scenarios. To address this, we first explore a Chain-of-Thought (CoT) framework
that integrates single-image grounding with multi-image comprehension. While
partially effective, it remains unstable and struggles to capture abstract
visual information due to its non-end-to-end nature. Therefore, we introduce
Migician, the first multi-image grounding model capable of performing free-form
and accurate grounding across multiple images. To support this, we present the
MGrounding-630k dataset, which comprises data for several multi-image grounding
tasks derived from existing datasets, along with newly generated free-form
grounding instruction-following data. Furthermore, we propose MIG-Bench, a
comprehensive benchmark specifically designed for evaluating multi-image
grounding capabilities. Experimental results demonstrate that our model
achieves significantly superior multi-image grounding capabilities,
outperforming the best existing MLLMs by 21.61% and even surpassing much larger
70B models. Our code, model, dataset, and benchmark are fully open-sourced at
https://migician-vg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Amortizing intrac<span class="highlight-title">table</span> inference in diffusion models for vision,
  language, and control <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddarth Venkatraman, Moksh Jain, Luca Scimeca, Minsu Kim, Marcin Sendera, Mohsin Hasan, Luke Rowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, Alexandre Adam, Jarrid Rector-Brooks, <span class="highlight-author">Yoshua Bengio</span>, Glen Berseth, Nikolay Malkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as effective distribution estimators in vision,
language, and reinforcement learning, but their use as priors in downstream
tasks poses an intractable posterior inference problem. This paper studies
amortized sampling of the posterior over data, $\mathbf{x}\sim p^{\rm
post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists
of a diffusion generative model prior $p(\mathbf{x})$ and a black-box
constraint or likelihood function $r(\mathbf{x})$. We state and prove the
asymptotic correctness of a data-free learning objective, relative trajectory
balance, for training a diffusion model that samples from this posterior, a
problem that existing methods solve only approximately or in restricted cases.
Relative trajectory balance arises from the generative flow network perspective
on diffusion models, which allows the use of deep reinforcement learning
techniques to improve mode coverage. Experiments illustrate the broad potential
of unbiased inference of arbitrary posteriors under diffusion priors: in vision
(classifier guidance), language (infilling under a discrete diffusion LLM), and
multimodal data (text-to-image generation). Beyond generative modeling, we
apply relative trajectory balance to the problem of continuous control with a
score-based behavior prior, achieving state-of-the-art results on benchmarks in
offline reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; code: https://github.com/GFNOrg/diffusion-finetuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructOCR: Instruction Boosting Scene Text Spotting <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Duan, Qianyi Jiang, Pei Fu, Jiamin Chen, Shengxi Li, Zining Wang, Shan Guo, Junfeng Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of scene text spotting, previous OCR methods primarily relied on
image encoders and pre-trained text information, but they often overlooked the
advantages of incorporating human language instructions. To address this gap,
we propose InstructOCR, an innovative instruction-based scene text spotting
model that leverages human language instructions to enhance the understanding
of text within images. Our framework employs both text and image encoders
during training and inference, along with instructions meticulously designed
based on text attributes. This approach enables the model to interpret text
more accurately and flexibly. Extensive experiments demonstrate the
effectiveness of our model and we achieve state-of-the-art results on widely
used benchmarks. Furthermore, the proposed framework can be seamlessly applied
to scene text VQA tasks. By leveraging instruction strategies during
pre-training, the performance on downstream VQA tasks can be significantly
improved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on
the ST-VQA dataset. These experimental results provide insights into the
benefits of incorporating human language instructions for OCR-related tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ II-Bench: An Image Implication Understanding Benchmark for Multimodal
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05862v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05862v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Liu, Feiteng Fang, Xi Feng, Xinrun Du, Chenhao Zhang, Zekun Wang, Yuelin Bai, Qixuan Zhao, Liyang Fan, Chengguang Gan, Hongquan Lin, Jiaming Li, Yuansheng Ni, Haihong Wu, Yaswanth Narsupalli, Zhigang Zheng, Chengming Li, Xiping Hu, Ruifeng Xu, Xiaojun Chen, Min Yang, Jiaheng Liu, Ruibo Liu, Wenhao Huang, Ge Zhang, Shiwen Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in the development of multimodal large language models
(MLLMs) have consistently led to new breakthroughs on various benchmarks. In
response, numerous challenging and comprehensive benchmarks have been proposed
to more accurately assess the capabilities of MLLMs. However, there is a dearth
of exploration of the higher-order perceptual capabilities of MLLMs. To fill
this gap, we propose the Image Implication understanding Benchmark, II-Bench,
which aims to evaluate the model's higher-order perception of images. Through
extensive experiments on II-Bench across multiple MLLMs, we have made
significant findings. Initially, a substantial gap is observed between the
performance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs
attains 74.8%, whereas human accuracy averages 90%, peaking at an impressive
98%. Subsequently, MLLMs perform worse on abstract and complex images,
suggesting limitations in their ability to understand high-level semantics and
capture image details. Finally, it is observed that most models exhibit
enhanced accuracy when image sentiment polarity hints are incorporated into the
prompts. This observation underscores a notable deficiency in their inherent
understanding of image sentiment. We believe that II-Bench will inspire the
community to develop the next generation of MLLMs, advancing the journey
towards expert artificial general intelligence (AGI). II-Bench is publicly
available at https://huggingface.co/datasets/m-a-p/II-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>100 pages, 82 figures, add citations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EM-DARTS: Hierarchical Differentiable Architecture Search for Eye
  Movement Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huafeng Qin, Hongyu Zhu, Xin Jin, Xin Yu, Mounim A. El-Yacoubi, Shuqiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eye movement biometrics has received increasing attention thanks to its
highly secure identification. Although deep learning (DL) models have shown
success in eye movement recognition, their architectures largely rely on human
prior knowledge. Differentiable Neural Architecture Search (DARTS) automates
the manual process of architecture design with high search efficiency. However,
DARTS typically stacks multiple cells to form a convolutional network, which
limits the diversity of architecture. Furthermore, DARTS generally searches for
architectures using shallower networks than those used in the evaluation,
creating a significant disparity in architecture depth between the search and
evaluation phases. To address this issue, we propose EM-DARTS, a hierarchical
differentiable architecture search algorithm to automatically design the DL
architecture for eye movement recognition. First, we define a supernet and
propose a global and local alternate Neural Architecture Search method to
search the optimal architecture alternately with a differentiable neural
architecture search. The local search strategy aims to find an optimal
architecture for different cells while the global search strategy is
responsible for optimizing the architecture of the target network. To minimize
redundancy, transfer entropy is proposed to compute the information amount of
each layer, thereby further simplifying the network search process.
Experimental results on three public datasets demonstrate that the proposed
EM-DARTS is capable of producing an optimal architecture that leads to
state-of-the-art recognition performance, {Specifically, the recognition models
developed using EM-DARTS achieved the lowest EERs of 0.0453 on the GazeBase
dataset, 0.0377 on the JuDo1000 dataset, and 0.1385 on the EMglasses dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submited to IEEE Transactions on Instrumentation and Measurement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for
  Transcription-only Supervised Text Spotting <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Wu, Zhengyao Fang, Pengyuan Lyu, Chengquan Zhang, Fanglin Chen, Guangming Lu, Wenjie Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transcription-only Supervised Text Spotting aims to learn text spotters
relying only on transcriptions but no text boundaries for supervision, thus
eliminating expensive boundary annotation. The crux of this task lies in
locating each transcription in scene text images without location annotations.
In this work, we formulate this challenging problem as a Weakly Supervised
Cross-modality Contrastive Learning problem, and design a simple yet effective
model dubbed WeCromCL that is able to detect each transcription in a scene
image in a weakly supervised manner. Unlike typical methods for cross-modality
contrastive learning that focus on modeling the holistic semantic correlation
between an entire image and a text description, our WeCromCL conducts atomistic
contrastive learning to model the character-wise appearance consistency between
a text transcription and its correlated region in a scene image to detect an
anchor point for the transcription in a weakly supervised manner. The detected
anchor points by WeCromCL are further used as pseudo location labels to guide
the learning of text spotting. Extensive experiments on four challenging
benchmarks demonstrate the superior performance of our model over other
methods. Code will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Driven Diabetic Retinopathy Screening: Multicentric Validation of
  AIDRSS in India 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05826v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05826v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Kr Dey, Pradeep Walia, Girish Somvanshi, Abrar Ali, Sagarnil Das, Pallabi Paul, Minakhi Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Diabetic retinopathy (DR) is a major cause of vision loss,
particularly in India, where access to retina specialists is limited in rural
areas. This study aims to evaluate the Artificial Intelligence-based Diabetic
Retinopathy Screening System (AIDRSS) for DR detection and prevalence
assessment, addressing the growing need for scalable, automated screening
solutions in resource-limited settings.
  Approach: A multicentric, cross-sectional study was conducted in Kolkata,
India, involving 5,029 participants and 10,058 macula-centric retinal fundus
images. The AIDRSS employed a deep learning algorithm with 50 million trainable
parameters, integrated with Contrast Limited Adaptive Histogram Equalization
(CLAHE) preprocessing for enhanced image quality. DR was graded using the
International Clinical Diabetic Retinopathy (ICDR) Scale, categorizing disease
into five stages (DR0 to DR4). Statistical metrics including sensitivity,
specificity, and prevalence rates were evaluated against expert retina
specialist assessments.
  Results: The prevalence of DR in the general population was 13.7%, rising to
38.2% among individuals with elevated random blood glucose levels. The AIDRSS
achieved an overall sensitivity of 92%, specificity of 88%, and 100%
sensitivity for detecting referable DR (DR3 and DR4). These results demonstrate
the system's robust performance in accurately identifying and grading DR in a
diverse population.
  Conclusions: AIDRSS provides a reliable, scalable solution for early DR
detection in resource-constrained environments. Its integration of advanced AI
techniques ensures high diagnostic accuracy, with potential to significantly
reduce the burden of diabetes-related vision loss in underserved regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1812.07105 by other authors without attribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeadGAP: Few-Shot 3D Head Avatar via Generalizable Gaussian Priors <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel 3D head avatar creation approach capable of
generalizing from few-shot in-the-wild data with high-fidelity and animatable
robustness. Given the underconstrained nature of this problem, incorporating
prior knowledge is essential. Therefore, we propose a framework comprising
prior learning and avatar creation phases. The prior learning phase leverages
3D head priors derived from a large-scale multi-view dynamic dataset, and the
avatar creation phase applies these priors for few-shot personalization. Our
approach effectively captures these priors by utilizing a Gaussian
Splatting-based auto-decoder network with part-based dynamic modeling. Our
method employs identity-shared encoding with personalized latent codes for
individual identities to learn the attributes of Gaussian primitives. During
the avatar creation phase, we achieve fast head avatar personalization by
leveraging inversion and fine-tuning strategies. Extensive experiments
demonstrate that our model effectively exploits head priors and successfully
generalizes them to few-shot personalization, achieving photo-realistic
rendering quality, multi-view consistency, and stable animation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 3DV 2025. Project page: https://headgap.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized
  Narratives from Open-Source Histopathology Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet Saygin Seyfioglu, Wisdom O. Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, Linda Shapiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diagnosis in histopathology requires a global whole slide images (WSIs)
analysis, requiring pathologists to compound evidence from different WSI
patches. The gigapixel scale of WSIs poses a challenge for histopathology
multi-modal models. Training multi-model models for histopathology requires
instruction tuning datasets, which currently contain information for individual
image patches, without a spatial grounding of the concepts within each patch
and without a wider view of the WSI. Therefore, they lack sufficient diagnostic
capacity for histopathology. To bridge this gap, we introduce Quilt-Instruct, a
large-scale dataset of 107,131 histopathology-specific instruction
question/answer pairs, grounded within diagnostically relevant image patches
that make up the WSI. Our dataset is collected by leveraging educational
histopathology videos from YouTube, which provides spatial localization of
narrations by automatically extracting the narrators' cursor positions.
Quilt-Instruct supports contextual reasoning by extracting diagnosis and
supporting facts from the entire WSI. Using Quilt-Instruct, we train
Quilt-LLaVA, which can reason beyond the given single image patch, enabling
diagnostic reasoning across patches. To evaluate Quilt-LLaVA, we propose a
comprehensive evaluation dataset created from 985 images and 1283
human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using
public histopathology datasets, where Quilt-LLaVA significantly outperforms
SOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set
VQA. Our code, data, and model are publicly accessible at
quilt-llava.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplifying CLIP: Unleashing the Power of Large-Scale Models on
  Consumer-level Computers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has attracted a surge of
attention for its superior zero-shot performance and excellent transferability
to downstream tasks. However, training such large-scale models usually requires
substantial computation and storage, which poses barriers for general users
with consumer-level computers. Motivated by this observation, in this paper we
investigate how to achieve competitive performance on only one Nvidia RTX3090
GPU and with one terabyte for storing dataset. On one hand, we simplify the
transformer block structure and combine Weight Inheritance with multi-stage
Knowledge Distillation (WIKD), thereby reducing the parameters and improving
the inference speed during training along with deployment. On the other hand,
confronted with the convergence challenge posed by small dataset, we generate
synthetic captions for each sample as data augmentation, and devise a novel
Pair Matching (PM) loss to fully exploit the distinguishment among positive and
negative image-text pairs. Extensive experiments demonstrate that our model can
achieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which
could further popularize the CLIP model in the related research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Buster: Implanting Semantic Backdoor into Text Encoder to Mitigate NSFW
  Content Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhao, Xiaojun Chen, Yuexin Xuan, Zhendong Zhao, Xiaojun Jia, Xinfeng Li, Xiaofeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of deep learning models in the digital era has raised substantial
concerns regarding the generation of Not-Safe-for-Work (NSFW) content. Existing
defense methods primarily involve model fine-tuning and post-hoc content
moderation. Nevertheless, these approaches largely lack scalability in
eliminating harmful content, degrade the quality of benign image generation, or
incur high inference costs. To address these challenges, we propose an
innovative framework named \textit{Buster}, which injects backdoors into the
text encoder to prevent NSFW content generation. Buster leverages deep semantic
information rather than explicit prompts as triggers, redirecting NSFW prompts
towards targeted benign prompts. Additionally, Buster employs energy-based
training data generation through Langevin dynamics for adversarial knowledge
augmentation, thereby ensuring robustness in harmful concept definition. This
approach demonstrates exceptional resilience and scalability in mitigating NSFW
content. Particularly, Buster fine-tunes the text encoder of Text-to-Image
models within merely five minutes, showcasing its efficiency. Our extensive
experiments denote that Buster outperforms nine state-of-the-art baselines,
achieving a superior NSFW content removal rate of at least 91.2\% while
preserving the quality of harmless images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PViT: Prior-augmented Vision Transformer for Out-of-distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao Zhang, Zhixiang Chen, Lyudmila S. Mihaylova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have achieved remarkable success over various
vision tasks, yet their robustness against data distribution shifts and
inherent inductive biases remain underexplored. To enhance the robustness of
ViT models for image Out-of-Distribution (OOD) detection, we introduce a novel
and generic framework named Prior-augmented Vision Transformer (PViT). Taking
as input the prior class logits from a pretrained model, we train PViT to
predict the class logits. During inference, PViT identifies OOD samples by
quantifying the divergence between the predicted class logits and the prior
logits obtained from pre-trained models. Unlike existing state-of-the-art(SOTA)
OOD detection methods, PViT shapes the decision boundary between ID and OOD by
utilizing the proposed prior guided confidence, without requiring additional
data modeling, generation methods, or structural modifications. Extensive
experiments on the large-scale ImageNet benchmark, evaluated against over seven
OOD datasets, demonstrate that PViT significantly outperforms existing SOTA OOD
detection methods in terms of FPR95 and AUROC. The codebase is publicly
available at https://github.com/RanchoGoose/PViT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Attention Vectors: Generative Multimodal Model Features Are
  Discriminative Vision-Language Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chancharik Mitra, Brandon Huang, Tianning Chai, Zhiqiu Lin, Assaf Arbelle, Rogerio Feris, Leonid Karlinsky, Trevor Darrell, Deva Ramanan, Roei Herzig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a
wide variety of vision-language (VL) tasks such as image captioning or visual
question answering. Despite strong performance, LMMs are not directly suited
for foundational discriminative vision-language tasks (i.e., tasks requiring
discrete label predictions) such as image classification and multiple-choice
VQA. One key challenge in utilizing LMMs for discriminative tasks is the
extraction of useful features from generative models. To overcome this issue,
we propose an approach for finding features in the model's latent space to more
effectively leverage LMMs for discriminative tasks. Toward this end, we present
Sparse Attention Vectors (SAVs) -- a finetuning-free method that leverages
sparse attention head activations (fewer than 1\% of the heads) in LMMs as
strong features for VL tasks. With only few-shot examples, SAVs demonstrate
state-of-the-art performance compared to a variety of few-shot and finetuned
baselines on a collection of discriminative tasks. Our experiments also imply
that SAVs can scale in performance with additional examples and generalize to
similar tasks, establishing SAVs as both effective and robust multimodal
feature representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ed Vision-Language Models Learn Discoverable Visual Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zang, Tian Yun, Hao Tan, Trung Bui, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do vision-language models (VLMs) pre-trained to caption an image of a
"durian" learn visual concepts such as "brown" (color) and "spiky" (texture) at
the same time? We aim to answer this question as visual concepts learned "for
free" would enable wide applications such as neuro-symbolic reasoning or
human-interpretable object classification. We assume that the visual concepts,
if captured by pre-trained VLMs, can be extracted by their vision-language
interface with text-based concept prompts. We observe that recent works
prompting VLMs with concepts often differ in their strategies to define and
evaluate the visual concepts, leading to conflicting conclusions. We propose a
new concept definition strategy based on two observations: First, certain
concept prompts include shortcuts that recognize correct concepts for wrong
reasons; Second, multimodal information (e.g. visual discriminativeness, and
textual knowledge) should be leveraged when selecting the concepts. Our
proposed concept discovery and learning (CDL) framework is thus designed to
identify a diverse list of generic visual concepts (e.g. "spiky" as opposed to
"spiky durian"), which are ranked and selected based on visual and language
mutual information. We carefully design quantitative and human evaluations of
the discovered concepts on six diverse visual recognition datasets, which
confirm that pre-trained VLMs do learn visual concepts that provide accurate
and thorough descriptions for the recognized objects. All code and models are
publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Transactions on Machine Learning Research, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extracting Manifold Information from Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Guidotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A kernel based method is proposed for the construction of signature
(defining) functions of subsets of $\mathbb{R}^d$. The subsets can range from
full dimensional manifolds (open subsets) to point clouds (a finite number of
points) and include bounded smooth manifolds of any codimension. The
interpolation and analysis of point clouds are the main application. Two
extreme cases in terms of regularity are considered, where the data set is
interpolated by an analytic surface, at the one extreme, and by a H\"older
continuous surface, at the other. The signature function can be computed as a
linear combination of translated kernels, the coefficients of which are the
solution of a finite dimensional linear problem. Once it is obtained, it can be
used to estimate the dimension as well as the normal and the curvatures of the
interpolated surface. The method is global and does not require explicit
knowledge of local neighborhoods or any other structure present in the data
set. It admits a variational formulation with a natural ``regularized''
counterpart, that proves to be useful in dealing with data sets corrupted by
numerical error or noise. The underlying analytical structure of the approach
is presented in general before it is applied to the case of point clouds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 16 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExACT: Teaching AI Agents to Explore with Reflective-MCTS and
  Exploratory Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02052v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02052v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have demonstrated significant potential in automating
complex multistep decision-making tasks. However, even state-of-the-art
vision-language models (VLMs), such as GPT-4o, still fall short of human-level
performance, particularly in intricate web environments and long-horizon tasks.
To address these limitations, we present ExACT, an approach to combine
test-time search and self-learning to build o1-like models for agentic
applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a
novel test time algorithm designed to enhance AI agents' ability to explore
decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating
contrastive reflection, allowing agents to learn from past interactions and
dynamically improve their search efficiency; and 2) using multi-agent debate
for reliable state evaluation. Next, we introduce Exploratory Learning, a novel
learning strategy to teach agents to search at inference time without relying
on any external search algorithms. On the challenging VisualWebArena benchmark,
our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across
various tasks compared to the previous state-of-the-art. Additionally, we show
that the knowledge and experience gained from test-time search can be
effectively transferred back to GPT-4o via fine-tuning. After Exploratory
Learning, GPT-4o 1) demonstrates the ability to explore the environment,
evaluate a state, and backtrack to viable ones when it detects that the current
state cannot lead to success, and 2) matches 87% of R-MCTS's performance while
using significantly less compute. Notably, our work demonstrates the compute
scaling properties in both training - data collection with R-MCTS - and testing
time. These results suggest a promising research direction to enhance VLMs'
capabilities for agentic applications via test-time search and self-learning.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal semantic <span class="highlight-title">retrie</span>val for product search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Esther Lopez Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic retrieval (also known as dense retrieval) based on textual data has
been extensively studied for both web search and product search application
fields, where the relevance of a query and a potential target document is
computed by their dense vector representation comparison. Product image is
crucial for e-commence search interactions and is a key factor for customers at
product explorations. But its impact for semantic retrieval has not been well
studied yet. In this research, we build a multimodal representation for product
items in e-commerece search in contrast to pure-text representation of
products, and investigate the impact of such representations. The models are
developed and evaluated on e-commerce datasets. We demonstrate that a
multimodal representation scheme for a product can show improvement either on
purchase recall or relevance accuracy in semantic retrieval. Additionally, we
provide numerical analysis for exclusive matches retrieved by a multimodal
semantic retrieval model versus a text-only semantic retrieval model, to
demonstrate the validation of multimodal solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span>-Agnostic <span class="highlight-title">Recommend</span>er Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tri Kurniawan Wijaya, Edoardo D'Amico, Xinyang Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  [This is a position paper and does not contain any empirical or theoretical
results] Recommender systems have become a cornerstone of personalized user
experiences, yet their development typically involves significant manual
intervention, including dataset-specific feature engineering, hyperparameter
tuning, and configuration. To this end, we introduce a novel paradigm:
Dataset-Agnostic Recommender Systems (DAReS) that aims to enable a single
codebase to autonomously adapt to various datasets without the need for
fine-tuning, for a given recommender system task. Central to this approach is
the Dataset Description Language (DsDL), a structured format that provides
metadata about the dataset's features and labels, and allow the system to
understand dataset's characteristics, allowing it to autonomously manage
processes like feature selection, missing values imputation, noise removal, and
hyperparameter optimization. By reducing the need for domain-specific expertise
and manual adjustments, DAReS offers a more efficient and scalable solution for
building recommender systems across diverse application domains. It addresses
critical challenges in the field, such as reusability, reproducibility, and
accessibility for non-expert users or entry-level researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Future-Conditioned <span class="highlight-title">Recommend</span>ations with Multi-Objective Controllable
  Decision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongming Gao, Kexin Huang, Ziang Fei, Jiaju Chen, Jiawei Chen, Jianshan Sun, Shuchang Liu, Qingpeng Cai, Peng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Securing long-term success is the ultimate aim of recommender systems,
demanding strategies capable of foreseeing and shaping the impact of decisions
on future user satisfaction. Current recommendation strategies grapple with two
significant hurdles. Firstly, the future impacts of recommendation decisions
remain obscured, rendering it impractical to evaluate them through direct
optimization of immediate metrics. Secondly, conflicts often emerge between
multiple objectives, like enhancing accuracy versus exploring diverse
recommendations. Existing strategies, trapped in a "training, evaluation, and
retraining" loop, grow more labor-intensive as objectives evolve. To address
these challenges, we introduce a future-conditioned strategy for
multi-objective controllable recommendations, allowing for the direct
specification of future objectives and empowering the model to generate item
sequences that align with these goals autoregressively. We present the
Multi-Objective Controllable Decision Transformer (MocDT), an offline
Reinforcement Learning (RL) model capable of autonomously learning the mapping
from multiple objectives to item sequences, leveraging extensive offline data.
Consequently, it can produce recommendations tailored to any specified
objectives during the inference stage. Our empirical findings emphasize the
controllable recommendation strategy's ability to produce item sequences
according to different objectives while maintaining performance that is
competitive with current recommendation strategies across various objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructing Set-Compositional and Negated Representations for
  First-Stage Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonios Minas Krasakis, Andrew Yates, Evangelos Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Set compositional and negated queries are crucial for expressing complex
information needs and enable the discovery of niche items like Books about
non-European monarchs. Despite the recent advances in LLMs, first-stage ranking
remains challenging due to the requirement of encoding documents and queries
independently from each other. This limitation calls for constructing
compositional query representations that encapsulate logical operations or
negations, and can be used to match relevant documents effectively. In the
first part of this work, we explore constructing such representations in a
zero-shot setting using vector operations between lexically grounded Learned
Sparse Retrieval (LSR) representations. Specifically, we introduce Disentangled
Negation that penalizes only the negated parts of a query, and a Combined
Pseudo-Term approach that enhances LSRs ability to handle intersections. We
find that our zero-shot approach is competitive and often outperforms
retrievers fine-tuned on compositional data, highlighting certain limitations
of LSR and Dense Retrievers. Finally, we address some of these limitations and
improve LSRs representation power for negation, by allowing them to attribute
negative term scores and effectively penalize documents containing the negated
terms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models: New Opportunities for Access to Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jutta Schnabel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adaptation of Large Language Models like ChatGPT for information
retrieval from scientific data, software and publications is offering new
opportunities to simplify access to and understanding of science for persons
from all levels of expertise. They can become tools to both enhance the
usability of the open science environment we are building as well as help to
provide systematic insight to a long-built corpus of scientific publications.
The uptake of Retrieval Augmented Generation-enhanced chat applications in the
construction of the open science environment of the KM3NeT neutrino detectors
serves as a focus point to explore and exemplify prospects for the wider
application of Large Language Models for our science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference proceeding to ADASS XXXIV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ListConRanker: A Contrastive Text Reranker with Listwise Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlong Liu, Yue Ma, Ruihui Zhao, Junhao Zheng, Qianli Ma, Yangyang Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranker models aim to re-rank the passages based on the semantics similarity
between the given query and passages, which have recently received more
attention due to the wide application of the Retrieval-Augmented Generation.
Most previous methods apply pointwise encoding, meaning that it can only encode
the context of the query for each passage input into the model. However, for
the reranker model, given a query, the comparison results between passages are
even more important, which is called listwise encoding. Besides, previous
models are trained using the cross-entropy loss function, which leads to issues
of unsmooth gradient changes during training and low training efficiency. To
address these issues, we propose a novel Listwise-encoded Contrastive text
reRanker (ListConRanker). It can help the passage to be compared with other
passages during the encoding process, and enhance the contrastive information
between positive examples and between positive and negative examples. At the
same time, we use the circle loss to train the model to increase the
flexibility of gradients and solve the problem of training efficiency.
Experimental results show that ListConRanker achieves state-of-the-art
performance on the reranking benchmark of Chinese Massive Text Embedding
Benchmark, including the cMedQA1.0, cMedQA2.0, MMarcoReranking, and T2Reranking
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multimodal Fusion via Meta-Learning Towards Micro-Video
  <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Liu, Yinwei Wei, Fan Liu, Wenjie Wang, Liqiang Nie, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal information (e.g., visual, acoustic, and textual) has been widely
used to enhance representation learning for micro-video recommendation. For
integrating multimodal information into a joint representation of micro-video,
multimodal fusion plays a vital role in the existing micro-video recommendation
approaches. However, the static multimodal fusion used in previous studies is
insufficient to model the various relationships among multimodal information of
different micro-videos. In this paper, we develop a novel meta-learning-based
multimodal fusion framework called Meta Multimodal Fusion (MetaMMF), which
dynamically assigns parameters to the multimodal fusion function for each
micro-video during its representation learning. Specifically, MetaMMF regards
the multimodal fusion of each micro-video as an independent task. Based on the
meta information extracted from the multimodal features of the input task,
MetaMMF parameterizes a neural network as the item-specific fusion function via
a meta learner. We perform extensive experiments on three benchmark datasets,
demonstrating the significant improvements over several state-of-the-art
multimodal recommendation models, like MMGCN, LATTICE, and InvRL. Furthermore,
we lighten our model by adopting canonical polyadic decomposition to improve
the training efficiency, and validate its effectiveness through experimental
results. Codes are available at https://github.com/hanliu95/MetaMMF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ACM Transactions on Information
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Interest Disentanglement and Item-Aware Intent Contrastive
  Learning for Sequential <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijin Choi, Chiehyeon Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems aim to provide personalized item recommendations by
capturing user behaviors derived from their interaction history. Considering
that user interactions naturally occur sequentially based on users' intents in
mind, user behaviors can be interpreted as user intents. Therefore,
intent-based sequential recommendations are actively studied recently to model
user intents from historical interactions for a more precise user understanding
beyond traditional studies that often overlook the underlying semantics behind
user interactions. However, existing studies face three challenges: 1) the
limited understanding of user behaviors by focusing solely on intents, 2) the
lack of robustness in categorizing intents due to arbitrary fixed numbers of
intent categories, and 3) the neglect of interacted items in modeling of user
intents. To address these challenges, we propose Intent-Interest
Disentanglement and Item-Aware Intent Contrastive Learning for Sequential
Recommendation (IDCLRec). IDCLRec disentangles user behaviors into intents
which are dynamic motivations and interests which are stable tastes of users
for a comprehensive understanding of user behaviors. A causal cross-attention
mechanism is used to identify consistent interests across interactions, while
residual behaviors are modeled as intents by modeling their temporal dynamics
through a similarity adjustment loss. In addition, without predefining the
number of intent categories, an importance-weighted attention mechanism
captures user-specific categorical intent considering the importance of intent
for each interaction. Furthermore, we introduce item-aware contrastive learning
which aligns intents that occurred the same interaction and aligns intent with
item combinations occurred by the corresponding intent. Extensive experiments
conducted on real-world datasets demonstrate the effectiveness of IDCLRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on the Online Update Method for <span class="highlight-title">Retrie</span>val-Augmented Generation
  (RAG) Model with Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Fan, Yuxiang Wang, Lipeng Liu, Xirui Tang, Na Sun, Zidong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the contemporary context of rapid advancements in information technology
and the exponential growth of data volume, language models are confronted with
significant challenges in effectively navigating the dynamic and ever-evolving
information landscape to update and adapt to novel knowledge in real time. In
this work, an online update method is proposed, which is based on the existing
Retrieval Enhanced Generation (RAG) model with multiple innovation mechanisms.
Firstly, the dynamic memory is used to capture the emerging data samples, and
then gradually integrate them into the core model through a tunable knowledge
distillation strategy. At the same time, hierarchical indexing and multi-layer
gating mechanism are introduced into the retrieval module to ensure that the
retrieved content is more targeted and accurate. Finally, a multi-stage network
structure is established for different types of inputs in the generation stage,
and cross-attention matching and screening are carried out on the intermediate
representations of each stage to ensure the effective integration and iterative
update of new and old knowledge. Experimental results show that the proposed
method is better than the existing mainstream comparison models in terms of
knowledge retention and inference accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Proposed Large Language Model-Based Smart Search for Archive System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha Dung Nguyen, Thi-Hoang Anh Nguyen, Thanh Binh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel framework for smart search in digital archival
systems, leveraging the capabilities of Large Language Models (LLMs) to enhance
information retrieval. By employing a Retrieval-Augmented Generation (RAG)
approach, the framework enables the processing of natural language queries and
transforming non-textual data into meaningful textual representations. The
system integrates advanced metadata generation techniques, a hybrid retrieval
mechanism, a router query engine, and robust response synthesis, the results
proved search precision and relevance. We present the architecture and
implementation of the system and evaluate its performance in four experiments
concerning LLM efficiency, hybrid retrieval optimizations, multilingual query
handling, and the impacts of individual components. Obtained results show
significant improvements over conventional approaches and have demonstrated the
potential of AI-powered systems to transform modern archival practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 13th International Symposium on Information and Communication
  Technology (SOICT 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Graph</span> Contrastive Learning on Multi-label Classification for
  <span class="highlight-title">Recommend</span>ations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Wu, Wensheng Gan, Huashen Lu, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In business analysis, providing effective recommendations is essential for
enhancing company profits. The utilization of graph-based structures, such as
bipartite graphs, has gained popularity for their ability to analyze complex
data relationships. Link prediction is crucial for recommending specific items
to users. Traditional methods in this area often involve identifying patterns
in the graph structure or using representational techniques like graph neural
networks (GNNs). However, these approaches encounter difficulties as the volume
of data increases. To address these challenges, we propose a model called Graph
Contrastive Learning for Multi-label Classification (MCGCL). MCGCL leverages
contrastive learning to enhance recommendation effectiveness. The model
incorporates two training stages: a main task and a subtask. The main task is
holistic user-item graph learning to capture user-item relationships. The
homogeneous user-user (item-item) subgraph is constructed to capture user-user
and item-item relationships in the subtask. We assessed the performance using
real-world datasets from Amazon Reviews in multi-label classification tasks.
Comparative experiments with state-of-the-art methods confirm the effectiveness
of MCGCL, highlighting its potential for improving recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Multimodal Large Language Models for Multimodal Sequential
  <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09698v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09698v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Ye, Zhi Zheng, Yishan Shen, Tianshu Wang, Hengruo Zhang, Peijun Zhu, Runlong Yu, Kai Zhang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have demonstrated significant
potential in the field of Recommendation Systems (RSs). Most existing studies
have focused on converting user behavior logs into textual prompts and
leveraging techniques such as prompt tuning to enable LLMs for recommendation
tasks. Meanwhile, research interest has recently grown in multimodal
recommendation systems that integrate data from images, text, and other sources
using modality fusion techniques. This introduces new challenges to the
existing LLM-based recommendation paradigm which relies solely on text modality
information. Moreover, although Multimodal Large Language Models (MLLMs)
capable of processing multi-modal inputs have emerged, how to equip MLLMs with
multi-modal recommendation capabilities remains largely unexplored. To this
end, in this paper, we propose the Multimodal Large Language Model-enhanced
Multimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic
user preference, we design a two-stage user preference summarization method.
Specifically, we first utilize an MLLM-based item-summarizer to extract image
feature given an item and convert the image into text. Then, we employ a
recurrent user preference summarization generation paradigm to capture the
dynamic changes in user preferences based on an LLM-based user-summarizer.
Finally, to enable the MLLM for multi-modal recommendation task, we propose to
fine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)
techniques. Extensive evaluations across various datasets validate the
effectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt
to the evolving dynamics of user preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference-Consistent Knowledge Distillation for <span class="highlight-title">Recommend</span>er System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchi Zhu, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature-based knowledge distillation has been applied to compress modern
recommendation models, usually with projectors that align student (small)
recommendation models' dimensions with teacher dimensions. However, existing
studies have only focused on making the projected features (i.e., student
features after projectors) similar to teacher features, overlooking
investigating whether the user preference can be transferred to student
features (i.e., student features before projectors) in this manner. In this
paper, we find that due to the lack of restrictions on projectors, the process
of transferring user preferences will likely be interfered with. We refer to
this phenomenon as preference inconsistency. It greatly wastes the power of
feature-based knowledge distillation. To mitigate preference inconsistency, we
propose PCKD, which consists of two regularization terms for projectors. We
also propose a hybrid method that combines the two regularization terms. We
focus on items with high preference scores and significantly mitigate
preference inconsistency, improving the performance of feature-based knowledge
distillation. Extensive experiments on three public datasets and three
backbones demonstrate the effectiveness of PCKD. The code of our method is
provided in https://github.com/woriazzc/KDs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TKDE 2024 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Feature-based Knowledge Distillation for <span class="highlight-title">Recommend</span>er System: A
  Frequency Perspective <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchi Zhu, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we analyze the feature-based knowledge distillation for
recommendation from the frequency perspective. By defining knowledge as
different frequency components of the features, we theoretically demonstrate
that regular feature-based knowledge distillation is equivalent to equally
minimizing losses on all knowledge and further analyze how this equal loss
weight allocation method leads to important knowledge being overlooked. In
light of this, we propose to emphasize important knowledge by redistributing
knowledge weights. Furthermore, we propose FreqD, a lightweight knowledge
reweighting method, to avoid the computational cost of calculating losses on
each knowledge. Extensive experiments demonstrate that FreqD consistently and
significantly outperforms state-of-the-art knowledge distillation methods for
recommender systems. Our code is available at https://github.com/woriazzc/KDs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM KDD 2025 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-granularity Interest <span class="highlight-title">Retrie</span>val and Refinement Network for
  Long-Term User Behavior Modeling in CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15005v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15005v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Xu, Hao Wang, Wei Guo, Luankang Zhang, Wanshan Yang, Runlong Yu, Yong Liu, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through Rate (CTR) prediction is crucial for online personalization
platforms. Recent advancements have shown that modeling rich user behaviors can
significantly improve the performance of CTR prediction. Current long-term user
behavior modeling algorithms predominantly follow two cascading stages. The
first stage retrieves subsequence related to the target item from the long-term
behavior sequence, while the second stage models the relationship between the
subsequence and the target item. Despite significant progress, these methods
have two critical flaws. First, the retrieval query typically includes only
target item information, limiting the ability to capture the user's diverse
interests. Second, relational information, such as sequential and interactive
information within the subsequence, is frequently overlooked. Therefore, it
requires to be further mined to more accurately model user interests.
  To this end, we propose Multi-granularity Interest Retrieval and Refinement
Network (MIRRN). Specifically, we first construct queries based on behaviors
observed at different time scales to obtain subsequences, each capturing users'
interest at various granularities. We then introduce an noval multi-head
Fourier transformer to efficiently learn sequential and interactive information
within the subsequences, leading to more accurate modeling of user interests.
Finally, we employ multi-head target attention to adaptively assess the impact
of these multi-granularity interests on the target item. Extensive experiments
have demonstrated that MIRRN significantly outperforms state-of-the-art
baselines. Furthermore, an A/B test shows that MIRRN increases the average
number of listening songs by 1.32% and the average time of listening songs by
0.55% on the Huawei Music App. The implementation code is publicly available at
https://github.com/USTC-StarTeam/MIRRN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic-Aware Knowledge <span class="highlight-title">Graph</span> with Large Language Models for
  Interoperability in <span class="highlight-title">Recommend</span>er Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minhye Jeon, Seokho Ahn, Young-Duk Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of knowledge graphs in recommender systems has become one of the
common approaches to addressing data sparsity and cold start problems. Recent
advances in large language models (LLMs) offer new possibilities for processing
side and context information within knowledge graphs. However, consistent
integration across various systems remains challenging due to the need for
domain expert intervention and differences in system characteristics. To
address these issues, we propose a consistent approach that extracts both
general and specific topics from both side and context information using LLMs.
First, general topics are iteratively extracted and updated from side
information. Then, specific topics are extracted using context information.
Finally, to address synonymous topics generated during the specific topic
extraction process, a refining algorithm processes and resolves these issues
effectively. This approach allows general topics to capture broad knowledge
across diverse item characteristics, while specific topics emphasize detailed
attributes, providing a more comprehensive understanding of the semantic
features of items and the preferences of users. Experimental results
demonstrate significant improvements in recommendation performance across
diverse knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BeFA: A General Behavior-driven Feature Adapter for Multimedia
  <span class="highlight-title">Recommend</span>ation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qile Fan, Penghang Yu, Zhiyi Tan, Bing-Kun Bao, Guanming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia recommender systems focus on utilizing behavioral information and
content information to model user preferences. Typically, it employs
pre-trained feature encoders to extract content features, then fuses them with
behavioral features. However, pre-trained feature encoders often extract
features from the entire content simultaneously, including excessive
preference-irrelevant details. We speculate that it may result in the extracted
features not containing sufficient features to accurately reflect user
preferences. To verify our hypothesis, we introduce an attribution analysis
method for visually and intuitively analyzing the content features. The results
indicate that certain products' content features exhibit the issues of
information drift}and information omission,reducing the expressive ability of
features. Building upon this finding, we propose an effective and efficient
general Behavior-driven Feature Adapter (BeFA) to tackle these issues. This
adapter reconstructs the content feature with the guidance of behavioral
information, enabling content features accurately reflecting user preferences.
Extensive experiments demonstrate the effectiveness of the adapter across all
multimedia recommendation methods. Our code is made publicly available on
https://github.com/fqldom/BeFA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E2ESlack: An End-to-End <span class="highlight-title">Graph</span>-Based Framework for Pre-Routing Slack
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Bodhe, Zhanguang Zhang, Atia Hamidizadeh, Shixiong Kai, Yingxue Zhang, Mingxuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-routing slack prediction remains a critical area of research in
Electronic Design Automation (EDA). Despite numerous machine learning-based
approaches targeting this task, there is still a lack of a truly end-to-end
framework that engineers can use to obtain TNS/WNS metrics from raw circuit
data at the placement stage. Existing works have demonstrated effectiveness in
Arrival Time (AT) prediction but lack a mechanism for Required Arrival Time
(RAT) prediction, which is essential for slack prediction and obtaining TNS/WNS
metrics. In this work, we propose E2ESlack, an end-to-end graph-based framework
for pre-routing slack prediction. The framework includes a TimingParser that
supports DEF, SDF and LIB files for feature extraction and graph construction,
an arrival time prediction model and a fast RAT estimation module. To the best
of our knowledge, this is the first work capable of predicting path-level
slacks at the pre-routing stage. We perform extensive experiments and
demonstrate that our proposed RAT estimation method outperforms the SOTA
ML-based prediction method and also pre-routing STA tool. Additionally, the
proposed E2ESlack framework achieves TNS/WNS values comparable to post-routing
STA results while saving up to 23x runtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Prototype Rehearsal for Continual Learning in ECG Arrhythmia
  Detection <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sana Rahmani, Reetam Chatterjee, Ali Etemad, Javad Hashemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning (CL) methods aim to learn from a sequence of tasks while
avoiding the challenge of forgetting previous knowledge. We present DREAM-CL, a
novel CL method for ECG arrhythmia detection that introduces dynamic prototype
rehearsal memory. DREAM-CL selects representative prototypes by clustering data
based on learning behavior during each training session. Within each cluster,
we apply a smooth sorting operation that ranks samples by training difficulty,
compressing extreme values and removing outliers. The more challenging samples
are then chosen as prototypes for the rehearsal memory, ensuring effective
knowledge retention across sessions. We evaluate our method on
time-incremental, class-incremental, and lead-incremental scenarios using two
widely used ECG arrhythmia datasets, Chapman and PTB-XL. The results
demonstrate that DREAM-CL outperforms the state-of-the-art in CL for ECG
arrhythmia detection. Detailed ablation and sensitivity studies are performed
to validate the different design choices of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2025 International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imagine while Reasoning in Space: Multimodal Visualization-of-Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) prompting has proven highly effective for enhancing
complex reasoning in Large Language Models (LLMs) and Multimodal Large Language
Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks.
Nonetheless, human cognition extends beyond language alone, enabling the
remarkable capability to think in both words and images. Inspired by this
mechanism, we propose a new reasoning paradigm, Multimodal
Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by
generating image visualizations of their reasoning traces. To ensure
high-quality visualization, we introduce token discrepancy loss into
autoregressive MLLMs. This innovation significantly improves both visual
coherence and fidelity. We validate this approach through several dynamic
spatial reasoning tasks. Experimental results reveal that MVoT demonstrates
competitive performance across tasks. Moreover, it exhibits robust and reliable
improvements in the most challenging scenarios where CoT fails. Ultimately,
MVoT establishes new possibilities for complex reasoning tasks where visual
thinking can effectively complement verbal reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables
  including references and appendices)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ML Mule: Mobile-Driven Context-Aware Collaborative Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiang Yu, Javier Berrocal, Christine Julien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has been integrated into nearly every aspect of daily
life, powering applications from object detection with computer vision to large
language models for writing emails and compact models in smart homes. These
machine learning models cater to individual users but are often detached from
them, as they are typically stored and processed in centralized data centers.
This centralized approach raises privacy concerns, incurs high infrastructure
costs, and struggles with personalization. Federated and fully decentralized
learning methods have been proposed to address these issues, but they still
depend on centralized servers or face slow convergence due to communication
constraints. To overcome these challenges, we propose ML Mule, a approach that
utilizes individual mobile devices as 'Mules' to train and transport model
snapshots as they move through physical spaces, sharing these models with the
physical 'Spaces' they inhabit. This method implicitly forms affinity groups
among devices associated with users who share particular spaces, enabling
collaborative model evolution, and protecting users' privacy. Our approach
addresses several major shortcomings of traditional, federated, and fully
decentralized learning systems. The proposed framework represents a new class
of machine learning methods that are more robust, distributed, and
personalized, bringing the field closer to realizing the original vision of
intelligent, adaptive, and genuinely context-aware smart environments. The
results show that ML Mule converges faster and achieves higher model accuracy
compared to other existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Map-Based Path Loss Models: A Study of Feature
  Representations in Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan G. Dempsey, Jonathan Ethier, Halim Yanikomeroglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path loss prediction is a beneficial tool for efficient use of the radio
frequency spectrum. Building on prior research on high-resolution map-based
path loss models, this paper studies convolutional neural network input
representations in more detail. We investigate different methods of
representing scalar features in convolutional neural networks. Specifically, we
compare using frequency and distance as input channels to convolutional layers
or as scalar inputs to regression layers. We assess model performance using
three different feature configurations and find that representing scalar
features as image channels results in the strongest generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RadAlign: Advancing Radiology Report Generation with Vision-Language
  Concept Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Difei Gu, Yunhe Gao, Yang Zhou, Mu Zhou, Dimitris Metaxas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated chest radiographs interpretation requires both accurate disease
classification and detailed radiology report generation, presenting a
significant challenge in the clinical workflow. Current approaches either focus
on classification accuracy at the expense of interpretability or generate
detailed but potentially unreliable reports through image captioning
techniques. In this study, we present RadAlign, a novel framework that combines
the predictive accuracy of vision-language models (VLMs) with the reasoning
capabilities of large language models (LLMs). Inspired by the radiologist's
workflow, RadAlign first employs a specialized VLM to align visual features
with key medical concepts, achieving superior disease classification with an
average AUC of 0.885 across multiple diseases. These recognized medical
conditions, represented as text-based concepts in the aligned visual-language
space, are then used to prompt LLM-based report generation. Enhanced by a
retrieval-augmented generation mechanism that grounds outputs in similar
historical cases, RadAlign delivers superior report quality with a GREEN score
of 0.678, outperforming state-of-the-art methods' 0.634. Our framework
maintains strong clinical interpretability while reducing hallucinations,
advancing automated medical imaging and report analysis through integrated
predictive and generative AI. Code is available at
https://github.com/difeigu/RadAlign.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving DeFi Accessibility through Efficient Liquidity Provisioning
  with Deep Reinforcement Learning <span class="chip">AAAI
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Xu, Alessio Brini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper applies deep reinforcement learning (DRL) to optimize liquidity
provisioning in Uniswap v3, a decentralized finance (DeFi) protocol
implementing an automated market maker (AMM) model with concentrated liquidity.
We model the liquidity provision task as a Markov Decision Process (MDP) and
train an active liquidity provider (LP) agent using the Proximal Policy
Optimization (PPO) algorithm. The agent dynamically adjusts liquidity positions
by using information about price dynamics to balance fee maximization and
impermanent loss mitigation. We use a rolling window approach for training and
testing, reflecting realistic market conditions and regime shifts. This study
compares the data-driven performance of the DRL-based strategy against common
heuristics adopted by small retail LP actors that do not systematically modify
their liquidity positions. By promoting more efficient liquidity management,
this work aims to make DeFi markets more accessible and inclusive for a broader
range of participants. Through a data-driven approach to liquidity management,
this work seeks to contribute to the ongoing development of more efficient and
user-friendly DeFi markets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures. Accepted at AI for Social Impact: Bridging
  Innovations in Finance, Social Media, and Crime Prevention Workshop at AAAI
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RbRL2.0: Integrated Reward and Policy Learning for Rating-based
  Reinforcement Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkang Wu, Devin White, Vernon Lawhern, Nicholas R. Waytowich, Yongcan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL), a common tool in decision making, learns
policies from various experiences based on the associated cumulative
return/rewards without treating them differently. On the contrary, humans often
learn to distinguish from different levels of performance and extract the
underlying trends towards improving their decision making for best performance.
Motivated by this, this paper proposes a novel RL method that mimics humans'
decision making process by differentiating among collected experiences for
effective policy learning. The main idea is to extract important directional
information from experiences with different performance levels, named ratings,
so that policies can be updated towards desired deviation from these
experiences with different ratings. Specifically, we propose a new policy loss
function that penalizes distribution similarities between the current policy
and failed experiences with different ratings, and assign different weights to
the penalty terms based on the rating classes. Meanwhile, reward learning from
these rated samples can be integrated with the new policy loss towards an
integrated reward and policy learning from rated samples. Optimizing the
integrated reward and policy loss function will lead to the discovery of
directions for policy improvement towards maximizing cumulative rewards and
penalizing most from the lowest performance level while least from the highest
performance level. To evaluate the effectiveness of the proposed method, we
present results for experiments on a few typical environments that show
improved convergence and overall performance over the existing rating-based
reinforcement learning method with only reward learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Collaborative AI and Modeling of Humans Bridge
  Program at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring and Mitigating Adversarial Manipulation of Voting-Based
  Leaderboards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangsibo Huang, Milad Nasr, Anastasios Angelopoulos, Nicholas Carlini, Wei-Lin Chiang, Christopher A. Choquette-Choo, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Ken Ziyu Liu, Ion Stoica, Florian Tramer, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is now common to evaluate Large Language Models (LLMs) by having humans
manually vote to evaluate model outputs, in contrast to typical benchmarks that
evaluate knowledge or skill at some particular task. Chatbot Arena, the most
popular benchmark of this type, ranks models by asking users to select the
better response between two randomly selected models (without revealing which
model was responsible for the generations). These platforms are widely trusted
as a fair and accurate measure of LLM capabilities. In this paper, we show that
if bot protection and other defenses are not implemented, these voting-based
benchmarks are potentially vulnerable to adversarial manipulation.
Specifically, we show that an attacker can alter the leaderboard (to promote
their favorite model or demote competitors) at the cost of roughly a thousand
votes (verified in a simulated, offline version of Chatbot Arena). Our attack
consists of two steps: first, we show how an attacker can determine which model
was used to generate a given reply with more than $95\%$ accuracy; and then,
the attacker can use this information to consistently vote for (or against) a
target model. Working with the Chatbot Arena developers, we identify, propose,
and implement mitigations to improve the robustness of Chatbot Arena against
adversarial manipulation, which, based on our analysis, substantially increases
the cost of such attacks. Some of these defenses were present before our
collaboration, such as bot protection with Cloudflare, malicious user
detection, and rate limiting. Others, including reCAPTCHA and login are being
integrated to strengthen the security in Chatbot Arena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrecipDiff: Leveraging image diffusion models to enhance satellite-based
  precipitation observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Yu Dai, Hayato Ushijima-Mwesigwa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent report from the World Meteorological Organization (WMO) highlights
that water-related disasters have caused the highest human losses among natural
disasters over the past 50 years, with over 91\% of deaths occurring in
low-income countries. This disparity is largely due to the lack of adequate
ground monitoring stations, such as weather surveillance radars (WSR), which
are expensive to install. For example, while the US and Europe combined possess
over 600 WSRs, Africa, despite having almost one and half times their landmass,
has fewer than 40. To address this issue, satellite-based observations offer a
global, near-real-time monitoring solution. However, they face several
challenges like accuracy, bias, and low spatial resolution. This study
leverages the power of diffusion models and residual learning to address these
limitations in a unified framework. We introduce the first diffusion model for
correcting the inconsistency between different precipitation products. Our
method demonstrates the effectiveness in downscaling satellite precipitation
estimates from 10 km to 1 km resolution. Extensive experiments conducted in the
Seattle region demonstrate significant improvements in accuracy, bias
reduction, and spatial detail. Importantly, our approach achieves these results
using only precipitation data, showcasing the potential of a purely computer
vision-based approach for enhancing satellite precipitation products and paving
the way for further advancements in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesis and Analysis of Data as Probability Measures with
  Entropy-Regularized Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brendan Mallery, James M. Murphy, Shuchin Aeron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider synthesis and analysis of probability measures using the
entropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn
divergence. The synthesis problem consists of computing the barycenter, with
respect to these costs, of $m$ reference measures given a set of coefficients
belonging to the $m$-dimensional simplex. The analysis problem consists of
finding the coefficients for the closest barycenter in the Wasserstein-2
distance to a given measure $\mu$. Under the weakest assumptions on the
measures thus far in the literature, we compute the derivative of the
entropy-regularized Wasserstein-2 cost. We leverage this to establish a
characterization of regularized barycenters as solutions to a fixed-point
equation for the average of the entropic maps from the barycenter to the
reference measures. This characterization yields a finite-dimensional, convex,
quadratic program for solving the analysis problem when $\mu$ is a barycenter.
It is shown that these coordinates, as well as the value of the barycenter
functional, can be estimated from samples with dimension-independent rates of
convergence, a hallmark of entropy-regularized optimal transport, and we verify
these rates experimentally. We also establish that barycentric coordinates are
stable with respect to perturbations in the Wasserstein-2 metric, suggesting a
robustness of these coefficients to corruptions. We employ the barycentric
coefficients as features for classification of corrupted point cloud data, and
show that compared to neural network baselines, our approach is more efficient
in small training data regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages. Code to reproduce experiments:
  https://github.com/brendanmallery9/Entropic-Barycenters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pairwise Comparisons without Stochastic Transitivity: Model, Theory and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sze Ming Lee, Yunxiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most statistical models for pairwise comparisons, including the Bradley-Terry
(BT) and Thurstone models and many extensions, make a relatively strong
assumption of stochastic transitivity. This assumption imposes the existence of
an unobserved global ranking among all the players/teams/items and monotone
constraints on the comparison probabilities implied by the global ranking.
However, the stochastic transitivity assumption does not hold in many
real-world scenarios of pairwise comparisons, especially games involving
multiple skills or strategies. As a result, models relying on this assumption
can have suboptimal predictive performance. In this paper, we propose a general
family of statistical models for pairwise comparison data without a stochastic
transitivity assumption, substantially extending the BT and Thurstone models.
In this model, the pairwise probabilities are determined by a (approximately)
low-dimensional skew-symmetric matrix. Likelihood-based estimation methods and
computational algorithms are developed, which allow for sparse data with only a
small proportion of observed pairs. Theoretical analysis shows that the
proposed estimator achieves minimax-rate optimality, which adapts effectively
to the sparsity level of the data. The spectral theory for skew-symmetric
matrices plays a crucial role in the implementation and theoretical analysis.
The proposed method's superiority against the BT model, along with its broad
applicability across diverse scenarios, is further supported by simulations and
real data analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distance Measure Based on an Embedding of the Manifold of K-Component
  Gaussian Mixture Models into the Manifold of Symmetric Positive Definite
  Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Vishwakarma, KS Subrahamanian Moosath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a distance between the Gaussian Mixture Models(GMMs) is
obtained based on an embedding of the K-component Gaussian Mixture Model into
the manifold of the symmetric positive definite matrices. Proof of embedding of
K-component GMMs into the manifold of symmetric positive definite matrices is
given and shown that it is a submanifold. Then, proved that the manifold of
GMMs with the pullback of induced metric is isometric to the submanifold with
the induced metric. Through this embedding we obtain a general lower bound for
the Fisher-Rao metric. This lower bound is a distance measure on the manifold
of GMMs and we employ it for the similarity measure of GMMs. The effectiveness
of this framework is demonstrated through an experiment on standard machine
learning benchmarks, achieving accuracy of 98%, 92%, and 93.33% on the UIUC,
KTH-TIPS, and UMD texture recognition datasets respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVICAD2: Multi-View Independent Component Analysis with Delays and
  Dilations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ambroise Heurtebise, Omar Chehab, Pierre Ablin, Alexandre Gramfort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning techniques in multi-view settings face significant
challenges, particularly when integrating heterogeneous data, aligning feature
spaces, and managing view-specific biases. These issues are prominent in
neuroscience, where data from multiple subjects exposed to the same stimuli are
analyzed to uncover brain activity dynamics. In magnetoencephalography (MEG),
where signals are captured at the scalp level, estimating the brain's
underlying sources is crucial, especially in group studies where sources are
assumed to be similar for all subjects. Common methods, such as Multi-View
Independent Component Analysis (MVICA), assume identical sources across
subjects, but this assumption is often too restrictive due to individual
variability and age-related changes. Multi-View Independent Component Analysis
with Delays (MVICAD) addresses this by allowing sources to differ up to a
temporal delay. However, temporal dilation effects, particularly in auditory
stimuli, are common in brain dynamics, making the estimation of time delays
alone insufficient. To address this, we propose Multi-View Independent
Component Analysis with Delays and Dilations (MVICAD2), which allows sources to
differ across subjects in both temporal delays and dilations. We present a
model with identifiable sources, derive an approximation of its likelihood in
closed form, and use regularization and optimization techniques to enhance
performance. Through simulations, we demonstrate that MVICAD2 outperforms
existing multi-view ICA methods. We further validate its effectiveness using
the Cam-CAN dataset, and showing how delays and dilations are related to aging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Investigation into Seasonal Variations in Energy Forecasting for
  Student Residences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Umair Danish, Mathumitha Sureshkumar, Thanuri Fonseka, Umeshika Uthayakumar, Vinura Galwaduge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research provides an in-depth evaluation of various machine learning
models for energy forecasting, focusing on the unique challenges of seasonal
variations in student residential settings. The study assesses the performance
of baseline models, such as LSTM and GRU, alongside state-of-the-art
forecasting methods, including Autoregressive Feedforward Neural Networks,
Transformers, and hybrid approaches. Special attention is given to predicting
energy consumption amidst challenges like seasonal patterns, vacations,
meteorological changes, and irregular human activities that cause sudden
fluctuations in usage. The findings reveal that no single model consistently
outperforms others across all seasons, emphasizing the need for season-specific
model selection or tailored designs. Notably, the proposed Hyper Network based
LSTM and MiniAutoEncXGBoost models exhibit strong adaptability to seasonal
variations, effectively capturing abrupt changes in energy consumption during
summer months. This study advances the energy forecasting field by emphasizing
the critical role of seasonal dynamics and model-specific behavior in achieving
accurate predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PROTECT: Protein circadian time prediction using unsupervised learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aram Ansary Ogholbake, Qiang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Circadian rhythms regulate the physiology and behavior of humans and animals.
Despite advancements in understanding these rhythms and predicting circadian
phases at the transcriptional level, predicting circadian phases from proteomic
data remains elusive. This challenge is largely due to the scarcity of time
labels in proteomic datasets, which are often characterized by small sample
sizes, high dimensionality, and significant noise. Furthermore, existing
methods for predicting circadian phases from transcriptomic data typically rely
on prior knowledge of known rhythmic genes, making them unsuitable for
proteomic datasets. To address this gap, we developed a novel computational
method using unsupervised deep learning techniques to predict circadian sample
phases from proteomic data without requiring time labels or prior knowledge of
proteins or genes. Our model involves a two-stage training process optimized
for robust circadian phase prediction: an initial greedy one-layer-at-a-time
pre-training which generates informative initial parameters followed by
fine-tuning. During fine-tuning, a specialized loss function guides the model
to align protein expression levels with circadian patterns, enabling it to
accurately capture the underlying rhythmic structure within the data. We tested
our method on both time-labeled and unlabeled proteomic data. For labeled data,
we compared our predictions to the known time labels, achieving high accuracy,
while for unlabeled human datasets, including postmortem brain regions and
urine samples, we explored circadian disruptions. Notably, our analysis
identified disruptions in rhythmic proteins between Alzheimer's disease and
control subjects across these samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Derivation of effective gradient flow equations and dynamical truncation
  of training data in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We derive explicit equations governing the cumulative biases and weights in
Deep Learning with ReLU activation function, based on gradient descent for the
Euclidean cost in the input layer, and under the assumption that the weights
are, in a precise sense, adapted to the coordinate system distinguished by the
activations. We show that gradient descent corresponds to a dynamical process
in the input layer, whereby clusters of data are progressively reduced in
complexity ("truncated") at an exponential rate that increases with the number
of data points that have already been truncated. We provide a detailed
discussion of several types of solutions to the gradient flow equations. A main
motivation for this work is to shed light on the interpretability question in
supervised learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AMS Latex, 35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretic Dual Memory System for Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        RunQing Wu, KaiHui Huang, HanYi Zhang, QiHe Liu, GuoJin Yu, JingSong Deng, Fei Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuously acquiring new knowledge from a dynamic environment is a
fundamental capability for animals, facilitating their survival and ability to
address various challenges. This capability is referred to as continual
learning, which focuses on the ability to learn a sequence of tasks without the
detriment of previous knowledge. A prevalent strategy to tackle continual
learning involves selecting and storing numerous essential data samples from
prior tasks within a fixed-size memory buffer. However, the majority of current
memory-based techniques typically utilize a single memory buffer, which poses
challenges in concurrently managing newly acquired and previously learned
samples. Drawing inspiration from the Complementary Learning Systems (CLS)
theory, which defines rapid and gradual learning mechanisms for processing
information, we propose an innovative dual memory system called the
Information-Theoretic Dual Memory System (ITDMS). This system comprises a fast
memory buffer designed to retain temporary and novel samples, alongside a slow
memory buffer dedicated to preserving critical and informative samples. The
fast memory buffer is optimized employing an efficient reservoir sampling
process. Furthermore, we introduce a novel information-theoretic memory
optimization strategy that selectively identifies and retains diverse and
informative data samples for the slow memory buffer. Additionally, we propose a
novel balanced sample selection procedure that automatically identifies and
eliminates redundant memorized samples, thus freeing up memory capacity for new
data acquisitions, which can deal with a growing array of tasks. Our
methodology is rigorously assessed through a series of continual learning
experiments, with empirical results underscoring the effectiveness of the
proposed system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 9 figures, submitted to Knowledge-Based Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynami-CAL <span class="highlight-title">Graph</span>Net: A Physics-Informed <span class="highlight-title">Graph</span> Neural Network Conserving
  Linear and Angular Momentum for Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinay Sharma, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate, interpretable, and real-time modeling of multi-body dynamical
systems is essential for predicting behaviors and inferring physical properties
in natural and engineered environments. Traditional physics-based models face
scalability challenges and are computationally demanding, while data-driven
approaches like Graph Neural Networks (GNNs) often lack physical consistency,
interpretability, and generalization. In this paper, we propose Dynami-CAL
GraphNet, a Physics-Informed Graph Neural Network that integrates the learning
capabilities of GNNs with physics-based inductive biases to address these
limitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and
angular momentum for interacting nodes using edge-local reference frames that
are equivariant to rotational symmetries, invariant to translations, and
equivariant to node permutations. This design ensures physically consistent
predictions of node dynamics while offering interpretable, edge-wise linear and
angular impulses resulting from pairwise interactions. Evaluated on a 3D
granular system with inelastic collisions, Dynami-CAL GraphNet demonstrates
stable error accumulation over extended rollouts, effective extrapolations to
unseen configurations, and robust handling of heterogeneous interactions and
external forces. Dynami-CAL GraphNet offers significant advantages in fields
requiring accurate, interpretable, and real-time modeling of complex multi-body
dynamical systems, such as robotics, aerospace engineering, and materials
science. By providing physically consistent and scalable predictions that
adhere to fundamental conservation laws, it enables the inference of forces and
moments while efficiently handling heterogeneous interactions and external
forces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating the Hubbard Model with Equivariant Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Schuh, Janik Kreit, Evan Berkowitz, Lena Funcke, Thomas Luu, Kim A. Nicoli, Marcel Rodekamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models, particularly normalizing flows, have shown exceptional
performance in learning probability distributions across various domains of
physics, including statistical mechanics, collider physics, and lattice field
theory. In the context of lattice field theory, normalizing flows have been
successfully applied to accurately learn the Boltzmann distribution, enabling a
range of tasks such as direct estimation of thermodynamic observables and
sampling independent and identically distributed (i.i.d.) configurations.
  In this work, we present a proof-of-concept demonstration that normalizing
flows can be used to learn the Boltzmann distribution for the Hubbard model.
This model is widely employed to study the electronic structure of graphene and
other carbon nanomaterials. State-of-the-art numerical simulations of the
Hubbard model, such as those based on Hybrid Monte Carlo (HMC) methods, often
suffer from ergodicity issues, potentially leading to biased estimates of
physical observables. Our numerical experiments demonstrate that leveraging
i.i.d.\ sampling from the normalizing flow effectively addresses these issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, contribution to the 41st International Symposium
  on Lattice Field Theory (Lattice 2024), July 28th - August 3rd, 2024,
  Liverpool, UK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal semantic <span class="highlight-title">retrie</span>val for product search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Esther Lopez Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic retrieval (also known as dense retrieval) based on textual data has
been extensively studied for both web search and product search application
fields, where the relevance of a query and a potential target document is
computed by their dense vector representation comparison. Product image is
crucial for e-commence search interactions and is a key factor for customers at
product explorations. But its impact for semantic retrieval has not been well
studied yet. In this research, we build a multimodal representation for product
items in e-commerece search in contrast to pure-text representation of
products, and investigate the impact of such representations. The models are
developed and evaluated on e-commerce datasets. We demonstrate that a
multimodal representation scheme for a product can show improvement either on
purchase recall or relevance accuracy in semantic retrieval. Additionally, we
provide numerical analysis for exclusive matches retrieved by a multimodal
semantic retrieval model versus a text-only semantic retrieval model, to
demonstrate the validation of multimodal solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TimberVision: A Multi-Task <span class="highlight-title">Dataset</span> and Framework for Log-Component
  Segmentation and Tracking in Autonomous Forestry Operations <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Steininger, Julia Simon, Andreas Trondl, Markus Murschitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timber represents an increasingly valuable and versatile resource. However,
forestry operations such as harvesting, handling and measuring logs still
require substantial human labor in remote environments posing significant
safety risks. Progressively automating these tasks has the potential of
increasing their efficiency as well as safety, but requires an accurate
detection of individual logs as well as live trees and their context. Although
initial approaches have been proposed for this challenging application domain,
specialized data and algorithms are still too scarce to develop robust
solutions. To mitigate this gap, we introduce the TimberVision dataset,
consisting of more than 2k annotated RGB images containing a total of 51k trunk
components including cut and lateral surfaces, thereby surpassing any existing
dataset in this domain in terms of both quantity and detail by a large margin.
Based on this data, we conduct a series of ablation experiments for oriented
object detection and instance segmentation and evaluate the influence of
multiple scene parameters on model performance. We introduce a generic
framework to fuse the components detected by our models for both tasks into
unified trunk representations. Furthermore, we automatically derive geometric
properties and apply multi-object tracking to further enhance robustness. Our
detection and tracking approach provides highly descriptive and accurate trunk
representations solely from RGB image data, even under challenging
environmental conditions. Our solution is suitable for a wide range of
application scenarios and can be readily combined with other sensor modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Winter Conference on Applications of Computer Vision
  (WACV) 2025. Code and dataset available at
  https://github.com/timbervision/timbervision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Generative Clustering with VAEs and Expectation-Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Adipoetra, Ségolène Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel deep clustering method that integrates Variational
Autoencoders (VAEs) into the Expectation-Maximization (EM) framework. Our
approach models the probability distribution of each cluster with a VAE and
alternates between updating model parameters by maximizing the Evidence Lower
Bound (ELBO) of the log-likelihood and refining cluster assignments based on
the learned distributions. This enables effective clustering and generation of
new samples from each cluster. Unlike existing VAE-based methods, our approach
eliminates the need for a Gaussian Mixture Model (GMM) prior or additional
regularization techniques. Experiments on MNIST and FashionMNIST demonstrate
superior clustering performance compared to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Online Reinforcement Learning with Meta-Learned Objective from
  Offline Data <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Deng, Zetao Zheng, Hongcai He, Paul Weng, Jie Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in Reinforcement Learning (RL) is the difficulty of
learning an optimal policy from sparse rewards. Prior works enhance online RL
with conventional Imitation Learning (IL) via a handcrafted auxiliary
objective, at the cost of restricting the RL policy to be sub-optimal when the
offline data is generated by a non-expert policy. Instead, to better leverage
valuable information in offline data, we develop Generalized Imitation Learning
from Demonstration (GILD), which meta-learns an objective that distills
knowledge from offline data and instills intrinsic motivation towards the
optimal policy. Distinct from prior works that are exclusive to a specific RL
algorithm, GILD is a flexible module intended for diverse vanilla off-policy RL
algorithms. In addition, GILD introduces no domain-specific hyperparameter and
minimal increase in computational cost. In four challenging MuJoCo tasks with
sparse rewards, we show that three RL algorithms enhanced with GILD
significantly outperform state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025 (this version includes supplementary material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Operating Mode Classification of Real-World Amateur Radio
  Transmissions <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Bundscherer, Thomas H. Schmitt, Ilja Baumann, Tobias Bocklet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents an ML approach for classifying digital radio operating
modes evaluated on real-world transmissions. We generated 98 different
parameterized radio signals from 17 digital operating modes, transmitted each
of them on the 70 cm (UHF) amateur radio band, and recorded our transmissions
with two different architectures of SDR receivers. Three lightweight ML models
were trained exclusively on spectrograms of limited non-transmitted signals
with random characters as payloads. This training involved an online data
augmentation pipeline to simulate various radio channel impairments. Our best
model, EfficientNetB0, achieved an accuracy of 93.80% across the 17 operating
modes and 85.47% across all 98 parameterized radio signals, evaluated on our
real-world transmissions with Wikipedia articles as payloads. Furthermore, we
analyzed the impact of varying signal durations & the number of FFT bins on
classification, assessed the effectiveness of our simulated channel
impairments, and tested our models across multiple simulated SNRs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference IEEE ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TempoGPT: Enhancing Temporal Reasoning via Quantizing Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochuan Zhang, Chunhua Yang, Jie Han, Liyang Qin, Xiaoli Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal language model has made advanced progress in vision and audio,
but still faces significant challenges in dealing with complex reasoning tasks
in the time series domain. The reasons are twofold. First, labels for
multi-modal time series data are coarse and devoid of analysis or reasoning
processes. Training with these data cannot improve the model's reasoning
capabilities. Second, due to the lack of precise tokenization in processing
time series, the representation patterns for temporal and textual information
are inconsistent, which hampers the effectiveness of multi-modal alignment. To
address these challenges, we propose a multi-modal time series data
construction approach and a multi-modal time series language model (TLM),
TempoGPT. Specially, we construct multi-modal data for complex reasoning tasks
by analyzing the variable-system relationships within a white-box system.
Additionally, proposed TempoGPT achieves consistent representation between
temporal and textual information by quantizing temporal embeddings, where
temporal embeddings are quantized into a series of discrete tokens using a
predefined codebook; subsequently, a shared embedding layer processes both
temporal and textual tokens. Extensive experiments demonstrate that TempoGPT
accurately perceives temporal information, logically infers conclusions, and
achieves state-of-the-art in the constructed complex time series reasoning
tasks. Moreover, we quantitatively demonstrate the effectiveness of quantizing
temporal embeddings in enhancing multi-modal alignment and the reasoning
capabilities of TLMs. Code and data are available at
https://github.com/zhanghaochuan20/TempoGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Foundation</span> Models at Work: Fine-Tuning for Fairness in Algorithmic
  Hiring <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Buse Sibel Korkmaz, Rahul Nair, Elizabeth M. Daly, Evangelos Anagnostopoulos, Christos Varytimidis, Antonio del Rio Chanona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models require fine-tuning to ensure their generative outputs
align with intended results for specific tasks. Automating this fine-tuning
process is challenging, as it typically needs human feedback that can be
expensive to acquire. We present AutoRefine, a method that leverages
reinforcement learning for targeted fine-tuning, utilizing direct feedback from
measurable performance improvements in specific downstream tasks. We
demonstrate the method for a problem arising in algorithmic hiring platforms
where linguistic biases influence a recommendation system. In this setting, a
generative model seeks to rewrite given job specifications to receive more
diverse candidate matches from a recommendation engine which matches jobs to
candidates. Our model detects and regulates biases in job descriptions to meet
diversity and fairness criteria. The experiments on a public hiring dataset and
a real-world hiring platform showcase how large language models can assist in
identifying and mitigation biases in the real world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025, AI Governance Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Artificial Intelligence Methods for Lead Time Prediction
  in Non-Cycled Areas of Automotive Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cornelius Hake, Jonas Weigele, Frederik Reichert, Christian Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The present study examines the effectiveness of applying Artificial
Intelligence methods in an automotive production environment to predict unknown
lead times in a non-cycle-controlled production area. Data structures are
analyzed to identify contextual features and then preprocessed using one-hot
encoding. Methods selection focuses on supervised machine learning techniques.
In supervised learning methods, regression and classification methods are
evaluated. Continuous regression based on target size distribution is not
feasible. Classification methods analysis shows that Ensemble Learning and
Support Vector Machines are the most suitable. Preliminary study results
indicate that gradient boosting algorithms LightGBM, XGBoost, and CatBoost
yield the best results. After further testing and extensive hyperparameter
optimization, the final method choice is the LightGBM algorithm. Depending on
feature availability and prediction interval granularity, relative prediction
accuracies of up to 90% can be achieved. Further tests highlight the importance
of periodic retraining of AI models to accurately represent complex production
processes using the database. The research demonstrates that AI methods can be
effectively applied to highly variable production data, adding business value
by providing an additional metric for various control tasks while outperforming
current non AI-based systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, CLC2024 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variable Bregman Majorization-Minimization Algorithm and its Application
  to Dirichlet Maximum Likelihood Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ségolène Martin, Jean-Christophe Pesquet, Gabriele Steidl, Ismail Ben Ayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel Bregman descent algorithm for minimizing a convex function
that is expressed as the sum of a differentiable part (defined over an open
set) and a possibly nonsmooth term. The approach, referred to as the Variable
Bregman Majorization-Minimization (VBMM) algorithm, extends the Bregman
Proximal Gradient method by allowing the Bregman function used in the
divergence to adaptively vary at each iteration, provided it satisfies a
majorizing condition on the objective function. This adaptive framework enables
the algorithm to approximate the objective more precisely at each iteration,
thereby allowing for accelerated convergence compared to the traditional
Bregman Proximal Gradient descent. We establish the convergence of the VBMM
algorithm to a minimizer under mild assumptions on the family of metrics used.
Furthermore, we introduce a novel application of both the Bregman Proximal
Gradient method and the VBMM algorithm to the estimation of the
multidimensional parameters of a Dirichlet distribution through the
maximization of its log-likelihood. Numerical experiments confirm that the VBMM
algorithm outperforms existing approaches in terms of convergence speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code and Pixels: Multi-Modal Contrastive <span class="highlight-title">Pre-train</span>ing for Enhanced
  <span class="highlight-title">Tabular</span> Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kankana Roy, Lars Krämer, Sebastian Domaschke, Malik Haris, Roland Aydin, Fabian Isensee, Martin Held
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from tabular data is of paramount importance, as it complements the
conventional analysis of image and video data by providing a rich source of
structured information that is often critical for comprehensive understanding
and decision-making processes. We present Multi-task Contrastive Masked Tabular
Modeling (MT-CMTM), a novel method aiming to enhance tabular models by
leveraging the correlation between tabular data and corresponding images.
MT-CMTM employs a dual strategy combining contrastive learning with masked
tabular modeling, optimizing the synergy between these data modalities.
  Central to our approach is a 1D Convolutional Neural Network with residual
connections and an attention mechanism (1D-ResNet-CBAM), designed to
efficiently process tabular data without relying on images. This enables
MT-CMTM to handle purely tabular data for downstream tasks, eliminating the
need for potentially costly image acquisition and processing.
  We evaluated MT-CMTM on the DVM car dataset, which is uniquely suited for
this particular scenario, and the newly developed HIPMP dataset, which connects
membrane fabrication parameters with image data. Our MT-CMTM model outperforms
the proposed tabular 1D-ResNet-CBAM, which is trained from scratch, achieving a
relative 1.48% improvement in relative MSE on HIPMP and a 2.38% increase in
absolute accuracy on DVM. These results demonstrate MT-CMTM's robustness and
its potential to advance the field of multi-modal learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Lessons of Developing Process Reward Models in Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process Reward Models (PRMs) emerge as a promising approach for process
supervision in mathematical reasoning of Large Language Models (LLMs), which
aim to identify and mitigate intermediate errors in the reasoning processes.
However, the development of effective PRMs faces significant challenges,
particularly in data annotation and evaluation methodologies. In this paper,
through extensive experiments, we demonstrate that commonly used Monte Carlo
(MC) estimation-based data synthesis for PRMs typically yields inferior
performance and generalization compared to LLM-as-a-judge and human annotation
methods. MC estimation relies on completion models to evaluate current-step
correctness, leading to inaccurate step verification. Furthermore, we identify
potential biases in conventional Best-of-N (BoN) evaluation strategies for
PRMs: (1) The unreliable policy models generate responses with correct answers
but flawed processes, leading to a misalignment between the evaluation criteria
of BoN and the PRM objectives of process verification. (2) The tolerance of
PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a
significant proportion of minimum scores concentrated on the final answer
steps, revealing the shift from process to outcome-based assessment in BoN
Optimized PRMs. To address these challenges, we develop a consensus filtering
mechanism that effectively integrates MC estimation with LLM-as-a-judge and
advocates a more comprehensive evaluation framework that combines
response-level and step-level metrics. Based on the mechanisms, we
significantly improve both model performance and data efficiency in the BoN
evaluation and the step-wise error identification task. Finally, we release a
new state-of-the-art PRM that outperforms existing open-source alternatives and
provides practical guidelines for future research in building process
supervision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span>-Agnostic <span class="highlight-title">Recommend</span>er Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tri Kurniawan Wijaya, Edoardo D'Amico, Xinyang Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  [This is a position paper and does not contain any empirical or theoretical
results] Recommender systems have become a cornerstone of personalized user
experiences, yet their development typically involves significant manual
intervention, including dataset-specific feature engineering, hyperparameter
tuning, and configuration. To this end, we introduce a novel paradigm:
Dataset-Agnostic Recommender Systems (DAReS) that aims to enable a single
codebase to autonomously adapt to various datasets without the need for
fine-tuning, for a given recommender system task. Central to this approach is
the Dataset Description Language (DsDL), a structured format that provides
metadata about the dataset's features and labels, and allow the system to
understand dataset's characteristics, allowing it to autonomously manage
processes like feature selection, missing values imputation, noise removal, and
hyperparameter optimization. By reducing the need for domain-specific expertise
and manual adjustments, DAReS offers a more efficient and scalable solution for
building recommender systems across diverse application domains. It addresses
critical challenges in the field, such as reusability, reproducibility, and
accessibility for non-expert users or entry-level researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating quantum relative entropies on quantum computers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Lu, Kun Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum relative entropy, a quantum generalization of the well-known
Kullback-Leibler divergence, serves as a fundamental measure of the
distinguishability between quantum states and plays a pivotal role in quantum
information science. Despite its importance, efficiently estimating quantum
relative entropy between two quantum states on quantum computers remains a
significant challenge. In this work, we propose the first quantum algorithm for
estimating quantum relative entropy and Petz R\'{e}nyi divergence from two
unknown quantum states on quantum computers, addressing open problems
highlighted in [Phys. Rev. A 109, 032431 (2024)] and [IEEE Trans. Inf. Theory
70, 5653-5680 (2024)]. This is achieved by combining quadrature approximations
of relative entropies, the variational representation of quantum f-divergences,
and a new technique for parameterizing Hermitian polynomial operators to
estimate their traces with quantum states. Notably, the circuit size of our
algorithm is at most 2n+1 with n being the number of qubits in the quantum
states and it is directly applicable to distributed scenarios, where quantum
states to be compared are hosted on cross-platform quantum computers. We
validate our algorithm through numerical simulations, laying the groundwork for
its future deployment on quantum hardware devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures; comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Smart Meter Gaps: A Benchmark of Statistical, Machine Learning
  and Time Series <span class="highlight-title">Foundation</span> Models for Data Imputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Sartipi, Joaquin Delgado Fernandez, Sergio Potenciano Menci, Alessio Magitteri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integrity of time series data in smart grids is often compromised by
missing values due to sensor failures, transmission errors, or disruptions.
Gaps in smart meter data can bias consumption analyses and hinder reliable
predictions, causing technical and economic inefficiencies. As smart meter data
grows in volume and complexity, conventional techniques struggle with its
nonlinear and nonstationary patterns. In this context, Generative Artificial
Intelligence offers promising solutions that may outperform traditional
statistical methods. In this paper, we evaluate two general-purpose Large
Language Models and five Time Series Foundation Models for smart meter data
imputation, comparing them with conventional Machine Learning and statistical
models. We introduce artificial gaps (30 minutes to one day) into an anonymized
public dataset to test inference capabilities. Results show that Time Series
Foundation Models, with their contextual understanding and pattern recognition,
could significantly enhance imputation accuracy in certain cases. However, the
trade-off between computational cost and performance gains remains a critical
consideration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Poisoning Attacks against Ridge Regression Models with
  Categorical Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monse Guedes-Ayala, Lars Schewe, Zeynep Suvak, Miguel Anjos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) models have become a very powerful tool to extract
information from large datasets and use it to make accurate predictions and
automated decisions. However, ML models can be vulnerable to external attacks,
causing them to underperform or deviate from their expected tasks. One way to
attack ML models is by injecting malicious data to mislead the algorithm during
the training phase, which is referred to as a poisoning attack. We can prepare
for such situations by designing anticipated attacks, which are later used for
creating and testing defence strategies. In this paper, we propose an algorithm
to generate strong poisoning attacks for a ridge regression model containing
both numerical and categorical features that explicitly models and poisons
categorical features. We model categorical features as SOS-1 sets and formulate
the problem of designing poisoning attacks as a bilevel optimization problem
that is nonconvex mixed-integer in the upper-level and unconstrained convex
quadratic in the lower-level. We present the mathematical formulation of the
problem, introduce a single-level reformulation based on the Karush-Kuhn-Tucker
(KKT) conditions of the lower level, find bounds for the lower-level variables
to accelerate solver performance, and propose a new algorithm to poison
categorical features. Numerical experiments show that our method improves the
mean squared error of all datasets compared to the previous benchmark in the
literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOS-Attack: A Scalable Multi-objective Adversarial Attack Framework <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Guo, Cheng Gong, Xi Lin, Fei Liu, Zhichao Lu, Qingfu Zhang, Zhenkun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crafting adversarial examples is crucial for evaluating and enhancing the
robustness of Deep Neural Networks (DNNs), presenting a challenge equivalent to
maximizing a non-differentiable 0-1 loss function.
  However, existing single objective methods, namely adversarial attacks focus
on a surrogate loss function, do not fully harness the benefits of engaging
multiple loss functions, as a result of insufficient understanding of their
synergistic and conflicting nature.
  To overcome these limitations, we propose the Multi-Objective Set-based
Attack (MOS Attack), a novel adversarial attack framework leveraging multiple
loss functions and automatically uncovering their interrelations.
  The MOS Attack adopts a set-based multi-objective optimization strategy,
enabling the incorporation of numerous loss functions without additional
parameters.
  It also automatically mines synergistic patterns among various losses,
facilitating the generation of potent adversarial attacks with fewer
objectives.
  Extensive experiments have shown that our MOS Attack outperforms
single-objective attacks. Furthermore, by harnessing the identified synergistic
patterns, MOS Attack continues to show superior results with a reduced number
of loss functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review of CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpre<span class="highlight-title">table</span> machine-learning for predicting molecular weight of PLA
  based on artificial bee colony optimization algorithm and adaptive neurofuzzy
  inference system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Pouya Masoumi, Leo Creedon, Ramen Ghosh, Nimra Munir, Ross McMorrow, Marion McAfee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article discusses the integration of the Artificial Bee Colony (ABC)
algorithm with two supervised learning methods, namely Artificial Neural
Networks (ANNs) and Adaptive Network-based Fuzzy Inference System (ANFIS), for
feature selection from Near-Infrared (NIR) spectra for predicting the molecular
weight of medical-grade Polylactic Acid (PLA). During extrusion processing of
PLA, in-line NIR spectra were captured along with extrusion process and machine
setting data. With a dataset comprising 63 observations and 512 input features,
appropriate machine learning tools are essential for interpreting data and
selecting features to improve prediction accuracy. Initially, the ABC
optimization algorithm is coupled with ANN/ANFIS to forecast PLA molecular
weight. The objective functions of the ABC algorithm are to minimize the root
mean square error (RMSE) between experimental and predicted PLA molecular
weights while also minimizing the number of input features. Results indicate
that employing ABC-ANFIS yields the lowest RMSE of 282 Da and identifies four
significant parameters (NIR wavenumbers 6158 cm-1, 6310 cm-1, 6349 cm-1, and
melt temperature) for prediction. These findings demonstrate the effectiveness
of using the ABC algorithm with ANFIS for selecting a minimal set of features
to predict PLA molecular weight with high accuracy during processing
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqing Wen, Ping Luo, Jiahuan Wang, Xiaoge Deng, Jinping Zou, Kun Yuan, Tao Sun, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive performance across a range
of natural language processing tasks. However, their vast number of parameters
introduces significant memory challenges during training, particularly when
using memory-intensive optimizers like Adam. Existing memory-efficient
algorithms often rely on techniques such as singular value decomposition
projection or weight freezing. While these approaches help alleviate memory
constraints, they generally produce suboptimal results compared to full-rank
updates. In this paper, we investigate the memory-efficient method beyond
low-rank training, proposing a novel solution called Gradient Wavelet Transform
(GWT), which applies wavelet transforms to gradients in order to significantly
reduce the memory requirements for maintaining optimizer states. We demonstrate
that GWT can be seamlessly integrated with memory-intensive optimizers,
enabling efficient training without sacrificing performance. Through extensive
experiments on both pre-training and fine-tuning tasks, we show that GWT
achieves state-of-the-art performance compared with advanced memory-efficient
optimizers and full-rank approaches in terms of both memory usage and training
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A data-driven approach to discover and quantify systemic lupus
  erythematosus etiological heterogeneity from electronic health records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Barbero Mota, John M. Still, Jorge L. Gamboa, Eric V. Strobl, Charles M. Stein, Vivian K. Kawai, Thomas A. Lasko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systemic lupus erythematosus (SLE) is a complex heterogeneous disease with
many manifestational facets. We propose a data-driven approach to discover
probabilistic independent sources from multimodal imperfect EHR data. These
sources represent exogenous variables in the data generation process causal
graph that estimate latent root causes of the presence of SLE in the health
record. We objectively evaluated the sources against the original variables
from which they were discovered by training supervised models to discriminate
SLE from negative health records using a reduced set of labelled instances. We
found 19 predictive sources with high clinical validity and whose EHR
signatures define independent factors of SLE heterogeneity. Using the sources
as input patient data representation enables models to provide with rich
explanations that better capture the clinical reasons why a particular record
is (not) an SLE case. Providers may be willing to trade patient-level
interpretability for discrimination especially in challenging cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Received Runner-up Knowledge Discovery and Data Mining Innovation
  Award at the American Medical Informatics Association Annual Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Enhanced Zeroth-Order Stochastic Frank-Wolfe Framework for
  Constrained Finite-Sum Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haishan Ye, Yinghui Huang, Hao Di, Xiangyu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an enhanced zeroth-order stochastic Frank-Wolfe framework to
address constrained finite-sum optimization problems, a structure prevalent in
large-scale machine-learning applications. Our method introduces a novel double
variance reduction framework that effectively reduces the gradient
approximation variance induced by zeroth-order oracles and the stochastic
sampling variance from finite-sum objectives. By leveraging this framework, our
algorithm achieves significant improvements in query efficiency, making it
particularly well-suited for high-dimensional optimization tasks. Specifically,
for convex objectives, the algorithm achieves a query complexity of O(d
\sqrt{n}/\epsilon ) to find an epsilon-suboptimal solution, where d is the
dimensionality and n is the number of functions in the finite-sum objective.
For non-convex objectives, it achieves a query complexity of
O(d^{3/2}\sqrt{n}/\epsilon^2 ) without requiring the computation ofd partial
derivatives at each iteration. These complexities are the best known among
zeroth-order stochastic Frank-Wolfe algorithms that avoid explicit gradient
calculations. Empirical experiments on convex and non-convex machine learning
tasks, including sparse logistic regression, robust classification, and
adversarial attacks on deep networks, validate the computational efficiency and
scalability of our approach. Our algorithm demonstrates superior performance in
both convergence rate and query complexity compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lung Cancer detection using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Chaudhari, Ankush Singh, Sanchi Gajbhiye, Pratham Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we discuss lung cancer detection using hybrid model of
Convolutional-Neural-Networks (CNNs) and Support-Vector-Machines-(SVMs) in
order to gain early detection of tumors, benign or malignant. The work uses
this hybrid model by training upon the Computed Tomography scans (CT scans) as
dataset. Using deep learning for detecting lung cancer early is a cutting-edge
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-Train</span>ed Large Language Model Based Remaining Useful Life Transfer
  Prediction of Bearing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laifa Tao, Zhengduo Zhao, Xuesong Wang, Bin Li, Wenchao Zhan, Xuanyuan Su, Shangyu Li, Qixuan Huang, Haifei Liu, Chen Lu, Zhixuan Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting the remaining useful life (RUL) of rotating machinery,
such as bearings, is essential for ensuring equipment reliability and
minimizing unexpected industrial failures. Traditional data-driven deep
learning methods face challenges in practical settings due to inconsistent
training and testing data distributions and limited generalization for
long-term predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable <span class="highlight-title">Graph</span> Neural Networks for Robust Power Grid Topology
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthijs de Jong, Jan Viebahn, Yuliya Shapovalova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The energy transition necessitates new congestion management methods. One
such method is controlling the grid topology with machine learning (ML). This
approach has gained popularity following the Learning to Run a Power Network
(L2RPN) competitions. Graph neural networks (GNNs) are a class of ML models
that reflect graph structure in their computation, which makes them suitable
for power grid modeling. Various GNN approaches for topology control have thus
been proposed. We propose the first GNN model for grid topology control that
uses only GNN layers. Additionally, we identify the busbar information
asymmetry problem that the popular homogeneous graph representation suffers
from, and propose a heterogeneous graph representation to resolve it. We train
both homogeneous and heterogeneous GNNs and fully connected neural networks
(FCNN) baselines on an imitation learning task. We evaluate the models
according to their classification accuracy and grid operation ability. We find
that the heterogeneous GNNs perform best on in-distribution networks, followed
by the FCNNs, and lastly, the homogeneous GNNs. We also find that both GNN
types generalize better to out-of-distribution networks than FCNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Guarantees on Automated Precision Weeding using Conformal
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Melki, Lionel Bombrun, Boubacar Diallo, Jérôme Dias, Jean-Pierre da Costa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision agriculture in general, and precision weeding in particular, have
greatly benefited from the major advancements in deep learning and computer
vision. A large variety of commercial robotic solutions are already available
and deployed. However, the adoption by farmers of such solutions is still low
for many reasons, an important one being the lack of trust in these systems.
This is in great part due to the opaqueness and complexity of deep neural
networks and the manufacturers' inability to provide valid guarantees on their
performance. Conformal prediction, a well-established methodology in the
machine learning community, is an efficient and reliable strategy for providing
trustworthy guarantees on the predictions of any black-box model under very
minimal constraints. Bridging the gap between the safe machine learning and
precision agriculture communities, this article showcases conformal prediction
in action on the task of precision weeding through deep learning-based image
classification. After a detailed presentation of the conformal prediction
methodology and the development of a precision spraying pipeline based on a
''conformalized'' neural network and well-defined spraying decision rules, the
article evaluates this pipeline on two real-world scenarios: one under
in-distribution conditions, the other reflecting a near out-of-distribution
setting. The results show that we are able to provide formal, i.e. certifiable,
guarantees on spraying at least 90% of the weeds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Distillation and Enhanced Subdomain Adaptation Using <span class="highlight-title">Graph</span>
  Convolutional Network for Resource-Constrained Bearing Fault Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Kavianpour, Parisa Kavianpour, Amin Ramezani, Mohammad TH Beheshti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bearing fault diagnosis under varying working conditions faces challenges,
including a lack of labeled data, distribution discrepancies, and resource
constraints. To address these issues, we propose a progressive knowledge
distillation framework that transfers knowledge from a complex teacher model,
utilizing a Graph Convolutional Network (GCN) with Autoregressive moving
average (ARMA) filters, to a compact and efficient student model. To mitigate
distribution discrepancies and labeling uncertainty, we introduce Enhanced
Local Maximum Mean Squared Discrepancy (ELMMSD), which leverages mean and
variance statistics in the Reproducing Kernel Hilbert Space (RKHS) and
incorporates a priori probability distributions between labels. This approach
increases the distance between clustering centers, bridges subdomain gaps, and
enhances subdomain alignment reliability. Experimental results on benchmark
datasets (CWRU and JNU) demonstrate that the proposed method achieves superior
diagnostic accuracy while significantly reducing computational costs.
Comprehensive ablation studies validate the effectiveness of each component,
highlighting the robustness and adaptability of the approach across diverse
working conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anomalous Agreement: How to find the Ideal Number of Anomaly Classes in
  Correlated, Multivariate Time Series Data <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferdinand Rewicki, Joachim Denzler, Julia Niebling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and classifying abnormal system states is critical for condition
monitoring, but supervised methods often fall short due to the rarity of
anomalies and the lack of labeled data. Therefore, clustering is often used to
group similar abnormal behavior. However, evaluating cluster quality without
ground truth is challenging, as existing measures such as the Silhouette Score
(SSC) only evaluate the cohesion and separation of clusters and ignore possible
prior knowledge about the data. To address this challenge, we introduce the
Synchronized Anomaly Agreement Index (SAAI), which exploits the synchronicity
of anomalies across multivariate time series to assess cluster quality. We
demonstrate the effectiveness of SAAI by showing that maximizing SAAI improves
accuracy on the task of finding the true number of anomaly classes K in
correlated time series by 0.23 compared to SSC and by 0.32 compared to X-Means.
We also show that clusters obtained by maximizing SAAI are easier to interpret
compared to SSC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acccepted at AAAI Workshop on AI for Time Series Analysis (AI4TS)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlphaNet: Scaling Up Local Frame-based Atomistic <span class="highlight-title">Foundation</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangchen Yin, Jiaao Wang, Weitao Du, Pengbo Wang, Penghua Ying, Haojun Jia, Zisheng Zhang, Yuanqi Du, Carla P. Gomes, Chenru Duan, Hai Xiao, Graeme Henkelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AlphaNet, a local frame-based equivariant model designed to
achieve both accurate and efficient simulations for atomistic systems.
Recently, machine learning force fields (MLFFs) have gained prominence in
molecular dynamics simulations due to their advantageous efficiency-accuracy
balance compared to classical force fields and quantum mechanical calculations,
alongside their transferability across various systems. Despite the
advancements in improving model accuracy, the efficiency and scalability of
MLFFs remain significant obstacles in practical applications. AlphaNet enhances
computational efficiency and accuracy by leveraging the local geometric
structures of atomic environments through the construction of equivariant local
frames and learnable frame transitions. We substantiate the efficacy of
AlphaNet across diverse datasets, including defected graphene, formate
decomposition, zeolites, and surface reactions. AlphaNet consistently surpasses
well-established models, such as NequIP and DeepPot, in terms of both energy
and force prediction accuracy. Notably, AlphaNet offers one of the best
trade-offs between computational efficiency and accuracy among existing models.
Moreover, AlphaNet exhibits scalability across a broad spectrum of system and
dataset sizes, affirming its versatility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIMRL: A Novel Meta-Reinforcement Learning Framework for Non-Stationary
  and Multi-Task Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Qi, Huiping Li, Panfeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, meta-reinforcement learning (meta-RL) algorithm has been
proposed to improve sample efficiency in the field of decision-making and
control, enabling agents to learn new knowledge from a small number of samples.
However, most research uses the Gaussian distribution to extract task
representation, which is poorly adapted to tasks that change in non-stationary
environment. To address this problem, we propose a novel meta-reinforcement
learning method by leveraging Gaussian mixture model and the transformer
network to construct task inference model. The Gaussian mixture model is
utilized to extend the task representation and conduct explicit encoding of
tasks. Specifically, the classification of tasks is encoded through transformer
network to determine the Gaussian component corresponding to the task. By
leveraging task labels, the transformer network is trained using supervised
learning. We validate our method on MuJoCo benchmarks with non-stationary and
multi-task environments. Experimental results demonstrate that the proposed
method dramatically improves sample efficiency and accurately recognizes the
classification of the tasks, while performing excellently in the environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A User's Guide to $\texttt{KSig}$: GPU-Accelerated Computation of the
  Signature Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Csaba Tóth, Danilo Jr Dela Cruz, Harald Oberhauser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The signature kernel is a positive definite kernel for sequential and
temporal data that has become increasingly popular in machine learning
applications due to powerful theoretical guarantees, strong empirical
performance, and recently introduced various scalable variations. In this
chapter, we give a short introduction to $\texttt{KSig}$, a
$\texttt{Scikit-Learn}$ compatible Python package that implements various
GPU-accelerated algorithms for computing signature kernels, and performing
downstream learning tasks. We also introduce a new algorithm based on tensor
sketches which gives strong performance compared to existing algorithms. The
package is available at
$\href{https://github.com/tgcsaba/ksig}{\texttt{https://github.com/tgcsaba/ksig}}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM360 K2: Scaling Up 360-Open-Source Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengzhong Liu, Bowen Tan, Hongyi Wang, Willie Neiswanger, Tianhua Tao, Haonan Li, Fajri Koto, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Liqun Ma, Liping Tang, Nikhil Ranjan, Yonghao Zhuang, Guowei He, Renxi Wang, Mingkai Deng, Robin Algayres, Yuanzhi Li, Zhiqiang Shen, Preslav Nakov, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We detail the training of the LLM360 K2-65B model, scaling up our 360-degree
OPEN SOURCE approach to the largest and most powerful models under project
LLM360. While open-source LLMs continue to advance, the answer to "How are the
largest LLMs trained?" remains unclear within the community. The implementation
details for such high-capacity models are often protected due to business
considerations associated with their high cost. This lack of transparency
prevents LLM researchers from leveraging valuable insights from prior
experience, e.g., "What are the best practices for addressing loss spikes?" The
LLM360 K2 project addresses this gap by providing full transparency and access
to resources accumulated during the training of LLMs at the largest scale. This
report highlights key elements of the K2 project, including our first model, K2
DIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals
LLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the
implementation steps and present a longitudinal analysis of K2 DIAMOND's
capabilities throughout its training process. We also outline ongoing projects
such as TXT360, setting the stage for future models in the series. By offering
previously unavailable resources, the K2 project also resonates with the
360-degree OPEN SOURCE principles of transparency, reproducibility, and
accessibility, which we believe are vital in the era of resource-intensive AI
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring Interpre<span class="highlight-title">table</span> Models of Fragmentation Functions using Symbolic
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nour Makke, Sanjay Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is rapidly making its path into natural sciences, including
high-energy physics. We present the first study that infers, directly from
experimental data, a functional form of fragmentation functions. The latter
represent a key ingredient to describe physical observables measured in
high-energy physics processes that involve hadron production, and predict their
values at different energy. Fragmentation functions can not be calculated in
theory and have to be determined instead from data. Traditional approaches rely
on global fits of experimental data using a pre-assumed functional form
inspired from phenomenological models to learn its parameters. This novel
approach uses a ML technique, namely symbolic regression, to learn an
analytical model from measured charged hadron multiplicities. The function
learned by symbolic regression resembles the Lund string function and describes
the data well, thus representing a potential candidate for use in global FFs
fits. This study represents an approach to follow in such QCD-related
phenomenology studies and more generally in sciences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SecAlign: Defending Against <span class="highlight-title">Prompt</span> Injection with Preference
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sizhe Chen, Arman Zharmagambetov, Saeed Mahloujifar, Kamalika Chaudhuri, David Wagner, Chuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are becoming increasingly prevalent in modern
software systems, interfacing between the user and the Internet to assist with
tasks that require advanced language understanding. To accomplish these tasks,
the LLM often uses external data sources such as user documents, web retrieval,
results from API calls, etc. This opens up new avenues for attackers to
manipulate the LLM via prompt injection. Adversarial prompts can be injected
into external data sources to override the system's intended instruction and
instead execute a malicious instruction.
  To mitigate this vulnerability, we propose a new defense called SecAlign
based on the technique of preference optimization. Our defense first constructs
a preference dataset with prompt-injected inputs, secure outputs (ones that
respond to the legitimate instruction), and insecure outputs (ones that respond
to the injection). We then perform preference optimization on this dataset to
teach the LLM to prefer the secure output over the insecure one. This provides
the first known method that reduces the success rates of various prompt
injections to around 0%, even against attacks much more sophisticated than ones
seen during training. This indicates our defense generalizes well against
unknown and yet-to-come attacks. Also, our defended models are still practical
with similar utility to the one before our defensive training. Our code is at
https://github.com/facebookresearch/SecAlign
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Key words: prompt injection defense, LLM security, LLM-integrated
  applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Task Learning through Inverse Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviv Netanyahu, Yilun Du, Antonia Bronars, Jyothish Pari, Joshua Tenenbaum, Tianmin Shu, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning the intents of an agent, defined by its goals or motion style, is
often extremely challenging from just a few examples. We refer to this problem
as task concept learning and present our approach, Few-Shot Task Learning
through Inverse Generative Modeling (FTL-IGM), which learns new task concepts
by leveraging invertible neural generative models. The core idea is to pretrain
a generative model on a set of basic concepts and their demonstrations. Then,
given a few demonstrations of a new concept (such as a new goal or a new
action), our method learns the underlying concepts through backpropagation
without updating the model weights, thanks to the invertibility of the
generative model. We evaluate our method in five domains -- object
rearrangement, goal-oriented navigation, motion caption of human actions,
autonomous driving, and real-world table-top manipulation. Our experimental
results demonstrate that via the pretrained generative model, we successfully
learn novel concepts and generate agent plans or motion corresponding to these
concepts in (1) unseen environments and (2) in composition with training
concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added acknowledgment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Performance of Echo State Networks Through State Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter J. Ehlers, Hendra I. Nurdin, Daniel Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reservoir computing, using nonlinear dynamical systems, offers a
cost-effective alternative to neural networks for complex tasks involving
processing of sequential data, time series modeling, and system identification.
Echo state networks (ESNs), a type of reservoir computer, mirror neural
networks but simplify training. They apply fixed, random linear transformations
to the internal state, followed by nonlinear changes. This process, guided by
input signals and linear regression, adapts the system to match target
characteristics, reducing computational demands. A potential drawback of ESNs
is that the fixed reservoir may not offer the complexity needed for specific
problems. While directly altering (training) the internal ESN would reintroduce
the computational burden, an indirect modification can be achieved by
redirecting some output as input. This feedback can influence the internal
reservoir state, yielding ESNs with enhanced complexity suitable for broader
challenges. In this paper, we demonstrate that by feeding some component of the
reservoir state back into the network through the input, we can drastically
improve upon the performance of a given ESN. We rigorously prove that, for any
given ESN, feedback will almost always improve the accuracy of the output. For
a set of three tasks, each representing different problem classes, we find that
with feedback the average error measures are reduced by $30\%-60\%$.
Remarkably, feedback provides at least an equivalent performance boost to
doubling the initial number of computational nodes, a computationally expensive
and technologically challenging alternative. These results demonstrate the
broad applicability and substantial usefulness of this feedback scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quilt-1M: One Million Image-Text Pairs for Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11207v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11207v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, Linda Shapiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent accelerations in multi-modal applications have been made possible with
the plethora of image and text data available online. However, the scarcity of
analogous data in the medical field, specifically in histopathology, has slowed
comparable progress. To enable similar representation learning for
histopathology, we turn to YouTube, an untapped resource of videos, offering
$1,087$ hours of valuable educational histopathology videos from expert
clinicians. From YouTube, we curate QUILT: a large-scale vision-language
dataset consisting of $802, 144$ image and text pairs. QUILT was automatically
curated using a mixture of models, including large language models, handcrafted
algorithms, human knowledge databases, and automatic speech recognition. In
comparison, the most comprehensive datasets curated for histopathology amass
only around $200$K samples. We combine QUILT with datasets from other sources,
including Twitter, research papers, and the internet in general, to create an
even larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it
as the largest vision-language histopathology dataset to date. We demonstrate
the value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model
outperforms state-of-the-art models on both zero-shot and linear probing tasks
for classifying new histopathology images across $13$ diverse patch-level
datasets of $8$ different sub-pathologies and cross-modal retrieval tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Higher-Order Topological Directionality and Directed Simplicial Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Lecha, Andrea Cavallo, Francesca Dominici, Elvin Isufi, Claudio Battiloro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological Deep Learning (TDL) has emerged as a paradigm to process and
learn from signals defined on higher-order combinatorial topological spaces,
such as simplicial or cell complexes. Although many complex systems have an
asymmetric relational structure, most TDL models forcibly symmetrize these
relationships. In this paper, we first introduce a novel notion of higher-order
directionality and we then design Directed Simplicial Neural Networks
(Dir-SNNs) based on it. Dir-SNNs are message-passing networks operating on
directed simplicial complexes able to leverage directed and possibly asymmetric
interactions among the simplices. To our knowledge, this is the first TDL model
using a notion of higher-order directionality. We theoretically and empirically
prove that Dir-SNNs are more expressive than their directed graph counterpart
in distinguishing isomorphic directed graphs. Experiments on a synthetic source
localization task demonstrate that Dir-SNNs outperform undirected SNNs when the
underlying complex is directed, and perform comparably when the underlying
complex is undirected.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhance Eye Disease Detection using Learnable Probabilistic Discrete
  Latents in Machine Learning Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16865v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16865v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Prabhakaran, YeKun Xiao, Ching-Yu Cheng, Dianbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ocular diseases, including diabetic retinopathy and glaucoma, present a
significant public health challenge due to their high prevalence and potential
for causing vision impairment. Early and accurate diagnosis is crucial for
effective treatment and management. In recent years, deep learning models have
emerged as powerful tools for analysing medical images, such as retina imaging.
However, challenges persist in model relibability and uncertainty estimation,
which are critical for clinical decision-making. This study leverages the
probabilistic framework of Generative Flow Networks (GFlowNets) to learn the
posterior distribution over latent discrete dropout masks for the
classification and analysis of ocular diseases using fundus images. We develop
a robust and generalizable method that utilizes GFlowOut integrated with
ResNet18 and ViT models as the backbone in identifying various ocular
conditions. This study employs a unique set of dropout masks - none, random,
bottomup, and topdown - to enhance model performance in analyzing these fundus
images. Our results demonstrate that our learnable probablistic latents
significantly improves accuracy, outperforming the traditional dropout
approach. We utilize a gradient map calculation method, Grad-CAM, to assess
model explainability, observing that the model accurately focuses on critical
image regions for predictions. The integration of GFlowOut in neural networks
presents a promising advancement in the automated diagnosis of ocular diseases,
with implications for improving clinical workflows and patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Path Loss Prediction Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan G. Dempsey, Jonathan Ethier, Halim Yanikomeroglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio deployments and spectrum planning benefit from path loss predictions.
Obstructions along a communications link are often considered implicitly or
through derived metrics such as representative clutter height or total
obstruction depth. In this paper, we propose a path-specific path loss
prediction method that uses convolutional neural networks to automatically
perform feature extraction from high-resolution obstruction height maps. Our
methods result in low prediction error in a variety of environments without
requiring derived metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlashRNN: Optimizing Traditional RNNs on Modern Hardware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Transformers and other sequence-parallelizable neural network
architectures seem like the current state of the art in sequence modeling, they
specifically lack state-tracking capabilities. These are important for
time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,
as well as modern variants like sLSTM do have these capabilities at the cost of
strictly sequential processing. While this is often seen as a strong
limitation, we show how fast these networks can get with our
hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the
register level on modern GPUs. We extend traditional RNNs with a
parallelization variant that processes multiple RNNs of smaller hidden state in
parallel, similar to the head-wise processing in Transformers. To enable
flexibility on different GPU variants, we introduce a new optimization
framework for hardware-internal cache sizes, memory and compute handling. It
models the hardware in a setting using polyhedral-like constraints, including
the notion of divisibility. This speeds up the solution process in our
ConstrINT library for general integer constraint satisfaction problems (integer
CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla
PyTorch implementation and allow 40x larger hidden sizes compared to our Triton
implementation. Our open-source kernels and the optimization library are
released here to boost research in the direction of state-tracking enabled RNNs
and sequence modeling: \url{https://github.com/NX-AI/flashrnn}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Top-Down Global Causal Discovery with Local Search for Linear and
  Nonlinear Additive Noise Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14496v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14496v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujai Hiremath, Jacqueline R. M. A. Maasch, Mengxiao Gao, Promit Ghosal, Kyra Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning the unique directed acyclic graph corresponding to an unknown causal
model is a challenging task. Methods based on functional causal models can
identify a unique graph, but either suffer from the curse of dimensionality or
impose strong parametric assumptions. To address these challenges, we propose a
novel hybrid approach for global causal discovery in observational data that
leverages local causal substructures. We first present a topological sorting
algorithm that leverages ancestral relationships in linear structural causal
models to establish a compact top-down hierarchical ordering, encoding more
causal information than linear orderings produced by existing methods. We
demonstrate that this approach generalizes to nonlinear settings with arbitrary
noise. We then introduce a nonparametric constraint-based algorithm that prunes
spurious edges by searching for local conditioning sets, achieving greater
accuracy than current methods. We provide theoretical guarantees for
correctness and worst-case polynomial time complexities, with empirical
validation on synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the Thirty-Eighth Annual Conference on Neural
  Information Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Approach to Extract Interpre<span class="highlight-title">table</span> Rules from Tree Ensembles
  via Integer Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Bonasera, Emilio Carrizosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tree ensembles are very popular machine learning models, known for their
effectiveness in supervised classification and regression tasks. Their
performance derives from aggregating predictions of multiple decision trees,
which are renowned for their interpretability properties. However, tree
ensemble models do not reliably exhibit interpretable output. Our work aims to
extract an optimized list of rules from a trained tree ensemble, providing the
user with a condensed, interpretable model that retains most of the predictive
power of the full model. Our approach consists of solving a set partitioning
problem formulated through Integer Programming. The proposed method works with
either tabular or time series data, for both classification and regression
tasks, and its flexible formulation can include any arbitrary loss or
regularization functions. Our extensive computational experiments offer
statistically significant evidence that our method is competitive with other
rule extraction methods in terms of predictive performance and fidelity towards
the tree ensemble. Moreover, we empirically show that the proposed method
effectively extracts interpretable rules from tree ensemble that are designed
for time series data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>- Improved overall manuscript flow and clearness - Added related work
  on explanation fidelity - Added computational results on fidelity - Fixed
  some flaws on data inference - Optimization problem with weighted objectives
  - Added appendix containing qualitative examples - New computational results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Steering Large Language Models using Conceptors: Improving
  Addition-Based Activation Engineering <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16314v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16314v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joris Postmus, Steven Abreu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have transformed AI, yet reliably controlling their
outputs remains a challenge. This paper explores activation engineering, where
outputs of pre-trained LLMs are controlled by manipulating their activations at
inference time. Unlike traditional methods using a single steering vector, we
introduce conceptors - mathematical constructs that represent sets of
activation vectors as ellipsoidal regions. Conceptors act as soft projection
matrices and offer more precise control over complex activation patterns. Our
experiments demonstrate that conceptors outperform traditional methods across
multiple steering tasks. We further use Boolean operations on conceptors for
combined steering goals that empirically outperform additively combining
steering vectors on a set of tasks. These results highlight conceptors as a
promising tool for more effective steering of LLMs. Our code is available on
github.com/jorispos/conceptorsteering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the MINT workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Decoders for Transformer-based Semantic Segmentation: A
  Compression Perspective <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qishuai Wen, Chun-Guang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for Transformer-based semantic segmentation
typically adopt Transformer decoders that are used to extract additional
embeddings from image embeddings via cross-attention, refine either or both
types of embeddings via self-attention, and project image embeddings onto the
additional embeddings via dot-product. Despite their remarkable success, these
empirical designs still lack theoretical justifications or interpretations,
thus hindering potentially principled improvements. In this paper, we argue
that there are fundamental connections between semantic segmentation and
compression, especially between the Transformer decoders and Principal
Component Analysis (PCA). From such a perspective, we derive a white-box, fully
attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the
interpretations as follows: 1) the self-attention operator refines image
embeddings to construct an ideal principal subspace that aligns with the
supervision and retains most information; 2) the cross-attention operator seeks
to find a low-rank approximation of the refined image embeddings, which is
expected to be a set of orthonormal bases of the principal subspace and
corresponds to the predefined classes; 3) the dot-product operation yields
compact representation for image embeddings as segmentation masks. Experiments
conducted on dataset ADE20K find that DEPICT consistently outperforms its
black-box counterpart, Segmenter, and it is light weight and more robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automation of Quantum Dot Measurement Analysis via Explainable Machine
  Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13699v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13699v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of quantum dot (QD) devices for quantum computing has
necessitated more efficient and automated methods for device characterization
and tuning. This work demonstrates the feasibility and advantages of applying
explainable machine learning techniques to the analysis of quantum dot
measurements, paving the way for further advances in automated and transparent
QD device tuning. Many of the measurements acquired during the tuning process
come in the form of images that need to be properly analyzed to guide the
subsequent tuning steps. By design, features present in such images capture
certain behaviors or states of the measured QD devices. When considered
carefully, such features can aid the control and calibration of QD devices. An
important example of such images are so-called $\textit{triangle plots}$, which
visually represent current flow and reveal characteristics important for QD
device calibration. While image-based classification tools, such as
convolutional neural networks (CNNs), can be used to verify whether a given
measurement is $\textit{good}$ and thus warrants the initiation of the next
phase of tuning, they do not provide any insights into how the device should be
adjusted in the case of $\textit{bad}$ images. This is because CNNs sacrifice
prediction and model intelligibility for high accuracy. To ameliorate this
trade-off, a recent study introduced an image vectorization approach that
relies on the Gabor wavelet transform (Schug $\textit{et al.}$ 2024
$\textit{Proc. XAI4Sci: Explainable Machine Learning for Sciences Workshop
(AAAI 2024) (Vancouver, Canada)}$ pp 1-6). Here we propose an alternative
vectorization method that involves mathematical modeling of synthetic triangles
to mimic the experimental data. Using explainable boosting machines, we show
that this new method offers superior explainability of model prediction without
sacrificing accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, abbreviated version published in Proceedings of
  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,
  (Vancouver, Canada)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked
  EHR and Pathology Lab <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Dai, Brian Sullivan, Axel Montout, Amy Dillon, Chris Waller, Peter Acs, Rachel Denholm, Philip Williams, Alastair D Hay, Raul Santos-Rodriguez, Andrew Dowsey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of machine learning and AI on electronic health records (EHRs) holds
substantial potential for clinical insight. However, this approach faces
challenges due to data heterogeneity, sparsity, temporal misalignment, and
limited labeled outcomes. In this context, we leverage a linked EHR dataset of
approximately one million de-identified individuals from Bristol, North
Somerset, and South Gloucestershire, UK, to characterize urinary tract
infections (UTIs). We implemented a data pre-processing and curation pipeline
that transforms the raw EHR data into a structured format suitable for
developing predictive models focused on data fairness, accountability and
transparency. Given the limited availability and biases of ground truth UTI
outcomes, we introduce a UTI risk estimation framework informed by clinical
expertise to estimate UTI risk across individual patient timelines. Pairwise
XGBoost models are trained using this framework to differentiate UTI risk
categories with explainable AI techniques applied to identify key predictors
and support interpretability. Our findings reveal differences in clinical and
demographic predictors across risk groups. While this study highlights the
potential of AI-driven insights to support UTI clinical decision-making,
further investigation of patient sub-strata and extensive validation are needed
to ensure robustness and applicability in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Light Transport-aware Diffusion Posterior Sampling for Single-View
  Reconstruction of 3D Volumes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludwic Leonard, Nils Thuerey, Ruediger Westermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a single-view reconstruction technique of volumetric fields in
which multiple light scattering effects are omnipresent, such as in clouds. We
model the unknown distribution of volumetric fields using an unconditional
diffusion model trained on a novel benchmark dataset comprising 1,000
synthetically simulated volumetric density fields. The neural diffusion model
is trained on the latent codes of a novel, diffusion-friendly, monoplanar
representation. The generative model is used to incorporate a tailored
parametric diffusion posterior sampling technique into different reconstruction
tasks. A physically-based differentiable volume renderer is employed to provide
gradients with respect to light transport in the latent space. This stands in
contrast to classic NeRF approaches and makes the reconstructions better
aligned with observed data. Through various experiments, we demonstrate
single-view reconstruction of volumetric clouds at a previously unattainable
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards an Information Theoretic Framework of Context-Based Offline
  Meta-Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02429v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02429v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanqing Li, Hai Zhang, Xinyu Zhang, Shatong Zhu, Yang Yu, Junqiao Zhao, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a marriage between offline RL and meta-RL, the advent of offline
meta-reinforcement learning (OMRL) has shown great promise in enabling RL
agents to multi-task and quickly adapt while acquiring knowledge safely. Among
which, context-based OMRL (COMRL) as a popular paradigm, aims to learn a
universal policy conditioned on effective task representations. In this work,
by examining several key milestones in the field of COMRL, we propose to
integrate these seemingly independent methodologies into a unified framework.
Most importantly, we show that the pre-existing COMRL algorithms are
essentially optimizing the same mutual information objective between the task
variable $M$ and its latent representation $Z$ by implementing various
approximate bounds. Such theoretical insight offers ample design freedom for
novel algorithms. As demonstrations, we propose a supervised and a
self-supervised implementation of $I(Z; M)$, and empirically show that the
corresponding optimization algorithms exhibit remarkable generalization across
a broad spectrum of RL benchmarks, context shift scenarios, data qualities and
deep learning architectures. This work lays the information theoretic
foundation for COMRL methods, leading to a better understanding of task
representation learning in the context of reinforcement learning. Given its
generality, we envision our framework as a promising offline pre-training
paradigm of foundation models for decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures, 7 tables. TLDR: We propose a novel information
  theoretic framework of the context-based offline meta-RL paradigm, which
  unifies several mainstream methods and leads to two robust algorithm
  implementations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Project Tracyn: Generative Artificial Intelligence based Peripherals
  Trace Synthesizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibai Huang, Yihan Shen, Yongchen Xie, Zhixiang Wei, Yun wang, Fangxin Liu, Tao Song, Zhengwei Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peripheral Component Interconnect Express (PCIe) is the de facto interconnect
standard for high-speed peripherals and CPUs. Prototyping and optimizing PCIe
devices for emerging scenarios is an ongoing challenge. Since Transaction Layer
Packets (TLPs) capture device-CPU interactions, it is crucial to analyze and
generate realistic TLP traces for effective device design and optimization.
Generative AI offers a promising approach for creating intricate, custom TLP
traces necessary for PCIe hardware and software development. However, existing
models often generate impractical traces due to the absence of PCIe-specific
constraints, such as TLP ordering and causality. This paper presents Phantom,
the first framework that treats TLP trace generation as a generative AI problem
while incorporating PCIe-specific constraints. We validate Phantom's
effectiveness by generating TLP traces for an actual PCIe network interface
card. Experimental results show that Phantom produces practical, large-scale
TLP traces, significantly outperforming existing models, with improvements of
up to 1000$\times$ in task-specific metrics and up to 2.19$\times$ in Frechet
Inception Distance (FID) compared to backbone-only methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design of 2D Skyrmionic Metamaterial Through Controlled Assembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qichen Xu, Zhuanglin Shen, Alexander Edström, I. P. Miranda, Zhiwei Lu, Anders Bergman, Danny Thonig, Wanjian Yin, Olle Eriksson, Anna Delin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite extensive research on magnetic skyrmions and antiskyrmions, a
significant challenge remains in crafting nontrivial high-order skyrmionic
textures with varying, or even tailor-made, topologies. We address this
challenge, by focusing on a construction pathway of skyrmionic metamaterials
within a monolayer thin film and suggest several skyrmionic metamaterials that
are surprisingly stable, i.e., long-lived, due to a self-stabilization
mechanism. This makes these new textures promising for applications. Central to
our approach is the concept of 'simulated controlled assembly', in short, a
protocol inspired by 'click chemistry' that allows for positioning topological
magnetic structures where one likes, and then allowing for energy minimization
to elucidate the stability. Utilizing high-throughput atomistic-spin-dynamic
simulations alongside state-of-the-art AI-driven tools, we have isolated
skyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium
(Q=0). These entities serve as foundational 'skyrmionic building blocks' to
form the here reported intricate textures. In this work, two key contributions
are introduced to the field of skyrmionic systems. First, we present a a novel
combination of atomistic spin dynamics simulations and controlled assembly
protocols for the stabilization and investigation of new topological magnets.
Second, using the aforementioned methods we report on the discovery of
skyrmionic metamaterials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BayesAdapter: enhanced uncertainty estimation in CLIP few-shot
  adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Morales-Álvarez, Stergios Christodoulidis, Maria Vakalopoulou, Pablo Piantanida, Jose Dolz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large pre-trained vision-language models (VLMs) represents a
paradigm shift in machine learning, with unprecedented results in a broad span
of visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited
remarkable zero-shot and transfer learning capabilities in classification. To
transfer CLIP to downstream tasks, adapters constitute a parameter-efficient
approach that avoids backpropagation through the large model (unlike related
prompt learning methods). However, CLIP adapters have been developed to target
discriminative performance, and the quality of their uncertainty estimates has
been overlooked. In this work we show that the discriminative performance of
state-of-the-art CLIP adapters does not always correlate with their uncertainty
estimation capabilities, which are essential for a safe deployment in
real-world scenarios. We also demonstrate that one of such adapters is obtained
through MAP inference from a more general probabilistic framework. Based on
this observation we introduce BayesAdapter, which leverages Bayesian inference
to estimate a full probability distribution instead of a single point, better
capturing the variability inherent in the parameter space. In a comprehensive
empirical evaluation we show that our approach obtains high quality uncertainty
estimates in the predictions, standing out in calibration and selective
classification. Our code will be publicly available upon acceptance of the
paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 5 figures, 23 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring energy minimization to model strain localization as a strong
  discontinuity using Physics Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar León, Víctor Rivera, Angel Vázquez-Patiño, Jacinto Ulloa, Esteban Samaniego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the possibilities of using energy minimization for the numerical
modeling of strain localization in solids as a sharp discontinuity in the
displacement field. For this purpose, we consider (regularized) strong
discontinuity kinematics in elastoplastic solids. The corresponding
mathematical model is discretized using Artificial Neural Networks (ANNs),
aiming to predict both the magnitude and location of the displacement jump from
energy minimization, $\textit{i.e.}$, within a variational setting. The
architecture takes care of the kinematics, while the loss function takes care
of the variational statement of the boundary value problem. The main idea
behind this approach is to solve both the equilibrium problem and the location
of the localization band by means of trainable parameters in the ANN. As a
proof of concept, we show through both 1D and 2D numerical examples that the
computational modeling of strain localization for elastoplastic solids using
energy minimization is feasible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QuadWBG: Generalizable Quadrupedal Whole-Body Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jilong Wang, Javokhirbek Rajabov, Chaoyi Xu, Yiming Zheng, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legged robots with advanced manipulation capabilities have the potential to
significantly improve household duties and urban maintenance. Despite
considerable progress in developing robust locomotion and precise manipulation
methods, seamlessly integrating these into cohesive whole-body control for
real-world applications remains challenging. In this paper, we present a
modular framework for robust and generalizable whole-body loco-manipulation
controller based on a single arm-mounted camera. By using reinforcement
learning (RL), we enable a robust low-level policy for command execution over 5
dimensions (5D) and a grasp-aware high-level policy guided by a novel metric,
Generalized Oriented Reachability Map (GORM). The proposed system achieves
state-of-the-art one-time grasping accuracy of 89% in the real world, including
challenging tasks such as grasping transparent objects. Through extensive
simulations and real-world experiments, we demonstrate that our system can
effectively manage a large workspace, from floor level to above body height,
and perform diverse whole-body loco-manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Driven Early Mental Health Screening: Analyzing Selfies of Pregnant
  Women <span class="chip">ALT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo A. Basílio, Thiago B. Pereira, Alessandro L. Koerich, Hermano Tavares, Ludmila Dias, Maria das Graças da S. Teixeira, Rafael T. Sousa, Wilian H. Hisatugu, Amanda S. Mota, Anilton S. Garcia, Marco Aurélio K. Galletta, Thiago M. Paixão
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Major Depressive Disorder and anxiety disorders affect millions globally,
contributing significantly to the burden of mental health issues. Early
screening is crucial for effective intervention, as timely identification of
mental health issues can significantly improve treatment outcomes. Artificial
intelligence (AI) can be valuable for improving the screening of mental
disorders, enabling early intervention and better treatment outcomes. AI-driven
screening can leverage the analysis of multiple data sources, including facial
features in digital images. However, existing methods often rely on controlled
environments or specialized equipment, limiting their broad applicability. This
study explores the potential of AI models for ubiquitous depression-anxiety
screening given face-centric selfies. The investigation focuses on high-risk
pregnant patients, a population that is particularly vulnerable to mental
health issues. To cope with limited training data resulting from our clinical
setup, pre-trained models were utilized in two different approaches:
fine-tuning convolutional neural networks (CNNs) originally designed for facial
expression recognition and employing vision-language models (VLMs) for
zero-shot analysis of facial expressions. Experimental results indicate that
the proposed VLM-based method significantly outperforms CNNs, achieving an
accuracy of 77.6%. Although there is significant room for improvement, the
results suggest that VLMs can be a promising approach for mental health
screening.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication in HEALTHINF25 at the
  18th International Joint Conference on Biomedical Engineering Systems and
  Technologies (BIOSTEC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral complexity of deep neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simmaco Di Lillo, Domenico Marinucci, Michele Salvi, Stefano Vigogna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that randomly initialized, push-forward, fully-connected
neural networks weakly converge to isotropic Gaussian processes, in the limit
where the width of all layers goes to infinity. In this paper, we propose to
use the angular power spectrum of the limiting field to characterize the
complexity of the network architecture. In particular, we define sequences of
random variables associated with the angular power spectrum, and provide a full
characterization of the network complexity in terms of the asymptotic
distribution of these sequences as the depth diverges. On this basis, we
classify neural networks as low-disorder, sparse, or high-disorder; we show how
this classification highlights a number of distinct features for standard
activation functions, and in particular, sparsity properties of ReLU networks.
Our theoretical results are also validated by numerical simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Forward Compatibility in Class Incremental Learning by
  Increasing Representation Rank and Feature Richness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeill Kim, Wonseok Lee, Moonjung Eo, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class Incremental Learning (CIL) constitutes a pivotal subfield within
continual learning, aimed at enabling models to progressively learn new
classification tasks while retaining knowledge obtained from prior tasks.
Although previous studies have predominantly focused on backward compatible
approaches to mitigate catastrophic forgetting, recent investigations have
introduced forward compatible methods to enhance performance on novel tasks and
complement existing backward compatible methods. In this study, we introduce an
effective-Rank based Feature Richness enhancement (RFR) method, designed for
improving forward compatibility. Specifically, this method increases the
effective rank of representations during the base session, thereby facilitating
the incorporation of more informative features pertinent to unseen novel tasks.
Consequently, RFR achieves dual objectives in backward and forward
compatibility: minimizing feature extractor modifications and enhancing novel
task performance, respectively. To validate the efficacy of our approach, we
establish a theoretical connection between effective rank and the Shannon
entropy of representations. Subsequently, we conduct comprehensive experiments
by integrating RFR into eleven well-known CIL methods. Our results demonstrate
the effectiveness of our approach in enhancing novel-task performance while
mitigating catastrophic forgetting. Furthermore, our method notably improves
the average incremental accuracy across all eleven cases examined.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUACK: Quantum Aligned Centroid Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00304v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00304v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kilian Tscharke, Sebastian Issel, Pascal Debus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing (QC) seems to show potential for application in machine
learning (ML). In particular quantum kernel methods (QKM) exhibit promising
properties for use in supervised ML tasks. However, a major disadvantage of
kernel methods is their unfavorable quadratic scaling with the number of
training samples. Together with the limits imposed by currently available
quantum hardware (NISQ devices) with their low qubit coherence times, small
number of qubits, and high error rates, the use of QC in ML at an industrially
relevant scale is currently impossible. As a small step in improving the
potential applications of QKMs, we introduce QUACK, a quantum kernel algorithm
whose time complexity scales linear with the number of samples during training,
and independent of the number of training samples in the inference stage. In
the training process, only the kernel entries for the samples and the centers
of the classes are calculated, i.e. the maximum shape of the kernel for n
samples and c classes is (n, c). During training, the parameters of the quantum
kernel and the positions of the centroids are optimized iteratively. In the
inference stage, for every new sample the circuit is only evaluated for every
centroid, i.e. c times. We show that the QUACK algorithm nevertheless provides
satisfactory results and can perform at a similar level as classical kernel
methods with quadratic scaling during training. In addition, our (simulated)
algorithm is able to handle high-dimensional datasets such as MNIST with 784
features without any dimensionality reduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2nd place Best Paper award in QML track @ IEEE International
  Conference on Quantum Computing and Engineering (QCE) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel
  Orthogonal Learner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentyn Melnychuk, Stefan Feuerriegel, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating causal quantities from observational data is crucial for
understanding the safety and effectiveness of medical treatments. However, to
make reliable inferences, medical practitioners require not only estimating
averaged causal quantities, such as the conditional average treatment effect,
but also understanding the randomness of the treatment effect as a random
variable. This randomness is referred to as aleatoric uncertainty and is
necessary for understanding the probability of benefit from treatment or
quantiles of the treatment effect. Yet, the aleatoric uncertainty of the
treatment effect has received surprisingly little attention in the causal
machine learning community. To fill this gap, we aim to quantify the aleatoric
uncertainty of the treatment effect at the covariate-conditional level, namely,
the conditional distribution of the treatment effect (CDTE). Unlike average
causal quantities, the CDTE is not point identifiable without strong additional
assumptions. As a remedy, we employ partial identification to obtain sharp
bounds on the CDTE and thereby quantify the aleatoric uncertainty of the
treatment effect. We then develop a novel, orthogonal learner for the bounds on
the CDTE, which we call AU-learner. We further show that our AU-learner has
several strengths in that it satisfies Neyman-orthogonality and, thus,
quasi-oracle efficiency. Finally, we propose a fully-parametric deep learning
instantiation of our AU-learner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Counterfactual Image Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20287v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20287v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez, Athanasios Vlontzos, Yannis Panagakis, Giorgos Papanastasiou, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has revolutionised visual content editing, empowering users to
effortlessly modify images and videos. However, not all edits are equal. To
perform realistic edits in domains such as natural image or medical imaging,
modifications must respect causal relationships inherent to the data generation
process. Such image editing falls into the counterfactual image generation
regime. Evaluating counterfactual image generation is substantially complex:
not only it lacks observable ground truths, but also requires adherence to
causal constraints. Although several counterfactual image generation methods
and evaluation metrics exist, a comprehensive comparison within a unified
setting is lacking. We present a comparison framework to thoroughly benchmark
counterfactual image generation methods. We integrate all models that have been
used for the task at hand and expand them to novel datasets and causal graphs,
demonstrating the superiority of Hierarchical VAEs across most datasets and
metrics. Our framework is implemented in a user-friendly Python package that
can be extended to incorporate additional SCMs, causal methods, generative
models, and datasets for the community to build on. Code:
https://github.com/gulnazaki/counterfactual-benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at NeurIPS 2024 Datasets and
  Benchmarks Track https://openreview.net/forum?id=0T8xRFrScB Project page:
  https://gulnazaki.github.io/counterfactual-benchmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitating from auxiliary imperfect demonstrations via Adversarial
  Density Weighted Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20351v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20351v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhang, Zifeng Zhuang, Jingzehua Xu, Yiyuan Yang, Yubo Huang, Donglin Wang, Shuai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel one-step supervised imitation learning (IL) framework
called Adversarial Density Regression (ADR). This IL framework aims to correct
the policy learned on unknown-quality to match the expert distribution by
utilizing demonstrations, without relying on the Bellman operator.
Specifically, ADR addresses several limitations in previous IL algorithms:
First, most IL algorithms are based on the Bellman operator, which inevitably
suffer from cumulative offsets from sub-optimal rewards during multi-step
update processes. Additionally, off-policy training frameworks suffer from
Out-of-Distribution (OOD) state-actions. Second, while conservative terms help
solve the OOD issue, balancing the conservative term is difficult. To address
these limitations, we fully integrate a one-step density-weighted Behavioral
Cloning (BC) objective for IL with auxiliary imperfect demonstration.
Theoretically, we demonstrate that this adaptation can effectively correct the
distribution of policies trained on unknown-quality datasets to align with the
expert policy's distribution. Moreover, the difference between the empirical
and the optimal value function is proportional to the upper bound of ADR's
objective, indicating that minimizing ADR's objective is akin to approaching
the optimal value. Experimentally, we validated the performance of ADR by
conducting extensive evaluations. Specifically, ADR outperforms all of the
selected IL algorithms on tasks from the Gym-Mujoco domain. Meanwhile, it
achieves an 89.5% improvement over IQL when utilizing ground truth rewards on
tasks from the Adroit and Kitchen domains. Our codebase will be released at:
https://github.com/stevezhangzA/Adverserial_Density_Regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D3RM: A Discrete Denoising Diffusion Refinement Model for Piano
  Transcription <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hounsu Kim, Taegyun Kwon, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been widely used in the generative domain due to their
convincing performance in modeling complex data distributions. Moreover, they
have shown competitive results on discriminative tasks, such as image
segmentation. While diffusion models have also been explored for automatic
music transcription, their performance has yet to reach a competitive level. In
this paper, we focus on discrete diffusion model's refinement capabilities and
present a novel architecture for piano transcription. Our model utilizes
Neighborhood Attention layers as the denoising module, gradually predicting the
target high-resolution piano roll, conditioned on the finetuned features of a
pretrained acoustic model. To further enhance refinement, we devise a novel
strategy which applies distinct transition states during training and inference
stage of discrete diffusion models. Experiments on the MAESTRO dataset show
that our approach outperforms previous diffusion-based piano transcription
models and the baseline model in terms of F1 score. Our code is available in
https://github.com/hanshounsu/d3rm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are LLMs Good Cryptic Crossword Solvers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Sadallah, Daria Kotova, Ekaterina Kochmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptic crosswords are puzzles that rely not only on general knowledge but
also on the solver's ability to manipulate language on different levels and
deal with various types of wordplay. Previous research suggests that solving
such puzzles is a challenge even for modern NLP models. However, the abilities
of large language models (LLMs) have not yet been tested on this task. In this
paper, we establish the benchmark results for three popular LLMs -- LLaMA2,
Mistral, and ChatGPT -- showing that their performance on this task is still
far from that of humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object
  Interaction Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkun He, Yun Liu, Ruitao Liu, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing realistic human-object interaction motions is a critical problem
in VR/AR and human animation. Unlike the commonly studied scenarios involving a
single human or hand interacting with one object, we address a more generic
multi-body setting with arbitrary numbers of humans, hands, and objects. This
complexity introduces significant challenges in synchronizing motions due to
the high correlations and mutual influences among bodies. To address these
challenges, we introduce SyncDiff, a novel method for multi-body interaction
synthesis using a synchronized motion diffusion strategy. SyncDiff employs a
single diffusion model to capture the joint distribution of multi-body motions.
To enhance motion fidelity, we propose a frequency-domain motion decomposition
scheme. Additionally, we introduce a new set of alignment scores to emphasize
the synchronization of different body motions. SyncDiff jointly optimizes both
data sample likelihood and alignment likelihood through an explicit
synchronization strategy. Extensive experiments across four datasets with
various multi-body configurations demonstrate the superiority of SyncDiff over
existing state-of-the-art motion synthesis methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Initialization is Critical to Whether Transformers Fit Composite
  Functions by Reasoning or Memorizing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05409v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05409v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwang Zhang, Pengxiao Lin, Zhiwei Wang, Yaoyu Zhang, Zhi-Qin John Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have shown impressive capabilities across various tasks, but
their performance on compositional problems remains a topic of debate. In this
work, we investigate the mechanisms of how transformers behave on unseen
compositional tasks. We discover that the parameter initialization scale plays
a critical role in determining whether the model learns inferential
(reasoning-based) solutions, which capture the underlying compositional
primitives, or symmetric (memory-based) solutions, which simply memorize
mappings without understanding the compositional structure. By analyzing the
information flow and vector representations within the model, we reveal the
distinct mechanisms underlying these solution types. We further find that
inferential (reasoning-based) solutions exhibit low complexity bias, which we
hypothesize is a key factor enabling them to learn individual mappings for
single anchors. We validate our conclusions on various real-world datasets. Our
findings provide valuable insights into the role of initialization scale in
tuning the reasoning and memorizing ability and we propose the initialization
rate $\gamma$ to be a convenient tunable hyper-parameter in common deep
learning frameworks, where $1/d_{\mathrm{in}}^\gamma$ is the standard deviation
of parameters of the layer with $d_{\mathrm{in}}$ input neurons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GFairHint: Improving Individual Fairness for <span class="highlight-title">Graph</span> Neural Networks via
  Fairness Hint <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paiheng Xu, Yuhang Zhou, Bang An, Wei Ai, Furong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the growing concerns about fairness in machine learning and the
impressive performance of Graph Neural Networks (GNNs) on graph data learning,
algorithmic fairness in GNNs has attracted significant attention. While many
existing studies improve fairness at the group level, only a few works promote
individual fairness, which renders similar outcomes for similar individuals. A
desirable framework that promotes individual fairness should (1) balance
between fairness and performance, (2) accommodate two commonly-used individual
similarity measures (externally annotated and computed from input features),
(3) generalize across various GNN models, and (4) be computationally efficient.
Unfortunately, none of the prior work achieves all the desirables. In this
work, we propose a novel method, GFairHint, which promotes individual fairness
in GNNs and achieves all aforementioned desirables. GFairHint learns fairness
representations through an auxiliary link prediction task, and then
concatenates the representations with the learned node embeddings in original
GNNs as a "fairness hint". Through extensive experimental investigations on
five real-world graph datasets under three prevalent GNN models covering both
individual similarity measures above, GFairHint achieves the best fairness
results in almost all combinations of datasets with various backbone models,
while generating comparable utility results, with much less computational cost
compared to the previous state-of-the-art (SoTA) method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the ACM Transactions on Knowledge Discovery from Data
  (TKDD 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoNOAir: A Neural Operator for Forecasting Carbon Monoxide Evolution in
  Cities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanchit Bedi, Karn Tiwari, Prathosh A. P., Sri Harsha Kota, N. M. Anoop Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Carbon Monoxide (CO) is a dominant pollutant in urban areas due to the energy
generation from fossil fuels for industry, automobile, and domestic
requirements. Forecasting the evolution of CO in real-time can enable the
deployment of effective early warning systems and intervention strategies.
However, the computational cost associated with the physics and chemistry-based
simulation makes it prohibitive to implement such a model at the city and
country scale. To address this challenge, here, we present a machine learning
model based on neural operator, namely, Complex Neural Operator for Air Quality
(CoNOAir), that can effectively forecast CO concentrations. We demonstrate this
by developing a country-level model for short-term (hourly) and long-term
(72-hour) forecasts of CO concentrations. Our model outperforms
state-of-the-art models such as Fourier neural operators (FNO) and provides
reliable predictions for both short and long-term forecasts. We further analyse
the capability of the model to capture extreme events and generate forecasts in
urban cities in India. Interestingly, we observe that the model predicts the
next hour CO concentrations with R2 values greater than 0.95 for all the cities
considered. The deployment of such a model can greatly assist the governing
bodies to provide early warning, plan intervention strategies, and develop
effective strategies by considering several what-if scenarios. Altogether, the
present approach could provide a fillip to real-time predictions of CO
pollution in urban cities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 14 figures, under submission process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A monthly sub-national Harmonized Food Insecurity <span class="highlight-title">Dataset</span> for
  comprehensive analysis and predictive modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélissande Machefer, Michele Ronco, Anne-Claire Thomas, Michael Assouline, Melanie Rabier, Christina Corbane, Felix Rembold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food security is a complex, multidimensional concept challenging to measure
comprehensively. Effective anticipation, monitoring, and mitigation of food
crises require timely and comprehensive global data. This paper introduces the
Harmonized Food Insecurity Dataset (HFID), an open-source resource
consolidating four key data sources: the Integrated Food Security Phase
Classification (IPC)/Cadre Harmonis\'e (CH) phases, the Famine Early Warning
Systems Network (FEWS NET) IPC-compatible phases, and the World Food Program's
(WFP) Food Consumption Score (FCS) and reduced Coping Strategy Index (rCSI).
Updated monthly and using a common reference system for administrative units,
the HFID offers extensive spatial and temporal coverage. It serves as a vital
tool for food security experts and humanitarian agencies, providing a unified
resource for analyzing food security conditions and highlighting global data
disparities. The scientific community can also leverage the HFID to develop
data-driven predictive models, enhancing the capacity to forecast and prevent
future food crises.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The authors Melissande Machefer and Michele Ronco have contributed
  equally as both first authors to this work. This work is currently being
  reviewed in a peer-reviewed journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MusicLIME: Explainable Multimodal Music Understanding <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Sotirou, Vassilis Lyberatos, Orfeas Menis Mastromichalakis, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal models are critical for music understanding tasks, as they capture
the complex interplay between audio and lyrics. However, as these models become
more prevalent, the need for explainability grows-understanding how these
systems make decisions is vital for ensuring fairness, reducing bias, and
fostering trust. In this paper, we introduce MusicLIME, a model-agnostic
feature importance explanation method designed for multimodal music models.
Unlike traditional unimodal methods, which analyze each modality separately
without considering the interaction between them, often leading to incomplete
or misleading explanations, MusicLIME reveals how audio and lyrical features
interact and contribute to predictions, providing a holistic view of the
model's decision-making. Additionally, we enhance local explanations by
aggregating them into global explanations, giving users a broader perspective
of model behavior. Through this work, we contribute to improving the
interpretability of multimodal music models, empowering users to make informed
choices, and fostering more equitable, fair, and transparent music
understanding systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub repository: https://github.com/IamTheo2000/MusicLIME. To be
  presented at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07661v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07661v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Zhang, Shuyang Jiang, Jiangtao Feng, Lin Zheng, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer has achieved remarkable success in language, image, and speech
processing. Recently, various efficient attention architectures have been
proposed to improve transformer's efficiency while largely preserving its
efficacy, especially in modeling long sequences. A widely-used benchmark to
test these efficient methods' capability on long-range modeling is Long Range
Arena (LRA). However, LRA only focuses on the standard bidirectional (or
noncausal) self attention, and completely ignores cross attentions and
unidirectional (or causal) attentions, which are equally important to
downstream applications. In this paper, we propose Comprehensive Attention
Benchmark (CAB) under a fine-grained attention taxonomy with four
distinguishable attention patterns, namely, noncausal self, causal self,
noncausal cross, and causal cross attentions. CAB collects seven real-world
tasks from different research areas to evaluate efficient attentions under the
four attention patterns. Among these tasks, CAB validates efficient attentions
in eight backbone networks to show their generalization across neural
architectures. We conduct exhaustive experiments to benchmark the performances
of nine widely-used efficient attention architectures designed with different
philosophies on CAB. Extensive experimental results also shed light on the
fundamental problems of efficient attentions, such as efficiency length against
vanilla attention, performance consistency across attention patterns, the
benefit of attention mechanisms, and interpolation/extrapolation on
long-context language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Amortizing intrac<span class="highlight-title">table</span> inference in diffusion models for vision,
  language, and control <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddarth Venkatraman, Moksh Jain, Luca Scimeca, Minsu Kim, Marcin Sendera, Mohsin Hasan, Luke Rowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, Alexandre Adam, Jarrid Rector-Brooks, <span class="highlight-author">Yoshua Bengio</span>, Glen Berseth, Nikolay Malkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as effective distribution estimators in vision,
language, and reinforcement learning, but their use as priors in downstream
tasks poses an intractable posterior inference problem. This paper studies
amortized sampling of the posterior over data, $\mathbf{x}\sim p^{\rm
post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists
of a diffusion generative model prior $p(\mathbf{x})$ and a black-box
constraint or likelihood function $r(\mathbf{x})$. We state and prove the
asymptotic correctness of a data-free learning objective, relative trajectory
balance, for training a diffusion model that samples from this posterior, a
problem that existing methods solve only approximately or in restricted cases.
Relative trajectory balance arises from the generative flow network perspective
on diffusion models, which allows the use of deep reinforcement learning
techniques to improve mode coverage. Experiments illustrate the broad potential
of unbiased inference of arbitrary posteriors under diffusion priors: in vision
(classifier guidance), language (infilling under a discrete diffusion LLM), and
multimodal data (text-to-image generation). Beyond generative modeling, we
apply relative trajectory balance to the problem of continuous control with a
score-based behavior prior, achieving state-of-the-art results on benchmarks in
offline reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; code: https://github.com/GFNOrg/diffusion-finetuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Large <span class="highlight-title">Foundation</span> Models Design: A Perspective From Model and
  System Co-Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01990v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01990v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Yanxuan Yu, Zhixin Lai, Yite Wang, Jing Wu, Zhongwei Wan, Sina Alinejad, Benjamin Lengerich, Ying Nian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on modern efficient training and inference technologies on
foundation models and illustrates them from two perspectives: model and system
design. Model and System Design optimize LLM training and inference from
different aspects to save computational resources, making LLMs more efficient,
affordable, and more accessible. The paper list repository is available at
\url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic
  Regression on Heterogeneous Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Tao, Shizhao Peng, Tianyu Mei, Shoumo Li, Haogang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate nonlinear computation is a key challenge in privacy-preserving
machine learning (PPML). Most existing frameworks approximate it through linear
operations, resulting in significant precision loss. This paper proposes an
efficient, verifiable and accurate security 2-party logistic regression
framework (EVA-S2PLoR), which achieves accurate nonlinear function computation
through a novel secure element-wise multiplication protocol and its derived
protocols. Our framework primarily includes secure 2-party vector element-wise
multiplication, addition to multiplication, reciprocal, and sigmoid function
based on data disguising technology, where high efficiency and accuracy are
guaranteed by the simple computation flow based on the real number domain and
the few number of fixed communication rounds. We provide secure and robust
anomaly detection through dimension transformation and Monte Carlo methods.
EVA-S2PLoR outperforms many advanced frameworks in terms of precision
(improving the performance of the sigmoid function by about 10 orders of
magnitude compared to most frameworks) and delivers the best overall
performance in secure logistic regression experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimally Solving Simultaneous-Move Dec-POMDPs: The Sequential Central
  Planning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13139v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13139v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johan Peralez, Aurèlien Delage, Jacopo Castellini, Rafael F. Cunha, Jilles S. Dibangoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The centralized training for decentralized execution paradigm emerged as the
state-of-the-art approach to $\epsilon$-optimally solving decentralized
partially observable Markov decision processes. However, scalability remains a
significant issue. This paper presents a novel and more scalable alternative,
namely the sequential-move centralized training for decentralized execution.
This paradigm further pushes the applicability of the Bellman's principle of
optimality, raising three new properties. First, it allows a central planner to
reason upon sufficient sequential-move statistics instead of prior
simultaneous-move ones. Next, it proves that $\epsilon$-optimal value functions
are piecewise linear and convex in such sufficient sequential-move statistics.
Finally, it drops the complexity of the backup operators from double
exponential to polynomial at the expense of longer planning horizons. Besides,
it makes it easy to use single-agent methods, e.g., SARSA algorithm enhanced
with these findings, while still preserving convergence guarantees. Experiments
on two- as well as many-agent domains from the literature against
$\epsilon$-optimal simultaneous-move solvers confirm the superiority of our
novel approach. This paradigm opens the door for efficient planning and
reinforcement learning methods for multi-agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Feature-based Knowledge Distillation for <span class="highlight-title">Recommend</span>er System: A
  Frequency Perspective <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchi Zhu, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we analyze the feature-based knowledge distillation for
recommendation from the frequency perspective. By defining knowledge as
different frequency components of the features, we theoretically demonstrate
that regular feature-based knowledge distillation is equivalent to equally
minimizing losses on all knowledge and further analyze how this equal loss
weight allocation method leads to important knowledge being overlooked. In
light of this, we propose to emphasize important knowledge by redistributing
knowledge weights. Furthermore, we propose FreqD, a lightweight knowledge
reweighting method, to avoid the computational cost of calculating losses on
each knowledge. Extensive experiments demonstrate that FreqD consistently and
significantly outperforms state-of-the-art knowledge distillation methods for
recommender systems. Our code is available at https://github.com/woriazzc/KDs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM KDD 2025 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Metrics for the Assessment of Neurodegenerative Diseases
  through Handwriting Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Thebaud, Anna Favaro, Casey Chen, Gabrielle Chavez, Laureano Moro-Velazquez, Ankur Butala, Najim Dehak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motor dysfunction is a common sign of neurodegenerative diseases (NDs) such
as Parkinson's disease (PD) and Alzheimer's disease (AD), but may be difficult
to detect, especially in the early stages. In this work, we examine the
behavior of a wide array of explainable metrics extracted from the handwriting
signals of 113 subjects performing multiple tasks on a digital tablet, as part
of the Neurological Signals dataset. The aim is to measure their effectiveness
in characterizing NDs, including AD and PD. To this end, task-agnostic and
task-specific metrics are extracted from 14 distinct tasks. Subsequently,
through statistical analysis and a series of classification experiments, we
investigate which metrics provide greater discriminative power between NDs and
healthy controls and amongst different NDs. Preliminary results indicate that
the tasks at hand can all be effectively leveraged to distinguish between the
considered set of NDs, specifically by measuring the stability, the speed of
writing, the time spent not writing, and the pressure variations between groups
from our handcrafted explainable metrics, which shows p-values lower than
0.0001 for multiple tasks. Using various binary classification algorithms on
the computed metrics, we obtain up to 87 % accuracy for the discrimination
between AD and healthy controls (CTL), and up to 69 % for the discrimination
between PD and CTL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages including references, under review in IEEE JHBI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An empirical study of LLaMA3 quantization: from LLMs to MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14047v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14047v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huang, Xingyu Zheng, Xudong Ma, Haotong Qin, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The LLaMA family, a collection of foundation language models ranging from 7B
to 65B parameters, has become one of the most powerful open-source large
language models (LLMs) and the popular LLM backbone of multi-modal large
language models (MLLMs), widely used in computer vision and natural language
understanding tasks. In particular, LLaMA3 models have recently been released
and have achieved impressive performance in various domains with super-large
scale pre-training on over 15T tokens of data. Given the wide application of
low-bit quantization for LLMs in resource-constrained scenarios, we explore
LLaMA3's capabilities when quantized to low bit-width. This exploration can
potentially provide new insights and challenges for the low-bit quantization of
LLaMA3 and other future LLMs, especially in addressing performance degradation
issues that suffer in LLM compression. Specifically, we comprehensively
evaluate the 10 existing post-training quantization and LoRA fine-tuning
(LoRA-FT) methods of LLaMA3 on 1-8 bits and various datasets to reveal the
low-bit quantization performance of LLaMA3. To uncover the capabilities of
low-bit quantized MLLM, we assessed the performance of the LLaMA3-based
LLaVA-Next-8B model under 2-4 ultra-low bits with post-training quantization
methods. Our experimental results indicate that LLaMA3 still suffers from
non-negligible degradation in linguistic and visual contexts, particularly
under ultra-low bit widths. This highlights the significant performance gap at
low bit-width that needs to be addressed in future developments. We expect that
this empirical study will prove valuable in advancing future models, driving
LLMs and MLLMs to achieve higher accuracy at lower bit to enhance practicality.
Our project is released on https://github.com/Macaronlin/LLaMA3-Quantization ,
and quantized models are released at https://huggingface.co/Efficient-ML .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning with Strategic Selection and Forgetting for Network
  Intrusion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinchen Zhang, Running Zhao, Zhihan Jiang, Handi Chen, Yulong Ding, Edith C. H. Ngai, Shuang-Hua Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intrusion Detection Systems (IDS) are crucial for safeguarding digital
infrastructure. In dynamic network environments, both threat landscapes and
normal operational behaviors are constantly changing, resulting in concept
drift. While continuous learning mitigates the adverse effects of concept
drift, insufficient attention to drift patterns and excessive preservation of
outdated knowledge can still hinder the IDS's adaptability. In this paper, we
propose SSF (Strategic Selection and Forgetting), a novel continual learning
method for IDS, providing continuous model updates with a constantly refreshed
memory buffer. Our approach features a strategic sample selection algorithm to
select representative new samples and a strategic forgetting mechanism to drop
outdated samples. The proposed strategic sample selection algorithm prioritizes
new samples that cause the `drifted' pattern, enabling the model to better
understand the evolving landscape. Additionally, we introduce strategic
forgetting upon detecting significant drift by discarding outdated samples to
free up memory, allowing the incorporation of more recent data. SSF captures
evolving patterns effectively and ensures the model is aligned with the change
of data patterns, significantly enhancing the IDS's adaptability to concept
drift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15
datasets demonstrates its superior adaptability to concept drift for network
intrusion detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE International Conference on Computer Communications
  (INFOCOM) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-Agnostic Cosmological Inference with SDSS-IV eBOSS: Simultaneous
  Probing for Background and Perturbed Universe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Purba Mukherjee, Anjan A. Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Here we explore certain subtle features imprinted in data from the completed
Sloan Digital Sky Survey IV (SDSS-IV) extended Baryon Oscillation Spectroscopic
Survey (eBOSS) as a combined probe for the background and perturbed Universe.
We reconstruct the baryon Acoustic Oscillation (BAO) and Redshift Space
Distortion (RSD) observables as functions of redshift, using measurements from
SDSS alone. We apply the Multi-Task Gaussian Process (MTGP) framework to model
the interdependencies of cosmological observables $D_M(z)/r_d$, $D_H(z)/r_d$,
and $f\sigma_8(z)$, and track their evolution across different redshifts.
Subsequently, we obtain constrained three-dimensional phase space containing
$D_M(z)/r_d$, $D_H(z)/r_d$, and $f\sigma_8(z)$ at different redshifts probed by
the SDSS-IV eBOSS survey. Furthermore, assuming the $\Lambda$CDM model, we
obtain constraints on model parameters $\Omega_{m}$, $H_{0}r_{d}$, $\sigma_{8}$
and $S_{8}$ at each redshift probed by SDSS-IV eBOSS. This indicates
redshift-dependent trends in $H_0$, $\Omega_m$, $\sigma_8$ and $S_8$ in the
$\Lambda$CDM model, suggesting a possible inconsistency in the $\Lambda$CDM
model. Ours is a template for model-independent extraction of information for
both background and perturbed Universe using a single galaxy survey taking into
account all the existing correlations between background and perturbed
observables and this can be easily extended to future DESI-3YR as well as
Euclid results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 sets of figures, 3 tables. Comments are welcome. New
  references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaPRL: Adaptive Pairwise Regression Learning with Uncertainty
  Estimation for Universal Regression Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuhang Liang, Rucong Xu, Deng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current deep regression models usually learn in point-wise way that treat
each sample as an independent input, neglecting the relative ordering among
different data. Consequently, the regression model could neglect the data 's
interrelationships, potentially resulting in suboptimal performance. Moreover,
the existence of aleatoric uncertainty in the training data may drive the model
to capture non-generalizable patterns, contributing to increased overfitting.
To address these issues, we propose a novel adaptive pairwise learning
framework (AdaPRL) for regression tasks which leverages the relative
differences between data points and integrates with deep probabilistic models
to quantify the uncertainty associated with the predictions. Additionally, we
adapt AdaPRL for applications in multi-task learning and multivariate time
series forecasting. Extensive experiments with several real-world regression
datasets including recommendation systems, age estimation, time series
forecasting, natural language understanding, finance, and industry datasets
show that AdaPRL is compatible with different backbone networks in various
tasks and achieves state-of-the-art performance on the vast majority of tasks,
highlighting its notable potential including enhancing prediction accuracy and
ranking ability, increasing generalization capability, improving robustness to
noisy data, improving resilience to reduced data, and enhancing
interpretability, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIO: A <span class="highlight-title">Foundation</span> Model on Multimodal Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17692v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17692v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. Codes and models are available in
  https://github.com/MIO-Team/MIO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplifying CLIP: Unleashing the Power of Large-Scale Models on
  Consumer-level Computers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) has attracted a surge of
attention for its superior zero-shot performance and excellent transferability
to downstream tasks. However, training such large-scale models usually requires
substantial computation and storage, which poses barriers for general users
with consumer-level computers. Motivated by this observation, in this paper we
investigate how to achieve competitive performance on only one Nvidia RTX3090
GPU and with one terabyte for storing dataset. On one hand, we simplify the
transformer block structure and combine Weight Inheritance with multi-stage
Knowledge Distillation (WIKD), thereby reducing the parameters and improving
the inference speed during training along with deployment. On the other hand,
confronted with the convergence challenge posed by small dataset, we generate
synthetic captions for each sample as data augmentation, and devise a novel
Pair Matching (PM) loss to fully exploit the distinguishment among positive and
negative image-text pairs. Extensive experiments demonstrate that our model can
achieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which
could further popularize the CLIP model in the related research community.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Ma, Zhuo Chen, Yuping Wang, Eng Siong Chng, Xie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Audio-Language Models (LALMs) have demonstrated remarkable performance
in tasks involving audio perception and understanding, such as speech
recognition and audio captioning. However, their reasoning capabilities -
critical for solving complex real-world problems - remain underexplored. In
this work, we conduct the first exploration into integrating Chain-of-Thought
(CoT) reasoning into LALMs to enhance their reasoning ability across auditory
modalities. We evaluate representative CoT methods, analyzing their performance
in both information extraction and reasoning tasks across sound, music, and
speech domains. Our findings reveal that CoT methods significantly improve
performance on easy and medium tasks but encounter challenges with hard tasks,
where reasoning chains can confuse the model rather than improve accuracy.
Additionally, we identify a positive correlation between reasoning path length
and accuracy, demonstrating the potential of scaling inference for advanced
instruction-following and reasoning. This study not only highlights the promise
of CoT in enhancing LALM reasoning capabilities but also identifies key
limitations and provides actionable directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth and Image Fusion for Road Obstacle Detection Using Stereo Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Perezyabov, Mikhail Gavrilenkov, Ilya Afanasyev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is devoted to the detection of objects on a road, performed with a
combination of two methods based on both the use of depth information and video
analysis of data from a stereo camera. Since neither the time of the appearance
of an object on the road, nor its size and shape is known in advance,
ML/DL-based approaches are not applicable. The task becomes more complicated
due to variations in artificial illumination, inhomogeneous road surface
texture, and unknown character and features of the object. To solve this
problem we developed the depth and image fusion method that complements a
search of small contrast objects by RGB-based method, and obstacle detection by
stereo image-based approach with SLIC superpixel segmentation. We conducted
experiments with static and low speed obstacles in an underground parking lot
and demonstrated the successful work of the developed technique for detecting
and even tracking small objects, which can be parking infrastructure objects,
things left on the road, wheels, dropped boxes, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multimodal Fusion via Meta-Learning Towards Micro-Video
  <span class="highlight-title">Recommend</span>ation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Liu, Yinwei Wei, Fan Liu, Wenjie Wang, Liqiang Nie, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal information (e.g., visual, acoustic, and textual) has been widely
used to enhance representation learning for micro-video recommendation. For
integrating multimodal information into a joint representation of micro-video,
multimodal fusion plays a vital role in the existing micro-video recommendation
approaches. However, the static multimodal fusion used in previous studies is
insufficient to model the various relationships among multimodal information of
different micro-videos. In this paper, we develop a novel meta-learning-based
multimodal fusion framework called Meta Multimodal Fusion (MetaMMF), which
dynamically assigns parameters to the multimodal fusion function for each
micro-video during its representation learning. Specifically, MetaMMF regards
the multimodal fusion of each micro-video as an independent task. Based on the
meta information extracted from the multimodal features of the input task,
MetaMMF parameterizes a neural network as the item-specific fusion function via
a meta learner. We perform extensive experiments on three benchmark datasets,
demonstrating the significant improvements over several state-of-the-art
multimodal recommendation models, like MMGCN, LATTICE, and InvRL. Furthermore,
we lighten our model by adopting canonical polyadic decomposition to improve
the training efficiency, and validate its effectiveness through experimental
results. Codes are available at https://github.com/hanliu95/MetaMMF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ACM Transactions on Information
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pedestrian Trajectory Prediction Based on Social Interactions Learning
  With Random Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajia Xie, Sheng Zhang, Beihao Xia, Zhu Xiao, Hongbo Jiang, Siwang Zhou, Zheng Qin, Hongyang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrian trajectory prediction is a critical technology in the evolution of
self-driving cars toward complete artificial intelligence. Over recent years,
focusing on the trajectories of pedestrians to model their social interactions
has surged with great interest in more accurate trajectory predictions.
However, existing methods for modeling pedestrian social interactions rely on
pre-defined rules, struggling to capture non-explicit social interactions. In
this work, we propose a novel framework named DTGAN, which extends the
application of Generative Adversarial Networks (GANs) to graph sequence data,
with the primary objective of automatically capturing implicit social
interactions and achieving precise predictions of pedestrian trajectory. DTGAN
innovatively incorporates random weights within each graph to eliminate the
need for pre-defined interaction rules. We further enhance the performance of
DTGAN by exploring diverse task loss functions during adversarial training,
which yields improvements of 16.7\% and 39.3\% on metrics ADE and FDE,
respectively. The effectiveness and accuracy of our framework are verified on
two public datasets. The experimental results show that our proposed DTGAN
achieves superior performance and is well able to understand pedestrians'
intentions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,7 figures,Accepted to IEEE Transactions on Multimedia (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient NVoD Scheme Using Implicit Error Correction and Subchannels
  for Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Asorey-Cacheda, Antonio-Javier Garcia-Sanchez, Joan Garcia-Haro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Error Correction (IEC) is a near Video-on-Demand (nVoD) scheme that
trades bandwidth utilization for initial playback delay to potentially support
an infinite number of users. Additionally, it provides error protection without
any further bandwidth increase by exploiting the implicit redundancy of nVoD
protocols, using linear combinations of the segments transmitted in a given
time slot. However, IEC packet loss protection is weaker at the beginning of
the playback due to the lack of implicit redundancy and lower decoding
efficiency, resulting in worse subjective playback quality. In tackling this
issue, this paper contributes with an extension of the original nVoD
architecture, enhancing its performance by adding a new element namely,
subchannels. These subdivisions of the original channels do not provide further
packet loss protection but significantly improve the decoding efficiency, which
in turn increases playback quality, especially at the beginning. Even for very
high packet loss probabilities, subchannels are designed to obtain higher
decoding efficiency which results in greater packet loss protection than that
provided by IEC. The proposed scheme is especially useful in wireless
cooperative networks using techniques such as network coding, as content
transmissions can be split into different subchannels in order to maximize
network efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Sound of Water: Inferring Physical Properties from Pouring Liquids <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush Bagad, Makarand Tapaswi, Cees G. M. Snoek, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the connection between audio-visual observations and the underlying
physics of a mundane yet intriguing everyday activity: pouring liquids. Given
only the sound of liquid pouring into a container, our objective is to
automatically infer physical properties such as the liquid level, the shape and
size of the container, the pouring rate and the time to fill. To this end, we:
(i) show in theory that these properties can be determined from the fundamental
frequency (pitch); (ii) train a pitch detection model with supervision from
simulated data and visual data with a physics-inspired objective; (iii)
introduce a new large dataset of real pouring videos for a systematic study;
(iv) show that the trained model can indeed infer these physical properties for
real data; and finally, (v) we demonstrate strong generalization to various
container shapes, other datasets, and in-the-wild YouTube videos. Our work
presents a keen understanding of a narrow yet rich problem at the intersection
of acoustics, physics, and learning. It opens up applications to enhance
multisensory perception in robotic pouring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://bpiyush.github.io/pouring-water-website.
  Short version accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BeFA: A General Behavior-driven Feature Adapter for Multimedia
  <span class="highlight-title">Recommend</span>ation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qile Fan, Penghang Yu, Zhiyi Tan, Bing-Kun Bao, Guanming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia recommender systems focus on utilizing behavioral information and
content information to model user preferences. Typically, it employs
pre-trained feature encoders to extract content features, then fuses them with
behavioral features. However, pre-trained feature encoders often extract
features from the entire content simultaneously, including excessive
preference-irrelevant details. We speculate that it may result in the extracted
features not containing sufficient features to accurately reflect user
preferences. To verify our hypothesis, we introduce an attribution analysis
method for visually and intuitively analyzing the content features. The results
indicate that certain products' content features exhibit the issues of
information drift}and information omission,reducing the expressive ability of
features. Building upon this finding, we propose an effective and efficient
general Behavior-driven Feature Adapter (BeFA) to tackle these issues. This
adapter reconstructs the content feature with the guidance of behavioral
information, enabling content features accurately reflecting user preferences.
Extensive experiments demonstrate the effectiveness of the adapter across all
multimedia recommendation methods. Our code is made publicly available on
https://github.com/fqldom/BeFA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is
  Not AGI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rolf Pfister, Hansueli Jud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenAI's o3 achieves a high score of 87.5 % on ARC-AGI, a benchmark proposed
to measure intelligence. This raises the question whether systems based on
Large Language Models (LLMs), particularly o3, demonstrate intelligence and
progress towards artificial general intelligence (AGI). Building on the
distinction between skills and intelligence made by Fran\c{c}ois Chollet, the
creator of ARC-AGI, a new understanding of intelligence is introduced: an agent
is the more intelligent, the more efficiently it can achieve the more diverse
goals in the more diverse worlds with the less knowledge. An analysis of the
ARC-AGI benchmark shows that its tasks represent a very specific type of
problem that can be solved by massive trialling of combinations of predefined
operations. This method is also applied by o3, achieving its high score through
the extensive use of computing power. However, for most problems in the
physical world and in the human domain, solutions cannot be tested in advance
and predefined operations are not available. Consequently, massive trialling of
predefined operations, as o3 does, cannot be a basis for AGI - instead, new
approaches are required that can reliably solve a wide variety of problems
without existing skills. To support this development, a new benchmark for
intelligence is outlined that covers a much higher diversity of unknown tasks
to be solved, thus enabling a comprehensive assessment of intelligence and of
progress towards AGI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the effects of logical database design on database size, query
  complexity, query performance, and energy consumption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni Taipalus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Database normalization theory is the basis for logical design of relational
databases. Normalization reduces data redundancy and consequently eliminates
potential data anomalies, while increasing the computational cost of read
operations. Despite decades worth of applications of normalization theory, it
still remains largely unclear to what extent normalization affects database
size and efficiency. In this study, we study the effects of database
normalization using the Internet Movie Database (IMDb) public dataset and
PostgreSQL. The results indicate, rather intuitively, that (i) database size on
disk is reduced through normalization from 1NF to 2NF by 10%, but not from 2NF
to 4NF, (ii) the number of tables and table rows in total increase
monotonically from 1NF to 2NF to 4NF, and that (iii) query complexity increases
with further normalization. Surprisingly, however, the results also indicate
that (iv) normalization from 1NF to 2NF increases throughput by a factor of 4,
and consequently, (v) energy consumption per transaction reduces by 74% with
normalization from 1NF to 2NF. The results imply that the gains of
normalization from 2NF to 4NF in terms of throughput and energy consumption are
minimal, yet increase the storage space requirements by approximately 7%. While
these results represent merely one specific case, they provide needed empirical
evaluation on the practical effects and magnitude of database normalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuji Chai, Mujin Kwen, David Brooks, Gu-Yeon Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying LLMs on edge devices presents serious technical challenges. Memory
elasticity is crucial for edge devices with unified memory, where memory is
shared and fluctuates dynamically. Existing solutions suffer from either poor
transition granularity or high storage costs. We propose FlexQuant, a novel
elasticity framework that generates an ensemble of quantized models, providing
an elastic hosting solution with 15x granularity improvement and 10x storage
reduction compared to SoTA methods. FlexQuant works with most quantization
methods and creates a family of trade-off options under various storage limits
through our pruning method. It brings great performance and flexibility to the
edge deployment of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Database <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the effects of logical database design on database size, query
  complexity, query performance, and energy consumption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toni Taipalus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Database normalization theory is the basis for logical design of relational
databases. Normalization reduces data redundancy and consequently eliminates
potential data anomalies, while increasing the computational cost of read
operations. Despite decades worth of applications of normalization theory, it
still remains largely unclear to what extent normalization affects database
size and efficiency. In this study, we study the effects of database
normalization using the Internet Movie Database (IMDb) public dataset and
PostgreSQL. The results indicate, rather intuitively, that (i) database size on
disk is reduced through normalization from 1NF to 2NF by 10%, but not from 2NF
to 4NF, (ii) the number of tables and table rows in total increase
monotonically from 1NF to 2NF to 4NF, and that (iii) query complexity increases
with further normalization. Surprisingly, however, the results also indicate
that (iv) normalization from 1NF to 2NF increases throughput by a factor of 4,
and consequently, (v) energy consumption per transaction reduces by 74% with
normalization from 1NF to 2NF. The results imply that the gains of
normalization from 2NF to 4NF in terms of throughput and energy consumption are
minimal, yet increase the storage space requirements by approximately 7%. While
these results represent merely one specific case, they provide needed empirical
evaluation on the practical effects and magnitude of database normalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An ontology-based description of nano computed tomo<span class="highlight-title">graph</span>y measurements
  in electronic laboratory notebooks: from metadata schema to first user
  experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Kirchner, D. C. Florian Wieland, Sarah Irvine, Sven Schimek, Jan Reimers, Rossella Aversa, Alexey Boubnov, Christian Lucas, Silja Flenner, Imke Greving, André Lopes Marinho, Tak Ming Wong, Regine Willumeit-Römer, Catriona Eschke, Berit Zeller-Plumhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the importance of well-documented metadata has been
discussed increasingly in many research fields. Making all metadata generated
during scientific research available in a findable, accessible, interoperable,
and reusable (FAIR) manner remains a significant challenge for researchers
across fields. Scientific communities are agreeing to achieve this by making
all data available in a semantically annotated knowledge graph using semantic
web technologies. Most current approaches do not gather metadata in a
consistent and community-agreed standardized way, and there are insufficient
tools to support the process of turning them into a knowledge graph. We present
an example solution in which the creation of a schema and ontology are placed
at the beginning of the scientific process which is then - using the electronic
laboratory notebook framework Herbie - turned into a bespoke data collection
platform to facilitate validation and semantic annotation of the metadata
immediately during an experiment. Using the example of synchrotron
radiation-based nano computed tomography measurements, we present a holistic
approach which can capture the complex metadata of such research instruments in
a flexible and straightforward manner. Different instrument setups of this
beamline can be considered, allowing a user-friendly experience. We show how
Herbie turns all semantic documents into an accessible user interface, where
all data entered automatically fulfills all requirements of being FAIR, and
present how data can be directly extracted via competency questions without
requiring familiarity with the fine-grained structure of the knowledge graph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 13 figures, 5 tables. Fabian Kirchner and Florian Wieland
  have contributed equally to the manuscript. Corresponding authors:
  fabian.kirchner@hereon.de, catriona.eschke@hereon.de</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multiple Temporal Network Kernel Density Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Shao, Peng Cheng, Xiang Lian, Lei Chen, Wangze Ni, Xuemin Lin, Chen Zhang, Liping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel density estimation (KDE) has become a popular method for visual
analysis in various fields, such as financial risk forecasting, crime
clustering, and traffic monitoring. KDE can identify high-density areas from
discrete datasets. However, most existing works only consider planar distance
and spatial data. In this paper, we introduce a new model, called TN-KDE, that
applies KDE-based techniques to road networks with temporal data. Specifically,
we introduce a novel solution, Range Forest Solution (RFS), which can
efficiently compute KDE values on spatiotemporal road networks. To support the
insertion operation, we present a dynamic version, called Dynamic Range Forest
Solution (DRFS). We also propose an optimization called Lixel Sharing (LS) to
share similar KDE values between two adjacent lixels. Furthermore, our
solutions support many non-polynomial kernel functions and still report exact
values. Experimental results show that our solutions achieve up to 6 times
faster than the state-of-the-art method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADKGD: Anomaly Detection in Knowledge <span class="highlight-title">Graph</span>s with Dual-Channel Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Wu, Wensheng Gan, Jiahao Zhang, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current development of large language models (LLMs), it is important
to ensure the accuracy and reliability of the underlying data sources. LLMs are
critical for various applications, but they often suffer from hallucinations
and inaccuracies due to knowledge gaps in the training data. Knowledge graphs
(KGs), as a powerful structural tool, could serve as a vital external
information source to mitigate the aforementioned issues. By providing a
structured and comprehensive understanding of real-world data, KGs enhance the
performance and reliability of LLMs. However, it is common that errors exist in
KGs while extracting triplets from unstructured data to construct KGs. This
could lead to degraded performance in downstream tasks such as
question-answering and recommender systems. Therefore, anomaly detection in KGs
is essential to identify and correct these errors. This paper presents an
anomaly detection algorithm in knowledge graphs with dual-channel learning
(ADKGD). ADKGD leverages a dual-channel learning approach to enhance
representation learning from both the entity-view and triplet-view
perspectives. Furthermore, using a cross-layer approach, our framework
integrates internal information aggregation and context information
aggregation. We introduce a kullback-leibler (KL)-loss component to improve the
accuracy of the scoring function between the dual channels. To evaluate ADKGD's
performance, we conduct empirical studies on three real-world KGs: WN18RR,
FB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms
the state-of-the-art anomaly detection algorithms. The source code and datasets
are publicly available at https://github.com/csjywu1/ADKGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Outlier Connections Detection in Databases Network Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Rodniansky, Tania Butovsky, Mikhail Shpak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The article describes a practical method for detecting outlier database
connections in real-time. Outlier connections are detected with a specified
level of confidence. The method is based on generalized security rules and a
simple but effective real-time machine learning mechanism. The described method
is non-intrusive to the database and does not depend on the type of database.
The method is used to proactively control access even before database
connection is established, minimize false positives, and maintain the required
response speed to detected database connection outliers. The capabilities of
the system are demonstrated with several examples of outliers in real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Historical Butterfly Counting in Large Temporal Bipartite
  Networks via <span class="highlight-title">Graph</span> Structure-aware <span class="highlight-title">Index</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuyang Mang, Jingbang Chen, Hangrui Zhou, Yu Gao, Yingli Zhou, Qingyu Shi, Richard Peng, Yixiang Fang, Chenhao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bipartite graphs are ubiquitous in many domains, e.g., e-commerce platforms,
social networks, and academia, by modeling interactions between distinct entity
sets. Within these graphs, the butterfly motif, a complete 2*2 biclique,
represents the simplest yet significant subgraph structure, crucial for
analyzing complex network patterns. Counting the butterflies offers significant
benefits across various applications, including community analysis and
recommender systems. Additionally, the temporal dimension of bipartite graphs,
where edges activate within specific time frames, introduces the concept of
historical butterfly counting, i.e., counting butterflies within a given time
interval. This temporal analysis sheds light on the dynamics and evolution of
network interactions, offering new insights into their mechanisms. Despite its
importance, no existing algorithm can efficiently solve the historical
butterfly counting task. To address this, we design two novel indices whose
memory footprints are dependent on #butterflies and #wedges, respectively.
Combining these indices, we propose a graph structure-aware indexing approach
that significantly reduces memory usage while preserving exceptional query
speed. We theoretically prove that our approach is particularly advantageous on
power-law graphs, a common characteristic of real-world bipartite graphs, by
surpassing traditional complexity barriers for general graphs. Extensive
experiments reveal that our query algorithms outperform existing methods by up
to five magnitudes, effectively balancing speed with manageable memory
requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subspace Collision: An Efficient and Accurate Framework for
  High-dimensional Approximate Nearest Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuqi Wei, Xiaodong Lee, Zhenyu Liao, Themis Palpanas, Botao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor (ANN) search in high-dimensional Euclidean
spaces is a fundamental problem with a wide range of applications. However,
there is currently no ANN method that performs well in both indexing and query
answering performance, while providing rigorous theoretical guarantees for the
quality of the answers. In this paper, we first design SC-score, a metric that
we show follows the Pareto principle and can act as a proxy for the Euclidean
distance between data points. Inspired by this, we propose a novel ANN search
framework called Subspace Collision (SC), which can provide theoretical
guarantees on the quality of its results. We further propose SuCo, which
achieves efficient and accurate ANN search by designing a clustering-based
lightweight index and query strategies for our proposed subspace collision
framework. Extensive experiments on real-world datasets demonstrate that both
the indexing and query answering performance of SuCo outperform
state-of-the-art ANN methods that can provide theoretical guarantees,
performing 1-2 orders of magnitude faster query answering with only up to
one-tenth of the index memory footprint. Moreover, SuCo achieves top
performance (best for hard datasets) even when compared to methods that do not
provide theoretical guarantees. This paper was published in SIGMOD 2025.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-12T00:00:00Z">2025-01-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Transferability of Multimodal Adversarial Samples for
  Vision-Language <span class="highlight-title">Pre-train</span>ing Models with Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12636v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12636v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youze Wang, Wenbo Hu, Yinpeng Dong, Hanwang Zhang, Hang Su, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of visual and textual data in Vision-Language Pre-training
(VLP) models is crucial for enhancing vision-language understanding. However,
the adversarial robustness of these models, especially in the alignment of
image-text features, has not yet been sufficiently explored. In this paper, we
introduce a novel gradient-based multimodal adversarial attack method,
underpinned by contrastive learning, to improve the transferability of
multimodal adversarial samples in VLP models. This method concurrently
generates adversarial texts and images within imperceptive perturbation,
employing both image-text and intra-modal contrastive loss. We evaluate the
effectiveness of our approach on image-text retrieval and visual entailment
tasks, using publicly available datasets in a black-box setting. Extensive
experiments indicate a significant advancement over existing single-modal
transfer-based adversarial attack methods and current multimodal adversarial
attack approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image
  Compression <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08505v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08505v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Shenyuan Gao, Zhening Liu, Jiawei Shao, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing learning-based stereo image codec adopt sophisticated transformation
with simple entropy models derived from single image codecs to encode latent
representations. However, those entropy models struggle to effectively capture
the spatial-disparity characteristics inherent in stereo images, which leads to
suboptimal rate-distortion results. In this paper, we propose a stereo image
compression framework, named CAMSIC. CAMSIC independently transforms each image
to latent representation and employs a powerful decoder-free Transformer
entropy model to capture both spatial and disparity dependencies, by
introducing a novel content-aware masked image modeling (MIM) technique. Our
content-aware MIM facilitates efficient bidirectional interaction between prior
information and estimated tokens, which naturally obviates the need for an
extra Transformer decoder. Experiments show that our stereo image codec
achieves state-of-the-art rate-distortion performance on two stereo image
datasets Cityscapes and InStereo2K with fast encoding and decoding speed. Code
is available at https://github.com/Xinjie-Q/CAMSIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Optimizing Locality of <span class="highlight-title">Graph</span> Transposition on Modern Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Koohi Esfahani, Hans Vandierendonck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the shared-memory Graph Transposition (GT) problem, a
fundamental graph algorithm that is widely used in graph analytics and
scientific computing.
  Previous GT algorithms have significant memory requirements that are
proportional to the number of vertices and threads which obstructs their use on
large graphs. Moreover, atomic memory operations have become comparably fast on
recent CPU architectures, which creates new opportunities for improving the
performance of concurrent atomic accesses in GT.
  We design PoTra, a GT algorithm which leverages graph structure and processor
and memory architecture to optimize locality and performance. PoTra limits the
size of additional data structures close to CPU cache sizes and utilizes the
skewed degree distribution of graph datasets to optimize locality and
performance. We present the performance model of PoTra to explain the
connection between cache and memory response times and graph locality.
  Our evaluation of PoTra on three CPU architectures and 20 real-world and
synthetic graph datasets with up to 128 billion edges demonstrates that PoTra
achieves up to 8.7 times speedup compared to previous works and if there is a
performance loss it remains limited to 15.7%, on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Structure-Aware Framework for Learning Device Placements on
  Computation <span class="highlight-title">Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shukai Duan, Heng Ping, Nikos Kanakaris, Xiongye Xiao, Panagiotis Kyriakis, Nesreen K. Ahmed, Peiyu Zhang, Guixiang Ma, Mihai Capota, Shahin Nazarian, Theodore L. Willke, Paul Bogdan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computation graphs are Directed Acyclic Graphs (DAGs) where the nodes
correspond to mathematical operations and are used widely as abstractions in
optimizations of neural networks. The device placement problem aims to identify
optimal allocations of those nodes to a set of (potentially heterogeneous)
devices. Existing approaches rely on two types of architectures known as
grouper-placer and encoder-placer, respectively. In this work, we bridge the
gap between encoder-placer and grouper-placer techniques and propose a novel
framework for the task of device placement, relying on smaller computation
graphs extracted from the OpenVINO toolkit. The framework consists of five
steps, including graph coarsening, node representation learning and policy
optimization. It facilitates end-to-end training and takes into account the DAG
nature of the computation graphs. We also propose a model variant, inspired by
graph parsing networks and complex network analysis, enabling graph
representation learning and jointed, personalized graph partitioning, using an
unspecified number of groups. To train the entire framework, we use
reinforcement learning using the execution time of the placement as a reward.
We demonstrate the flexibility and effectiveness of our approach through
multiple experiments with three benchmark models, namely Inception-V3, ResNet,
and BERT. The robustness of the proposed framework is also highlighted through
an ablation study. The suggested placements improve the inference speed for the
benchmark models by up to 58.2% over CPU execution and by up to 60.24% compared
to other commonly used baselines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patent Novelty Assessment Accelerating Innovation and Patent Prosecution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kapil Kashyap, Sean Fargose, Gandhar Dhonde, Aditya Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving landscape of technological innovation, safeguarding
intellectual property rights through patents is crucial for fostering progress
and stimulating research and development investments. This report introduces a
ground-breaking Patent Novelty Assessment and Claim Generation System,
meticulously crafted to dissect the inventive aspects of intellectual property
and simplify access to extensive patent claim data. Addressing a crucial gap in
academic institutions, our system provides college students and researchers
with an intuitive platform to navigate and grasp the intricacies of patent
claims, particularly tailored for the nuances of Chinese patents. Unlike
conventional analysis systems, our initiative harnesses a proprietary Chinese
API to ensure unparalleled precision and relevance. The primary challenge lies
in the complexity of accessing and comprehending diverse patent claims,
inhibiting effective innovation upon existing ideas. Our solution aims to
overcome these barriers by offering a bespoke approach that seamlessly
retrieves comprehensive claim information, finely tuned to the specifics of the
Chinese patent landscape. By equipping users with efficient access to
comprehensive patent claim information, our transformative platform seeks to
ignite informed exploration and innovation in the ever-evolving domain of
intellectual property. Its envisioned impact transcends individual colleges,
nurturing an environment conducive to research and development while deepening
the understanding of patented concepts within the academic community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Claims in Economics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Garg, Thiemo Fetzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze over 44,000 NBER and CEPR working papers from 1980 to 2023 using a
custom language model to construct knowledge graphs that map economic concepts
and their relationships. We distinguish between general claims and those
documented via causal inference methods (e.g., DiD, IV, RDD, RCTs). We document
a substantial rise in the share of causal claims-from roughly 4% in 1990 to
nearly 28% in 2020-reflecting the growing influence of the "credibility
revolution." We find that causal narrative complexity (e.g., the depth of
causal chains) strongly predicts both publication in top-5 journals and higher
citation counts, whereas non-causal complexity tends to be uncorrelated or
negatively associated with these outcomes. Novelty is also pivotal for top-5
publication, but only when grounded in credible causal methods: introducing
genuinely new causal edges or paths markedly increases both the likelihood of
acceptance at leading outlets and long-run citations, while non-causal novelty
exhibits weak or even negative effects. Papers engaging with central, widely
recognized concepts tend to attract more citations, highlighting a divergence
between factors driving publication success and long-term academic impact.
Finally, bridging underexplored concept pairs is rewarded primarily when
grounded in causal methods, yet such gap filling exhibits no consistent link
with future citations. Overall, our findings suggest that methodological rigor
and causal innovation are key drivers of academic recognition, but sustained
impact may require balancing novel contributions with conceptual integration
into established economic discourse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For data, interactive tools, and additional project information,
  visit https://www.causal.claims/. The website contains resources such as data
  downloads, interactive author and paper-level knowledge graphs, and more</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Temporal Trends in 19th Century Literature: An Information
  <span class="highlight-title">Retrie</span>val Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suchana Datta, Dwaipayan Roy, Derek Greene, Gerardine Meaney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In English literature, the 19th century witnessed a significant transition in
styles, themes, and genres. Consequently, the novels from this period display
remarkable diversity. This paper explores these variations by examining the
evolution of term usage in 19th century English novels through the lens of
information retrieval. By applying a query expansion-based approach to a
decade-segmented collection of fiction from the British Library, we examine how
related terms vary over time. Our analysis employs multiple standard metrics
including Kendall's tau, Jaccard similarity, and Jensen-Shannon divergence to
assess overlaps and shifts in expanded query term sets. Our results indicate a
significant degree of divergence in the related terms across decades as
selected by the query expansion technique, suggesting substantial linguistic
and conceptual changes throughout the 19th century novels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at JCDL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models, Knowledge <span class="highlight-title">Graph</span>s and Search Engines: A Crossroads
  for Answering Users' Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aidan Hogan, Xin Luna Dong, Denny Vrandečić, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much has been discussed about how Large Language Models, Knowledge Graphs and
Search Engines can be combined in a synergistic manner. A dimension largely
absent from current academic discourse is the user perspective. In particular,
there remain many open questions regarding how best to address the diverse
information needs of users, incorporating varying facets and levels of
difficulty. This paper introduces a taxonomy of user information needs, which
guides us to study the pros, cons and possible synergies of Large Language
Models, Knowledge Graphs and Search Engines. From this study, we derive a
roadmap for future research.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Database <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a visually interpre<span class="highlight-title">table</span> analysis of Two-Phase Locking
  membership 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Martinenghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-phase locking (2PL) is a consolidated policy commonly adopted by Database
Management Systems to enforce serializability of a schedule. While the policy
is well understood, both in its standard and in the strict version,
automatically deriving a suitable tabular/graphical analysis of schedules with
respect to 2PL is far from trivial, and requires several technicalities that do
not straightforwardly translate to visual cues. In this paper, we delve into
the details of the development of a tool for 2PL analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Data Sketches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Zhang, Mohsen Heidari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in quantum technologies, particularly in quantum sensing
and simulation, have facilitated the generation and analysis of inherently
quantum data. This progress underscores the necessity for developing efficient
and scalable quantum data management strategies. This goal faces immense
challenges due to the exponential dimensionality of quantum data and its unique
quantum properties such as no-cloning and measurement stochasticity.
Specifically, classical storage and manipulation of an arbitrary n-qubit
quantum state requires exponential space and time. Hence, there is a critical
need to revisit foundational data management concepts and algorithms for
quantum data. In this paper, we propose succinct quantum data sketches to
support basic database operations such as search and selection. We view our
work as an initial step towards the development of quantum data management
model, opening up many possibilities for future research in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-11T00:00:00Z">2025-01-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NVS-SQA: Exploring <span class="highlight-title">Self-Supervised</span> Quality Representation Learning for
  Neurally Synthesized Scenes without References 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Weidong Cai, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,
effectively creates photorealistic scenes from sparse viewpoints, typically
evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,
these full-reference methods, which compare synthesized views to reference
views, may not fully capture the perceptual quality of neurally synthesized
scenes (NSS), particularly due to the limited availability of dense reference
views. Furthermore, the challenges in acquiring human perceptual labels hinder
the creation of extensive labeled datasets, risking model overfitting and
reduced generalizability. To address these issues, we propose NVS-SQA, a NSS
quality assessment method to learn no-reference quality representations through
self-supervision without reliance on human labels. Traditional self-supervised
learning predominantly relies on the "same instance, similar representation"
assumption and extensive datasets. However, given that these conditions do not
apply in NSS quality assessment, we employ heuristic cues and quality scores as
learning objectives, along with a specialized contrastive pair preparation
process to improve the effectiveness and efficiency of learning. The results
show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,
on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second
best) and even exceeds 16 full-reference methods across all evaluation metrics
(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual question answering: from early developments to recent advances --
  a <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngoc Dung Huynh, Mohamed Reda Bouadjenek, Sunil Aryal, Imran Razzak, Hakim Hacid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is an evolving research field aimed at
enabling machines to answer questions about visual content by integrating image
and language processing techniques such as feature extraction, object
detection, text embedding, natural language understanding, and language
generation. With the growth of multimodal data research, VQA has gained
significant attention due to its broad applications, including interactive
educational tools, medical image diagnosis, customer service, entertainment,
and social media captioning. Additionally, VQA plays a vital role in assisting
visually impaired individuals by generating descriptive content from images.
This survey introduces a taxonomy of VQA architectures, categorizing them based
on design choices and key components to facilitate comparative analysis and
evaluation. We review major VQA approaches, focusing on deep learning-based
methods, and explore the emerging field of Large Visual Language Models (LVLMs)
that have demonstrated success in multimodal tasks like VQA. The paper further
examines available datasets and evaluation metrics essential for measuring VQA
system performance, followed by an exploration of real-world VQA applications.
Finally, we highlight ongoing challenges and future directions in VQA research,
presenting open questions and potential areas for further development. This
survey serves as a comprehensive resource for researchers and practitioners
interested in the latest advancements and future
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 papers</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Recommend</span>ing the right academic programs: An interest mining approach
  using BERTopic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Hill, Kalen Goo, Puneet Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prospective students face the challenging task of selecting a university
program that will shape their academic and professional careers. For
decision-makers and support services, it is often time-consuming and extremely
difficult to match personal interests with suitable programs due to the vast
and complex catalogue information available. This paper presents the first
information system that provides students with efficient recommendations based
on both program content and personal preferences. BERTopic, a powerful topic
modeling algorithm, is used that leverages text embedding techniques to
generate topic representations. It enables us to mine interest topics from all
course descriptions, representing the full body of knowledge taught at the
institution. Underpinned by the student's individual choice of topics, a
shortlist of the most relevant programs is computed through statistical
backtracking in the knowledge map, a novel characterization of the
program-course relationship. This approach can be applied to a wide range of
educational settings, including professional and vocational training. A case
study at a post-secondary school with 80 programs and over 5,000 courses shows
that the system provides immediate and effective decision support. The
presented interest topics are meaningful, leading to positive effects such as
serendipity, personalization, and fairness, as revealed by a qualitative study
involving 65 students. Over 98% of users indicated that the recommendations
aligned with their interests, and about 94% stated they would use the tool in
the future. Quantitative analysis shows the system can be configured to ensure
fairness, achieving 98% program coverage while maintaining a personalization
score of 0.77. These findings suggest that this real-time, user-centered,
data-driven system could improve the program selection process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Data Mining and Knowledge Discovery (Springer)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing the Role of Context in Forecasting with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gerrit Mutschlechner, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the forecasting performance of recent language models
(LLMs) on binary forecasting questions. We first introduce a novel dataset of
over 600 binary forecasting questions, augmented with related news articles and
their concise question-related summaries. We then explore the impact of input
prompts with varying level of context on forecasting performance. The results
indicate that incorporating news articles significantly improves performance,
while using few-shot examples leads to a decline in accuracy. We find that
larger models consistently outperform smaller models, highlighting the
potential of LLMs in enhancing automated forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Sequential <span class="highlight-title">Recommend</span>ations with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artun Boz, Wouter Zorgdrager, Zoe Kotti, Jesse Harte, Panos Louridas, Dietmar Jannach, Vassilios Karakoidas, Marios Fragkoulis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sequential recommendation problem has attracted considerable research
attention in the past few years, leading to the rise of numerous recommendation
models. In this work, we explore how Large Language Models (LLMs), which are
nowadays introducing disruptive effects in many AI-based applications, can be
used to build or improve sequential recommendation approaches. Specifically, we
design three orthogonal approaches and hybrids of those to leverage the power
of LLMs in different ways. In addition, we investigate the potential of each
approach by focusing on its comprising technical aspects and determining an
array of alternative choices for each one. We conduct extensive experiments on
three datasets and explore a large variety of configurations, including
different language models and baseline recommendation models, to obtain a
comprehensive picture of the performance of each approach. Among other
observations, we highlight that initializing state-of-the-art sequential
recommendation models such as BERT4Rec or SASRec with embeddings obtained from
an LLM can lead to substantial performance gains in terms of accuracy.
Furthermore, we find that fine-tuning an LLM for recommendation tasks enables
it to learn not only the tasks, but also concepts of a domain to some extent.
We also show that fine-tuning OpenAI GPT leads to considerably better
performance than fine-tuning Google PaLM 2. Overall, our extensive experiments
indicate a huge potential value of leveraging LLMs in future recommendation
approaches. We publicly share the code and data of our experiments to ensure
reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 12 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Database <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TWIX: Automatically Reconstructing Structured Data from Templatized
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Lin, Mawil Hasan, Rohan Kosalge, Alvin Cheung, Aditya G. Parameswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many documents, that we call templatized documents, are programmatically
generated by populating fields in a visual template. Effective data extraction
from these documents is crucial to supporting downstream analytical tasks.
Current data extraction tools often struggle with complex document layouts,
incur high latency and/or cost on large datasets, and often require significant
human effort, when extracting tables or values given user-specified fields from
documents. The key insight of our tool, TWIX, is to predict the underlying
template used to create such documents, modeling the visual and structural
commonalities across documents. Data extraction based on this predicted
template provides a more principled, accurate, and efficient solution at a low
cost. Comprehensive evaluations on 34 diverse real-world datasets show that
uncovering the template is crucial for data extraction from templatized
documents. TWIX achieves over 90% precision and recall on average,
outperforming tools from industry: Textract and Azure Document Intelligence,
and vision-based LLMs like GPT-4-Vision, by over 25% in precision and recall.
TWIX scales easily to large datasets and is 734X faster and 5836X cheaper than
vision-based LLMs for extracting data from a large document collection with 817
pages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aster: Enhancing LSM-structures for Scalable <span class="highlight-title">Graph</span> Database <span class="chip">SIGMOD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingheng Mo, Junfeng Liu, Fan Wang, Siqiang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a proliferation of applications requiring the management of
large-scale, evolving graphs under workloads with intensive graph updates and
lookups. Driven by this challenge, we introduce Poly-LSM, a high-performance
key-value storage engine for graphs with the following novel techniques: (1)
Poly-LSM is embedded with a new design of graph-oriented LSM-tree structure
that features a hybrid storage model for concisely and effectively storing
graph data. (2) Poly-LSM utilizes an adaptive mechanism to handle edge
insertions and deletions on graphs with optimized I/O efficiency. (3) Poly-LSM
exploits the skewness of graph data to encode the key-value entries. Building
upon this foundation, we further implement Aster, a robust and versatile graph
database that supports Gremlin query language facilitating various graph
applications. In our experiments, we compared Aster against several mainstream
real-world graph databases. The results demonstrate that Aster outperforms all
baseline graph databases, especially on large-scale graphs. Notably, on the
billion-scale Twitter graph dataset, Aster achieves up to 17x throughput
improvement compared to the best-performing baseline graph system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGMOD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Matrix Multiplication meets the Submodular Width 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Abo-Khamis, Xiao Hu, Dan Suciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One fundamental question in database theory is the following: Given a Boolean
conjunctive query Q, what is the best complexity for computing the answer to Q
in terms of the input database size N? When restricted to the class of
combinatorial algorithms, it is known that the best known complexity for any
query Q is captured by the submodular width of Q. However, beyond combinatorial
algorithms, certain queries are known to admit faster algorithms that often
involve a clever combination of fast matrix multiplication and data
partitioning. Nevertheless, there is no systematic way to derive and analyze
the complexity of such algorithms for arbitrary queries Q.
  In this work, we introduce a general framework that captures the best
complexity for answering any Boolean conjunctive query Q using matrix
multiplication. Our framework unifies both combinatorial and non-combinatorial
techniques under the umbrella of information theory. It generalizes the notion
of submodular width to a new stronger notion called the omega-submodular width
that naturally incorporates the power of fast matrix multiplication. We
describe a matching algorithm that computes the answer to any query Q in time
corresponding to the omega-submodular width of Q. We show that our framework
recovers the best known complexities for Boolean queries that have been studied
in the literature, to the best of our knowledge, and also discovers new
algorithms for some classes of queries that improve upon the best known
complexities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Marketplace: A Benchmark for Data Management in Microservices <span class="chip">SIGMOD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Laigner, Zhexiang Zhang, Yijian Liu, Leonardo Freitas Gomes, Yongluan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microservice architectures have become a popular approach for designing
scalable distributed applications. Despite their extensive use in industrial
settings for over a decade, there is limited understanding of the data
management challenges that arise in these applications. Consequently, it has
been difficult to advance data system technologies that effectively support
microservice applications. To fill this gap, we present Online Marketplace, a
microservice benchmark that highlights core data management challenges that
existing benchmarks fail to address. These challenges include transaction
processing, query processing, event processing, constraint enforcement, and
data replication. We have defined criteria for various data management issues
to enable proper comparison across data systems and platforms.
  Through case studies with state-of-the-art data platforms, we discuss the
issues encountered while implementing and meeting Online Marketplace's
criteria. By capturing the overhead of meeting the key data management
requirements that are overlooked by existing benchmarks, we gain actionable
insights into the experimental platforms. This highlights the significance of
Online Marketplace in advancing future data systems to meet the needs of
microservice practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version accepted at SIGMOD'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-01-19T05:27:12.412103233Z">
            2025-01-19 05:27:12 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
